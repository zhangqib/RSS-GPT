<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.21080</link>
<guid>https://arxiv.org/abs/2508.21080</guid>
<content:encoded><![CDATA[
<div> workshop, out-of-distribution hazard detection, vision-language models, benchmarking, safe autonomous driving practices

Summary: The computer vision community is working on advancing autonomous driving algorithms, but the challenge of novel scenarios remains a key barrier to safe self-driving cars. The 2COOOL workshop aims to address this challenge by focusing on novelty handling, including out-of-distribution hazard detection, vision-language models for hazard understanding, new benchmarking and methodologies, and safe autonomous driving practices. The workshop, to be held at ICCV 2025 in Honolulu, Hawaii, will bring together researchers and industry experts to inspire the development of new algorithms and systems for hazard avoidance. Drawing on ideas from anomaly detection, open-set recognition, and domain adaptation, the workshop seeks to push the state of the art in autonomous driving technology. <div>
arXiv:2508.21080v1 Announce Type: new 
Abstract: As the computer vision community advances autonomous driving algorithms, integrating vision-based insights with sensor data remains essential for improving perception, decision making, planning, prediction, simulation, and control. Yet we must ask: Why don't we have entirely safe self-driving cars yet? A key part of the answer lies in addressing novel scenarios, one of the most critical barriers to real-world deployment. Our 2COOOL workshop provides a dedicated forum for researchers and industry experts to push the state of the art in novelty handling, including out-of-distribution hazard detection, vision-language models for hazard understanding, new benchmarking and methodologies, and safe autonomous driving practices. The 2nd Workshop on the Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held at the International Conference on Computer Vision (ICCV) 2025 in Honolulu, Hawaii, on October 19, 2025. We aim to inspire the development of new algorithms and systems for hazard avoidance, drawing on ideas from anomaly detection, open-set recognition, open-vocabulary modeling, domain adaptation, and related fields. Building on the success of its inaugural edition at the Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop will feature a mix of academic and industry participation.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images</title>
<link>https://arxiv.org/abs/2508.21088</link>
<guid>https://arxiv.org/abs/2508.21088</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, dental conditions, panoramic X-ray, classification, hybrid models <br>
Summary: <br> 
- A study was conducted on automated classification of dental conditions in panoramic X-ray images using deep learning methods.
- The dataset comprised 1,512 radiographs with annotations for four conditions: fillings, cavities, implants, and impacted teeth.
- Three approaches were evaluated: custom CNN, hybrid models combining CNN with traditional classifiers, and fine-tuned pre-trained architectures.
- The hybrid CNN Random Forest model achieved the highest performance with 85.4% accuracy.
- Pre-trained models like VGG16, Xception, and ResNet50 also showed good accuracy, highlighting the potential of hybrid models for automated dental diagnostic support.
- Results underscore the need for larger datasets and further clinical validation to enhance the efficiency and reliability of automated dental diagnostic systems. <br> 
Summary: <div>
arXiv:2508.21088v1 Announce Type: new 
Abstract: This study investigates deep learning methods for automated classification of dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs with 11,137 expert-verified annotations across four conditions fillings, cavities, implants, and impacted teeth was used. After preprocessing and class balancing, three approaches were evaluated: a custom convolutional neural network (CNN), hybrid models combining CNN feature extraction with traditional classifiers, and fine-tuned pre-trained architectures. Experiments employed 5 fold cross validation with accuracy, precision, recall, and F1 score as evaluation metrics. The hybrid CNN Random Forest model achieved the highest performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%. Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by Xception and ResNet50. Results show that hybrid models improve discrimination of morphologically similar conditions and provide efficient, reliable performance. These findings suggest that combining CNN-based feature extraction with ensemble classifiers offers a practical path toward automated dental diagnostic support, while also highlighting the need for larger datasets and further clinical validation.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment</title>
<link>https://arxiv.org/abs/2508.21090</link>
<guid>https://arxiv.org/abs/2508.21090</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot appearance transfer, large-scale image generation, attention leakage, Q-Align, semantic alignment

Summary: 
Q-Align addresses the challenge of attention leakage in zero-shot appearance transfer by utilizing Query-Query alignment, Key-Value rearrangement, and attention refinement. By incorporating Query-Query alignment, sophisticated spatial semantic mapping between images is facilitated, enhancing feature correspondence through realignment with Key-Value rearrangement. Attention refinement is utilized with rearranged keys and values to maintain semantic consistency. Extensive experiments and analysis validate the effectiveness of Q-Align, showcasing superior appearance fidelity compared to state-of-the-art methods while still preserving competitive structure. Through Q-Align, semantic alignment in zero-shot appearance transfer is significantly improved. 

<br><br>Summary: <div>
arXiv:2508.21090v1 Announce Type: new 
Abstract: We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion</title>
<link>https://arxiv.org/abs/2508.21091</link>
<guid>https://arxiv.org/abs/2508.21091</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, feature caching, ERTACache, inference speedup, visual quality improvement

Summary: 
ERTACache addresses the computational overhead of diffusion models by introducing a caching framework that reduces errors caused by feature shift and step amplification. It employs an offline residual profiling stage to identify reusable steps and dynamically adjusts integration intervals for efficient sampling. ERTACache analytically approximates cache-induced errors, enabling up to 2x inference speedup while maintaining or enhancing visual quality. Experimental results on image and video generation benchmarks demonstrate the effectiveness of ERTACache, with significant improvements in efficiency observed on the Wan2.1 video diffusion model. The code for ERTACache is publicly available on GitHub. <br><br>Summary: <div>
arXiv:2508.21091v1 Announce Type: new 
Abstract: Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-LLMs with Temporal Visual Screening</title>
<link>https://arxiv.org/abs/2508.21094</link>
<guid>https://arxiv.org/abs/2508.21094</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Visual Screening, Video Large Language Models, fine-grained temporal semantics, cognitive load, video-language understanding

Summary:
Temporal Visual Screening (TVS) is a new task that preprocesses video question answering and instruction tuning data by focusing on critical video segments, aligning queries with visual information, and optimizing reasoning burden. It is integrated as a front-end adapter task in Video Large Language Models to improve understanding of video-language interactions. The proposed benchmark, ReSimplifyIt, outperforms previous approaches on video trimming and query rewriting, highlighting the effectiveness of TVS. Experimental results show relative gains in training and inference accuracy, demonstrating the impact of temporal information screening in enhancing video-language understanding.<br><br>Summary: <div>
arXiv:2508.21094v1 Announce Type: new 
Abstract: Humans naturally perform temporal screening by dragging the progress bar and focusing on salient temporal segments, but current Video Large Language Models (Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse frame sampling and insufficient inter-frame reasoning supervision during their training. To address this, Inspired by well-established cognitive science principles, we propose Temporal Visual Screening (TVS), a new task that universally pre-processes video question answering and instruction tuning data by: (1) retaining focus-critical video segments, (2) synchronously reconstructing queries to their most direct form while preserving answer consistency, and (3) keeping the invariance and consistency for any possible answer. TVS is formulated as a modular front-end adapter task that can be seamlessly integrated into both Video Instruction Tuning (training) and Video Question Answering (inference) pipelines. TVS optimizes distribution of reasoning burden and cognitive load; during training, it aligns queries with focus-critical visual information; at inference, it enables query-aware segment focus and streamlined query representations. In particular, we curate the first benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming while achieving competitive query rewriting performance. Experiments demonstrate that incorporating TVS yields relative gains of 7.33% (training) and 34.6% (inference), demonstrating the effectiveness of temporal information screening for improving video-language understanding.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments</title>
<link>https://arxiv.org/abs/2508.21096</link>
<guid>https://arxiv.org/abs/2508.21096</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical tools, localisation, deep learning, pose annotations, dataset

Summary: 
The article introduces the importance of localising surgical tools in computer-assisted interventions and proposes using skeletal pose annotations as an efficient annotation approach. The new dataset, ROBUST-MIPS, combines tool pose and tool instance segmentation data to enable a comprehensive study of these annotation styles. The dataset aims to facilitate the comparison of different annotation methods for various tasks. A benchmark using popular pose estimation methods shows promising results with high-quality outcomes. The authors also provide benchmark models and custom tool pose annotation software to encourage the adoption of pose annotations. Overall, the article advocates for the use of pose annotations as a more effective and scalable solution for surgical tool localisation in medical technologies. 

<br><br>Summary: <div>
arXiv:2508.21096v1 Announce Type: new 
Abstract: Localisation of surgical tools constitutes a foundational building block for computer-assisted interventional technologies. Works in this field typically focus on training deep learning models to perform segmentation tasks. Performance of learning-based approaches is limited by the availability of diverse annotated data. We argue that skeletal pose annotations are a more efficient annotation approach for surgical tools, striking a balance between richness of semantic information and ease of annotation, thus allowing for accelerated growth of available annotated data. To encourage adoption of this annotation style, we present, ROBUST-MIPS, a combined tool pose and tool instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our enriched dataset facilitates the joint study of these two annotation styles and allow head-to-head comparison on various downstream tasks. To demonstrate the adequacy of pose annotations for surgical tool localisation, we set up a simple benchmark using popular pose estimation methods and observe high-quality results. To ease adoption, together with the dataset, we release our benchmark models and custom tool pose annotation software.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</title>
<link>https://arxiv.org/abs/2508.21099</link>
<guid>https://arxiv.org/abs/2508.21099</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image generation, safety mechanisms, Safe-Control, data-driven strategies, plug-and-play design 

Summary: 
Safe-Control is a new safety patch designed to mitigate unsafe content generation in Text-to-Image models. It addresses limitations of existing safety mechanisms by injecting safety control signals into models and allowing for the construction of various safety patches to meet evolving requirements. Safe-Control is effective in reducing unsafe content generation while maintaining image quality and text alignment. It outperforms seven state-of-the-art safety mechanisms in reducing unsafe content generation. Safe-Control reduces the probability of unsafe content generation to 7% compared to approximately 20% for baseline methods under unsafe prompts and adversarial attacks. This innovative plug-and-play design makes Safe-Control compatible with similar T2I models and ensures adaptability. 

<br><br>Summary: <div>
arXiv:2508.21099v1 Announce Type: new 
Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions</title>
<link>https://arxiv.org/abs/2508.21102</link>
<guid>https://arxiv.org/abs/2508.21102</guid>
<content:encoded><![CDATA[
<div> Keywords: location identification, natural language instruction, front camera image, stuff-type target regions, segmentation

Summary:
GENNAV is a novel method for identifying the location of target regions from natural language instructions and front camera images captured by a mobility device. It focuses on predicting target existence and generating segmentation masks for stuff-type target regions with ambiguous boundaries. The method outperforms baseline methods on standard evaluation metrics and demonstrates robustness in real-world environments across diverse urban areas. The GRiN-Drive benchmark, which includes samples with no target, single target, and multi-target scenarios, was utilized to evaluate GENNAV. Real-world experiments with four automobiles in five geographically distinct urban areas validated its zero-shot transfer performance. The project page for GENNAV is available at https://gennav.vercel.app/. 

<br><br>Summary: <div>
arXiv:2508.21102v1 Announce Type: new 
Abstract: We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility. This task is challenging because it requires both existence prediction and segmentation, particularly for stuff-type target regions with ambiguous boundaries. Existing methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets. To overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. To evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target. GENNAV achieved superior performance over baseline methods on standard evaluation metrics. Furthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance. In these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments. The project page is available at https://gennav.vercel.app/.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning</title>
<link>https://arxiv.org/abs/2508.21113</link>
<guid>https://arxiv.org/abs/2508.21113</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multimodal, Auto-thinking, Bi-mode annealing, Policy Optimization
Summary:
R-4B is introduced as an auto-thinking Multimodal Large Language Model that dynamically determines when to engage in the thinking process based on problem complexity. The model is trained on a diverse dataset and undergoes a second training phase under an improved Bi-mode Policy Optimization framework. Through this approach, R-4B achieves state-of-the-art performance on 25 challenging benchmarks by outperforming Qwen2.5-VL-7B in most tasks and matching the performance of larger models like Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive tasks. This innovative model showcases enhanced accuracy in decision-making regarding when to activate the thinking mode, resulting in improved efficiency and computational cost-effectiveness. <div>
arXiv:2508.21113v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection</title>
<link>https://arxiv.org/abs/2508.21135</link>
<guid>https://arxiv.org/abs/2508.21135</guid>
<content:encoded><![CDATA[
<div> Keywords: hidden object detection, multimodal fusion, RGB, thermal, depth <br>
Summary: <br>
- Traditional RGB-based detection methods struggle with finding concealed objects due to factors like occlusion and lighting variations.
- The HiddenObject framework integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism to improve detection in challenging environments.
- By capturing modality-specific features and fusing them in a unified representation, the framework enhances detection of obscured or camouflaged targets.
- Validation on multiple benchmark datasets shows that HiddenObject achieves state-of-the-art or competitive performance compared to existing methods.
- The study suggests that Mamba-based fusion architectures can significantly advance multimodal object detection in visually degraded or complex conditions. <br> 
Summary: <div>
arXiv:2508.21135v1 Announce Type: new 
Abstract: Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and na\"ive fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration</title>
<link>https://arxiv.org/abs/2508.21154</link>
<guid>https://arxiv.org/abs/2508.21154</guid>
<content:encoded><![CDATA[
<div> Keywords: Computed Tomography, X-ray registration, image-guided navigation, RadGS reconstruction, vertebral-level

Summary: 
The article introduces a novel framework called RadGS-Reg for vertebral-level CT/X-ray registration. This framework combines 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration to achieve high accuracy and real-time performance in image-guided navigation. The RadGS reconstruction module utilizes a Counterfactual Attention Learning (CAL) mechanism to focus on vertebral regions in noisy X-rays. A patient-specific pre-training strategy is employed to adapt the framework from simulated to real data and learn vertebral shape prior knowledge. Experimental results on in-house datasets show that RadGS-Reg outperforms existing methods, demonstrating state-of-the-art performance for both tasks. The code for RadGS-Reg is available on GitHub for further exploration and implementation. 

<br><br>Summary: <div>
arXiv:2508.21154v1 Announce Type: new 
Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional "render and compare" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: https://github.com/shenao1995/RadGS_Reg.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4</title>
<link>https://arxiv.org/abs/2508.21169</link>
<guid>https://arxiv.org/abs/2508.21169</guid>
<content:encoded><![CDATA[
<div> building models, 3D, dataset, SYNBUILD-3D, generative AI algorithms

Summary:
SYNBUILD-3D is a new dataset that consists of over 6.2 million synthetic 3D residential buildings at Level of Detail (LoD) 4. Each building in the dataset is represented through three modalities: a semantically enriched 3D wireframe graph at LoD 4, floor plan images, and a LiDAR-like roof point cloud. The dataset includes semantic annotations such as rooms, doors, and windows derived from the floor plan images. SYNBUILD-3D aims to facilitate the development of generative AI algorithms for automating the creation of 3D building models at LoD 4 while ensuring semantic-geometric consistency. The dataset and code samples are publicly available on GitHub, providing a valuable resource for researchers working on applications in architecture, energy simulation, and navigation.<br><br>Summary: <div>
arXiv:2508.21169v1 Announce Type: new 
Abstract: 3D building models are critical for applications in architecture, energy simulation, and navigation. Yet, generating accurate and semantically rich 3D buildings automatically remains a major challenge due to the lack of large-scale annotated datasets in the public domain. Inspired by the success of synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse, and multi-modal dataset of over 6.2 million synthetic 3D residential buildings at Level of Detail (LoD) 4. In the dataset, each building is represented through three distinct modalities: a semantically enriched 3D wireframe graph at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a LiDAR-like roof point cloud (Modality III). The semantic annotations for each building wireframe are derived from the corresponding floor plan images and include information on rooms, doors, and windows. Through its tri-modal nature, future work can use SYNBUILD-3D to develop novel generative AI algorithms that automate the creation of 3D building models at LoD 4, subject to predefined floor plan layouts and roof geometries, while enforcing semantic-geometric consistency. Dataset and code samples are publicly available at https://github.com/kdmayer/SYNBUILD-3D.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radially Distorted Homographies, Revisited</title>
<link>https://arxiv.org/abs/2508.21190</link>
<guid>https://arxiv.org/abs/2508.21190</guid>
<content:encoded><![CDATA[
<div> homography, geometric computer vision, lens distortion, radial distortion, minimal solvers  
Summary:  
Homographies are essential in geometric computer vision, with homography estimation crucial for various tasks. Real images often have lens distortions, including radial distortion, which require simultaneous estimation with homography for accurate results. This paper presents a unified approach to solve three cases of radial distortion in homographies: distortion in one image, identical distortion in both images, and independent distortion in both images. The proposed minimal solvers are fast, stable, and accurate, outperforming existing solvers while maintaining similar precision. The solvers are tested on benchmarks, including fisheye camera images, demonstrating their effectiveness. The source code for the solvers will be available upon publication acceptance. <br><br>Summary: <div>
arXiv:2508.21190v1 Announce Type: new 
Abstract: Homographies are among the most prevalent transformations occurring in geometric computer vision and projective geometry, and homography estimation is consequently a crucial step in a wide assortment of computer vision tasks. When working with real images, which are often afflicted with geometric distortions caused by the camera lens, it may be necessary to determine both the homography and the lens distortion-particularly the radial component, called radial distortion-simultaneously to obtain anything resembling useful estimates. When considering a homography with radial distortion between two images, there are three conceptually distinct configurations for the radial distortion; (i) distortion in only one image, (ii) identical distortion in the two images, and (iii) independent distortion in the two images. While these cases have been addressed separately in the past, the present paper provides a novel and unified approach to solve all three cases. We demonstrate how the proposed approach can be used to construct new fast, stable, and accurate minimal solvers for radially distorted homographies. In all three cases, our proposed solvers are faster than the existing state-of-the-art solvers while maintaining similar accuracy. The solvers are tested on well-established benchmarks including images taken with fisheye cameras. The source code for our solvers will be made available in the event our paper is accepted for publication.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability</title>
<link>https://arxiv.org/abs/2508.21197</link>
<guid>https://arxiv.org/abs/2508.21197</guid>
<content:encoded><![CDATA[
<div> Keywords: Concept Activation Vectors, Global Concept Activation Vector, contrastive learning, attention-based fusion, deep neural networks

Summary:
Concept Activation Vectors (CAVs) are used to interpret deep neural networks by quantifying their sensitivity to predefined concepts. However, CAVs computed independently at different layers often show inconsistencies, leading to unreliable cross-layer comparisons. To address this issue, a novel framework called Global Concept Activation Vector (GCAV) is proposed, which unifies CAVs into a single, semantically consistent representation using contrastive learning and attention-based fusion. GCAV significantly reduces variance in TCAV scores, enhances concept relevance, and ensures more stable and reliable concept attributions. A method called Testing with Global Concept Activation Vectors (TGCAV) is introduced to apply TCAV to GCAV-based representations. Through extensive experiments on various deep neural networks, GCAV proves to mitigate concept inconsistency, improve concept localization, and enhance robustness against adversarial perturbations. Overall, GCAV offers a more comprehensive and interpretable understanding of how deep learning models encode human-defined concepts.<br><br>Summary: <div>
arXiv:2508.21197v1 Announce Type: new 
Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for interpreting deep neural networks by quantifying their sensitivity to human-defined concepts. However, when computed independently at different layers, CAVs often exhibit inconsistencies, making cross-layer comparisons unreliable. To address this issue, we propose the Global Concept Activation Vector (GCAV), a novel framework that unifies CAVs into a single, semantically consistent representation. Our method leverages contrastive learning to align concept representations across layers and employs an attention-based fusion mechanism to construct a globally integrated CAV. By doing so, our method significantly reduces the variance in TCAV scores while preserving concept relevance, ensuring more stable and reliable concept attributions. To evaluate the effectiveness of GCAV, we introduce Testing with Global Concept Activation Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We conduct extensive experiments on multiple deep neural networks, demonstrating that our method effectively mitigates concept inconsistency across layers, enhances concept localization, and improves robustness against adversarial perturbations. By integrating cross-layer information into a coherent framework, our method offers a more comprehensive and interpretable understanding of how deep learning models encode human-defined concepts. Code and models are available at https://github.com/Zhenghao-He/GCAV.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Object Re-Identification via Visual In-Context Prompting</title>
<link>https://arxiv.org/abs/2508.21222</link>
<guid>https://arxiv.org/abs/2508.21222</guid>
<content:encoded><![CDATA[
<div> self-supervised learning, object re-identification, Visual In-Context Prompting, generalization, novel categories<br>
Summary:<br>
Current object re-identification methods face challenges in generalization and data labeling. This paper introduces Visual In-Context Prompting (VICP), a novel framework that enables models to generalize to novel categories using in-context examples as prompts. VICP combines language-based models (LLMs) and vision foundation models (VFMs) to extract identity-sensitive features. By aligning semantic concepts from LLMs with pre-trained priors of VFMs, VICP eliminates the need for dataset-specific retraining. The proposed ShopID10K dataset facilitates evaluation. Experimental results on diverse datasets show that VICP outperforms baselines on unseen categories. The code is available at the specified GitHub repository. <br> <div>
arXiv:2508.21222v1 Announce Type: new 
Abstract: Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting~(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer semantic identity rules from few-shot positive/negative pairs through task-specific prompting, which then guides a VFM (\eg, DINO) to extract ID-discriminative features via \textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at https://github.com/Hzzone/VICP.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg</title>
<link>https://arxiv.org/abs/2508.21227</link>
<guid>https://arxiv.org/abs/2508.21227</guid>
<content:encoded><![CDATA[
<div> MRI, pancreatic tumors, segmentation, deep learning, dataset <br>
Summary: 
- The study focused on automated segmentation of pancreatic tumors using MRI data, which is crucial for diagnosis and treatment planning.
- SegResNet models within the Auto3DSeg architecture were trained and evaluated on two different MRI tasks, achieving moderate performance.
- The challenges of segmentation were highlighted due to limited dataset availability and anatomical variability.
- Results showed better performance on Task 1 compared to Task 2, indicating the influence of different MRI sequences.
- The study emphasizes the need for larger and standardized MRI datasets to improve model robustness and clinical utility. <div>
arXiv:2508.21227v1 Announce Type: new 
Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis, treatment planning, and outcome assessment, yet automated segmentation remains challenging due to anatomical variability and limited dataset availability. In this study, SegResNet models, as part of the Auto3DSeg architecture, were trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold cross-validation with STAPLE ensembling after focusing on an anatomically relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI with expert annotated pancreas and tumor labels. The Pancreatic Tumor Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases with expert annotated pancreas and tumor labels. Algorithm-automated segmentation performance of pancreatic tumor was assessed using Dice Similarity Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1, the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203 mm. These findings illustrate the challenges of MRI-based pancreatic tumor segmentation with small datasets, highlighting variability introduced by different MRI sequences. Despite modest performance, the results demonstrate potential for automated delineation and emphasize the need for larger, standardized MRI datasets to improve model robustness and clinical utility.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2508.21254</link>
<guid>https://arxiv.org/abs/2508.21254</guid>
<content:encoded><![CDATA[
<div> Keywords: pretrained segmentation model, cardiac MRI, generalization, reverse imaging, spin properties<br>
Summary:<br>
Pretrained segmentation models for cardiac MRI often struggle to generalize across different imaging sequences due to variations in image contrast resulting from changes in imaging protocols. To address this challenge, a novel physics-driven method called Reverse Imaging is introduced. This method leverages the core spin properties governing all MR images to infer these properties from observed images, enabling data augmentation and domain adaptation. By solving ill-posed inverse problems regularized by the prior distribution of spin properties learned from a specific dataset, Reverse Imaging estimates spin properties to facilitate image synthesis for segmentation tasks on diverse imaging protocols. This approach leads to highly accurate segmentation results across various contrasts and protocols, demonstrating enhanced generalization capability for cardiac MRI segmentation tasks.<br> <div>
arXiv:2508.21254v1 Announce Type: new 
Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem. Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this "spin prior" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable "latent variable" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHD: Personalized 3D Human Body Fitting with Point Diffusion</title>
<link>https://arxiv.org/abs/2508.21257</link>
<guid>https://arxiv.org/abs/2508.21257</guid>
<content:encoded><![CDATA[
<div> pose estimation, 3D human mesh recovery, personalized, body fitting, Point Diffusion Transformer

Summary:
The article introduces a personalized approach, PHD, for 3D human mesh recovery (HMR) and body fitting that focuses on leveraging user-specific shape information to improve pose estimation accuracy from videos. Traditional HMR methods are typically designed to be user-agnostic, leading to compromises in 3D accuracy while refining poses using constraints from 2D images. PHD decouples this process by first calibrating the user's body shape and then employing a personalized pose fitting process based on that shape. A body shape-conditioned 3D pose prior, implemented as a Point Diffusion Transformer, guides the pose fitting via a Point Distillation Sampling loss, effectively mitigating errors from excessive reliance on 2D constraints. This approach improves pelvis-aligned pose accuracy as well as absolute pose accuracy, which is often overlooked in prior work. Additionally, PHD is highly data-efficient, requiring only synthetic data for training, and can be seamlessly integrated with existing 3D pose estimators to enhance performance. <div>
arXiv:2508.21257v1 Announce Type: new 
Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery (HMR) and body fitting that leverages user-specific shape information to improve pose estimation accuracy from videos. Traditional HMR methods are designed to be user-agnostic and optimized for generalization. While these methods often refine poses using constraints derived from the 2D image to improve alignment, this process compromises 3D accuracy by failing to jointly account for person-specific body shapes and the plausibility of 3D poses. In contrast, our pipeline decouples this process by first calibrating the user's body shape and then employing a personalized pose fitting process conditioned on that shape. To achieve this, we develop a body shape-conditioned 3D pose prior, implemented as a Point Diffusion Transformer, which iteratively guides the pose fitting via a Point Distillation Sampling loss. This learned 3D pose prior effectively mitigates errors arising from an over-reliance on 2D constraints. Consequently, our approach improves not only pelvis-aligned pose accuracy but also absolute pose accuracy -- an important metric often overlooked by prior work. Furthermore, our method is highly data-efficient, requiring only synthetic data for training, and serves as a versatile plug-and-play module that can be seamlessly integrated with existing 3D pose estimators to enhance their performance. Project page: https://phd-pose.github.io/
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</title>
<link>https://arxiv.org/abs/2508.21363</link>
<guid>https://arxiv.org/abs/2508.21363</guid>
<content:encoded><![CDATA[
<div> Efficient Diffusion-Based 3D Human Pose Estimation; Hierarchical Temporal Pruning; Temporal Correlation-Enhanced Pruning; Sparse-Focused Temporal MHSA; Mask-Guided Pose Token Pruner

Summary:

- The paper introduces an Efficient Diffusion-Based 3D Human Pose Estimation framework with Hierarchical Temporal Pruning (HTP).
- HTP dynamically prunes redundant pose tokens across frame and semantic levels while preserving critical motion dynamics.
- HTP operates in stages: Temporal Correlation-Enhanced Pruning (TCEP), Sparse-Focused Temporal MHSA (SFT MHSA), and Mask-Guided Pose Token Pruner (MGPTP).
- Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5%, inference MACs by 56.8%, and improves inference speed by an average of 81.1% compared to prior diffusion-based methods.
- The proposed framework achieves state-of-the-art performance in 3D human pose estimation tasks.

Summary: <div>
arXiv:2508.21363v1 Announce Type: new 
Abstract: Diffusion models have demonstrated strong capabilities in generating high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis requirements incur substantial computational cost. In this paper, we propose an Efficient Diffusion-Based 3D Human Pose Estimation framework with a Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes redundant pose tokens across both frame and semantic levels while preserving critical motion dynamics. HTP operates in a staged, top-down manner: (1) Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by analyzing inter-frame motion correlations through adaptive temporal graph construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the resulting frame-level sparsity to reduce attention computation, focusing on motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs fine-grained semantic pruning via clustering, retaining only the most informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves inference speed by an average of 81.1\% compared to prior diffusion-based methods, while achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image</title>
<link>https://arxiv.org/abs/2508.21371</link>
<guid>https://arxiv.org/abs/2508.21371</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Coherence Tomography, biometric recognition, synthetic data generation, deep learning, fingerprint recognition<br>
Summary:<br>
This paper introduces Print2Volume, a framework for generating synthetic 3D fingerprints from 2D fingerprint images using Optical Coherence Tomography (OCT). The framework consists of three stages: a style transfer module, a 3D structure expansion network, and an OCT realism refiner. By utilizing this framework, a large-scale synthetic dataset of 420,000 samples was created. The synthetic data showed high quality and significantly improved recognition performance when used for pre-training a model. The Equal Error Rate (EER) was reduced from 15.62% to 2.50% on the ZJUT-EIFD benchmark by fine-tuning the model on a small real-world dataset. This approach effectively overcomes the data scarcity issue in OCT-based fingerprint recognition, enabling the development of advanced algorithms, particularly deep learning models. <div>
arXiv:2508.21371v1 Announce Type: new 
Abstract: Optical Coherence Tomography (OCT) enables the acquisition of high-resolution, three-dimensional fingerprint data, capturing rich subsurface structures for robust biometric recognition. However, the high cost and time-consuming nature of OCT data acquisition have led to a scarcity of large-scale public datasets, significantly hindering the development of advanced algorithms, particularly data-hungry deep learning models. To address this critical bottleneck, this paper introduces Print2Volume, a novel framework for generating realistic, synthetic OCT-based 3D fingerprints from 2D fingerprint image. Our framework operates in three sequential stages: (1) a 2D style transfer module that converts a binary fingerprint into a grayscale images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that renders the structural volume with authentic textures, speckle noise, and other imaging characteristics. Using Print2Volume, we generated a large-scale synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the high quality of our synthetic data and its significant impact on recognition performance. By pre-training a recognition model on our synthetic data and fine-tuning it on a small real-world dataset, we achieved a remarkable reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD benchmark, proving the effectiveness of our approach in overcoming data scarcity.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLENDA: Gynecologic Laparoscopy Endometriosis Dataset</title>
<link>https://arxiv.org/abs/2508.21398</link>
<guid>https://arxiv.org/abs/2508.21398</guid>
<content:encoded><![CDATA[
<div> laparoscopy, minimally invasive surgery, computer vision, machine learning, endometriosis
Summary:
Gynecologic laparoscopy is a form of minimally invasive surgery used for various treatments, with the ability to record videos crucial for post-surgical activities. The manual analysis of these recordings is time-consuming, leading to the development of computer vision and machine learning approaches. However, the scarcity of sample data in the medical field poses a challenge. In response, the Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) has been created, containing region-based annotations of endometriosis, a common medical condition involving the displacement of uterine-like tissue. This dataset, developed in collaboration with medical experts, is the first of its kind and aims to support research and advancements in the field of minimally invasive surgery and medical image analysis. 
<br><br>Summary: <div>
arXiv:2508.21398v1 Announce Type: new 
Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is performed via a live feed of a patient's abdomen surveying the insertion and handling of various instruments for conducting treatment. Adopting this kind of surgical intervention not only facilitates a great variety of treatments, the possibility of recording said video streams is as well essential for numerous post-surgical activities, such as treatment planning, case documentation and education. Nonetheless, the process of manually analyzing surgical recordings, as it is carried out in current practice, usually proves tediously time-consuming. In order to improve upon this situation, more sophisticated computer vision as well as machine learning approaches are actively developed. Since most of such approaches heavily rely on sample data, which especially in the medical field is only sparsely available, with this work we publish the Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset containing region-based annotations of a common medical condition named endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the first of its kind and it has been created in collaboration with leading medical experts in the field.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation</title>
<link>https://arxiv.org/abs/2508.21399</link>
<guid>https://arxiv.org/abs/2508.21399</guid>
<content:encoded><![CDATA[
<div> segmentation, recognition, surgical instruments, laparoscopic gynecology, convolutional network <br>
<br>
Summary: 
This study focuses on segmenting and recognizing surgical instruments in videos from laparoscopic gynecology surgeries. The researchers used a region-based fully convolutional network for both instance-aware instrument segmentation and instrument recognition. While they were successful in segmenting instrument regions accurately with a low number of training examples, identifying the specific instrument proved to be a challenging task due to the high similarity among surgical instruments. The study highlights the importance of automatic content indexing in medical video archives and the complexity of distinguishing and recognizing surgical instruments in recorded surgery footage. <div>
arXiv:2508.21399v1 Announce Type: new 
Abstract: Recorded videos from surgeries have become an increasingly important information source for the field of medical endoscopy, since the recorded footage shows every single detail of the surgery. However, while video recording is straightforward these days, automatic content indexing - the basis for content-based search in a medical video archive - is still a great challenge due to the very special video content. In this work, we investigate segmentation and recognition of surgical instruments in videos recorded from laparoscopic gynecology. More precisely, we evaluate the achievable performance of segmenting surgical instruments from their background by using a region-based fully convolutional network for instance-aware (1) instrument segmentation as well as (2) instrument recognition. While the first part addresses only binary segmentation of instances (i.e., distinguishing between instrument or background) we also investigate multi-class instrument recognition (i.e., identifying the type of instrument). Our evaluation results show that even with a moderately low number of training examples, we are able to localize and segment instrument regions with a pretty high accuracy. However, the results also reveal that determining the particular instrument is still very challenging, due to the inherently high similarity of surgical instruments.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing</title>
<link>https://arxiv.org/abs/2508.21402</link>
<guid>https://arxiv.org/abs/2508.21402</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, remote sensing imagery, SatDINO, representation learning, contrastive learning

Summary:
SatDINO, a model specifically designed for self-supervised representation learning in satellite imagery, is introduced in this study. Through various experiments on multiple datasets, SatDINO outperformed existing methods using masked autoencoders and showed competitive results in different benchmarks. An ablation study was conducted to evaluate the individual components of SatDINO, and novel enhancements were proposed, including GSD encoding integration and adaptive view sampling. These enhancements can be independently applied to the SatDINO model. The code and trained models for SatDINO are publicly available on GitHub at https://github.com/strakaj/SatDINO. <br><br>Summary: SatDINO, a self-supervised learning model tailored for satellite imagery, demonstrates superior performance compared to existing methods. The study includes an ablation study, novel enhancements, and offers open-access resources for further research and development. <div>
arXiv:2508.21402v1 Announce Type: new 
Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing, where large amounts of unlabeled data are available. In this work, we investigate the use of DINO, a contrastive self-supervised method, for pretraining on remote sensing imagery. We introduce SatDINO, a model tailored for representation learning in satellite imagery. Through extensive experiments on multiple datasets in multiple testing setups, we demonstrate that SatDINO outperforms other state-of-the-art methods based on much more common masked autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual components. Finally, we propose a few novel enhancements, such as a new way to incorporate ground sample distance (GSD) encoding and adaptive view sampling. These enhancements can be used independently on our SatDINO model. Our code and trained models are available at: https://github.com/strakaj/SatDINO.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives</title>
<link>https://arxiv.org/abs/2508.21418</link>
<guid>https://arxiv.org/abs/2508.21418</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole Slide Image, AI algorithm development, Metadata, Tissue map, Clinical pathology<br>
Summary: <br>
The article introduces the concept of Whole Slide Images (WSIs) and their significance in various fields such as pathology and oncology. Currently, there is a lack of standard metadata for WSI selection, making manual inspection for algorithm training cumbersome. To address this, a framework is proposed to generate a 2D index map and profiling mechanism, focusing on clinical pathology. This approach aims to enhance WSI collections with detailed tissue maps, categorizing segments based on source, tissue type, and pathological alterations. The standardization of syntax and semantics ensures interoperability between different catalogs. The benefits of this standard are demonstrated through examples in WSI catalogs, Machine Learning applications, and graph-based representations. By providing fine-grained information about WSI content, this framework offers a systematic approach to selecting cohorts for AI algorithm training or validation. <br> <div>
arXiv:2508.21418v1 Announce Type: new 
Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by scanning an entire glass slide containing a biological specimen, such as tissue sections or cell samples, at multiple magnifications. These images can be viewed, analyzed, shared digitally, and are used today for Artificial Intelligence (AI) algorithm development. WSIs are used in a variety of fields, including pathology for diagnosing diseases and oncology for cancer research. They are also utilized in neurology, veterinary medicine, hematology, microbiology, dermatology, pharmacology, toxicology, immunology, and forensic science.
  When assembling cohorts for the training or validation of an AI algorithm, it is essential to know what is present on such a WSI. However, there is currently no standard for this metadata, so such selection has mainly been done through manual inspection, which is not suitable for large collections with several million objects.
  We propose a general framework to generate a 2D index map for WSI and a profiling mechanism for specific application domains. We demonstrate this approach in the field of clinical pathology, using common syntax and semantics to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that provides fine-grained information about the WSI content. The tissue map is organized into three layers: source, tissue type, and pathological alterations, with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard through specific examples in WSI catalogs, Machine Learning (ML), and graph-based WSI representations.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels</title>
<link>https://arxiv.org/abs/2508.21424</link>
<guid>https://arxiv.org/abs/2508.21424</guid>
<content:encoded><![CDATA[
<div> Proposed keyword: Deep learning, Computer vision, Class-Incremental Learning, Unsupervised incremental learning, Pseudo-labels 

Summary:
Class-Incremental Learning (CIL) methods have made significant advancements in enabling models to learn novel classes while retaining knowledge of previous ones. However, these methods usually require fully labeled datasets, which are often unrealistic in practice. In this work, an Unsupervised Incremental Learning method using Confidence-based Pseudo-labels (ICPL) is proposed to address this limitation. By integrating pseudo-labels into various CIL methods with confidence-based selection, ICPL achieves competitive results on CIFAR100 and ImageNet100 datasets. Compared to popular Class Incremental Novel Category Discovery (class-iNCD) methods, ICPL outperforms them by more than 5% in final accuracy. Furthermore, the method is shown to be applicable to fine-grained datasets, demonstrating real-world practicality. ICPL also exhibits low computational complexity, making it suitable for resource-constrained environments. 

<br><br>Summary: <div>
arXiv:2508.21424v1 Announce Type: new 
Abstract: Deep learning models have achieved state-of-the-art performance in many computer vision tasks. However, in real-world scenarios, novel classes that were unseen during training often emerge, requiring models to acquire new knowledge incrementally. Class-Incremental Learning (CIL) methods enable a model to learn novel classes while retaining knowledge of previous classes. However, these methods make the strong assumption that the incremental dataset is fully labeled, which is unrealistic in practice. In this work, we propose an unsupervised Incremental Learning method using Confidence-based Pseudo-labels (ICPL), which replaces human annotations with pseudo-labels, enabling incremental learning from unlabeled datasets. We integrate these pseudo-labels into various CIL methods with confidence-based selection and evaluate performance degradation on CIFAR100 and ImageNet100. Then, we compare our approach to popular Class Incremental Novel Category Discovery (class-iNCD) methods addressing similar challenges. Additionally, we apply our method to fine-grained datasets to demonstrate its real-world practicality and measure its computational complexity to validate its suitability for resource-constrained environments. ICPL achieves competitive results compared to supervised methods and outperforms state-of-the-art class-iNCD methods by more than 5% in final accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation</title>
<link>https://arxiv.org/abs/2508.21435</link>
<guid>https://arxiv.org/abs/2508.21435</guid>
<content:encoded><![CDATA[
<div> Synthetic medical data, X-ray images, domain translation, MedShift, X-DigiSkull <br>
Summary:
The paper introduces MedShift, a generative model for cross-domain translation between synthetic and real X-ray images of the head. It addresses discrepancies in attenuation behavior, noise characteristics, and soft tissue representation. MedShift is based on Flow Matching and Schrodinger Bridges, enabling high-fidelity, unpaired image translation across multiple domains. X-DigiSkull, a new dataset, is introduced for benchmarking domain translation models. MedShift learns a shared domain-agnostic latent space, supporting seamless translation between any pair of seen domains. Despite its smaller size compared to diffusion-based approaches, MedShift offers strong performance and flexibility at inference, allowing tuning for perceptual fidelity or structural consistency. It provides a scalable and generalizable solution for domain adaptation in medical imaging. The code and dataset are available for access. <br> <br>Summary: <div>
arXiv:2508.21435v1 Announce Type: new 
Abstract: Synthetic medical data offers a scalable solution for training robust models, but significant domain gaps limit its generalizability to real-world clinical settings. This paper addresses the challenge of cross-domain translation between synthetic and real X-ray images of the head, focusing on bridging discrepancies in attenuation behavior, noise characteristics, and soft tissue representation. We propose MedShift, a unified class-conditional generative model based on Flow Matching and Schrodinger Bridges, which enables high-fidelity, unpaired image translation across multiple domains. Unlike prior approaches that require domain-specific training or rely on paired data, MedShift learns a shared domain-agnostic latent space and supports seamless translation between any pair of domains seen during training. We introduce X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays under varying radiation doses, to benchmark domain translation models. Experimental results demonstrate that, despite its smaller model size compared to diffusion-based approaches, MedShift offers strong performance and remains flexible at inference time, as it can be tuned to prioritize either perceptual fidelity or structural consistency, making it a scalable and generalizable solution for domain adaptation in medical imaging. The code and dataset are available at https://caetas.github.io/medshift.html
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trees as Gaussians: Large-Scale Individual Tree Mapping</title>
<link>https://arxiv.org/abs/2508.21437</link>
<guid>https://arxiv.org/abs/2508.21437</guid>
<content:encoded><![CDATA[
<div> deep learning, tree detection, global scale, PlanetScope imagery, lidar data<br>
Summary:<br>
This study introduces a deep learning approach for detecting large individual trees globally using 3-m resolution PlanetScope imagery. By simulating tree crowns with scalable Gaussian kernels, the model can accurately identify tree centers and generate binary tree cover maps. Trained on billions of lidar data points, the model achieves state-of-the-art performance in tree detection, with an R$^2$ value of 0.81 compared to aerial lidar. The approach demonstrates balanced detection metrics across different biomes and can be further improved through fine-tuning with manual labels. This method provides a scalable framework for high-resolution tree monitoring on a global scale and can be adapted for future satellite missions with improved imagery.<br><br>Summary: <div>
arXiv:2508.21437v1 Announce Type: new 
Abstract: Trees are key components of the terrestrial biosphere, playing vital roles in ecosystem function, climate regulation, and the bioeconomy. However, large-scale monitoring of individual trees remains limited by inadequate modelling. Available global products have focused on binary tree cover or canopy height, which do not explicitely identify trees at individual level. In this study, we present a deep learning approach for detecting large individual trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree crowns with Gaussian kernels of scalable size, allowing the extraction of crown centers and the generation of binary tree cover maps. Training is based on billions of points automatically extracted from airborne lidar data, enabling the model to successfully identify trees both inside and outside forests. We compare against existing tree cover maps and airborne lidar with state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial lidar), report balanced detection metrics across biomes, and demonstrate how detection can be further improved through fine-tuning with manual labels. Our method offers a scalable framework for global, high-resolution tree monitoring, and is adaptable to future satellite missions offering improved imagery.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content</title>
<link>https://arxiv.org/abs/2508.21444</link>
<guid>https://arxiv.org/abs/2508.21444</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, real-time rendering, dynamic scenes, Gaussian spheres, training efficiency

Summary: 
3D Gaussian Splatting (3DGS) is crucial for high-fidelity real-time rendering in immersive applications. However, challenges arise when extending 3DGS to dynamic scenes due to the substantial data volume of dense Gaussians and prolonged training times. In response, this paper introduces a scalable framework called \M that enhances training efficiency for streaming tasks. By hierarchically organizing Gaussian spheres within an anchor-based structure, \M efficiently represents both low-resolution structures and high-fidelity details. A hybrid deformation and spawning strategy models motion between frames, reducing computational overhead. Additionally, a bidirectional adaptive masking mechanism removes static regions and prioritizes informative viewpoints during training. Experimental results show that \M achieves superior visual quality while significantly reducing training time compared to existing methods. <div>
arXiv:2508.21444v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist</title>
<link>https://arxiv.org/abs/2508.21451</link>
<guid>https://arxiv.org/abs/2508.21451</guid>
<content:encoded><![CDATA[
<div> Keywords: Image captioning, lightweight model, multimodal large language models, visual specialist, Sharp-Eyed Refinement <br>
Summary: 
Image captioning is essential for various applications but deploying large language models on local devices is challenging. To address this, a specialist based on a smaller language model is explored for lightweight captioning. Despite its smaller size, the specialist performs comparably to large generalists, making it suitable for on-device applications. However, like other large models, it suffers from visual blindness leading to semantic errors. A novel captioning framework, Sharp-Eyed Refinement, is developed to improve visual grounding and caption quality by focusing on informative regions. Experiments confirm the specialist's advantages over both small captioning models and large generalists, highlighting the effectiveness of the framework in enhancing caption quality. <br><br>Summary: <div>
arXiv:2508.21451v1 Announce Type: new 
Abstract: Image captioning is fundamental for applications like video instruction systems and exploration robots, yet deploying such models on local devices is challenging due to the high computational demands of multimodal large language models (MLLMs). To address this, we first explore lightweight captioning by implementing a specialist based on a 125M-parameter language model, 56 times smaller than LLaMA-7B, and evaluating its performance on both single-sentence and detailed captioning tasks. Surprisingly, we find that our model can achieve performance comparable to large multimodal generalists, suggesting its potential to serve as a strong visual specialist for on-device applications. While promising, our model also exhibits a limitation: like other MLLMs, it suffers from visual blindness, occasionally resulting in semantic captioning errors. We carry out toy experiments and investigate the underlying causes, where we observe that the problems arise from ineffective attention mechanisms and limited visual representations. To alleviate them, we develop a novel captioning framework, Sharp-Eyed Refinement, which enhances caption quality through improved visual grounding. At its core, our DeepLens extracts detailed visual representations by concentrating on informative regions identified during the initial glance. Our experiments confirm both the advantages of our specialist over prior small captioning models and large generalists and the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification</title>
<link>https://arxiv.org/abs/2508.21458</link>
<guid>https://arxiv.org/abs/2508.21458</guid>
<content:encoded><![CDATA[
<div> benchmarking study, foundation models, federated learning, dementia diagnosis, brain MRI data

Summary:
In this benchmarking study, the impact of key design choices on federated FM tuning for AI-based dementia diagnosis using brain MRI data was systematically evaluated. The architecture of the classification head was found to substantially influence performance, with freezing the FM encoder yielding comparable results to full fine-tuning. Advanced aggregation methods were shown to outperform standard federated averaging, enhancing the efficiency of the FL system. These results offer practical insights for deploying FMs in decentralized clinical settings and highlight important trade-offs for future method development. Overall, the study provides valuable guidance for optimizing the performance and efficiency of FM tuning in federated learning systems for AI-based dementia diagnosis using brain MRI data.<br><br>Summary: <div>
arXiv:2508.21458v1 Announce Type: new 
Abstract: While foundation models (FMs) offer strong potential for AI-based dementia diagnosis, their integration into federated learning (FL) systems remains underexplored. In this benchmarking study, we systematically evaluate the impact of key design choices: classification head architecture, fine-tuning strategy, and aggregation method, on the performance and efficiency of federated FM tuning using brain MRI data. Using a large multi-cohort dataset, we find that the architecture of the classification head substantially influences performance, freezing the FM encoder achieves comparable results to full fine-tuning, and advanced aggregation methods outperform standard federated averaging. Our results offer practical insights for deploying FMs in decentralized clinical settings and highlight trade-offs that should guide future method development.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Method Ensemble for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2508.21463</link>
<guid>https://arxiv.org/abs/2508.21463</guid>
<content:encoded><![CDATA[
<div> feature truncation, scoring functions, OOD detection, Multi-Method Ensemble, neural networks <br>
Summary: <br>
Detecting out-of-distribution (OOD) samples is crucial for neural networks in open-world scenarios. Current methods improve OOD detection by combining feature truncation and scoring functions. By merging state-of-the-art techniques, the proposed Multi-Method Ensemble (MME) score enhances robustness against various types of OOD samples. The MME score unifies existing OOD detectors into a more effective scoring function. Experiment results on large-scale and small-scale benchmarks demonstrate that MME outperforms recent state-of-the-art methods across all benchmarks. Notably, using the BiT model, MME achieves an average FPR95 of 27.57% on ImageNet-1K, showing a 6% improvement over the best existing baseline. <div>
arXiv:2508.21463v1 Announce Type: new 
Abstract: Detecting out-of-distribution (OOD) samples is essential for neural networks operating in open-world settings, particularly in safety-critical applications. Existing methods have improved OOD detection by leveraging two main techniques: feature truncation, which increases the separation between in-distribution (ID) and OOD samples, and scoring functions, which assign scores to distinguish between ID and OOD data. However, most approaches either focus on a single family of techniques or evaluate their effectiveness on a specific type of OOD dataset, overlooking the potential of combining multiple existing solutions. Motivated by this observation, we theoretically and empirically demonstrate that state-of-the-art feature truncation and scoring functions can be effectively combined. Moreover, we show that aggregating multiple scoring functions enhances robustness against various types of OOD samples. Based on these insights, we propose the Multi-Method Ensemble (MME) score, which unifies state-of-the-art OOD detectors into a single, more effective scoring function. Extensive experiments on both large-scale and small-scale benchmarks, covering near-OOD and far-OOD scenarios, show that MME significantly outperforms recent state-of-the-art methods across all benchmarks. Notably, using the BiT model, our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K benchmark, improving performance by 6% over the best existing baseline.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Patch Attack for Ship Detection via Localized Augmentation</title>
<link>https://arxiv.org/abs/2508.21472</link>
<guid>https://arxiv.org/abs/2508.21472</guid>
<content:encoded><![CDATA[
<div> Localization, Augmentation, Adversarial Patch Attacks, Object Detection, Transferability

Summary:
This paper introduces a novel approach to enhance ship detection techniques in remote sensing imagery by tackling the vulnerability of deep neural networks to adversarial patch attacks. The proposed method focuses on localized augmentation, applying augmentation only to target regions to avoid interference with non-target areas. By reducing background interference, the model can better focus on the impact of adversarial patches, leading to improved attack success rates and transferability. Experiments on the HRSC2016 dataset validate the effectiveness of this approach in increasing the success rate of adversarial attacks and enhancing their transferability.<br><br>Summary: <div>
arXiv:2508.21472v1 Announce Type: new 
Abstract: Current ship detection techniques based on remote sensing imagery primarily rely on the object detection capabilities of deep neural networks (DNNs). However, DNNs are vulnerable to adversarial patch attacks, which can lead to misclassification by the detection model or complete evasion of the targets. Numerous studies have demonstrated that data transformation-based methods can improve the transferability of adversarial examples. However, excessive augmentation of image backgrounds or irrelevant regions may introduce unnecessary interference, resulting in false detections of the object detection model. These errors are not caused by the adversarial patches themselves but rather by the over-augmentation of background and non-target areas. This paper proposes a localized augmentation method that applies augmentation only to the target regions, avoiding any influence on non-target areas. By reducing background interference, this approach enables the loss function to focus more directly on the impact of the adversarial patch on the detection model, thereby improving the attack success rate. Experiments conducted on the HRSC2016 dataset demonstrate that the proposed method effectively increases the success rate of adversarial patch attacks and enhances their transferability.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.21496</link>
<guid>https://arxiv.org/abs/2508.21496</guid>
<content:encoded><![CDATA[
<div> Hallucination, Video-MLLMs, Semantic Aggregation Hallucination, ELV-Halluc benchmark, long videos
<br>
Summary: 
The article introduces the concept of Semantic Aggregation Hallucination (SAH) in large language models for video understanding, particularly in long videos. The ELV-Halluc benchmark is created to address SAH in the context of long videos, showcasing an increase in SAH with semantic complexity. Models are more susceptible to SAH in rapidly changing semantic environments. Positional encoding and DPO strategies are proposed to mitigate SAH, leading to a significant 27.7% reduction in SAH ratio. The study includes experiments confirming the presence of SAH and potential approaches to alleviate it, supported by a dataset of 8K adversarial data pairs. These strategies show improvements on both ELV-Halluc and Video-MME benchmarks. <div>
arXiv:2508.21496v1 Announce Type: new 
Abstract: Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</title>
<link>https://arxiv.org/abs/2508.21529</link>
<guid>https://arxiv.org/abs/2508.21529</guid>
<content:encoded><![CDATA[
<div> upsampling, feature foundation models, microscopy images, segmentation, convolutional neural network

Summary:<br>
This paper introduces a method to enhance the representation of fine features in microscopy images using an upsampling convolutional neural network. Feature foundation models, often based on vision transformers, typically struggle with large image sizes and fine details in micrographs. By training a network to upsample low-resolution features derived from the foundation model, the authors are able to improve segmentation and object detection tasks on various microscopy images, including plant cells, lithium-ion battery cathodes, and organic crystals. The upsampled features enable better separation of challenging phases, such as hairline cracks. The approach also facilitates interactive segmentation, allowing for high-quality segmentations with minimal labeling effort compared to traditional convolutional networks. This method demonstrates efficient and effective processing of microscopy images for various applications. 

Summary: <div>
arXiv:2508.21529v1 Announce Type: new 
Abstract: Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</title>
<link>https://arxiv.org/abs/2508.21539</link>
<guid>https://arxiv.org/abs/2508.21539</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language-Guided Drones, Vision-Language Models, Hierarchical Cross-Granularity Contrastive, Matching Learning, GeoText-1652, ERA dataset

Summary:
The article introduces the concept of Natural Language-Guided Drones (NLGD) and the challenges faced in vision-language understanding in drone scenarios. The proposed solution is the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework, consisting of Region-Global Image-Text Contrastive Learning (RG-ITC) and Region-Global Image-Text Matching (RG-ITM) components. These components capture hierarchical local-to-global semantics and enhance compositional reasoning without strict constraints. The framework also incorporates Momentum Contrast and Distillation (MCD) to improve robustness against incomplete or ambiguous drone text descriptions. Experimental results on the GeoText-1652 dataset showcase state-of-the-art performance in image and text retrieval tasks, with strong zero-shot generalization demonstrated on the unseen ERA dataset. HCCM outperforms fine-tuned baselines and achieves significant improvements in Recall@1 and mean recall (mR) metrics.<br><br>Summary: <div>
arXiv:2508.21539v1 Announce Type: new 
Abstract: Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such as target matching and navigation. However, the wide field of view and complex compositional semantics in drone scenarios pose challenges for vision-language understanding. Mainstream Vision-Language Models (VLMs) emphasize global alignment while lacking fine-grained semantics, and existing hierarchical methods depend on precise entity partitioning and strict containment, limiting effectiveness in dynamic environments. To address this, we propose the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework with two components: (1) Region-Global Image-Text Contrastive Learning (RG-ITC), which avoids precise scene partitioning and captures hierarchical local-to-global semantics by contrasting local visual regions with global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM), which dispenses with rigid constraints and instead evaluates local semantic consistency within global cross-modal representations, enhancing compositional reasoning. Moreover, drone text descriptions are often incomplete or ambiguous, destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation (MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot generalization with 39.93% mean recall (mR), outperforming fine-tuned baselines.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2508.21542</link>
<guid>https://arxiv.org/abs/2508.21542</guid>
<content:encoded><![CDATA[
<div> latent diffusion model, Gaussian splats, 3D scene reconstruction, Variational AutoReconstructor, self-supervised learning <br>
<br>
Summary:
The article introduces a novel approach for reconstructing complete 3D scenes using a latent diffusion model with Gaussian splats from a single image. Traditional Gaussian splatting methods struggle with reconstructing occluded and unobserved areas, but the proposed model addresses this challenge by learning a distribution of 3D representations conditioned on a single input image. By using a generative formulation and self-supervised learning with a Variational AutoReconstructor, the model can generate faithful reconstructions and diverse samples, even completing occluded surfaces for high-quality 360-degree renderings. This new approach outperforms conventional methods by avoiding the blurriness and implausibility associated with predicting a single "mode" for occluded surfaces, offering a more accurate representation of complex scenes. <div>
arXiv:2508.21542v1 Announce Type: new 
Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single "mode" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting</title>
<link>https://arxiv.org/abs/2508.21550</link>
<guid>https://arxiv.org/abs/2508.21550</guid>
<content:encoded><![CDATA[
<div> Efficient, Pairwise comparison, Annotation, CLIP model, Human-in-the-loop <br> 
<br>Summary: 
Pairwise comparison is a preferred method for difficult or subjective annotation tasks for its improved reliability, but it can be labor-intensive. A new method called EZ-Sort combines the Contrastive Language-Image Pre-training (CLIP) model's zero-shot pre-ordering with automated comparisons to reduce human annotation costs by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to previous methods, especially with a dataset of n=100. The approach initilizes bucket-aware Elo scores and employs uncertainty-guided human-in-the-loop MergeSort. Validation on face-age estimation, historical image chronology, and retinal image quality assessment datasets demonstrates that EZ-Sort enhances or maintains inter-rater reliability while significantly reducing the annotation burden. This study highlights the effectiveness of integrating CLIP-based priors with uncertainty-aware sampling for scalable and efficient pairwise ranking tasks. <div>
arXiv:2508.21550v1 Announce Type: new 
Abstract: Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECHO: Ego-Centric modeling of Human-Object interactions</title>
<link>https://arxiv.org/abs/2508.21556</link>
<guid>https://arxiv.org/abs/2508.21556</guid>
<content:encoded><![CDATA[
<div> Keywords: human-object interactions, egocentric perspective, wearable devices, diffusion transformer architecture, state-of-the-art.

Summary: 
ECHO (Ego-Centric modeling of Human-Object interactions) introduces a novel approach to modeling human-object interactions from an egocentric perspective using only head and wrists tracking. This unified framework recovers human pose, object motion, and contact information through a unique three-variate diffusion process. Operating in a head-centric canonical space, ECHO offers robustness to global orientation. The method utilizes a conveyor-based inference to process sequences of any length by progressively increasing the diffusion timestamp. Through extensive evaluation, ECHO surpasses existing methods in egocentric HOI reconstruction, setting a new state-of-the-art. <div>
arXiv:2508.21556v1 Announce Type: new 
Abstract: Modeling human-object interactions (HOI) from an egocentric perspective is a largely unexplored yet important problem due to the increasing adoption of wearable devices, such as smart glasses and watches. We investigate how much information about interaction can be recovered from only head and wrists tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object interactions), which, for the first time, proposes a unified framework to recover three modalities: human pose, object motion, and contact from such minimal observation. ECHO employs a Diffusion Transformer architecture and a unique three-variate diffusion process, which jointly models human motion, object trajectory, and contact sequence, allowing for flexible input configurations. Our method operates in a head-centric canonical space, enhancing robustness to global orientation. We propose a conveyor-based inference, which progressively increases the diffusion timestamp with the frame position, allowing us to process sequences of any length. Through extensive evaluation, we demonstrate that ECHO outperforms existing methods that do not offer the same flexibility, setting a state-of-the-art in egocentric HOI reconstruction.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images</title>
<link>https://arxiv.org/abs/2508.21565</link>
<guid>https://arxiv.org/abs/2508.21565</guid>
<content:encoded><![CDATA[
<div> urban scenes, spatial reasoning, vision-language models, fine-tuning, synthetic dataset<br>
Summary:<br>
This study explores the transferability of vision-language models (VLMs) to urban scenes, focusing on spatial reasoning. Three VLMs were evaluated, showing reasonable zero-shot performance. However, fine-tuning with a synthetic VQA dataset specific to urban scenes significantly improved performance, especially on challenging question types like negation and counterfactuals. The dataset was constructed from street-view image predictions paired with Chain-of-Thought (CoT) answers for step-by-step reasoning supervision. This study highlights urban spatial reasoning as a novel challenge for VLMs and demonstrates the effectiveness of synthetic dataset construction in adapting general-purpose models to specialized domains. Overall, the findings suggest the potential for enhancing VLM performance in urban scene understanding through fine-tuning with domain-specific datasets. <br>Summary: <div>
arXiv:2508.21565v1 Announce Type: new 
Abstract: Effectively understanding urban scenes requires fine-grained spatial reasoning about objects, layouts, and depth cues. However, how well current vision-language models (VLMs), pretrained on general scenes, transfer these abilities to urban domain remains underexplored. To address this gap, we conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP, and LLaVA-1.5-evaluating both zero-shot performance and the effects of fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct such dataset from segmentation, depth, and object detection predictions of street-view images, pairing each question with LLM-generated Chain-of-Thought (CoT) answers for step-by-step reasoning supervision. Results show that while VLMs perform reasonably well in zero-shot settings, fine-tuning with our synthetic CoT-supervised dataset substantially boosts performance, especially for challenging question types such as negation and counterfactuals. This study introduces urban spatial reasoning as a new challenge for VLMs and demonstrates synthetic dataset construction as a practical path for adapting general-purpose models to specialized domains.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging</title>
<link>https://arxiv.org/abs/2508.21580</link>
<guid>https://arxiv.org/abs/2508.21580</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal dynamics, medical imaging, deep learning, generative trajectory, 4D medical image prediction

Summary: 
Temporal dynamics play a crucial role in medical imaging applications, such as disease progression modeling and treatment planning. Existing deep learning methods often focus on single temporal contexts or specific tasks, limiting their ability to make fine-grained spatial predictions. To address this gap, a new approach called Temporal Flow Matching (TFM) is introduced. TFM is a generative trajectory method that learns underlying temporal distributions and can predict the last context image as a fallback option. It supports 3D volumes, multiple prior scans, and irregular sampling. Benchmarking on three public datasets demonstrates that TFM outperforms other spatio-temporal methods, establishing itself as a state-of-the-art and robust baseline for 4D medical image prediction.<br><br>Summary: <div>
arXiv:2508.21580v1 Announce Type: new 
Abstract: Understanding temporal dynamics in medical imaging is crucial for applications such as disease progression modeling, treatment planning and anatomical development tracking. However, most deep learning methods either consider only single temporal contexts, or focus on tasks like classification or regression, limiting their ability for fine-grained spatial predictions. While some approaches have been explored, they are often limited to single timepoints, specific diseases or have other technical restrictions. To address this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified generative trajectory method that (i) aims to learn the underlying temporal distribution, (ii) by design can fall back to a nearest image predictor, i.e. predicting the last context image (LCI), as a special case, and (iii) supports $3D$ volumes, multiple prior scans, and irregular sampling. Extensive benchmarks on three public longitudinal datasets show that TFM consistently surpasses spatio-temporal methods from natural imaging, establishing a new state-of-the-art and robust baseline for $4D$ medical image prediction.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer</title>
<link>https://arxiv.org/abs/2508.21581</link>
<guid>https://arxiv.org/abs/2508.21581</guid>
<content:encoded><![CDATA[
<div> CT, histopathology, deep learning, recurrence risk, clear cell renal cell carcinoma <br>
Summary: 
This study focuses on estimating recurrence risk in clear cell renal cell carcinoma (ccRCC) using a combination of preoperative computed tomography (CT) and postoperative histopathology whole-slide images (WSIs). The researchers developed a modular deep learning framework with pretrained encoders and Cox-based survival modeling to predict recurrence risk. The results showed that WSI-based models outperformed CT-only models, highlighting the importance of pathology in prognosis. Intermediate fusion of CT and pathology data further improved performance, with the best model approaching the adjusted Leibovich score. Random tie-breaking narrowed the gap between clinical baseline and learned models, suggesting that discretization may overstate individualized performance. The study demonstrates the feasibility of integrating multiple modalities for personalized ccRCC risk prediction, and suggests exploring more expressive fusion strategies and larger multimodal datasets in the future. <br> <div>
arXiv:2508.21581v1 Announce Type: new 
Abstract: Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is essential for guiding postoperative surveillance and treatment. The Leibovich score remains widely used for stratifying distant recurrence risk but offers limited patient-level resolution and excludes imaging information. This study evaluates multimodal recurrence prediction by integrating preoperative computed tomography (CT) and postoperative histopathology whole-slide images (WSIs). A modular deep learning framework with pretrained encoders and Cox-based survival modeling was tested across unimodal, late fusion, and intermediate fusion setups. In a real-world ccRCC cohort, WSI-based models consistently outperformed CT-only models, underscoring the prognostic strength of pathology. Intermediate fusion further improved performance, with the best model (TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random tie-breaking narrowed the gap between the clinical baseline and learned models, suggesting discretization may overstate individualized performance. Using simple embedding concatenation, radiology added value primarily through fusion. These findings demonstrate the feasibility of foundation model-based multimodal integration for personalized ccRCC risk prediction. Future work should explore more expressive fusion strategies, larger multimodal datasets, and general-purpose CT encoders to better match pathology modeling capacity.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation</title>
<link>https://arxiv.org/abs/2508.21657</link>
<guid>https://arxiv.org/abs/2508.21657</guid>
<content:encoded><![CDATA[
<div> Deep Unfolding Network, Computer-generated holography, Deep learning, Adaptive bandwidth-preserving model, Phase-domain complex-valued denoiser <br>
<br>
Summary:
The paper introduces a new Deep Unfolding Network for Computer-generated holography (CGH) that addresses existing challenges in accurate and stable reconstruction. It decomposes gradient descent into two modules - an adaptive bandwidth-preserving model (ABPM) and a phase-domain complex-valued denoiser (PCD). This approach enhances flexibility and interpretability by considering the physical relationships in the reconstruction model. The ABPM module enables wider working distances compared to existing methods, while the PCD module utilizes complex-valued deformable self-attention to capture global features and improve performance, achieving a PSNR of over 35 dB. Experimental results on both simulated and real data demonstrate state-of-the-art performance of the proposed approach. <div>
arXiv:2508.21657v1 Announce Type: new 
Abstract: Computer-generated holography (CGH) has gained wide attention with deep learning-based algorithms. However, due to its nonlinear and ill-posed nature, challenges remain in achieving accurate and stable reconstruction. Specifically, ($i$) the widely used end-to-end networks treat the reconstruction model as a black box, ignoring underlying physical relationships, which reduces interpretability and flexibility. ($ii$) CNN-based CGH algorithms have limited receptive fields, hindering their ability to capture long-range dependencies and global context. ($iii$) Angular spectrum method (ASM)-based models are constrained to finite near-fields.In this paper, we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain complex-valued denoiser (PCD), providing more flexibility. ABPM allows for wider working distances compared to ASM-based methods. At the same time, PCD leverages its complex-valued deformable self-attention module to capture global features and enhance performance, achieving a PSNR over 35 dB. Experiments on simulated and real data show state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models</title>
<link>https://arxiv.org/abs/2508.21680</link>
<guid>https://arxiv.org/abs/2508.21680</guid>
<content:encoded><![CDATA[
<div> Keywords: PET/CT imaging, lesion segmentation, interactive segmentation, Euclidean Distance Transform, promptable models 

Summary: 
Whole-body PET/CT imaging in oncology often faces challenges in accurately segmenting lesions due to various factors. The autoPET/CT IV challenge introduces interactive segmentation tasks to enhance the accuracy of lesion segmentation. The submission from this work for Task 1 builds upon the nnU-Net pipeline and incorporates user prompts for foreground and background clicks. The study evaluates different representations for spatial prompts and finds that Euclidean Distance Transform encoding outperforms Gaussian kernels consistently. By simulating user interactions online and implementing a customized point sampling strategy, the proposed ensemble of models shows improved performance in cross-validation, reducing false positives and false negatives. These findings demonstrate the potential of promptable models in facilitating efficient user-guided segmentation workflows in multi-tracer, multi-center PET/CT imaging. The code for this work is available on GitHub for public access. 

<br><br>Summary: <div>
arXiv:2508.21680v1 Announce Type: new 
Abstract: Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate lesion segmentation remains challenging due to tracer heterogeneity, physiological uptake, and multi-center variability. While fully automated methods have advanced substantially, clinical practice benefits from approaches that keep humans in the loop to efficiently refine predicted masks. The autoPET/CT IV challenge addresses this need by introducing interactive segmentation tasks based on simulated user prompts. In this work, we present our submission to Task 1. Building on the winning autoPET III nnU-Net pipeline, we extend the framework with promptable capabilities by encoding user-provided foreground and background clicks as additional input channels. We systematically investigate representations for spatial prompts and demonstrate that Euclidean Distance Transform (EDT) encodings consistently outperform Gaussian kernels. Furthermore, we propose online simulation of user interactions and a custom point sampling strategy to improve robustness under realistic prompting conditions. Our ensemble of EDT-based models, trained with and without external data, achieves the strongest cross-validation performance, reducing both false positives and false negatives compared to baseline models. These results highlight the potential of promptable models to enable efficient, user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code is publicly available at https://github.com/MIC-DKFZ/autoPET-interactive
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</title>
<link>https://arxiv.org/abs/2508.21689</link>
<guid>https://arxiv.org/abs/2508.21689</guid>
<content:encoded><![CDATA[
<div> probabilistic projection, HD map, geometric mapping, confidence scores, temporal processing

Summary:<br>
The article introduces a novel approach for constructing high-definition (HD) maps from sensory input by accurately mapping road elements from image space to Bird's Eye View (BEV) space. The proposed method includes a probabilistic projection mechanism with confidence scores to refine the mapping and filter out irrelevant elements, improving the alignment with the scene and enhancing HD map generation. The approach also utilizes confidence scores for better temporal processing, selectively accumulating reliable information over time. Experimental results on nuScenes and Argoverse2 datasets show improved performance compared to existing methods, particularly in long perception ranges. The code and model checkpoints for the proposed approach are available on GitHub, providing a valuable resource for further research and development. <br><br>Summary: <div>
arXiv:2508.21689v1 Announce Type: new 
Abstract: Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Bird's Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at https://github.com/Fatih-Erdogan/mapping-like-skeptic .
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</title>
<link>https://arxiv.org/abs/2508.21693</link>
<guid>https://arxiv.org/abs/2508.21693</guid>
<content:encoded><![CDATA[
<div> OCR, word segmentation, language models, dataset, line-level OCR  
Summary:  
- Conventional OCR techniques segmented characters leading to errors in character segmentation and limited context usage.  
- Modern techniques in sequence to sequence translation focus on word detection for better language model utilization.   
- Transitioning from word to line-level OCR aims to improve accuracy and efficiency by providing larger sentence context.  
- A curated dataset of 251 English page images with line-level annotations is provided to train and benchmark line-level OCR.   
- Experimental results show a 5.4% accuracy improvement and 4 times efficiency enhancement compared to word-based pipelines.  
<br><br>Summary: <div>
arXiv:2508.21693v1 Announce Type: new 
Abstract: Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: https://nishitanand.github.io/line-level-ocr-website
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</title>
<link>https://arxiv.org/abs/2508.21712</link>
<guid>https://arxiv.org/abs/2508.21712</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based generative models, object detection, synthetic data generation, Flux LoRA Augmentation (FLORA), efficiency-focused approach

Summary:
Recent advancements in diffusion-based generative models have shown promise in supplementing limited datasets for object detection tasks, but often require extensive resources for full fine-tuning. A new approach called Flux LoRA Augmentation (FLORA) has been introduced to address these challenges by using a lightweight synthetic data generation pipeline. By fine-tuning the Flux 1.1 Dev diffusion model through Low-Rank Adaptation (LoRA), FLORA significantly reduces computational requirements, enabling synthetic dataset creation with a consumer-grade GPU. Empirical evaluations on seven different object detection datasets have shown that training object detectors with just 500 synthetic images generated by FLORA outperforms models trained on 5000 synthetic images from the baseline method. FLORA achieves up to 21.3% improvement in mAP@.50:.95, demonstrating the effectiveness of a quality and efficiency-focused approach in synthetic data creation for real-world applications. 

<br><br>Summary: <div>
arXiv:2508.21712v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2508.21715</link>
<guid>https://arxiv.org/abs/2508.21715</guid>
<content:encoded><![CDATA[
<div> Keywords: Convolutional Neural Networks, Adversarial perturbations, Activation entropy monitoring, Detection accuracy, Real-time deployment<br>
Summary:<br>
Convolutional Neural Networks (CNNs) are widely used in computer vision, but they are susceptible to adversarial perturbations that can cause misclassification. Existing detection methods are either costly or reduce performance on clean data. This study introduces a novel approach that leverages entropy signatures in CNN activations to detect adversarial inputs without modifying the model. By monitoring activation entropy on a VGG-16 network, the researchers found a consistent 7% shift in entropy in early convolutional layers for adversarial inputs, achieving a high detection accuracy of 90% with low false positive and negative rates. The separation between clean and adversarial entropy distributions demonstrates that CNNs inherently capture distribution shifts in their activation patterns. This innovation paves the way for real-time deployment of self-diagnostic vision systems that can detect adversarial inputs while maintaining original model performance.<br> 
Summary: <div>
arXiv:2508.21715v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have become the foundation of modern computer vision, achieving unprecedented accuracy across diverse image recognition tasks. While these networks excel on in-distribution data, they remain vulnerable to adversarial perturbations imperceptible input modifications that cause misclassification with high confidence. However, existing detection methods either require expensive retraining, modify network architecture, or degrade performance on clean inputs. Here we show that adversarial perturbations create immediate, detectable entropy signatures in CNN activations that can be monitored without any model modification. Using parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs consistently shift activation entropy by 7% in early convolutional layers, enabling 90% detection accuracy with false positives and false negative rates below 20%. The complete separation between clean and adversarial entropy distributions reveals that CNNs inherently encode distribution shifts in their activation patterns. This work establishes that CNN reliability can be assessed through activation entropy alone, enabling practical deployment of self-diagnostic vision systems that detect adversarial inputs in real-time without compromising original model performance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.21732</link>
<guid>https://arxiv.org/abs/2508.21732</guid>
<content:encoded><![CDATA[
<div> CAD2DMD-SET, synthetic data generation tool, LVLM, visual question answering, DMDs<br>
Summary:<br>
Recent advancements in Large Vision-Language Models (LVLMs) have led to impressive capabilities across multimodal tasks but struggle with reading values from Digital Measurement Devices (DMDs) in real-world conditions. CAD2DMD-SET, a synthetic data generation tool using 3D CAD models and advanced rendering, produces diverse datasets for LVLM fine-tuning. DMDBench, a validation set of 1,000 real-world images, evaluates model performance. Benchmarking LVLMs and fine-tuning with CAD2DMD-SET's dataset improved model performance, with a 200% increase in score for InternVL. CAD2DMD-SET enhances LVLM robustness and performance in challenging conditions. The tool will be released open-source for community use, enabling the generation of custom datasets.  <br> <br>Summary: <div>
arXiv:2508.21732v1 Announce Type: new 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Silence and Noise for Visual Sound Source Localization</title>
<link>https://arxiv.org/abs/2508.21761</link>
<guid>https://arxiv.org/abs/2508.21761</guid>
<content:encoded><![CDATA[
<div> Keywords: visual sound localization, audio-visual correspondence, self-supervised model, negative audio, IS3 dataset

Summary:
- Visual sound localization is a crucial task that can be challenging in cases with low audio-visual semantic correspondence, such as silence, noise, and offscreen sounds.
- Current methods often struggle in these situations, as they are predominantly evaluated on positive cases with a single visible sound source in the scene.
- The proposed SSL-SaN self-supervised model addresses these shortcomings by incorporating silence and noise during training, leading to improved performance and robustness against negative sounds.
- A new metric is introduced to quantify the alignment and separability of auditory and visual features across positive and negative audio-visual pairs.
- The IS3+ dataset, an extended and enhanced version of the IS3 synthetic dataset, includes negative audio to further enhance evaluation and training in visual sound localization tasks.

<br><br>Summary: <div>
arXiv:2508.21761v1 Announce Type: new 
Abstract: Visual sound source localization is a fundamental perception task that aims to detect the location of sounding sources in a video given its audio. Despite recent progress, we identify two shortcomings in current methods: 1) most approaches perform poorly in cases with low audio-visual semantic correspondence such as silence, noise, and offscreen sounds, i.e. in the presence of negative audio; and 2) most prior evaluations are limited to positive cases, where both datasets and metrics convey scenarios with a single visible sound source in the scene. To address this, we introduce three key contributions. First, we propose a new training strategy that incorporates silence and noise, which improves performance in positive cases, while being more robust against negative sounds. Our resulting self-supervised model, SSL-SaN, achieves state-of-the-art performance compared to other self-supervised models, both in sound localization and cross-modal retrieval. Second, we propose a new metric that quantifies the trade-off between alignment and separability of auditory and visual features across positive and negative audio-visual pairs. Third, we present IS3+, an extended and improved version of the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the https://xavijuanola.github.io/SSL-SaN/.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UItron: Foundational GUI Agent with Advanced Perception and Planning</title>
<link>https://arxiv.org/abs/2508.21767</link>
<guid>https://arxiv.org/abs/2508.21767</guid>
<content:encoded><![CDATA[
<div> GUI agent, Mobile/PC devices, artificial general intelligence, VLMs, UItron <br>
Summary:<br>
This article introduces UItron, an open-source foundational model for automatic GUI agents, addressing the challenges in GUI agent development. It emphasizes the importance of data engineering strategies and interactive infrastructure for training GUI agents. UItron utilizes supervised finetuning and curriculum reinforcement learning for perception and planning tasks, leading to superior performance in GUI scenarios. It successfully interacts with top-tier Chinese mobile apps, filling the gap in Chinese capabilities in existing solutions. By collecting and analyzing over one million operation trajectories, UItron showcases significant progress in Chinese app scenarios, advancing GUI agents closer to real-world application. <br> <div>
arXiv:2508.21767v1 Announce Type: new 
Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</title>
<link>https://arxiv.org/abs/2508.21769</link>
<guid>https://arxiv.org/abs/2508.21769</guid>
<content:encoded><![CDATA[

arXiv:2508.21769v1 Announce Type: new 
Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</title>
<link>https://arxiv.org/abs/2508.21770</link>
<guid>https://arxiv.org/abs/2508.21770</guid>
<content:encoded><![CDATA[

arXiv:2508.21770v1 Announce Type: new 
Abstract: Humans usually show exceptional generalisation and discovery ability in the open world, when being shown uncommon new concepts. Whereas most existing studies in the literature focus on common typical data from closed sets, open-world novel discovery is under-explored in videos. In this paper, we are interested in asking: \textit{What if atypical unusual videos are exposed in the learning process?} To this end, we collect a new video dataset consisting of various types of unusual atypical data (\eg sci-fi, animation, \etc). To study how such atypical data may benefit open-world learning, we feed them into the model training process for representation learning. Focusing on three key tasks in open-world learning: out-of-distribution (OOD) detection, novel category discovery (NCD), and zero-shot action recognition (ZSAR), we found that even straightforward learning approaches with atypical data consistently improve performance across various settings. Furthermore, we found that increasing the categorical diversity of the atypical samples further boosts OOD detection performance. Additionally, in the NCD task, using a smaller yet more semantically diverse set of atypical samples leads to better performance compared to using a larger but more typical dataset. In the ZSAR setting, the semantic diversity of atypical videos helps the model generalise better to unseen action classes. These observations in our extensive experimental evaluations reveal the benefits of atypical videos for visual representation learning in the open world, together with the newly proposed dataset, encouraging further studies in this direction.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering</title>
<link>https://arxiv.org/abs/2508.21773</link>
<guid>https://arxiv.org/abs/2508.21773</guid>
<content:encoded><![CDATA[

arXiv:2508.21773v1 Announce Type: new 
Abstract: We propose a realistic scenario for the unsupervised video learning where neither task boundaries nor labels are provided when learning a succession of tasks. We also provide a non-parametric learning solution for the under-explored problem of unsupervised video continual learning. Videos represent a complex and rich spatio-temporal media information, widely used in many applications, but which have not been sufficiently explored in unsupervised continual learning. Prior studies have only focused on supervised continual learning, relying on the knowledge of labels and task boundaries, while having labeled data is costly and not practical. To address this gap, we study the unsupervised video continual learning (uVCL). uVCL raises more challenges due to the additional computational and memory requirements of processing videos when compared to images. We introduce a general benchmark experimental protocol for uVCL by considering the learning of unstructured video data categories during each task. We propose to use the Kernel Density Estimation (KDE) of deep embedded video features extracted by unsupervised video transformer networks as a non-parametric probabilistic representation of the data. We introduce a novelty detection criterion for the incoming new task data, dynamically enabling the expansion of memory clusters, aiming to capture new knowledge when learning a succession of tasks. We leverage the use of transfer learning from the previous tasks as an initial state for the knowledge transfer to the current learning task. We found that the proposed methodology substantially enhances the performance of the model when successively learning many tasks. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, without using any labels or class boundaries.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI</title>
<link>https://arxiv.org/abs/2508.21775</link>
<guid>https://arxiv.org/abs/2508.21775</guid>
<content:encoded><![CDATA[

arXiv:2508.21775v1 Announce Type: new 
Abstract: Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is critical for clinical workflows but is hindered by poor tumor-tissue contrast and a scarcity of annotated data. This paper details our submission to the PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the nnU-Net framework and leverages a deep, multi-stage cascaded pre-training strategy, starting from a general anatomical foundation model and sequentially fine-tuning on CT pancreatic lesion datasets and the target MRI modalities. Through extensive five-fold cross-validation, we systematically evaluated data augmentation schemes and training schedules. Our analysis revealed a critical trade-off, where aggressive data augmentation produced the highest volumetric accuracy, while default augmentations yielded superior boundary precision (achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1). For our final submission, we exploited this finding by constructing custom, heterogeneous ensembles of specialist models, essentially creating a mix of experts. This metric-aware ensembling strategy proved highly effective, achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523 for Task 2. Our work presents a robust methodology for developing specialized, high-performance models in the context of limited data and complex medical imaging tasks (Team MIC-DKFZ).
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight</title>
<link>https://arxiv.org/abs/2508.21777</link>
<guid>https://arxiv.org/abs/2508.21777</guid>
<content:encoded><![CDATA[

arXiv:2508.21777v1 Announce Type: new 
Abstract: Introduction: Large language models (LLM) have shown great potential in clinical decision support. GPT-5 is a novel LLM system that has been specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300 multiple-choice items, and (ii) a curated set of 60 authentic radiation oncologic vignettes representing diverse disease sites and treatment indications. For the vignette evaluation, GPT-5 was instructed to generate concise therapeutic plans. Four board-certified radiation oncologists rated correctness, comprehensiveness, and hallucinations. Inter-rater reliability was quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%, outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's treatment recommendations were rated highly for correctness (mean 3.24/4, 95% CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69). Hallucinations were rare with no case reaching majority consensus for their presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for correctness), reflecting inherent variability in clinical judgment. Errors clustered in complex scenarios requiring precise trial knowledge or detailed clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation oncology multiple-choice benchmark. Although GPT-5 exhibited favorable performance in generating real-world radiation oncology treatment recommendations, correctness ratings indicate room for further improvement. While hallucinations were infrequent, the presence of substantive errors underscores that GPT-5-generated recommendations require rigorous expert oversight before clinical implementation.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank</title>
<link>https://arxiv.org/abs/2508.21795</link>
<guid>https://arxiv.org/abs/2508.21795</guid>
<content:encoded><![CDATA[

arXiv:2508.21795v1 Announce Type: new 
Abstract: Anomaly detection, which aims to identify anomalies deviating from normal patterns, is challenging due to the limited amount of normal data available. Unlike most existing unified methods that rely on carefully designed image feature extractors and memory banks to capture logical relationships between objects, we introduce a text memory bank to enhance the detection of logical anomalies. Specifically, we propose a Three-Memory framework for Unified structural and logical Anomaly Detection (TMUAD). First, we build a class-level text memory bank for logical anomaly detection by the proposed logic-aware text extractor, which can capture rich logical descriptions of objects from input images. Second, we construct an object-level image memory bank that preserves complete object contours by extracting features from segmented objects. Third, we employ visual encoders to extract patch-level image features for constructing a patch-level memory bank for structural anomaly detection. These three complementary memory banks are used to retrieve and compare normal images that are most similar to the query image, compute anomaly scores at multiple levels, and fuse them into a final anomaly score. By unifying structural and logical anomaly detection through collaborative memory banks, TMUAD achieves state-of-the-art performance across seven publicly available datasets involving industrial and medical domains. The model and code are available at https://github.com/SIA-IDE/TMUAD.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoCap: Video Object Captioning and Segmentation from Any Prompt</title>
<link>https://arxiv.org/abs/2508.21809</link>
<guid>https://arxiv.org/abs/2508.21809</guid>
<content:encoded><![CDATA[

arXiv:2508.21809v1 Announce Type: new 
Abstract: Understanding objects in videos in terms of fine-grained localization masks and detailed semantic properties is a fundamental task in video understanding. In this paper, we propose VoCap, a flexible video model that consumes a video and a prompt of various modalities (text, box or mask), and produces a spatio-temporal masklet with a corresponding object-centric caption. As such our model addresses simultaneously the tasks of promptable video object segmentation, referring expression segmentation, and object captioning. Since obtaining data for this task is tedious and expensive, we propose to annotate an existing large-scale segmentation dataset (SAV) with pseudo object captions. We do so by preprocessing videos with their ground-truth masks to highlight the object of interest and feed this to a large Vision Language Model (VLM). For an unbiased evaluation, we collect manual annotations on the validation set. We call the resulting dataset SAV-Caption. We train our VoCap model at scale on a SAV-Caption together with a mix of other image and video datasets. Our model yields state-of-the-art results on referring expression video object segmentation, is competitive on semi-supervised video object segmentation, and establishes a benchmark for video object captioning. Our dataset will be made available at https://github.com/google-deepmind/vocap.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</title>
<link>https://arxiv.org/abs/2508.21816</link>
<guid>https://arxiv.org/abs/2508.21816</guid>
<content:encoded><![CDATA[

arXiv:2508.21816v1 Announce Type: new 
Abstract: Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveQA: Passing the Driving Knowledge Test</title>
<link>https://arxiv.org/abs/2508.21824</link>
<guid>https://arxiv.org/abs/2508.21824</guid>
<content:encoded><![CDATA[

arXiv:2508.21824v1 Announce Type: new 
Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19153</link>
<guid>https://arxiv.org/abs/2508.19153</guid>
<content:encoded><![CDATA[

arXiv:2508.19153v1 Announce Type: cross 
Abstract: We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</title>
<link>https://arxiv.org/abs/2508.21095</link>
<guid>https://arxiv.org/abs/2508.21095</guid>
<content:encoded><![CDATA[

arXiv:2508.21095v1 Announce Type: cross 
Abstract: Unregistered surface meshes, especially raw 3D scans, present significant challenges for automatic computation of plausible deformations due to the lack of established point-wise correspondences and the presence of noise in the data. In this paper, we propose a new, rig-free, data-driven framework for motion prediction and transfer on such body meshes. Our method couples a robust motion embedding network with a learned per-vertex feature field to generate a spatio-temporal deformation field, which drives the mesh deformation. Extensive evaluations, including quantitative benchmarks and qualitative visuals on tasks such as walking and running, demonstrate the effectiveness and versatility of our approach on challenging unregistered meshes.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?</title>
<link>https://arxiv.org/abs/2508.21143</link>
<guid>https://arxiv.org/abs/2508.21143</guid>
<content:encoded><![CDATA[

arXiv:2508.21143v1 Announce Type: cross 
Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have garnered a lot of attention in recent times, with advances made in frontiers like coding, mathematics, and science. However, very limited experiments have been done to assess their performance in simple perception tasks performed over uncontaminated, generated images containing basic shapes and structures. To address this issue, the paper introduces a dataset, Percept-V, containing a total of 7200 program-generated images equally divided into 30 categories, each testing a combination of visual perception skills. Unlike previously proposed datasets, Percept-V comprises very basic tasks of varying complexity that test the perception abilities of MLLMs. This dataset is then tested on state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their performance. Contrary to the evidence that MLLMs excel in many complex tasks, our experiments show a significant drop in the models' performance with increasing problem complexity across all categories. An analysis of the performances also reveals that the tested MLLMs exhibit a similar trend in accuracy across categories, testing a particular cognitive skill and find some skills to be more difficult than others.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini Autonomous Car Driving based on 3D Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2508.21271</link>
<guid>https://arxiv.org/abs/2508.21271</guid>
<content:encoded><![CDATA[

arXiv:2508.21271v1 Announce Type: cross 
Abstract: Autonomous driving applications have become increasingly relevant in the automotive industry due to their potential to enhance vehicle safety, efficiency, and user experience, thereby meeting the growing demand for sophisticated driving assistance features. However, the development of reliable and trustworthy autonomous systems poses challenges such as high complexity, prolonged training periods, and intrinsic levels of uncertainty. Mini Autonomous Cars (MACs) are used as a practical testbed, enabling validation of autonomous control methodologies on small-scale setups. This simplified and cost-effective environment facilitates rapid evaluation and comparison of machine learning models, which is particularly useful for algorithms requiring online training. To address these challenges, this work presents a methodology based on RGB-D information and three-dimensional convolutional neural networks (3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the proposed approach against recurrent neural networks (RNNs), with architectures trained and tested on two simulated tracks with distinct environmental features. Performance was assessed using task completion success, lap-time metrics, and driving consistency. Results highlight how architectural modifications and track complexity influence the models' generalization capability and vehicle control performance. The proposed 3D CNN demonstrated promising results when compared with RNNs.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARGS: Advanced Regularization on Aligning Gaussians over the Surface</title>
<link>https://arxiv.org/abs/2508.21344</link>
<guid>https://arxiv.org/abs/2508.21344</guid>
<content:encoded><![CDATA[

arXiv:2508.21344v1 Announce Type: cross 
Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, "needle-like" shapes-by favoring more balanced, "disk-like" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.21430</link>
<guid>https://arxiv.org/abs/2508.21430</guid>
<content:encoded><![CDATA[

arXiv:2508.21430v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) hold significant potential in medical applications, including disease diagnosis and clinical decision-making. However, these tasks require highly accurate, context-sensitive, and professionally aligned responses, making reliable reward models and judges critical. Despite their importance, medical reward models (MRMs) and judges remain underexplored, with no dedicated benchmarks addressing clinical requirements. Existing benchmarks focus on general MLLM capabilities or evaluate models as solvers, neglecting essential evaluation dimensions like diagnostic accuracy and clinical relevance. To address this, we introduce Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and judges in medical scenarios. Med-RewardBench features a multimodal dataset spanning 13 organ systems and 8 clinical departments, with 1,026 expert-annotated cases. A rigorous three-step process ensures high-quality evaluation data across six clinically critical dimensions. We evaluate 32 state-of-the-art MLLMs, including open-source, proprietary, and medical-specific models, revealing substantial challenges in aligning outputs with expert judgment. Additionally, we develop baseline models that demonstrate substantial performance improvements through fine-tuning.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morae: Proactively Pausing UI Agents for User Choices</title>
<link>https://arxiv.org/abs/2508.21456</link>
<guid>https://arxiv.org/abs/2508.21456</guid>
<content:encoded><![CDATA[

arXiv:2508.21456v1 Announce Type: cross 
Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</title>
<link>https://arxiv.org/abs/2508.21635</link>
<guid>https://arxiv.org/abs/2508.21635</guid>
<content:encoded><![CDATA[

arXiv:2508.21635v1 Announce Type: cross 
Abstract: We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/rosariov2/.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is this chart lying to me? Automating the detection of misleading visualizations</title>
<link>https://arxiv.org/abs/2508.21675</link>
<guid>https://arxiv.org/abs/2508.21675</guid>
<content:encoded><![CDATA[

arXiv:2508.21675v1 Announce Type: cross 
Abstract: Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Subspaces for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2508.21695</link>
<guid>https://arxiv.org/abs/2508.21695</guid>
<content:encoded><![CDATA[

arXiv:2508.21695v1 Announce Type: cross 
Abstract: To ensure the reliability of deep models in real-world applications, out-of-distribution (OOD) detection methods aim to distinguish samples close to the training distribution (in-distribution, ID) from those farther away (OOD). In this work, we propose a novel OOD detection method that utilizes singular value decomposition of the weight matrix of the classification head to decompose the model's activations into decisive and insignificant components, which contribute maximally, respectively minimally, to the final classifier output. We find that the subspace of insignificant components more effectively distinguishes ID from OOD data than raw activations in regimes of large distribution shifts (Far-OOD). This occurs because the classification objective leaves the insignificant subspace largely unaffected, yielding features that are ''untainted'' by the target classification task. Conversely, in regimes of smaller distribution shifts (Near-OOD), we find that activation shaping methods profit from only considering the decisive subspace, as the insignificant component can cause interference in the activation space. By combining two findings into a single approach, termed ActSub, we achieve state-of-the-art results in various standard OOD benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</title>
<link>https://arxiv.org/abs/2508.21738</link>
<guid>https://arxiv.org/abs/2508.21738</guid>
<content:encoded><![CDATA[

arXiv:2508.21738v1 Announce Type: cross 
Abstract: With the deepening of poverty alleviation and rural revitalization strategies, improving the rural living environment and enhancing the quality of life have become key priorities. Rural livability is a key indicator for measuring the effectiveness of these efforts. Current measurement approaches face significant limitations, as questionnaire-based methods are difficult to scale, while urban-oriented visual perception methods are poorly suited for rural contexts. In this paper, a rural-specific livability assessment framework was proposed based on drone imagery and multimodal large language models (MLLMs). To comprehensively assess village livability, this study first used a top-down approach to collect large-scale drone imagery of 1,766 villages in 146 counties across China. In terms of the model framework, an efficient image comparison mechanism was developed, incorporating binary search interpolation to determine effective image pairs while reducing comparison iterations. Building on expert knowledge, a chain-of-thought prompting suitable for nationwide rural livability measurement was constructed, considering both living quality and ecological habitability dimensions. This approach enhanced the rationality and reliability of the livability assessment. Finally, this study characterized the spatial heterogeneity of rural livability across China and thoroughly analyzed its influential factors. The results show that: (1) The rural livability in China demonstrates a dual-core-periphery spatial pattern, radiating outward from Sichuan and Zhejiang provinces with declining gradients; (2) Among various influential factors, government fiscal expenditure emerged as the core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit enhancement in livability. The findings provide valuable insights for rural construction policy-making.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling</title>
<link>https://arxiv.org/abs/2508.21785</link>
<guid>https://arxiv.org/abs/2508.21785</guid>
<content:encoded><![CDATA[

arXiv:2508.21785v1 Announce Type: cross 
Abstract: Heart rate prediction is vital for personalized health monitoring and fitness, while it frequently faces a critical challenge when deploying in real-world: data heterogeneity. We classify it in two key dimensions: source heterogeneity from fragmented device markets with varying feature sets, and user heterogeneity reflecting distinct physiological patterns across individuals and activities. Existing methods either discard device-specific information, or fail to model user-specific differences, limiting their real-world performance. To address this, we propose a framework that learns latent representations agnostic to both heterogeneity, enabling downstream predictors to work consistently under heterogeneous data patterns. Specifically, we introduce a random feature dropout strategy to handle source heterogeneity, making the model robust to various feature sets. To manage user heterogeneity, we employ a time-aware attention module to capture long-term physiological traits and use a contrastive learning objective to build a discriminative representation space. To reflect the heterogeneous nature of real-world data, we created and publicly released a new benchmark dataset, ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that our model significantly outperforms existing baselines by 17% and 15%, respectively. Furthermore, analysis of the learned representations demonstrates their strong discriminative power, and one downstream application task confirm the practical value of our model.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation</title>
<link>https://arxiv.org/abs/2309.08289</link>
<guid>https://arxiv.org/abs/2309.08289</guid>
<content:encoded><![CDATA[

arXiv:2309.08289v3 Announce Type: replace 
Abstract: Accurate 3D modeling of human organs is critical for constructing digital phantoms in virtual imaging trials. However, organs such as the large intestine remain particularly challenging due to their complex geometry and shape variability. We propose CLAP, a novel Conditional LAtent Point-diffusion model that combines geometric deep learning with denoising diffusion models to enhance 3D representations of the large intestine. Given point clouds sampled from segmentation masks, we employ a hierarchical variational autoencoder to learn both global and local latent shape representations. Two conditional diffusion models operate within this latent space to refine the organ shape. A pretrained surface reconstruction model is then used to convert the refined point clouds into meshes. CLAP achieves substantial improvements in shape modeling accuracy, reducing Chamfer distance by 26% and Hausdorff distance by 36% relative to the initial suboptimal shapes. This approach offers a robust and extensible solution for high-fidelity organ modeling, with potential applicability to a wide range of anatomical structures.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALow-Cost Real-Time Framework for Industrial Action Recognition Using Foundation Models</title>
<link>https://arxiv.org/abs/2403.08420</link>
<guid>https://arxiv.org/abs/2403.08420</guid>
<content:encoded><![CDATA[

arXiv:2403.08420v2 Announce Type: replace 
Abstract: Action recognition (AR) in industrial environments -- particularly for identifying actions and operational gestures -- faces persistent challenges due to high deployment costs, poor cross-scenario generalization, and limited real-time performance. To address these issues, we propose a low-cost real-time framework for industrial action recognition using foundation models, denoted as LRIAR, to enhance recognition accuracy and transferability while minimizing human annotation and computational overhead. The proposed framework constructs an automatically labeled dataset by coupling Grounding DINO with the pretrained BLIP-2 image encoder, enabling efficient and scalable action labeling. Leveraging the constructed dataset, we train YOLOv5 for real-time action detection, and a Vision Transformer (ViT) classifier is deceloped via LoRA-based fine-tuning for action classification. Extensive experiments conducted in real-world industrial settings validate the effectiveness of LRIAR, demonstrating consistent improvements over state-of-the-art methods in recognition accuracy, scenario generalization, and deployment efficiency.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model</title>
<link>https://arxiv.org/abs/2408.01627</link>
<guid>https://arxiv.org/abs/2408.01627</guid>
<content:encoded><![CDATA[

arXiv:2408.01627v2 Announce Type: replace 
Abstract: In recent years, the talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high-quality video. However, no single model has yet achieved equivalence across all quantitative and qualitative metrics. We introduce Jamba, a hybrid Transformer-Mamba model, to animate a 3D face. Mamba, a pioneering Structured State Space Model (SSM) architecture, was developed to overcome the limitations of conventional Transformer architectures, particularly in handling long sequences. This challenge has constrained traditional models. Jamba combines the advantages of both the Transformer and Mamba approaches, offering a comprehensive solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and lip sync through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding a diffusion model using sliding windows</title>
<link>https://arxiv.org/abs/2411.10257</link>
<guid>https://arxiv.org/abs/2411.10257</guid>
<content:encoded><![CDATA[

arXiv:2411.10257v3 Announce Type: replace 
Abstract: Guidance is a widely used technique for diffusion models to enhance sample quality. Technically, guidance is realised by using an auxiliary model that generalises more broadly than the primary model. Using a 2D toy example, we first show that it is highly beneficial when the auxiliary model exhibits similar but stronger generalisation errors than the primary model. Based on this insight, we introduce \emph{masked sliding window guidance (M-SWG)}, a novel, training-free method. M-SWG upweights long-range spatial dependencies by guiding the primary model with itself by selectively restricting its receptive field. M-SWG requires neither access to model weights from previous iterations, additional training, nor class conditioning. M-SWG achieves a superior Inception score (IS) compared to previous state-of-the-art training-free approaches, without introducing sample oversaturation. In conjunction with existing guidance methods, M-SWG reaches state-of-the-art Frechet DINOv2 distance on ImageNet using EDM2-XXL and DiT-XL. The code is available at https://github.com/HHU-MMBS/swg_bmvc2025_official.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximising Kidney Glomeruli Segmentation using Minimal Labels via Self-Supervision</title>
<link>https://arxiv.org/abs/2412.15389</link>
<guid>https://arxiv.org/abs/2412.15389</guid>
<content:encoded><![CDATA[

arXiv:2412.15389v2 Announce Type: replace 
Abstract: Histopathology, the microscopic examination of tissue samples, is essential for disease diagnosis and prognosis. Accurate segmentation and identification of key regions in histopathology images are crucial for developing automated solutions. However, state-of-art deep learning segmentation methods like UNet require extensive labels, which is both costly and time-consuming, particularly when dealing with multiple stainings. To mitigate this, various multi-stain segmentation methods such as UDA-GAN have been developed, which reduce the need for labels by requiring only one (source) stain to be labelled. Nonetheless, obtaining source stain labels can still be challenging, and segmentation models fail when they are unavailable. This article shows that through self-supervised pre-training$-$including SimCLR, BYOL, and a novel approach, HR-CS-CO$-$the performance of these segmentation methods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably, with self-supervised pre-training and using only 5% labels, the performance drops are minimal: 5.9% for UNet and 6.2% for UDAGAN, compared to their respective fully supervised counterparts (without pre-training, using 100% labels). Furthermore, these findings are shown to generalise beyond their training distribution to public benchmark datasets. Im-
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHaRM: Conditioned Heatmap Regression Methodology for Accurate and Fast Dental Landmark Localization</title>
<link>https://arxiv.org/abs/2501.13073</link>
<guid>https://arxiv.org/abs/2501.13073</guid>
<content:encoded><![CDATA[

arXiv:2501.13073v5 Announce Type: replace 
Abstract: Identifying anatomical landmarks in 3D dental models is essential for orthodontic treatment, yet manual placement is labor-intensive and requires expert knowledge. While machine learning methods have been proposed for automatic landmark detection in 3D Intraoral Scans (IOS), none provide a fully end-to-end solution that avoids costly tooth segmentation.
  We present CHaRM (Conditioned Heatmap Regression Methodology), the first fully end-to-end deep learning approach for tooth landmark detection in 3D IOS. CHaRM integrates four components: a point cloud encoder, a decoder with a heatmap regression head, a teeth-presence classification head, and the novel CHaR module. The CHaR module leverages teeth-presence information to adapt to missing teeth, improving detection accuracy in complex dental cases. Unlike two-stage workflows that segment teeth before landmarking, CHaRM operates directly on IOS point clouds, reducing complexity, avoiding error propagation, and lowering computational cost.
  We evaluated CHaRM with five point cloud learning backbones on IOSLandmarks-1k, a new dataset of 1,214 annotated 3D dental models. Both the dataset and code will be publicly released to address the scarcity of open data in orthodontics and foster reproducible research.
  CHaRM with PointMLP, named CHaRNet, achieved the best accuracy and efficiency. Compared to state-of-the-art methods (TSMDL and ALIIOS), CHaRNet reduced mean Euclidean distance error to 0.56 mm on standard dental models and 1.12 mm across all dentition type, while delivering up to 14.8x faster inference on GPU. This end-to-end approach streamlines orthodontic workflows, enhances the precision of 3D IOS analysis, and enables efficient computer-assisted treatment planning.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration</title>
<link>https://arxiv.org/abs/2502.14156</link>
<guid>https://arxiv.org/abs/2502.14156</guid>
<content:encoded><![CDATA[

arXiv:2502.14156v3 Announce Type: replace 
Abstract: Vehicle-to-everything (V2X) collaborative perception has emerged as a promising solution to address the limitations of single-vehicle perception systems. However, existing V2X datasets are limited in scope, diversity, and quality. To address these gaps, we present Mixed Signals, a comprehensive V2X dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from three connected autonomous vehicles (CAVs) equipped with two different configurations of LiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides point clouds and bounding box annotations across 10 classes, ensuring reliable data for perception training. We provide detailed statistical analysis on the quality of our dataset and extensively benchmark existing V2X methods on it. The Mixed Signals dataset is ready-to-use, with precise alignment and consistent annotations across time and viewpoints. Dataset website is available at https://mixedsignalsdataset.cs.cornell.edu/.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Rectangular Attention Module</title>
<link>https://arxiv.org/abs/2503.10875</link>
<guid>https://arxiv.org/abs/2503.10875</guid>
<content:encoded><![CDATA[

arXiv:2503.10875v2 Announce Type: replace 
Abstract: In this paper, we introduce a novel spatial attention module that can be easily integrated to any convolutional network. This module guides the model to pay attention to the most discriminative part of an image. This enables the model to attain a better performance by an end-to-end training. In conventional approaches, a spatial attention map is typically generated in a position-wise manner. Thus, it is often resulting in irregular boundaries and so can hamper generalization to new samples. In our method, the attention region is constrained to be rectangular. This rectangle is parametrized by only 5 parameters, allowing for a better stability and generalization to new samples. In our experiments, our method systematically outperforms the position-wise counterpart. So that, we provide a novel useful spatial attention mechanism for convolutional models. Besides, our module also provides the interpretability regarding the \textit{where to look} question, as it helps to know the part of the input on which the model focuses to produce the prediction.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation</title>
<link>https://arxiv.org/abs/2503.16430</link>
<guid>https://arxiv.org/abs/2503.16430</guid>
<content:encoded><![CDATA[

arXiv:2503.16430v3 Announce Type: replace 
Abstract: Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.02617</link>
<guid>https://arxiv.org/abs/2504.02617</guid>
<content:encoded><![CDATA[

arXiv:2504.02617v2 Announce Type: replace 
Abstract: RGB-based novel object pose estimation is critical for rapid deployment in robotic applications, yet zero-shot generalization remains a key challenge. In this paper, we introduce PicoPose, a novel framework designed to tackle this task using a three-stage pixel-to-pixel correspondence learning process. Firstly, PicoPose matches features from the RGB observation with those from rendered object templates, identifying the best-matched template and establishing coarse correspondences. Secondly, PicoPose smooths the correspondences by globally regressing a 2D affine transformation, including in-plane rotation, scale, and 2D translation, from the coarse correspondence map. Thirdly, PicoPose applies the affine transformation to the feature map of the best-matched template and learns correspondence offsets within local regions to achieve fine-grained correspondences. By progressively refining the correspondences, PicoPose significantly improves the accuracy of object poses computed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the seven core datasets of the BOP benchmark, demonstrating exceptional generalization to novel objects. Code and trained models are available at https://github.com/foollh/PicoPose.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computer-Aided Design of Personalized Occlusal Positioning Splints Using Multimodal 3D Data</title>
<link>https://arxiv.org/abs/2504.12868</link>
<guid>https://arxiv.org/abs/2504.12868</guid>
<content:encoded><![CDATA[

arXiv:2504.12868v2 Announce Type: replace 
Abstract: Digital technology plays a crucial role in designing customized medical devices, such as occlusal splints, commonly used in the management of disorders of the stomatognathic system. This methodological proof-of-concept study presents a computer-aided approach for designing and evaluating occlusal positioning splints. The primary aim is to demonstrate the feasibility and geometric accuracy of the proposed method at the preclinical stage. In this approach, a three-dimensional splint is generated using a transformation matrix to represent the therapeutic mandibular position. An experienced operator defines this position using a virtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans, and a digitized plaster model. We introduce a novel method for generating splints that reproduces occlusal conditions in the therapeutic position and resolves surface conflicts through virtual embossing. The process for obtaining transformation matrices using dental tools and intraoral devices commonly employed in dental and laboratory workflows is described, and the geometric accuracy of both designed and printed splints is evaluated using profile and surface deviation analysis. The method supports reproducible, patient-specific splint fabrication and provides a transparent foundation for future validation studies, supporting multimodal image registration and quantification of occlusal discrepancies in research settings.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Camera Motions in Any Video</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[

arXiv:2504.15376v2 Announce Type: replace 
Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like "follow" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saliency-Guided Training for Fingerprint Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.02176</link>
<guid>https://arxiv.org/abs/2505.02176</guid>
<content:encoded><![CDATA[

arXiv:2505.02176v2 Announce Type: replace 
Abstract: Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated "pseudosaliency," including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation</title>
<link>https://arxiv.org/abs/2505.03401</link>
<guid>https://arxiv.org/abs/2505.03401</guid>
<content:encoded><![CDATA[

arXiv:2505.03401v2 Announce Type: replace 
Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. Existing LRRG approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. However, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. Consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in LRRG. To address this, we develop a novel dynamic difference-aware temporal residual network (DDaTR). In DDaTR, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. The Dynamic Feature Alignment Module (DFAM) is designed to align prior features across modalities for the integrity of prior clinical information. Prompted by the enriched prior features, the dynamic difference-aware module (DDAM) captures favorable difference information by identifying relationships across exams. Furthermore, our DDaTR employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. Extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both RRG and LRRG tasks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment</title>
<link>https://arxiv.org/abs/2506.06680</link>
<guid>https://arxiv.org/abs/2506.06680</guid>
<content:encoded><![CDATA[

arXiv:2506.06680v3 Announce Type: replace 
Abstract: Infertility has a considerable impact on individuals' quality of life, affecting them socially and psychologically, with projections indicating a rise in the upcoming years. In vitro fertilization (IVF) emerges as one of the primary techniques within economically developed nations, employed to address the rising problem of low fertility. Expert embryologists conventionally grade embryos by reviewing blastocyst images to select the most optimal for transfer, yet this process is time-consuming and lacks efficiency. Blastocyst images provide a valuable resource for assessing embryo viability. In this study, we introduce an explainable artificial intelligence (XAI) framework for classifying embryos, employing a fusion of convolutional neural network (CNN) and long short-term memory (LSTM) architecture, referred to as CNN-LSTM. Utilizing deep learning, our model achieves high accuracy in embryo classification while maintaining interpretability through XAI.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent and Invariant Generalization Learning for Short-video Misinformation Detection</title>
<link>https://arxiv.org/abs/2507.04061</link>
<guid>https://arxiv.org/abs/2507.04061</guid>
<content:encoded><![CDATA[

arXiv:2507.04061v3 Announce Type: replace 
Abstract: Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</title>
<link>https://arxiv.org/abs/2507.08340</link>
<guid>https://arxiv.org/abs/2507.08340</guid>
<content:encoded><![CDATA[

arXiv:2507.08340v2 Announce Type: replace 
Abstract: Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization</title>
<link>https://arxiv.org/abs/2507.12420</link>
<guid>https://arxiv.org/abs/2507.12420</guid>
<content:encoded><![CDATA[

arXiv:2507.12420v2 Announce Type: replace 
Abstract: Bounding box regression (BBR) is fundamental to object detection, where the regression loss is crucial for accurate localization. Existing IoU-based losses often incorporate handcrafted geometric penalties to address IoU's non-differentiability in non-overlapping cases and enhance BBR performance. However, these penalties are sensitive to box shape, size, and distribution, often leading to suboptimal optimization for small objects and undesired behaviors such as bounding box enlargement due to misalignment with the IoU objective. To address these limitations, we propose InterpIoU, a novel loss function that replaces handcrafted geometric penalties with a term based on the IoU between interpolated boxes and the target. By using interpolated boxes to bridge the gap between predictions and ground truth, InterpIoU provides meaningful gradients in non-overlapping cases and inherently avoids the box enlargement issue caused by misaligned penalties. Simulation results further show that IoU itself serves as an ideal regression target, while existing geometric penalties are both unnecessary and suboptimal. Building on InterpIoU, we introduce Dynamic InterpIoU, which dynamically adjusts interpolation coefficients based on IoU values, enhancing adaptability to scenarios with diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC show that our methods consistently outperform state-of-the-art IoU-based losses across various detection frameworks, with particularly notable improvements in small object detection, confirming their effectiveness.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention</title>
<link>https://arxiv.org/abs/2508.06107</link>
<guid>https://arxiv.org/abs/2508.06107</guid>
<content:encoded><![CDATA[

arXiv:2508.06107v2 Announce Type: replace 
Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task due to the inherent two-dimensional structure, varying symbol scales, and complex spatial relationships among symbols. In this paper, we present a self-supervised learning (SSL) framework for HMER that eliminates the need for expensive labeled data. Our approach begins by pretraining an image encoder using a combination of global and local contrastive loss, enabling the model to learn both holistic and fine-grained representations. A key contribution of this work is a novel self-supervised attention network, which is trained using a progressive spatial masking strategy. This attention mechanism is designed to learn semantically meaningful focus regions, such as operators, exponents, and nested mathematical notation, without requiring any supervision. The progressive masking curriculum encourages the network to become increasingly robust to missing or occluded visual information, ultimately improving structural understanding. Our complete pipeline consists of (1) self-supervised pretraining of the encoder, (2) self-supervised attention learning, and (3) supervised fine-tuning with a transformer decoder to generate LATEX sequences. Extensive experiments on CROHME benchmarks demonstrate that our method outperforms existing SSL and fully supervised baselines, validating the effectiveness of our progressive attention mechanism in enhancing HMER performance. Our codebase can be found here.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</title>
<link>https://arxiv.org/abs/2508.07313</link>
<guid>https://arxiv.org/abs/2508.07313</guid>
<content:encoded><![CDATA[

arXiv:2508.07313v2 Announce Type: replace 
Abstract: Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Region-Level Context-Aware Multimodal Understanding</title>
<link>https://arxiv.org/abs/2508.12263</link>
<guid>https://arxiv.org/abs/2508.12263</guid>
<content:encoded><![CDATA[

arXiv:2508.12263v2 Announce Type: replace 
Abstract: Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding -- an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objects' visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC\&amp;P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at https://github.com/hongliang-wei/RC-MLLM
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TorchCP: A Python Library for Conformal Prediction</title>
<link>https://arxiv.org/abs/2402.12683</link>
<guid>https://arxiv.org/abs/2402.12683</guid>
<content:encoded><![CDATA[

arXiv:2402.12683v4 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) is a powerful statistical framework that generates prediction intervals or sets with guaranteed coverage probability. While CP algorithms have evolved beyond traditional classifiers and regressors to sophisticated deep learning models like deep neural networks (DNNs), graph neural networks (GNNs), and large language models (LLMs), existing CP libraries often lack the model support and scalability for large-scale DL scenarios. This paper introduces TorchCP, a PyTorch-native library designed to integrate state-of-the-art CP algorithms into deep learning techniques, including DNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0 license, TorchCP comprises about 16k lines of code, validated with 100% unit test coverage and detailed documentation. Notably, TorchCP enables CP-specific training algorithms, online prediction, and GPU-accelerated batch processing, achieving up to 90% reduction in inference time on large datasets. With its low-coupling design, comprehensive suite of advanced methods, and full GPU scalability, TorchCP empowers researchers and practitioners to enhance uncertainty quantification across cutting-edge applications.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling</title>
<link>https://arxiv.org/abs/2409.08906</link>
<guid>https://arxiv.org/abs/2409.08906</guid>
<content:encoded><![CDATA[

arXiv:2409.08906v3 Announce Type: replace-cross 
Abstract: Diffusion models can generate a variety of high-quality images by modeling complex data distributions. Trained diffusion models can also be very effective image priors for solving inverse problems. Most of the existing diffusion-based methods integrate data consistency steps by approximating the likelihood function within the diffusion reverse sampling process. In this paper, we show that the existing approximations are either insufficient or computationally inefficient. To address these issues, we propose a unified likelihood approximation method that incorporates a covariance correction term to enhance the performance and avoids propagating gradients through the diffusion model. The correction term, when integrated into the reverse diffusion sampling process, achieves better convergence towards the true data posterior for selected distributions and improves performance on real-world natural image datasets. Furthermore, we present an efficient way to factorize and invert the covariance matrix of the likelihood function for several inverse problems. Our comprehensive experiments demonstrate the effectiveness of our method over several existing approaches. Code available at https://github.com/CSIPlab/CoDPS.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing Attention to CAD: Boundary Representation Learning via Transformer</title>
<link>https://arxiv.org/abs/2504.07134</link>
<guid>https://arxiv.org/abs/2504.07134</guid>
<content:encoded><![CDATA[

arXiv:2504.07134v2 Announce Type: replace-cross 
Abstract: The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, we propose a novel approach for adapting Transformers to B-rep learning, called the Boundary Representation Transformer (BRT). B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into Bezier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformer's attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Imitation Enables Contextual Humanoid Control</title>
<link>https://arxiv.org/abs/2505.03729</link>
<guid>https://arxiv.org/abs/2505.03729</guid>
<content:encoded><![CDATA[

arXiv:2505.03729v5 Announce Type: replace-cross 
Abstract: How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.04619</link>
<guid>https://arxiv.org/abs/2505.04619</guid>
<content:encoded><![CDATA[

arXiv:2505.04619v2 Announce Type: replace-cross 
Abstract: Vision is well-known for its use in manipulation, especially using visual servoing. Due to the 3D nature of the world, using multiple camera views and merging them creates better representations for Q-learning and in turn, trains more sample efficient policies. Nevertheless, these multi-view policies are sensitive to failing cameras and can be burdensome to deploy. To mitigate these issues, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while simultaneously disentangling views by augmenting multi-view feature inputs with single-view features. This produces robust policies and allows lightweight deployment. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling</title>
<link>https://arxiv.org/abs/2505.14177</link>
<guid>https://arxiv.org/abs/2505.14177</guid>
<content:encoded><![CDATA[

arXiv:2505.14177v2 Announce Type: replace-cross 
Abstract: We consider the problem of sampling distributions stemming from non-convex potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of the discrete-time ULA to drift approximations under the assumption that the potential is strongly convex at infinity. In many context, e.g. imaging inverse problems, potentials are non-convex and non-smooth. Proximal Stochastic Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such potentials. It combines the forward-backward optimization algorithm with a ULA step. Our main stability result combined with properties of the Moreau envelope allows us to derive the first proof of convergence of the PSGLA for non-convex potentials. We empirically validate our methodology on synthetic data and in the context of imaging inverse problems. In particular, we observe that PSGLA exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm for posterior sampling while preserving its restoration properties.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Residual-Based Scalable Image Coding for Humans and Machines</title>
<link>https://arxiv.org/abs/2506.19297</link>
<guid>https://arxiv.org/abs/2506.19297</guid>
<content:encoded><![CDATA[

arXiv:2506.19297v2 Announce Type: replace-cross 
Abstract: Scalable image compression is a technique that progressively reconstructs multiple versions of an image for different requirements. In recent years, images have increasingly been consumed not only by humans but also by image recognition models. This shift has drawn growing attention to scalable image compression methods that serve both machine and human vision (ICMH). Many existing models employ neural network-based codecs, known as learned image compression, and have made significant strides in this field by carefully designing the loss functions. In some cases, however, models are overly reliant on their learning capacity, and their architectural design is not sufficiently considered. In this paper, we enhance the coding efficiency and interpretability of ICMH framework by integrating an explicit residual compression mechanism, which is commonly employed in resolution scalable coding methods such as JPEG2000. Specifically, we propose two complementary methods: Feature Residual-based Scalable Coding (FR-ICMH) and Pixel Residual-based Scalable Coding (PR-ICMH). These proposed methods are applicable to various machine vision tasks. Moreover, they provide flexibility to choose between encoder complexity and compression performance, making it adaptable to diverse application requirements. Experimental results demonstrate the effectiveness of our proposed methods, with PR-ICMH achieving up to 29.57% BD-rate savings over the previous work.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar</title>
<link>https://arxiv.org/abs/2507.07331</link>
<guid>https://arxiv.org/abs/2507.07331</guid>
<content:encoded><![CDATA[

arXiv:2507.07331v2 Announce Type: replace-cross 
Abstract: In this paper, we present mmFlux: a novel framework for extracting underlying crowd motion patterns and inferring crowd semantics using mmWave radar. First, our proposed signal processing pipeline combines optical flow estimation concepts from vision with novel statistical and morphological noise filtering. This approach generates high-fidelity mmWave flow fields-compact 2D vector representations of crowd motion. We then introduce a novel approach that transforms these fields into directed geometric graphs. In these graphs, edges capture dominant flow currents, vertices mark crowd splitting or merging, and flow distribution is quantified across edges. Finally, we show that analyzing the local Jacobian and computing the corresponding curl and divergence enables extraction of key crowd semantics for both structured and diffused crowds. We conduct 21 experiments on crowds of up to 20 people across 3 areas, using commodity mmWave radar. Our framework achieves high-fidelity graph reconstruction of the underlying flow structure, even for complex crowd patterns, demonstrating strong spatial alignment and precise quantitative characterization of flow split ratios. Finally, our curl and divergence analysis accurately infers key crowd semantics, e.g., abrupt turns, boundaries where flow directions shift, dispersions, and gatherings. Overall, these findings validate mmFlux, underscoring its potential for various crowd analytics applications.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</title>
<link>https://arxiv.org/abs/2508.02512</link>
<guid>https://arxiv.org/abs/2508.02512</guid>
<content:encoded><![CDATA[

arXiv:2508.02512v2 Announce Type: replace-cross 
Abstract: Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization</title>
<link>https://arxiv.org/abs/2508.20181</link>
<guid>https://arxiv.org/abs/2508.20181</guid>
<content:encoded><![CDATA[
<div> hallucinations, MLLMs, alignment problem, CHAIR metric, Direct Preference Optimization

Summary: 
Multimodal Large Language Models (MLLMs) are powerful tools for various tasks, but they often suffer from hallucinations, generating answers not supported by visual input. This paper addresses hallucinations as an alignment problem and introduces CHAIR-DPO, a method that leverages the CHAIR metric to reduce hallucinations in MLLMs. By fine-tuning MLLMs using Direct Preference Optimization (DPO) with the CHAIR-based reward, CHAIR-DPO effectively decreases hallucinated answers on multiple benchmarks. Unlike previous approaches, CHAIR-DPO does not require complex pipelines or proprietary models, making it more accessible. The method demonstrates the importance of aligning MLLMs to prefer generating content without hallucinations, improving the overall performance of these models in various applications. The source code and trained models for CHAIR-DPO are available publicly for further research and development. <br /><br />Summary: <div>
arXiv:2508.20181v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to address a multitude of tasks, ranging from NLP to computer vision. Despite showcasing state-of-the-art results in many benchmarks, a long-standing issue is the tendency of MLLMs to hallucinate, that is to generate answers to the user's query that are not reflected in the visual input. In this paper, we address the problem of hallucinations as an alignment problem, seeking to steer the MLLM so that it prefers generating content without hallucinations. In contrast to recent approaches that require complicated pipelines to build synthetic preference data for alignment training, often relying on proprietary models, we capitalize on the well-known CHAIR metric, originally proposed to gauge the degree of hallucinations in image captioning. Given a pair of generated answers, we leverage CHAIR to distinguish winner and loser options (i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf MLLMs via Direct Preference Optimization (DPO). The resulting method, which we refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models are publicly available at https://github.com/aimagelab/CHAIR-DPO.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization</title>
<link>https://arxiv.org/abs/2508.20182</link>
<guid>https://arxiv.org/abs/2508.20182</guid>
<content:encoded><![CDATA[
<div> Keywords: Stable Diffusion, image manipulation, forgery localization, multi-modal processing, image forensics<br />
<br />
Summary:<br />
1. The article introduces a new approach that integrates image generation and perceptual capabilities of Stable Diffusion (SD) into an image forensic framework to enhance forgery localization.<br />
2. The multi-modal architecture of SD can be conditioned on forgery-related information, allowing for more efficient and accurate forgery localization results.<br />
3. The method leverages Stable DiffusionV3 (SD3) to enhance forgery localization performance by treating image forgery residuals as an explicit modality in the latent space.<br />
4. By fusing the modality of forgery residuals into the latent space during training, the model improves performance significantly on benchmarking datasets compared to current state-of-the-art models.<br />
5. The framework shows strong performance on forensic tasks involving both real-world document forgery images and natural scene forging images, even when the data was unseen during training. <div>
arXiv:2508.20182v1 Announce Type: new 
Abstract: Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study</title>
<link>https://arxiv.org/abs/2508.20188</link>
<guid>https://arxiv.org/abs/2508.20188</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, skin diseases, interpretability, Multimodal Large Language Models, quantitative attributes<br />
<br />
Summary: 
Artificial Intelligence models have shown promise in diagnosing skin diseases, including cancer, but the interpretability of their predictions needs improvement. This study explores the combination of Multimodal Large Language Models (MLLMs) and quantitative attribute usage to enhance interpretability. MLLMs offer a potentially more understandable way of reasoning for diagnosis, while quantitative attributes related to lesion appearance can improve predictive accuracy. By fine-tuning MLLMs to predict values of these attributes from images, the study demonstrates that the embedding spaces of MLLMs can be grounded in these attributes. An attribute-specific content-based image retrieval case study using the SLICE-3D dataset supports this grounding in the embedding space. This research provides a promising approach to enhancing the interpretability of AI models in diagnosing skin diseases. <br /><br />Summary: <div>
arXiv:2508.20188v1 Announce Type: new 
Abstract: Artificial Intelligence models have demonstrated significant success in diagnosing skin diseases, including cancer, showing the potential to assist clinicians in their analysis. However, the interpretability of model predictions must be significantly improved before they can be used in practice. To this end, we explore the combination of two promising approaches: Multimodal Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a potential avenue for increased interpretability, providing reasoning for diagnosis in natural language through an interactive format. Separately, a number of quantitative attributes that are related to lesion appearance (e.g., lesion area) have recently been found predictive of malignancy with high accuracy. Predictions grounded as a function of such concepts have the potential for improved interpretability. We provide evidence that MLLM embedding spaces can be grounded in such attributes, through fine-tuning to predict their values from images. Concretely, we evaluate this grounding in the embedding space through an attribute-specific content-based image retrieval case study using the SLICE-3D dataset.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels</title>
<link>https://arxiv.org/abs/2508.20193</link>
<guid>https://arxiv.org/abs/2508.20193</guid>
<content:encoded><![CDATA[
<div> Transformer, Modulation recognition, Supervised learning, Self-supervised learning, Reconstruction objective

Summary:<br />
Automatic modulation recognition (AMR) is essential for various wireless communication applications. Existing solutions for AMR often face challenges like requiring large labeled datasets or complex training pipelines, affecting scalability and generalization. To address this, a unified Vision Transformer (ViT) framework is proposed, combining supervised, self-supervised, and reconstruction objectives. The framework includes a ViT encoder, a lightweight convolutional decoder, and a linear classifier. By incorporating fine-grained I/Q structure and leveraging partial label supervision, the model achieves superior performance on the RML2018.01A dataset with limited labeled data, outperforming baseline models in low-label scenarios and maintaining accuracy across different signal-to-noise ratio (SNR) levels. Overall, this approach provides a simple, adaptable, and label-efficient solution for automatic modulation recognition. 

Summary:<br />
 <div>
arXiv:2508.20193v1 Announce Type: new 
Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio, spectrum monitoring, and secure wireless communication. However, existing solutions often rely on large labeled datasets or multi-stage training pipelines, which limit scalability and generalization in practice. We propose a unified Vision Transformer (ViT) framework that integrates supervised, self-supervised, and reconstruction objectives. The model combines a ViT encoder, a lightweight convolutional decoder, and a linear classifier; the reconstruction branch maps augmented signals back to their originals, anchoring the encoder to fine-grained I/Q structure. This strategy promotes robust, discriminative feature learning during pretraining, while partial label supervision in fine-tuning enables effective classification with limited labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and ViT baselines in low-label regimes, approaches ResNet-level accuracy with only 15-20% labeled data, and maintains strong performance across varying SNR levels. Overall, the framework provides a simple, generalizable, and label-efficient solution for AMR.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfinityHuman: Towards Long-Term Audio-Driven Human</title>
<link>https://arxiv.org/abs/2508.20210</link>
<guid>https://arxiv.org/abs/2508.20210</guid>
<content:encoded><![CDATA[
<div> Challenges, Audio-driven human animation, Video generation, Hand motions, Pose-guided refiner <br />
Summary: <br />
The article introduces a new framework called InfinityHuman for audio-driven human animation, addressing challenges in generating high-resolution, long-duration videos with consistent appearance and natural hand motions. The proposed method first generates audio-synchronized representations and refines them using a pose-guided refiner to improve video quality, identity preservation, hand accuracy, and lip-sync. The framework decouples pose sequences from appearance to resist error accumulation and temporal degradation, using stable poses and an initial frame as a visual anchor to reduce drift. A hand-specific reward mechanism is introduced to enhance semantic accuracy and gesture realism, trained with high-quality hand motion data. Experiments on the EMTD and HDTF datasets demonstrate state-of-the-art performance, with ablation studies confirming the effectiveness of each module. Code will be publicly available for further research and applications. <br /> <div>
arXiv:2508.20210v1 Announce Type: new 
Abstract: Audio-driven human animation has attracted wide attention thanks to its practical applications. However, critical challenges remain in generating high-resolution, long-duration videos with consistent appearance and natural hand motions. Existing methods extend videos using overlapping motion frames but suffer from error accumulation, leading to identity drift, color shifts, and scene instability. Additionally, hand movements are poorly modeled, resulting in noticeable distortions and misalignment with the audio. In this work, we propose InfinityHuman, a coarse-to-fine framework that first generates audio-synchronized representations, then progressively refines them into high-resolution, long-duration videos using a pose-guided refiner. Since pose sequences are decoupled from appearance and resist temporal degradation, our pose-guided refiner employs stable poses and the initial frame as a visual anchor to reduce drift and improve lip synchronization. Moreover, to enhance semantic accuracy and gesture realism, we introduce a hand-specific reward mechanism trained with high-quality hand motion data. Experiments on the EMTD and HDTF datasets show that InfinityHuman achieves state-of-the-art performance in video quality, identity preservation, hand accuracy, and lip-sync. Ablation studies further confirm the effectiveness of each module. Code will be made public.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos</title>
<link>https://arxiv.org/abs/2508.20221</link>
<guid>https://arxiv.org/abs/2508.20221</guid>
<content:encoded><![CDATA[
<div> saliency prediction, 360-degree videos, spatial audio, vision transformer, dataset <br />
Summary: 
This study introduces a new dataset called YT360-EyeTracking for predicting visual saliency in omnidirectional videos (ODVs). The research focuses on incorporating spatial audio cues to enhance saliency prediction in 360-degree environments. Two novel models, SalViT360 and SalViT360-AV, are proposed for this task, with the latter incorporating transformer adapters conditioned on audio input. Results show that these models outperform existing methods in predicting viewer attention in ODVs. The study suggests that integrating spatial audio cues in the model architecture is crucial for accurate saliency prediction in 360-degree videos. The dataset and code for the models will be made available for further research. <br /> <div>
arXiv:2508.20221v1 Announce Type: new 
Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual reality (VR) by offering an unprecedented full field-of-view (FOV). This study extends the domain of saliency prediction to 360-degree environments, addressing the complexities of spherical distortion and the integration of spatial audio. Contextually, ODVs have transformed user experience by adding a spatial audio dimension that aligns sound direction with the viewer's perspective in spherical scenes. Motivated by the lack of comprehensive datasets for 360-degree audio-visual saliency prediction, our study curates YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying audio-visual conditions. Our goal is to explore how to utilize audio-visual cues to effectively predict visual saliency in 360-degree videos. Towards this aim, we propose two novel saliency prediction models: SalViT360, a vision-transformer-based framework for ODVs equipped with spherical geometry-aware spatio-temporal attention layers, and SalViT360-AV, which further incorporates transformer adapters conditioned on audio input. Our results on a number of benchmark datasets, including our YT360-EyeTracking, demonstrate that SalViT360 and SalViT360-AV significantly outperform existing methods in predicting viewer attention in 360-degree scenes. Interpreting these results, we suggest that integrating spatial audio cues in the model architecture is crucial for accurate saliency prediction in omnidirectional videos. Code and dataset will be available at https://cyberiada.github.io/SalViT360.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Framework for Automated Explain Vision Model Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.20227</link>
<guid>https://arxiv.org/abs/2508.20227</guid>
<content:encoded><![CDATA[
<div> Keywords: vision models, explainability, xAI methods, dataset analysis, Vision-Language Models

Summary:<br />
This paper addresses the need for explainability in vision models by proposing a pipeline that can provide insights at both sample and dataset levels. While existing xAI methods focus on explaining models sample-by-sample, this pipeline aims to analyze the general behavior of vision models on large datasets. By understanding the trends and patterns of vision models on general images, biased judgments can be prevented. The proposed pipeline integrates vision model development with xAI analysis, allowing for the discovery of failure cases and gaining valuable insights with minimal effort. This approach can help advance image analysis by providing a deeper understanding of vision models' behavior. <div>
arXiv:2508.20227v1 Announce Type: new 
Abstract: The development of many vision models mainly focuses on improving their performance using metrics such as accuracy, IoU, and mAP, with less attention to explainability due to the complexity of applying xAI methods to provide a meaningful explanation of trained models. Although many existing xAI methods aim to explain vision models sample-by-sample, methods explaining the general behavior of vision models, which can only be captured after running on a large dataset, are still underexplored. Furthermore, understanding the behavior of vision models on general images can be very important to prevent biased judgments and help identify the model's trends and patterns. With the application of Vision-Language Models, this paper proposes a pipeline to explain vision models at both the sample and dataset levels. The proposed pipeline can be used to discover failure cases and gain insights into vision models with minimal effort, thereby integrating vision model development with xAI analysis to advance image analysis.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems</title>
<link>https://arxiv.org/abs/2508.20232</link>
<guid>https://arxiv.org/abs/2508.20232</guid>
<content:encoded><![CDATA[
<div> Keywords: ATMS-KD, lightweight CNN models, agricultural environments, knowledge distillation, Damask rose<br />
Summary: <br />
This study introduces ATMS-KD, a framework for creating lightweight CNN models for agriculture using adaptive temperature scheduling and mixed-sample augmentation. Three student configurations were tested on a dataset of Damask rose images from Morocco. Results showed improved accuracy using ATMS-KD over direct training, with all student models achieving over 96.7% validation accuracy. ATMS-KD outperformed eleven other knowledge distillation methods, achieving 97.11% accuracy with the compact model and the lowest inference latency. Knowledge retention rates were over 99% for all model configurations, indicating effective knowledge transfer regardless of model capacity. The framework offers promising results for resource-constrained agricultural environments in computer vision applications. <br /><br />Summary: <div>
arXiv:2508.20232v1 Announce Type: new 
Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge Distillation), a novel framework for developing lightweight CNN models suitable for resource-constrained agricultural environments. The framework combines adaptive temperature scheduling with mixed-sample augmentation to transfer knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to lightweight residual CNN students. Three student configurations were evaluated: Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M parameters). The dataset used in this study consists of images of \textit{Rosa damascena} (Damask rose) collected from agricultural fields in the Dades Oasis, southeastern Morocco, providing a realistic benchmark for agricultural computer vision applications under diverse environmental conditions. Experimental evaluation on the Damascena rose maturity classification dataset demonstrated significant improvements over direct training methods. All student models achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to 95--96\% with direct training. The framework outperformed eleven established knowledge distillation methods, achieving 97.11\% accuracy with the compact model -- a 1.60 percentage point improvement over the second-best approach while maintaining the lowest inference latency of 72.19\,ms. Knowledge retention rates exceeded 99\% for all configurations, demonstrating effective knowledge transfer regardless of student model capacity.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification</title>
<link>https://arxiv.org/abs/2508.20243</link>
<guid>https://arxiv.org/abs/2508.20243</guid>
<content:encoded><![CDATA[
<div> Framework, Microstructure, Additive manufacturing, Qualification, Vision-language<br />
<br />
Summary:<br />
This study presents a novel framework that combines microstructure informatics with expert characterization knowledge using customized vision-language representations. By leveraging deep semantic segmentation and pre-trained multi-modal models, the framework encodes visual and textual data into shared representations. A customized similarity-based representation allows for zero-shot classification of unseen microstructures based on expert annotations. Validation on an additively manufactured dataset shows the framework's ability to distinguish between acceptable and defective samples. The FLAVA model offers high visual sensitivity, while the CLIP model aligns consistently with textual criteria. Z-score normalization adjusts similarity scores for effective alignment and classification. This method improves traceability and interpretability in qualification pipelines, enabling human-in-the-loop decision-making without retraining models. By enhancing semantic interoperability between data and expert knowledge, this work contributes to scalable and adaptable qualification strategies in engineering informatics. <div>
arXiv:2508.20243v1 Announce Type: new 
Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck in industrial manufacturing, particularly for heterogeneous structures produced via non-conventional additive manufacturing processes. This study introduces a novel framework that links microstructure informatics with a range of expert characterization knowledge using customized and hybrid vision-language representations (VLRs). By integrating deep semantic segmentation with pre-trained multi-modal models (CLIP and FLAVA), we encode both visual microstructural data and textual expert assessments into shared representations. To overcome limitations in general-purpose embeddings, we develop a customized similarity-based representation that incorporates both positive and negative references from expert-annotated images and their associated textual descriptions. This allows zero-shot classification of previously unseen microstructures through a net similarity scoring approach. Validation on an additively manufactured metal matrix composite dataset demonstrates the framework's ability to distinguish between acceptable and defective samples across a range of characterization criteria. Comparative analysis reveals that FLAVA model offers higher visual sensitivity, while the CLIP model provides consistent alignment with the textual criteria. Z-score normalization adjusts raw unimodal and cross-modal similarity scores based on their local dataset-driven distributions, enabling more effective alignment and classification in the hybrid vision-language framework. The proposed method enhances traceability and interpretability in qualification pipelines by enabling human-in-the-loop decision-making without task-specific model retraining. By advancing semantic interoperability between raw data and expert knowledge, this work contributes toward scalable and domain-adaptable qualification strategies in engineering informatics.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces</title>
<link>https://arxiv.org/abs/2508.20256</link>
<guid>https://arxiv.org/abs/2508.20256</guid>
<content:encoded><![CDATA[
<div> Transformer-inspired 3D encoder-decoder convolutional network, automated PVS segmentation, diverse MRI datasets, MedNeXt-L-k5, voxel-level Dice scores<br />
Summary:<br />
- Enlarged perivascular spaces (PVS) are important biomarkers for various neurodegenerative diseases.<br />
- Manual segmentation of PVS is time-consuming, and existing automated models have limitations in generalization.<br />
- MedNeXt-L-k5, a Transformer-inspired network, was adapted for automated PVS segmentation.<br />
- Models trained on T2-weighted MRI scans achieved high Dice scores, comparable to inter-rater reliability.<br />
- Models trained on T1-weighted MRI scans had lower accuracy, indicating challenges in segmentation.<br />
- MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation across diverse MRI datasets.<br />
- Attention-based mechanisms in transformer-inspired models may not be necessary for high accuracy in PVS segmentation.<br /> 
Summary:<br /> <div>
arXiv:2508.20256v1 Announce Type: new 
Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers of cerebral small vessel disease, Alzheimer's disease, stroke, and aging-related neurodegeneration. However, manual segmentation of PVS is time-consuming and subject to moderate inter-rater reliability, while existing automated deep learning models have moderate performance and typically fail to generalize across diverse clinical and research MRI datasets. We adapted MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network, for automated PVS segmentation. Two models were trained: one using a homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model performance was evaluated using internal 5-fold cross validation (5FCV) and leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of 0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater reliability of that dataset, and the highest yet reported in the literature. The same models trained on the T1w images of the HCP-Aging dataset achieved a substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG). MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the nnU-Net, indicating that the attention-based mechanisms present in transformer-inspired models to provide global context are not required for high accuracy in PVS segmentation.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation</title>
<link>https://arxiv.org/abs/2508.20265</link>
<guid>https://arxiv.org/abs/2508.20265</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, segmentation, spatial coherence, feedback-driven, attention<br />
<br />
Summary: 
This paper addresses the issue of poor segmentation performance in CLIP due to the lack of spatial coherence in intermediate attention. By developing a training-free, feedback-driven framework, the authors adapt output-based patch-level correspondences back to intermediate attention, enhancing semantic consistency between internal representations and final predictions. Key modules such as attention isolation and confidence-based pruning enable sparse adaptation, while an adaptation ensemble effectively provides feedback on output coherence cues. The proposed method integrates seamlessly into state-of-the-art approaches with various backbones and attention types, consistently improving performance across multiple benchmarks. <div>
arXiv:2508.20265v1 Announce Type: new 
Abstract: CLIP exhibits strong visual-textual alignment but struggle with open-vocabulary segmentation due to poor localization. Prior methods enhance spatial coherence by modifying intermediate attention. But, this coherence isn't consistently propagated to the final output due to subsequent operations such as projections. Additionally, intermediate attention lacks direct interaction with text representations, such semantic discrepancy limits the full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. The output predictions, being the culmination of the model's processing, encapsulate the most comprehensive visual and textual semantics about each patch. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. We design key modules, including attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble, to effectively feedback the output coherence cues. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We further validate our framework across multiple attention types (Q-K, self-self, and Proxy augmented with MAE, SAM, and DINO). Our approach consistently improves their performance across eight benchmarks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding</title>
<link>https://arxiv.org/abs/2508.20279</link>
<guid>https://arxiv.org/abs/2508.20279</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, probing framework, visual grounding, semantic reasoning, model-agnostic approach

Summary:<br /><br />
In this work, a probing framework is introduced to systematically analyze the processing dynamics of Multimodal Large Language Models (MLLMs) across layers. By training linear classifiers to predict fine-grained visual categories from token embeddings at each layer, the authors demonstrate a stage-wise structure in MLLMs. Early layers focus on visual grounding, middle layers support lexical integration and semantic reasoning, and final layers prepare task-specific outputs. The study also evaluates the probes under three controlled prompt variations to uncover the functional roles of different layers. Results show that the overall stage-wise structure remains stable across variations, but the specific layer allocation to each stage shifts notably with changes in the base LLM architecture. This work provides insights into the layer-wise organization of MLLMs and offers a lightweight, model-agnostic approach for analyzing multimodal representation dynamics.<br /> <div>
arXiv:2508.20279v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance across a wide range of vision-language tasks, yet their internal processing dynamics remain underexplored. In this work, we introduce a probing framework to systematically analyze how MLLMs process visual and textual inputs across layers. We train linear classifiers to predict fine-grained visual categories (e.g., dog breeds) from token embeddings extracted at each layer, using a standardized anchor question. To uncover the functional roles of different layers, we evaluate these probes under three types of controlled prompt variations: (1) lexical variants that test sensitivity to surface-level changes, (2) semantic negation variants that flip the expected answer by modifying the visual concept in the prompt, and (3) output format variants that preserve reasoning but alter the answer format. Applying our framework to LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent stage-wise structure in which early layers perform visual grounding, middle layers support lexical integration and semantic reasoning, and final layers prepare task-specific outputs. We further show that while the overall stage-wise structure remains stable across variations in visual tokenization, instruction tuning data, and pretraining corpus, the specific layer allocation to each stage shifts notably with changes in the base LLM architecture. Our findings provide a unified perspective on the layer-wise organization of MLLMs and offer a lightweight, model-agnostic approach for analyzing multimodal representation dynamics.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)</title>
<link>https://arxiv.org/abs/2508.20322</link>
<guid>https://arxiv.org/abs/2508.20322</guid>
<content:encoded><![CDATA[
<div> keywords: Vision-language co-embedding networks, disentangled embedding space, supervised dictionary learning, concept-specific component vectors, concept-filtered image retrieval

Summary: 
- The article introduces a novel approach called Sparse Linear Concept Subspaces (SLiCS) to disentangle the embedding space of vision-language co-embedding networks.
- SLiCS decomposes the embedding into multiple concept-specific component vectors that lie in different subspaces, enabling a more precise concept-filtered image retrieval and conditional generation using image-to-prompt.
- A supervised dictionary learning approach is proposed to estimate a linear synthesis model consisting of sparse, non-negative combinations of concept-specific component vectors associated with labels.
- The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence.
- The approach leverages text co-embeddings to find semantically meaningful descriptions and enable zero-shot classification of images, providing instance-wise multi-labels. 

<br /><br />Summary: <div>
arXiv:2508.20322v1 Announce Type: new 
Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent embedding space with semantic information that is useful for downstream tasks. We hypothesize that the embedding space can be disentangled to separate the information on the content of complex scenes by decomposing the embedding into multiple concept-specific component vectors that lie in different subspaces. We propose a supervised dictionary learning approach to estimate a linear synthesis model consisting of sparse, non-negative combinations of groups of vectors in the dictionary (atoms), whose group-wise activity matches the multi-label information. Each concept-specific component is a non-negative combination of atoms associated to a label. The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence. Exploiting the text co-embeddings, we detail how semantically meaningful descriptions can be found based on text embeddings of words best approximated by a concept's group of atoms, and unsupervised dictionary learning can exploit zero-shot classification of training set images using the text embeddings of concept labels to provide instance-wise multi-labels. We show that the disentangled embeddings provided by our sparse linear concept subspaces (SLiCS) enable concept-filtered image retrieval (and conditional generation using image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed autoencoder embeddings from TiTok and the latent embedding from self-supervised DINOv2. Quantitative and qualitative results highlight the improved precision of the concept-filtered image retrieval for all embeddings.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models</title>
<link>https://arxiv.org/abs/2508.20345</link>
<guid>https://arxiv.org/abs/2508.20345</guid>
<content:encoded><![CDATA[
<div> Keywords: medical vision-language models, security concerns, MedFoundationHub, privacy-preserving inference, expert evaluation

Summary:<br /><br />Recent advances in medical vision-language models (VLMs) offer various clinical applications but also introduce security risks like PHI exposure and data leakage. To address these challenges, MedFoundationHub, a GUI toolkit, allows users to select and deploy different models easily, ensuring privacy-preserving inference. It requires only a local workstation with an NVIDIA A6000 GPU, making it secure and accessible. An evaluation using pathologists revealed limitations in current VLMs, such as off-target answers and inconsistent pathology terminology. <div>
arXiv:2508.20345v1 Announce Type: new 
Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction</title>
<link>https://arxiv.org/abs/2508.20376</link>
<guid>https://arxiv.org/abs/2508.20376</guid>
<content:encoded><![CDATA[
<div> Keywords: Bidirectional Interaction Scan, Multi-Scale Scan, Multi-task dense prediction, Computational efficiency, Scene modeling

Summary: 
The paper introduces a novel approach called Bidirectional Interaction Mamba (BIM) for multi-task dense prediction. BIM incorporates Bidirectional Interaction Scan (BI-Scan) and Multi-Scale Scan (MS-Scan) mechanisms to efficiently handle cross-task interactions and multi-granularity scene modeling. The BI-Scan constructs task-specific representations as bidirectional sequences using task-first and position-first scanning modes within a linear complexity architecture, preserving critical cross-task information. The MS-Scan mechanism enables multi-granularity scene modeling, meeting diverse task requirements and enhancing cross-task feature interactions. Experimental results on NYUD-V2 and PASCAL-Context benchmarks demonstrate the superior performance of BIM compared to existing methods in terms of computational efficiency and accuracy. <div>
arXiv:2508.20376v1 Announce Type: new 
Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Guided Visual Editing with Complex Multi-Modal Prompts</title>
<link>https://arxiv.org/abs/2508.20379</link>
<guid>https://arxiv.org/abs/2508.20379</guid>
<content:encoded><![CDATA[
<div> diffusion models, audio-guided editing, multi-modal encoder, zero-shot capabilities, complex editing tasks
<br />
Summary:
This study introduces an audio-guided visual editing framework that can handle complex tasks with multiple text and audio prompts without additional training. By leveraging a pre-trained multi-modal encoder with strong zero-shot capabilities, the framework integrates diverse audio into visual editing tasks. A novel approach is proposed to address complex scenarios with multiple and multi-modal editing prompts through separate noise branching and adaptive patch selection. Comprehensive experiments show that the framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fall short. <div>
arXiv:2508.20379v1 Announce Type: new 
Abstract: Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning</title>
<link>https://arxiv.org/abs/2508.20381</link>
<guid>https://arxiv.org/abs/2508.20381</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-label learning, Single Positive Multi-Label Learning (SPML), Generalized Pseudo-Label Robust Loss (GPR Loss), Dynamic Augmented Multi-focus Pseudo-labeling (DAMP), Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) 

Summary: 
The article introduces the Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) framework for multi-label learning, addressing challenges in Single Positive Multi-Label Learning (SPML) with partially annotated data. The framework incorporates the Generalized Pseudo-Label Robust Loss (GPR Loss) to effectively utilize diverse pseudo-labels and mitigate noise in training. Additionally, a Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique is introduced to enhance the learning process. Extensive experiments on benchmark datasets demonstrate that the AEVLP framework achieves state-of-the-art results in multi-label classification tasks. <div>
arXiv:2508.20381v1 Announce Type: new 
Abstract: Multi-label learning is a challenging computer vision task that requires assigning multiple categories to each image. However, fully annotating large-scale datasets is often impractical due to high costs and effort, motivating the study of learning from partially annotated data. In the extreme case of Single Positive Multi-Label Learning (SPML), each image is provided with only one positive label, while all other labels remain unannotated. Traditional SPML methods that treat missing labels as unknown or negative tend to yield inaccuracies and false negatives, and integrating various pseudo-labeling strategies can introduce additional noise. To address these challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a novel loss function that effectively learns from diverse pseudo-labels while mitigating noise. Complementing this, we introduce a simple yet effective Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate that our framework significantly advances multi-label classification, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection</title>
<link>https://arxiv.org/abs/2508.20392</link>
<guid>https://arxiv.org/abs/2508.20392</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, delay-spike approach, temporal-dependent Integrate-and-Fire neuron, visual detection tasks, ultra-low latency<br />
<br />
Summary:<br />
Spiking Neural Networks (SNNs) are efficient and fast on neuromorphic hardware and widely used in visual perception tasks. However, current ANN-SNN conversion methods struggle with visual detection tasks. To address this issue, a delay-spike approach is proposed to manage heterogeneous spiking patterns. Additionally, a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture is introduced, allowing neurons to adjust accumulation and firing based on temporal order. This approach enables more precise feature representation with lower time-steps, improving performance and achieving ultra-low latency in object detection and lane line detection tasks. Extensive evaluations show that the tdIF method outperforms current conversion approaches by achieving state-of-the-art performance within 5 time-steps. <div>
arXiv:2508.20392v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection</title>
<link>https://arxiv.org/abs/2508.20415</link>
<guid>https://arxiv.org/abs/2508.20415</guid>
<content:encoded><![CDATA[
<div> dynamic uncertainty propagation, multimodal collaborative reasoning network, salient object detection, small structures, edge regions<br />
<br />Summary: <br />The paper introduces a novel method called DUP-MCRNet for salient object detection, addressing issues of detail loss, edge blurring, and limited fusion of single-modal information in complex scenes. The network includes a dynamic uncertainty graph convolution module (DUGC) for uncertainty propagation and channel adaptive interaction, improving accuracy for small structures and edges. A multimodal collaborative fusion strategy (MCF) utilizes modality gating weights to fuse RGB, depth, and edge features, adjusting modality importance based on the scene. This enhances semantic complementarity and consistency between modalities, aiding salient region identification in challenging scenarios. Optimizations include multi-scale loss functions and uncertainty-guided supervision, improving performance at pixel and region levels. Extensive experiments demonstrate the superior performance of DUP-MCRNet over existing methods, particularly in edge clarity and robustness to complex backgrounds. <div>
arXiv:2508.20415v1 Announce Type: new 
Abstract: In view of the problems that existing salient object detection (SOD) methods are prone to losing details, blurring edges, and insufficient fusion of single-modal information in complex scenes, this paper proposes a dynamic uncertainty propagation and multimodal collaborative reasoning network (DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is designed to propagate uncertainty between layers through a sparse graph constructed based on spatial semantic distance, and combined with channel adaptive interaction, it effectively improves the detection accuracy of small structures and edge regions. Secondly, a multimodal collaborative fusion strategy (MCF) is proposed, which uses learnable modality gating weights to weightedly fuse the attention maps of RGB, depth, and edge features. It can dynamically adjust the importance of each modality according to different scenes, effectively suppress redundant or interfering information, and strengthen the semantic complementarity and consistency between cross-modalities, thereby improving the ability to identify salient regions under occlusion, weak texture or background interference. Finally, the detection performance at the pixel level and region level is optimized through multi-scale BCE and IoU loss, cross-scale consistency constraints, and uncertainty-guided supervision mechanisms. Extensive experiments show that DUP-MCRNet outperforms various SOD methods on most common benchmark datasets, especially in terms of edge clarity and robustness to complex backgrounds. Our code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection</title>
<link>https://arxiv.org/abs/2508.20447</link>
<guid>https://arxiv.org/abs/2508.20447</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-View Pedestrian Detection, Deep Learning, Multi-Scale Image Features, Feature Pyramid Network, GMVD Dataset

Summary:
Multi-Scale Multi-View Detection (MSMVD) is proposed to improve pedestrian detection in bird's eye view from multi-view images. Traditional deep learning methods struggle with detecting pedestrians of varying scales between views. MSMVD addresses this by generating multi-scale bird's eye view features using scale-specific image features from individual views. These multi-scale features enhance the detection of pedestrians with consistent scales in views and vastly different scales between views. By utilizing a feature pyramid network to combine information from different scales of multiple views, MSMVD achieves significant performance improvement over previous methods, surpassing the highest achieved MODA by 4.5 points on the GMVD dataset. This novel approach underscores the importance of leveraging multi-scale image features in multi-view pedestrian detection.

<br /><br />Summary: <div>
arXiv:2508.20447v1 Announce Type: new 
Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end trainable deep learning methods have progressed greatly. However, they often struggle to detect pedestrians with consistently small or large scales in views or with vastly different scales between views. This is because they do not exploit multi-scale image features to generate the BEV feature and detect pedestrians. To overcome this problem, we propose a novel MVPD method, called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV features by projecting multi-scale image features extracted from individual views into the BEV space, scale-by-scale. Each of these BEV features inherits the properties of its corresponding scale image features from multiple views. Therefore, these BEV features help the precise detection of pedestrians with consistently small or large scales in views. Then, MSMVD combines information at different scales of multiple views by processing the multi-scale BEV features using a feature pyramid network. This improves the detection of pedestrians with vastly different scales between views. Extensive experiments demonstrate that exploiting multi-scale image features via multi-scale BEV features greatly improves the detection performance, and MSMVD outperforms the previous highest MODA by $4.5$ points on the GMVD dataset.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection</title>
<link>https://arxiv.org/abs/2508.20449</link>
<guid>https://arxiv.org/abs/2508.20449</guid>
<content:encoded><![CDATA[
<div> real-time deepfake detection, SFMFNet, lightweight architecture, spatial-frequency hybrid aware module, efficient multi-level feature interaction, residual-enhanced blur pooling
<br />
Summary: 
The article introduces SFMFNet, a lightweight real-time deepfake detection architecture that effectively detects forged content in applications like video conferencing and social media. SFMFNet incorporates a spatial-frequency hybrid aware module that enhances sensitivity to subtle manipulations through a gated mechanism. It also features a token-selective cross attention mechanism for efficient multi-level feature interaction and a residual-enhanced blur pooling structure to retain key semantic cues during downsampling. Experimental results on benchmark datasets show that SFMFNet achieves a good balance between accuracy and efficiency, with strong generalization capabilities for real-time applications. This architecture addresses the challenge of heavy computational costs in deepfake detection, making it a practical and valuable solution for detecting forged content in real-time scenarios. 
<br /> <div>
arXiv:2508.20449v1 Announce Type: new 
Abstract: With the rapid advancement of real-time deepfake generation techniques, forged content is becoming increasingly realistic and widespread across applications like video conferencing and social media. Although state-of-the-art detectors achieve high accuracy on standard benchmarks, their heavy computational cost hinders real-time deployment in practical applications. To address this, we propose the Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture for real-time deepfake detection. We design a spatial-frequency hybrid aware module that jointly leverages spatial textures and frequency artifacts through a gated mechanism, enhancing sensitivity to subtle manipulations. A token-selective cross attention mechanism enables efficient multi-level feature interaction, while a residual-enhanced blur pooling structure helps retain key semantic cues during downsampling. Experiments on several benchmark datasets show that SFMFNet achieves a favorable balance between accuracy and efficiency, with strong generalization and practical value for real-time applications.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification</title>
<link>https://arxiv.org/abs/2508.20461</link>
<guid>https://arxiv.org/abs/2508.20461</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image classification, dual-model weight selection, self-knowledge distillation, lightweight models, computational efficiency 

Summary: 
This article presents a novel approach to medical image classification that addresses the challenges of deploying large-scale models in resource-constrained environments. The method combines dual-model weight selection and self-knowledge distillation to transfer knowledge efficiently from a pretrained model to lightweight models. By leveraging this strategy, the proposed approach achieves comparable performance to large-scale models while maintaining computational efficiency. The dual-model weight selection initializes two lightweight models with weights derived from the pretrained model, allowing for effective knowledge transfer. Self-knowledge distillation is then applied to refine the selected models without adding excessive computational cost. The method is evaluated on various medical image datasets, including chest X-ray images, lung CT scans, and brain MRI scans, demonstrating superior performance and robustness compared to existing methods. <div>
arXiv:2508.20461v1 Announce Type: new 
Abstract: We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2508.20466</link>
<guid>https://arxiv.org/abs/2508.20466</guid>
<content:encoded><![CDATA[
<div> Hierarchical feature propagation, lightweight modules, efficient context modeling, LiDAR point clouds, compression ratios <br />
Summary: 
This study proposes a novel framework for compressing LiDAR point clouds efficiently. Two lightweight modules, the Geometry Re-Densification Module and the Cross-scale Feature Propagation Module, work together to generate compact features for predictive coding. The Geometry Re-Densification Module re-densifies sparse geometry, extracts features at a denser scale, and re-sparsifies them for coding, avoiding costly computation on sparse details. The Cross-scale Feature Propagation Module leverages occupancy cues from multiple resolution levels to guide feature propagation, reducing redundancy and enriching features. This approach achieves state-of-the-art compression ratios and real-time performance, with encoding and decoding both running at 26 FPS for 12-bit quantization. The code for this method is available on GitHub at https://github.com/pengpeng-yu/FastPCC. <br /> <div>
arXiv:2508.20466v1 Announce Type: new 
Abstract: LiDAR point clouds are fundamental to various applications, yet high-precision scans incur substantial storage and transmission overhead. Existing methods typically convert unordered points into hierarchical octree or voxel structures for dense-to-sparse predictive coding. However, the extreme sparsity of geometric details hinders efficient context modeling, thereby limiting their compression performance and speed. To address this challenge, we propose to generate compact features for efficient predictive coding. Our framework comprises two lightweight modules. First, the Geometry Re-Densification Module re-densifies encoded sparse geometry, extracts features at denser scale, and then re-sparsifies the features for predictive coding. This module avoids costly computation on highly sparse details while maintaining a lightweight prediction head. Second, the Cross-scale Feature Propagation Module leverages occupancy cues from multiple resolution levels to guide hierarchical feature propagation. This design facilitates information sharing across scales, thereby reducing redundant feature extraction and providing enriched features for the Geometry Re-Densification Module. By integrating these two modules, our method yields a compact feature representation that provides efficient context modeling and accelerates the coding process. Experiments on the KITTI dataset demonstrate state-of-the-art compression ratios and real-time performance, achieving 26 FPS for both encoding and decoding at 12-bit quantization. Code is available at https://github.com/pengpeng-yu/FastPCC.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation</title>
<link>https://arxiv.org/abs/2508.20470</link>
<guid>https://arxiv.org/abs/2508.20470</guid>
<content:encoded><![CDATA[
<div> Keywords: Scaling laws, 3D asset generation, Video dataset, Generative model, Commonsense priors

Summary: 
This paper discusses the challenges of data scarcity in the 3D domain and proposes utilizing video data as a supervisory signal for 3D asset generation. By leveraging videos that capture multiple views of objects or scenes and contain rich semantic information, a generative model called Droplet3D is trained on the new Droplet3D-4M dataset. The model supports both image and dense text input, producing spatially consistent and semantically plausible 3D content. The approach demonstrates improved performance in comparison to existing 3D solutions, showcasing potential for scene-level applications. The open-sourced resources, including the dataset, code, technical framework, and model weights, are made available for further research and development. This study highlights the significance of commonsense priors from videos in enhancing 3D creation processes.<br /><br />Summary: <div>
arXiv:2508.20470v1 Announce Type: new 
Abstract: Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation</title>
<link>https://arxiv.org/abs/2508.20471</link>
<guid>https://arxiv.org/abs/2508.20471</guid>
<content:encoded><![CDATA[
<div> Framework, Object Editing, Autonomous Driving, Pose Control, Visual Fidelity

Summary:
G^2Editor is a new framework for photorealistic and precise object editing in driving videos. It uses a 3D Gaussian representation of the edited object to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is utilized to reconstruct occluded areas of non-target objects. Hierarchical fine-grained features guide appearance details of the edited object. The method supports object repositioning, insertion, and deletion within a single framework. G^2Editor outperforms existing methods in pose controllability and visual quality when tested on the Waymo Open Dataset. The framework is beneficial for training and validating autonomous driving systems, offering a cost-effective and safe alternative to collecting corner cases from the real world. <div>
arXiv:2508.20471v1 Announce Type: new 
Abstract: Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization</title>
<link>https://arxiv.org/abs/2508.20475</link>
<guid>https://arxiv.org/abs/2508.20475</guid>
<content:encoded><![CDATA[
<div> fetal brain segmentation, corpus callosum dysgenesis, deep learning models, synthetic data generation, biomarkers<br />
<br />
Summary: <br />
Accurate fetal brain segmentation is essential for assessing neurodevelopment, especially in rare conditions like corpus callosum dysgenesis (CCD). However, the scarcity of annotated data limits the generalization of deep learning models. This study proposes a pathology-informed domain randomization strategy that incorporates prior knowledge of CCD into a synthetic data generation pipeline. By simulating various brain alterations from healthy data, the approach enables robust segmentation without the need for pathological annotations. The method was validated on a cohort including healthy fetuses and those with CCD and other brain pathologies, showing significant improvements in CCD cases while maintaining performance on other cases. The predicted segmentations allowed for the derivation of clinically relevant biomarkers, such as corpus callosum length (LCC) and volume, which were useful in distinguishing CCD subtypes. The pathology-informed augmentation reduced estimation errors in healthy and CCD cases, resulting in improved topological consistency for shape-based analyses. Overall, this approach effectively addresses data scarcity and enhances the analysis of rare but clinically significant malformations. <br /> <div>
arXiv:2508.20475v1 Announce Type: new 
Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and assessing neurodevelopment, especially in conditions such as corpus callosum dysgenesis (CCD), which can induce drastic anatomical changes. However, the rarity of CCD severely limits annotated data, hindering the generalization of deep learning models. To address this, we propose a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline. By simulating diverse brain alterations from healthy data alone, our approach enables robust segmentation without requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with CCD, and 47 with other brain pathologies, achieving substantial improvements on CCD cases while maintaining performance on both healthy fetuses and those with other pathologies. From the predicted segmentations, we derive clinically relevant biomarkers, such as corpus callosum length (LCC) and volume, and show their utility in distinguishing CCD subtypes. Our pathology-informed augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these quantitative gains, our approach yields segmentations with improved topological consistency relative to available ground truth, enabling more reliable shape-based analyses. Overall, this work demonstrates that incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding</title>
<link>https://arxiv.org/abs/2508.20476</link>
<guid>https://arxiv.org/abs/2508.20476</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio, Sign Language Translation, Visual Speech Recognition, Modality-agnostic architecture, Lip movements<br />
Summary:<br />
- The paper introduces a unified framework for integrating sign language, lip movements, and audio for text generation.<br />
- The framework aims to effectively process diverse inputs and explore the synergy among different modalities.<br />
- Performance of the framework is on par with or better than specialized models in Sign Language Translation, Visual Speech Recognition, Automatic Speech Recognition, and Audio-Visual Speech Recognition.<br />
- Modeling lip movements as a separate modality significantly enhances Sign Language Translation performance.<br />
- The study highlights the importance of non-manual cues like lip movements in sign language comprehension. <br /> 

Summary: <div>
arXiv:2508.20476v1 Announce Type: new 
Abstract: Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.20478</link>
<guid>https://arxiv.org/abs/2508.20478</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-form video understanding, multi-turn reasoning framework, iterative reasoning process, bi-level reward system, end-to-end training

Summary: 
Video-MTR is a new multi-turn reasoning framework for long-form video understanding that allows for iterative key video segment selection and question comprehension. Unlike traditional methods, Video-MTR performs reasoning in multiple turns, progressively selecting video segments based on evolving understanding and the current question. A novel gated bi-level reward system is introduced to optimize both video segment selection and question comprehension during the reasoning process. This system eliminates the need for external visual-language models and enables end-to-end training. Extensive experiments on benchmark datasets show that Video-MTR outperforms existing methods in accuracy and efficiency, advancing the state-of-the-art in long video understanding. <div>
arXiv:2508.20478v1 Announce Type: new 
Abstract: Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts</title>
<link>https://arxiv.org/abs/2508.20488</link>
<guid>https://arxiv.org/abs/2508.20488</guid>
<content:encoded><![CDATA[
<div> Test-Time Adaptation, Monocular 3D Object Detection, Uncertainty Optimization, Autonomous Driving, Convex Optimization

Summary:
Dual Uncertainty Optimization (DUO) is proposed as a Test-Time Adaptation framework to improve Monocular 3D Object Detection. It tackles semantic and geometric uncertainty simultaneously by using a convex optimization approach with focal loss and introducing unsupervised uncertainty weighting. A semantic-aware normal field constraint helps in enhancing spatial perception and semantic classification. DUO's dual-branch mechanism creates a loop where spatial understanding improves semantic predictions and vice versa. Extensive experiments show that DUO outperforms existing methods across different datasets and domain shift types. In autonomous driving applications, DUO can significantly enhance the robustness and accuracy of 3D object detection. <br /><br />Summary: <div>
arXiv:2508.20488v1 Announce Type: new 
Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel unsupervised version, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information</title>
<link>https://arxiv.org/abs/2508.20491</link>
<guid>https://arxiv.org/abs/2508.20491</guid>
<content:encoded><![CDATA[
<div> swing posture, ball trajectory, deep learning, CaddieSet, golf swing analysis

Summary: 
The article presents a new dataset called CaddieSet, which gathers joint information and ball details from a single golf shot to provide insights for improving swing precision. CaddieSet utilizes computer vision techniques to extract joint information from swing videos and defines 15 key metrics related to golf swing to analyze outcomes. Experimental results show the dataset's effectiveness in predicting ball trajectories through interpretable models, highlighting the consistency of swing feedback with expert domain knowledge. This dataset offers a valuable tool for enhancing golf swing analysis in both academic research and the sports industry. <div>
arXiv:2508.20491v1 Announce Type: new 
Abstract: Recent advances in deep learning have led to more studies to enhance golfers' shot precision. However, these existing studies have not quantitatively established the relationship between swing posture and ball trajectory, limiting their ability to provide golfers with the necessary insights for swing improvement. In this paper, we propose a new dataset called CaddieSet, which includes joint information and various ball information from a single shot. CaddieSet extracts joint information from a single swing video by segmenting it into eight swing phases using a computer vision-based approach. Furthermore, based on expert golf domain knowledge, we define 15 key metrics that influence a golf swing, enabling the interpretation of swing outcomes through swing-related features. Through experiments, we demonstrated the feasibility of CaddieSet for predicting ball trajectories using various benchmarks. In particular, we focus on interpretable models among several benchmarks and verify that swing feedback using our joint features is quantitatively consistent with established domain knowledge. This work is expected to offer new insight into golf swing analysis for both academia and the sports industry.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.20492</link>
<guid>https://arxiv.org/abs/2508.20492</guid>
<content:encoded><![CDATA[
<div> Keywords: Surface anomaly detection, 3D point cloud, Importance-Aware Ensemble Network, Importance-Aware Fusion, MVTec 3D-AD

Summary: 
Surface anomaly detection in industrial manufacturing is crucial for ensuring product quality. While 2D image-based methods have been successful, 3D point cloud-based detection has not been extensively explored due to the lack of powerful pretrained models. To address this gap, the paper proposes the Importance-Aware Ensemble Network (IAENet), which combines 2D and 3D expert models using an Importance-Aware Fusion (IAF) module. This module dynamically assesses and reweights the anomaly scores from different sources to improve overall accuracy. Additionally, novel loss functions guide the optimization of IAF to preserve the unique strengths of each source expert. Experimental results on the MVTec 3D-AD dataset show that IAENet achieves a new state-of-the-art performance with a significantly lower false positive rate, demonstrating its practical value for industrial deployment.

<br /><br />Summary: Surface anomaly detection is vital in manufacturing, with 2D methods successful but 3D point cloud detection lacking powerful pretrained models. The Importance-Aware Ensemble Network (IAENet) combines 2D and 3D expert models using an Importance-Aware Fusion (IAF) module, dynamically assessing and reweighting anomaly scores. Novel loss functions guide IAF optimization to preserve source expert strengths. Experimental results on MVTec 3D-AD show IAENet outperforms existing methods with a lower false positive rate, highlighting its importance for industrial use. <div>
arXiv:2508.20492v1 Announce Type: new 
Abstract: Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce an novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet achieves a new state-of-the-art with a markedly lower false positive rate, underscoring its practical value for industrial deployment.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent</title>
<link>https://arxiv.org/abs/2508.20505</link>
<guid>https://arxiv.org/abs/2508.20505</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, semantic image editing, DescriptiveEdit, Cross-Attentive UNet, editing accuracy

Summary:<br />
- The article addresses challenges in semantic image editing by introducing a new framework called DescriptiveEdit.
- DescriptiveEdit reframes instruction-based image editing as reference-image-based text-to-image generation, utilizing a Cross-Attentive UNet model.
- The framework combines reference images and prompts to enhance the edit-image generation process, improving accuracy and consistency.
- DescriptiveEdit overcomes limitations in dataset quality and scale compared to existing inversion-based algorithms and instruction-based models.
- The approach integrates seamlessly with other extensions like ControlNet and IP-Adapter, making it more scalable and versatile.

Summary: <div>
arXiv:2508.20505v1 Announce Type: new 
Abstract: Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample</title>
<link>https://arxiv.org/abs/2508.20516</link>
<guid>https://arxiv.org/abs/2508.20516</guid>
<content:encoded><![CDATA[
<div> Dual-path feature consistency, confidence-aware sample learning, continual test-time adaptation, pseudo-labels, error accumulation
<br />
Summary: 
DCFS is introduced as a novel framework for Continual Test-time Adaptation (CTTA), addressing challenges in model adaptation to target domain data without access to source data. By leveraging dual-path feature consistency, DCFS disentangles target data features into semantic-related and domain-related features through dual classifiers, ensuring comprehensive feature capture. Confidence-aware sample learning further enhances model performance by calculating per-sample confidence scores, enabling loss-weighted self-supervised learning to reduce pseudo-label noise and mitigate error accumulation. Extensive experimentation across CIFAR10-C, CIFAR100-C, and ImageNet-C datasets validates DCFS efficacy in CTTA scenarios, demonstrating consistent performance improvements in adapting pre-trained models to new target domains. 
<br /> <div>
arXiv:2508.20516v1 Announce Type: new 
Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model to a stream of target domain data without accessing source data. Without access to source domain data, the model focuses solely on the feature characteristics of the target data. Relying exclusively on these features can lead to confusion and introduce learning biases. Currently, many existing methods generate pseudo-labels via model predictions. However, the quality of pseudo-labels cannot be guaranteed and the problem of error accumulation must be solved. To address these challenges, we propose DCFS, a novel CTTA framework that introduces dual-path feature consistency and confidence-aware sample learning. This framework disentangles the whole feature representation of the target data into semantic-related feature and domain-related feature using dual classifiers to learn distinct feature representations. By maintaining consistency between the sub-features and the whole feature, the model can comprehensively capture data features from multiple perspectives. Additionally, to ensure that the whole feature information of the target domain samples is not overlooked, we set a adaptive threshold and calculate a confidence score for each sample to carry out loss weighted self-supervised learning, effectively reducing the noise of pseudo-labels and alleviating the problem of error accumulation. The efficacy of our proposed method is validated through extensive experimentation across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C, demonstrating consistent performance in continual test-time adaptation scenarios.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adam SLAM - the last mile of camera calibration with 3DGS</title>
<link>https://arxiv.org/abs/2508.20526</link>
<guid>https://arxiv.org/abs/2508.20526</guid>
<content:encoded><![CDATA[
<div> camera calibration, novel view synthesis, 3DGS model, backpropagation, PSNR improvement

Summary:
The paper discusses the importance of camera calibration for novel view synthesis and proposes a method using a 3DGS model to fine-tune calibration by backpropagation of novel view color loss. The study shows that the new calibration alone leads to an average improvement of 0.4 dB PSNR on a reference dataset. While fine-tuning calibration may be time-consuming, it is crucial for scenes like Mip-NeRF 360 where novel view quality is paramount. The quality of calibration directly impacts reconstruction quality, with even small errors having a significant impact. By using a 3DGS model and backpropagation, researchers can ensure more precise calibration and subsequently enhance the quality of novel view synthesis. <div>
arXiv:2508.20526v1 Announce Type: new 
Abstract: The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation</title>
<link>https://arxiv.org/abs/2508.20528</link>
<guid>https://arxiv.org/abs/2508.20528</guid>
<content:encoded><![CDATA[
<div> Active learning, multi-modal medical data, deep neural networks, domain adaptation, gross tumor volume segmentation.

Summary:
This article introduces an active and sequential domain adaptation framework for dynamic multi-modal sample selection in accurate gross tumor volume segmentation on medical data. The proposed method aims to reduce annotation costs by selecting the most informative samples for labeling and training. A novel query strategy prioritizes samples based on their informativeness and representativeness. Empirical validation on various segmentation tasks shows superior performance compared to existing ADA methods. The approach addresses the challenge of negative transfer and limited access to source domain data in medical image segmentation. The code for the method is available on the GitHub repository. <div>
arXiv:2508.20528v1 Announce Type: new 
Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is critical for radiotherapy planning in nasopharyngeal carcinoma and glioblastoma. Recent advances in deep neural networks have brought promising results in medical image segmentation, leading to an increasing demand for labeled data. Since labeling medical images is time-consuming and labor-intensive, active learning has emerged as a solution to reduce annotation costs by selecting the most informative samples to label and adapting high-performance models with as few labeled samples as possible. Previous active domain adaptation (ADA) methods seek to minimize sample redundancy by selecting samples that are farthest from the source domain. However, such one-off selection can easily cause negative transfer, and access to source medical data is often limited. Moreover, the query strategy for multi-modal medical data remains unexplored. In this work, we propose an active and sequential domain adaptation framework for dynamic multi-modal sample selection in ADA. We derive a query strategy to prioritize labeling and training on the most valuable samples based on their informativeness and representativeness. Empirical validation on diverse gross tumor volume segmentation tasks demonstrates that our method achieves favorable segmentation performance, significantly outperforming state-of-the-art ADA methods. Code is available at the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.20530</link>
<guid>https://arxiv.org/abs/2508.20530</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, 3D object detection, RGB images, data-level fusion, nuScenes dataset <br />
Summary: 
This paper introduces a novel data-level fusion framework for unsupervised 3D object detection, combining LiDAR data and RGB images at an early stage. By leveraging vision foundation models for instance segmentation and depth estimation on images, a bi-directional fusion method is employed to enhance the quality of pseudo-box generation. A local and global filtering method is introduced to address noise from depth and segmentation estimations, improving accuracy. The proposed dynamic self-evolution strategy iteratively refines pseudo-boxes under a dense representation to enhance localization accuracy. Experimental results on the nuScenes dataset show that the detector trained using this method achieves a significant improvement, outperforming previous state-of-the-art methods with a 28.4% mAP on the nuScenes validation benchmark. <br /><br />Summary: <div>
arXiv:2508.20530v1 Announce Type: new 
Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation benchmark.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset</title>
<link>https://arxiv.org/abs/2508.20534</link>
<guid>https://arxiv.org/abs/2508.20534</guid>
<content:encoded><![CDATA[
<div> Keywords: Body Mass Index, machine learning, deep learning, image filtering, mobile deployment

Summary:
Estimating Body Mass Index (BMI) from camera images using machine learning models is crucial for rapid weight assessment in scenarios where traditional methods are not feasible. This study presents a deep learning-based BMI estimation method trained on a large dataset called WayBED, consisting of over 84,000 smartphone images from thousands of individuals. By implementing an automatic filtering process based on posture clustering and person detection, the dataset was curated to include 71,322 high-quality images for training. The resulting model achieved a Mean Absolute Percentage Error (MAPE) of 7.9% on the test set, the lowest reported value in the literature. Furthermore, the model demonstrated robust generalization on an unseen dataset and outperformed existing approaches. The complete pipeline, including image filtering and BMI estimation, was successfully deployed on Android devices using the CLAID framework. The code for model training, filtering, and mobile deployment is released as open-source contributions.

<br /><br />Summary: <div>
arXiv:2508.20534v1 Announce Type: new 
Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation Techniques for Natural and Medical Image Classification</title>
<link>https://arxiv.org/abs/2508.20537</link>
<guid>https://arxiv.org/abs/2508.20537</guid>
<content:encoded><![CDATA[
<div> Simulation studies, image classification, domain adaptation, medical datasets, Deep Subdomain Adaptation Network<br />
<br />
Summary:<br />
- The study investigates the benefits of domain adaptation (DA) techniques for image classification in both natural and medical datasets.
- 557 simulation studies were conducted using seven DA techniques on five natural and eight medical datasets, covering various scenarios like out-of-distribution and limited training samples.
- The Deep Subdomain Adaptation Network (DSAN) algorithm exhibited outstanding performance, achieving a feasible classification accuracy of 91.2% in the COVID-19 dataset using Resnet50 and showing significant accuracy improvement in dynamic data stream DA scenarios.
- DSAN also demonstrated a remarkable level of explainability when evaluated on COVID-19 and skin cancer datasets.
- The results contribute to the understanding of DA techniques and provide valuable insights into effectively adapting models to medical data. 

<br /><br /> <div>
arXiv:2508.20537v1 Announce Type: new 
Abstract: Domain adaptation (DA) techniques have the potential in machine learning to alleviate distribution differences between training and test sets by leveraging information from source domains. In image classification, most advances in DA have been made using natural images rather than medical data, which are harder to work with. Moreover, even for natural images, the use of mainstream datasets can lead to performance bias. {With the aim of better understanding the benefits of DA for both natural and medical images, this study performs 557 simulation studies using seven widely-used DA techniques for image classification in five natural and eight medical datasets that cover various scenarios, such as out-of-distribution, dynamic data streams, and limited training samples.} Our experiments yield detailed results and insightful observations highlighting the performance and medical applicability of these techniques. Notably, our results have shown the outstanding performance of the Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved feasible classification accuracy (91.2\%) in the COVID-19 dataset using Resnet50 and showed an important accuracy improvement in the dynamic data stream DA scenario (+6.7\%) compared to the baseline. Our results also demonstrate that DSAN exhibits remarkable level of explainability when evaluated on COVID-19 and skin cancer datasets. These results contribute to the understanding of DA techniques and offer valuable insight into the effective adaptation of models to medical data.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning through Auxiliary Branch for Video Object Detection</title>
<link>https://arxiv.org/abs/2508.20551</link>
<guid>https://arxiv.org/abs/2508.20551</guid>
<content:encoded><![CDATA[
<div> challenging task, image deterioration, video object detection, Contrastive Learning through Auxiliary Branch (CLAB), state-of-the-art performance <br />
<br />
Summary:
Video object detection is a complex task due to image degradation challenges like motion blur and occlusion. Prior methods have used feature aggregation and post-processing techniques to improve detection performance but have increased computational demands. To address this, a Contrastive Learning through Auxiliary Branch (CLAB) method is proposed, which enhances feature representation without adding to computational load during inference. By implementing a contrastive auxiliary branch with a dynamic loss weighting strategy, CLAB achieves state-of-the-art performance on the ImageNet VID dataset with ResNet-101 and ResNeXt-101 models. The method emphasizes auxiliary feature learning early in training and gradually shifts focus towards the detection task as training progresses. These results demonstrate consistent performance gains without the need for additional post-processing methods. <div>
arXiv:2508.20551v1 Announce Type: new 
Abstract: Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detector's backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Mechanistic Defenses Against Typographic Attacks in CLIP</title>
<link>https://arxiv.org/abs/2508.20570</link>
<guid>https://arxiv.org/abs/2508.20570</guid>
<content:encoded><![CDATA[
<div> Keywords: Typographic attacks, CLIP vision encoders, attention heads, defense method, dyslexic CLIP models 

Summary:
Typographic attacks on multi-modal systems involve injecting text into images to manipulate models. This study examines how the CLIP vision encoders respond to such attacks, identifying specialized attention heads that extract and transmit typographic information to the cls token. A defense method is introduced to protect CLIP models by selectively disabling a typographic circuit within attention heads. This approach significantly improves performance on a typographic variant of ImageNet-100 without requiring additional training, while maintaining high accuracy on standard ImageNet-100. The proposed training-free defense method is shown to be competitive with existing typographic defense techniques that involve fine-tuning. The release of dyslexic CLIP models further enhances robustness against typographic attacks, offering a viable solution for safety-critical applications where text manipulation poses a significant risk. 

<br /><br />Summary: <div>
arXiv:2508.20570v1 Announce Type: new 
Abstract: Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition</title>
<link>https://arxiv.org/abs/2508.20579</link>
<guid>https://arxiv.org/abs/2508.20579</guid>
<content:encoded><![CDATA[
<div> Facial expression recognition, Graph Neural Networks, GLaRE, emotion recognition, facial landmarks<br />
Summary:<br />
Facial expression recognition (FER) is a vital task in computer vision, but traditional systems face challenges like occlusion and expression variability. Graph Neural Networks (GNNs) offer a solution by modeling relationships between facial landmarks for structured learning. GLaRE is a new Graph-based Landmark Region Embedding network for emotion recognition, using 3D facial alignment and hierarchical coarsening for spatial structure preservation. It outperforms existing baselines on AffectNet and FERG datasets, achieving 64.89% and 94.24% accuracy respectively. Ablation studies show that region-level embeddings from quotient graphs improve prediction performance. <div>
arXiv:2508.20579v1 Announce Type: new 
Abstract: Facial expression recognition (FER) is a crucial task in computer vision with wide range of applications including human computer interaction, surveillance, and assistive technologies. However, challenges such as occlusion, expression variability, and lack of interpretability hinder the performance of traditional FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by modeling relational dependencies between facial landmarks, enabling structured and interpretable learning. In this paper, we propose GLaRE, a novel Graph-based Landmark Region Embedding network for emotion recognition. Facial landmarks are extracted using 3D facial alignment, and a quotient graph is constructed via hierarchical coarsening to preserve spatial structure while reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet and 94.24 percentage on FERG, outperforming several existing baselines. Additionally, ablation studies have demonstrated that region-level embeddings from quotient graphs have contributed to improved prediction performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models</title>
<link>https://arxiv.org/abs/2508.20586</link>
<guid>https://arxiv.org/abs/2508.20586</guid>
<content:encoded><![CDATA[
<div> cacheable diffusion architecture, FastFit, multi-reference virtual try-on, DressCode-MR dataset, Semi-Attention mechanism.

Summary:
FastFit is a novel framework for virtual try-on technology that overcomes challenges faced by current methods. It utilizes a cacheable diffusion architecture, Semi-Attention mechanism, and class embeddings for reference items, allowing for efficient computation and re-usage of reference features. This leads to a 3.5x speedup compared to existing methods while maintaining high fidelity in outfit composition. The framework is supported by the DressCode-MR dataset, which contains a large number of high-quality paired images for research purposes. FastFit outperforms state-of-the-art methods in terms of fidelity metrics on various datasets (VITON-HD, DressCode, DressCode-MR) and offers significant efficiency in inference speed. <div>
arXiv:2508.20586v1 Announce Type: new 
Abstract: Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching</title>
<link>https://arxiv.org/abs/2508.20594</link>
<guid>https://arxiv.org/abs/2508.20594</guid>
<content:encoded><![CDATA[
<div> Keywords: thermal camera, neuromorphic vision camera, autonomous driving, event camera, traffic signage<br />
<br />
Summary: <br />
This paper introduces a novel approach called UTA-Sign, which combines thermal and event camera technologies to enhance the perception of traffic signage in low-light environments. The thermal camera excels in low-light conditions but struggles with capturing signage, while event cameras detect changes in light intensity effectively. The proposed method utilizes a dual-boosting mechanism to fuse thermal frames and event signals, providing consistent signage representation over time. By aligning thermal frames for motion cues and incorporating subtle signage content from event signals, the method improves the understanding of the environment and enhances signage detection accuracy. Validated on real-world datasets, UTA-Sign demonstrates superior quality in sketching traffic signage and boosts detection accuracy at the perceptual level. <div>
arXiv:2508.20594v1 Announce Type: new 
Abstract: The thermal camera excels at perceiving outdoor environments under low-light conditions, making it ideal for applications such as nighttime autonomous driving and unmanned navigation. However, thermal cameras encounter challenges when capturing signage from objects made of similar materials, which can pose safety risks for accurately understanding semantics in autonomous driving systems. In contrast, the neuromorphic vision camera, also known as an event camera, detects changes in light intensity asynchronously and has proven effective in high-speed, low-light traffic environments. Recognizing the complementary characteristics of these two modalities, this paper proposes UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage in low-illumination environments, targeting elements such as license plates and roadblock indicators. To address the signage blind spots of thermal imaging and the non-uniform sampling of event cameras, we developed a dual-boosting mechanism that fuses thermal frames and event signals for consistent signage representation over time. The proposed method utilizes thermal frames to provide accurate motion cues as temporal references for aligning the uneven event signals. At the same time, event signals contribute subtle signage content to the raw thermal frames, enhancing the overall understanding of the environment. The proposed method is validated on datasets collected from real-world scenarios, demonstrating superior quality in traffic signage sketching and improved detection accuracy at the perceptual level.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations</title>
<link>https://arxiv.org/abs/2508.20595</link>
<guid>https://arxiv.org/abs/2508.20595</guid>
<content:encoded><![CDATA[
<div> Generative Adversarial Networks, Deepfake technology, Active defense method, Low-frequency perturbations, Face swapping manipulation<br />
Summary:<br />
The article proposes an active defense method utilizing low-frequency perceptual perturbations to disrupt deepfake manipulation, focusing on prevention rather than post-event analysis. By targeting the generative process of deepfake techniques through a combination of frequency and spatial domain features, the method reduces the effectiveness and naturalness of face swapping. The architecture includes an encoder, perturbation generator, and decoder, leveraging discrete wavelet transform to produce disruptions while maintaining visual plausibility. Experimental results on CelebA-HQ and LFW datasets demonstrate a significant decrease in face-swapping effectiveness, improved defense success rates, and preservation of visual quality. <div>
arXiv:2508.20595v1 Announce Type: new 
Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion</title>
<link>https://arxiv.org/abs/2508.20604</link>
<guid>https://arxiv.org/abs/2508.20604</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-motion generation, diversity, uncertainty, latent space, transformer-based methods

Summary:
Diverse-T2M is a novel text-to-motion generation method that aims to improve diversity in generated human motions while maintaining text consistency. The approach introduces uncertainty into the generation process, using noise signals to facilitate explicit modeling of uncertainty in transformer-based methods. By constructing a latent space for text representation and incorporating a latent space sampler for stochastic sampling, Diverse-T2M enhances the diversity and uncertainty of generated motions. Results on benchmark datasets show that the method significantly improves diversity while retaining state-of-the-art text consistency. <div>
arXiv:2508.20604v1 Announce Type: new 
Abstract: Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction</title>
<link>https://arxiv.org/abs/2508.20605</link>
<guid>https://arxiv.org/abs/2508.20605</guid>
<content:encoded><![CDATA[
<div> calibration, 3D Intravascular Ultrasound, liver surgery, intraoperative guidance, volume reconstruction
<br />
Summary: 
<br />
In liver surgery, interpreting intraoperative ultrasound images is challenging due to limited view and complex anatomy. A solution using 3D IntraVascular Ultrasound (IVUS) for organ reconstruction can bridge preoperative and intraoperative data, aiding surgical guidance. The proposed optimization-based calibration method, using a 3D-printed phantom, ensures precise alignment of IVUS data with preoperative CT images. Validation on swine liver images resulted in calibration errors of 0.88-1.80 mm and registration errors of 3.40-5.71 mm. This method offers accurate calibration and volume reconstruction, facilitating the registration of intraoperative ultrasound images with preoperative CT scans in liver surgery, thereby improving intraoperative navigation and guidance. 
<br /> <div>
arXiv:2508.20605v1 Announce Type: new 
Abstract: Intraoperative ultrasound images are inherently challenging to interpret in liver surgery due to the limited field of view and complex anatomical structures. Bridging the gap between preoperative and intraoperative data is crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS) offers a potential solution by enabling the reconstruction of the entire organ, which facilitates registration between preoperative computed tomography (CT) scans and intraoperative IVUS images. In this work, we propose an optimization-based calibration method using a 3D-printed phantom for accurate 3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise alignment of tracked IVUS data with preoperative CT images, improving intraoperative navigation. We validated our method using in vivo swine liver images, achieving a calibration error from 0.88 to 1.80 mm and a registration error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT scan. Our method provides a reliable and accurate means of calibration and volume reconstruction. It can be used to register intraoperative ultrasound images with preoperative CT images in the context of liver surgery, and enhance intraoperative guidance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics Informed Generative Models for Magnetic Field Images</title>
<link>https://arxiv.org/abs/2508.20612</link>
<guid>https://arxiv.org/abs/2508.20612</guid>
<content:encoded><![CDATA[
<div> Keywords: semiconductor manufacturing, defect detection, Magnetic Field Imaging, machine learning, generative models

Summary:
Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) is proposed to generate synthetic MFI samples by integrating specific physical information for defect localization in semiconductor manufacturing. The aim is to address the limited availability of MFI datasets for training machine learning models efficiently. Power shorts, the most common defect types, are targeted for generating MFI images. The model is compared to state-of-the-art generative models like variational autoencoder and diffusion methods. Domain expert evaluation and various image generation metrics validate the effectiveness of the generated samples. This approach shows promising results in optimizing the defect localization process to ensure product quality and yield in semiconductor manufacturing. 

<br /><br />Summary: <div>
arXiv:2508.20612v1 Announce Type: new 
Abstract: In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization</title>
<link>https://arxiv.org/abs/2508.20613</link>
<guid>https://arxiv.org/abs/2508.20613</guid>
<content:encoded><![CDATA[
<div> Privacy Risks, Split Inference, Deep Neural Networks, Data Reconstruction Attacks, GAN-based DRA framework<br />
Summary:<br />
The paper discusses Split Inference (SI) as a collaborative paradigm for Deep Neural Networks (DNNs) to reduce latency and protect user privacy. However, Data Reconstruction Attacks (DRAs) pose privacy risks by exploiting intermediate features in SI. Existing DRAs are limited in effectiveness and generalizability. The proposed GAN-based DRA framework with Progressive Feature Optimization (PFO) enhances reconstructed image quality by refining intermediate representations. An L1-ball constraint is introduced for image realism. Extensive experiments show the method outperforms prior attacks, especially in high-resolution, out-of-distribution, and complex DNN scenarios.<br /> <div>
arXiv:2508.20613v1 Announce Type: new 
Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoCAST: Emotional Talking Portrait via Emotive Text Description</title>
<link>https://arxiv.org/abs/2508.20615</link>
<guid>https://arxiv.org/abs/2508.20615</guid>
<content:encoded><![CDATA[
<div> Diffusion-based framework, emotional talking head synthesis, text-driven, emotive module, emotive audio attention module<br />
<br />
Summary: <br />
The article introduces EmoCAST, a diffusion-based framework for precise text-driven emotional talking head synthesis. It addresses limitations in control flexibility, motion naturalness, and expression quality by integrating emotional prompts through a text-guided decoupled emotive module for appearance modeling. An emotive audio attention module captures the relationship between audio and emotion, generating emotion-aware features for facial motion synthesis. The framework is optimized using an emotional talking head dataset with emotive text descriptions and employs emotion-aware sampling and progressive functional training strategies for improved performance. EmoCAST achieves state-of-the-art results in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. <div>
arXiv:2508.20615v1 Announce Type: new 
Abstract: Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification</title>
<link>https://arxiv.org/abs/2508.20621</link>
<guid>https://arxiv.org/abs/2508.20621</guid>
<content:encoded><![CDATA[
<div> Keywords: Breast cancer, Magnetic Resonance Imaging, AI-based solutions, Deep learning, ODELIA consortium

Summary:
Breast cancer is a significant cause of cancer-related deaths in women, emphasizing the importance of early detection. Magnetic Resonance Imaging (MRI) is a highly sensitive tool for detecting breast cancer, particularly in high-risk individuals or those with dense breast tissue. The ODELIA consortium conducted a multi-center challenge to promote AI-based solutions for breast cancer diagnosis and classification. Their dataset, comprising 511 studies from six European centers, was used to develop a SwinUNETR-based deep learning framework incorporating breast region masking, extensive data augmentation, and ensemble learning. This framework, which achieved second place in the challenge, demonstrates promising potential for aiding clinical breast MRI interpretation. The codebase for this framework is publicly available at https://github.com/smriti-joshi/bcnaim-odelia-challenge.git. <br /><br />Summary: Breast cancer poses a significant health risk to women, necessitating early detection through tools like MRI. The ODELIA consortium's challenge promoted AI solutions for breast cancer diagnosis, resulting in a deep learning framework that shows promise for improving clinical interpretation of breast MRI studies. <div>
arXiv:2508.20621v1 Announce Type: new 
Abstract: Breast cancer is one of the leading causes of cancer-related mortality in women, and early detection is essential for improving outcomes. Magnetic resonance imaging (MRI) is a highly sensitive tool for breast cancer detection, particularly in women at high risk or with dense breast tissue, where mammography is less effective. The ODELIA consortium organized a multi-center challenge to foster AI-based solutions for breast cancer diagnosis and classification. The dataset included 511 studies from six European centers, acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study was labeled for the left and right breast as no lesion, benign lesion, or malignant lesion. We developed a SwinUNETR-based deep learning framework that incorporates breast region masking, extensive data augmentation, and ensemble learning to improve robustness and generalizability. Our method achieved second place on the challenge leaderboard, highlighting its potential to support clinical breast MRI interpretation. We publicly share our codebase at https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</title>
<link>https://arxiv.org/abs/2508.20623</link>
<guid>https://arxiv.org/abs/2508.20623</guid>
<content:encoded><![CDATA[
arXiv:2508.20623v1 Announce Type: new 
Abstract: Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtFace: Towards Historical Portrait Face Identification via Model Adaptation</title>
<link>https://arxiv.org/abs/2508.20626</link>
<guid>https://arxiv.org/abs/2508.20626</guid>
<content:encoded><![CDATA[
arXiv:2508.20626v1 Announce Type: new 
Abstract: Identifying sitters in historical paintings is a key task for art historians, offering insight into their lives and how they chose to be seen. However, the process is often subjective and limited by the lack of data and stylistic variations. Automated facial recognition is capable of handling challenging conditions and can assist, but while traditional facial recognition models perform well on photographs, they struggle with paintings due to domain shift and high intra-class variation. Artistic factors such as style, skill, intent, and influence from other works further complicate recognition. In this work, we investigate the potential of foundation models to improve facial recognition in artworks. By fine-tuning foundation models and integrating their embeddings with those from conventional facial recognition networks, we demonstrate notable improvements over current state-of-the-art methods. Our results show that foundation models can bridge the gap where traditional methods are ineffective. Paper page at https://www.idiap.ch/paper/artface/
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</title>
<link>https://arxiv.org/abs/2508.20640</link>
<guid>https://arxiv.org/abs/2508.20640</guid>
<content:encoded><![CDATA[
arXiv:2508.20640v1 Announce Type: new 
Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Alignment in LVLMs with Debiased Self-Judgment</title>
<link>https://arxiv.org/abs/2508.20655</link>
<guid>https://arxiv.org/abs/2508.20655</guid>
<content:encoded><![CDATA[
arXiv:2508.20655v1 Announce Type: new 
Abstract: The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection</title>
<link>https://arxiv.org/abs/2508.20670</link>
<guid>https://arxiv.org/abs/2508.20670</guid>
<content:encoded><![CDATA[
arXiv:2508.20670v1 Announce Type: new 
Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCLIP2: Improving Multi-Modal Reinforced Training</title>
<link>https://arxiv.org/abs/2508.20691</link>
<guid>https://arxiv.org/abs/2508.20691</guid>
<content:encoded><![CDATA[
arXiv:2508.20691v1 Announce Type: new 
Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network</title>
<link>https://arxiv.org/abs/2508.20709</link>
<guid>https://arxiv.org/abs/2508.20709</guid>
<content:encoded><![CDATA[
arXiv:2508.20709v1 Announce Type: new 
Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent years. However, precise rate control remains a challenge due to the inherent limitations of learning-based codecs. To solve this issue, we propose a dynamic video compression framework designed for variable bitrate scenarios. First, to achieve variable bitrate implementation, we propose the Dynamic-Route Autoencoder with variable coding routes, each occupying partial computational complexity of the whole network and navigating to a distinct RD trade-off. Second, to approach the target bitrate, the Rate Control Agent estimates the bitrate of each route and adjusts the coding route of DRA at run time. To encompass a broad spectrum of variable bitrates while preserving overall RD performance, we employ the Joint-Routes Optimization strategy, achieving collaborative training of various routes. Extensive experiments on the HEVC and UVG datasets show that the proposed method achieves an average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods while maintaining an average bitrate error of 1.66%, achieving Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and bitrate-constrained applications. Our code is available at https://git.openi.org.cn/OpenAICoding/DynamicDVC.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network</title>
<link>https://arxiv.org/abs/2508.20734</link>
<guid>https://arxiv.org/abs/2508.20734</guid>
<content:encoded><![CDATA[
arXiv:2508.20734v1 Announce Type: new 
Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification</title>
<link>https://arxiv.org/abs/2508.20745</link>
<guid>https://arxiv.org/abs/2508.20745</guid>
<content:encoded><![CDATA[
arXiv:2508.20745v1 Announce Type: new 
Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet remain challenging to identify consistently, particularly under domain shift stemming from scanner, stain, and acquisition differences. We present a simple training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2. The approach (i) increases feature diversity via style perturbations inserted at early and mid backbone stages, (ii) aligns attention-refined features across sites using weak domain labels (Scanner, Origin, Species, Tumor) through an auxiliary alignment loss, and (iii) stabilizes predictions by distilling from an exponential moving average (EMA) teacher with temperature-scaled KL divergence. On the organizer-run preliminary leaderboard for atypical mitosis classification, our submission attains balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs negligible inference-time overhead, relies only on coarse domain metadata, and delivers strong, balanced performance, positioning it as a competitive submission for the MIDOG 2025 challenge.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.20751</link>
<guid>https://arxiv.org/abs/2508.20751</guid>
<content:encoded><![CDATA[
arXiv:2508.20751v1 Announce Type: new 
Abstract: Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.20754</link>
<guid>https://arxiv.org/abs/2508.20754</guid>
<content:encoded><![CDATA[
arXiv:2508.20754v1 Announce Type: new 
Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2508.20758</link>
<guid>https://arxiv.org/abs/2508.20758</guid>
<content:encoded><![CDATA[
arXiv:2508.20758v1 Announce Type: new 
Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using natural language descriptions. Although supervised methods achieve higher accuracy in constrained settings, zero-shot 3DVG holds greater promise for real-world applications since eliminating scene-specific training requirements. However, existing zero-shot methods face challenges of spatial-limited reasoning due to reliance on single-view localization, and contextual omissions or detail degradation. To address these issues, we propose SeqVLM, a novel zero-shot 3DVG framework that leverages multi-view real-world scene images with spatial information for target object reasoning. Specifically, SeqVLM first generates 3D instance proposals via a 3D semantic segmentation network and refines them through semantic filtering, retaining only semantic-relevant candidates. A proposal-guided multi-view projection strategy then projects these candidate proposals onto real scene image sequences, preserving spatial relationships and contextual details in the conversion process of 3D point cloud to images. Furthermore, to mitigate VLM computational overload, we implement a dynamic scheduling mechanism that iteratively processes sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to identify textually specified objects. Experiments on the ScanRefer and Nr3D benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%, respectively, which advance 3DVG toward greater generalization and real-world applicability. The code is available at https://github.com/JiawLin/SeqVLM.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occlusion Robustness of CLIP for Military Vehicle Classification</title>
<link>https://arxiv.org/abs/2508.20760</link>
<guid>https://arxiv.org/abs/2508.20760</guid>
<content:encoded><![CDATA[
arXiv:2508.20760v1 Announce Type: new 
Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer</title>
<link>https://arxiv.org/abs/2508.20762</link>
<guid>https://arxiv.org/abs/2508.20762</guid>
<content:encoded><![CDATA[
arXiv:2508.20762v1 Announce Type: new 
Abstract: Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding</title>
<link>https://arxiv.org/abs/2508.20765</link>
<guid>https://arxiv.org/abs/2508.20765</guid>
<content:encoded><![CDATA[
arXiv:2508.20765v1 Announce Type: new 
Abstract: The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid ``re-inventing the wheel'' as we start revisiting it in the era of multi-modal foundation models.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML</title>
<link>https://arxiv.org/abs/2508.20776</link>
<guid>https://arxiv.org/abs/2508.20776</guid>
<content:encoded><![CDATA[
arXiv:2508.20776v1 Announce Type: new 
Abstract: Recent advancements in skin lesion classification models have significantly improved accuracy, with some models even surpassing dermatologists' diagnostic performance. However, in medical practice, distrust in AI models remains a challenge. Beyond high accuracy, trustworthy, explainable diagnoses are essential. Existing explainability methods have reliability issues, with LIME-based methods suffering from inconsistency, while CAM-based methods failing to consider all classes. To address these limitations, we propose Global Class Activation Probabilistic Map Evaluation, a method that analyses all classes' activation probability maps probabilistically and at a pixel level. By visualizing the diagnostic process in a unified manner, it helps reduce the risk of misdiagnosis. Furthermore, the application of SafeML enhances the detection of false diagnoses and issues warnings to doctors and patients as needed, improving diagnostic reliability and ultimately patient safety. We evaluated our method using the ISIC datasets with MobileNetV2 and Vision Transformers.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Compositional Generalisation in VLMs and Diffusion Models</title>
<link>https://arxiv.org/abs/2508.20783</link>
<guid>https://arxiv.org/abs/2508.20783</guid>
<content:encoded><![CDATA[
arXiv:2508.20783v1 Announce Type: new 
Abstract: A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: https://github.com/otmive/diffusion_classifier_clip
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surfel-based 3D Registration with Equivariant SE(3) Features</title>
<link>https://arxiv.org/abs/2508.20789</link>
<guid>https://arxiv.org/abs/2508.20789</guid>
<content:encoded><![CDATA[
arXiv:2508.20789v1 Announce Type: new 
Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training</title>
<link>https://arxiv.org/abs/2508.20813</link>
<guid>https://arxiv.org/abs/2508.20813</guid>
<content:encoded><![CDATA[
arXiv:2508.20813v1 Announce Type: new 
Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing the detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning</title>
<link>https://arxiv.org/abs/2508.20817</link>
<guid>https://arxiv.org/abs/2508.20817</guid>
<content:encoded><![CDATA[
arXiv:2508.20817v1 Announce Type: new 
Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on optimizing fused image quality. Recent studies have begun incorporating downstream tasks, such as semantic segmentation and object detection, to provide semantic guidance for VIF. However, semantic segmentation requires extensive annotations, while object detection, despite reducing annotation efforts compared with segmentation, faces challenges in highly crowded scenes due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd counting has gained increasing attention in recent years, no studies have integrated VIF and crowd counting into a unified framework. To address these challenges, we propose FusionCounting, a novel multi-task learning framework that integrates crowd counting into the VIF process. Crowd counting provides a direct quantitative measure of population density with minimal annotation, making it particularly suitable for dense scenes. Our framework leverages both input images and population density information in a mutually beneficial multi-task design. To accelerate convergence and balance tasks contributions, we introduce a dynamic loss function weighting strategy. Furthermore, we incorporate adversarial training to enhance the robustness of both VIF and crowd counting, improving the model's stability and resilience to adversarial attacks. Experimental results on public datasets demonstrate that FusionCounting not only enhances image fusion quality but also achieves superior crowd counting performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2508.20830</link>
<guid>https://arxiv.org/abs/2508.20830</guid>
<content:encoded><![CDATA[
arXiv:2508.20830v1 Announce Type: new 
Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</title>
<link>https://arxiv.org/abs/2508.20835</link>
<guid>https://arxiv.org/abs/2508.20835</guid>
<content:encoded><![CDATA[
arXiv:2508.20835v1 Announce Type: new 
Abstract: Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis</title>
<link>https://arxiv.org/abs/2508.20851</link>
<guid>https://arxiv.org/abs/2508.20851</guid>
<content:encoded><![CDATA[
arXiv:2508.20851v1 Announce Type: new 
Abstract: Deep learning based automated pathological diagnosis has markedly improved diagnostic efficiency and reduced variability between observers, yet its clinical adoption remains limited by opaque model decisions and a lack of traceable rationale. To address this, recent multimodal visual reasoning architectures provide a unified framework that generates segmentation masks at the pixel level alongside semantically aligned textual explanations. By localizing lesion regions and producing expert style diagnostic narratives, these models deliver the transparent and interpretable insights necessary for dependable AI assisted pathology. Building on these advancements, we propose PathMR, a cell-level Multimodal visual Reasoning framework for Pathological image analysis. Given a pathological image and a textual query, PathMR generates expert-level diagnostic explanations while simultaneously predicting cell distribution patterns. To benchmark its performance, we evaluated our approach on the publicly available PathGen dataset as well as on our newly developed GADVR dataset. Extensive experiments on these two datasets demonstrate that PathMR consistently outperforms state-of-the-art visual reasoning methods in text generation quality, segmentation accuracy, and cross-modal alignment. These results highlight the potential of PathMR for improving interpretability in AI-driven pathological diagnosis. The code will be publicly available in https://github.com/zhangye-zoe/PathMR.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis</title>
<link>https://arxiv.org/abs/2508.20877</link>
<guid>https://arxiv.org/abs/2508.20877</guid>
<content:encoded><![CDATA[
arXiv:2508.20877v1 Announce Type: new 
Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and evaluating computer vision models through the lens of counterfactuals</title>
<link>https://arxiv.org/abs/2508.20881</link>
<guid>https://arxiv.org/abs/2508.20881</guid>
<content:encoded><![CDATA[
arXiv:2508.20881v1 Announce Type: new 
Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software</title>
<link>https://arxiv.org/abs/2508.20892</link>
<guid>https://arxiv.org/abs/2508.20892</guid>
<content:encoded><![CDATA[
arXiv:2508.20892v1 Announce Type: new 
Abstract: Autonomous vehicle perception typically relies on modular pipelines that decompose the task into detection, tracking, and prediction. While interpretable, these pipelines suffer from error accumulation and limited inter-task synergy. Unified perception has emerged as a promising paradigm that integrates these sub-tasks within a shared architecture, potentially improving robustness, contextual reasoning, and efficiency while retaining interpretable outputs. In this survey, we provide a comprehensive overview of unified perception, introducing a holistic and systemic taxonomy that categorizes methods along task integration, tracking formulation, and representation flow. We define three paradigms -Early, Late, and Full Unified Perception- and systematically review existing methods, their architectures, training strategies, datasets used, and open-source availability, while highlighting future research directions. This work establishes the first comprehensive framework for understanding and advancing unified perception, consolidates fragmented efforts, and guides future research toward more robust, generalizable, and interpretable perception.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2508.20909</link>
<guid>https://arxiv.org/abs/2508.20909</guid>
<content:encoded><![CDATA[
arXiv:2508.20909v1 Announce Type: new 
Abstract: Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at https://github.com/yifangao112/DinoUNet.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement</title>
<link>https://arxiv.org/abs/2508.20919</link>
<guid>https://arxiv.org/abs/2508.20919</guid>
<content:encoded><![CDATA[
arXiv:2508.20919v1 Announce Type: new 
Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading. Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult, as manual annotation is time-consuming and subjective. In this work an ensemble of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it reduced sensitivity and overall performance. The results show that deep ensembles perform well for AMF classification. RBR can increase specific metrics but requires further research.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans</title>
<link>https://arxiv.org/abs/2508.20920</link>
<guid>https://arxiv.org/abs/2508.20920</guid>
<content:encoded><![CDATA[
arXiv:2508.20920v1 Announce Type: new 
Abstract: In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement</title>
<link>https://arxiv.org/abs/2508.20954</link>
<guid>https://arxiv.org/abs/2508.20954</guid>
<content:encoded><![CDATA[
arXiv:2508.20954v1 Announce Type: new 
Abstract: In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\% accuracy rate, significantly surpassing the initial SAM performance of 82\%.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections</title>
<link>https://arxiv.org/abs/2508.20955</link>
<guid>https://arxiv.org/abs/2508.20955</guid>
<content:encoded><![CDATA[
arXiv:2508.20955v1 Announce Type: new 
Abstract: Many high-performance networks were not designed with lightweight application scenarios in mind from the outset, which has greatly restricted their scope of application. This paper takes ConvNeXt as the research object and significantly reduces the parameter scale and network complexity of ConvNeXt by integrating the Cross Stage Partial Connections mechanism and a series of optimized designs. The new network is named E-ConvNeXt, which can maintain high accuracy performance under different complexity configurations. The three core innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network (CSPNet) with ConvNeXt and adjusting the network structure, which reduces the model's network complexity by up to 80%; (2) Optimizing the Stem and Block structures to enhance the model's feature expression capability and operational efficiency; (3) Replacing Layer Scale with channel attention. Experimental validation on ImageNet classification demonstrates E-ConvNeXt's superior accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at 0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer learning tests on object detection tasks further confirm its generalization capability.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes</title>
<link>https://arxiv.org/abs/2508.20965</link>
<guid>https://arxiv.org/abs/2508.20965</guid>
<content:encoded><![CDATA[
arXiv:2508.20965v1 Announce Type: new 
Abstract: We present DrivingGaussian++, an efficient and effective framework for realistic reconstructing and controllable editing of surrounding dynamic autonomous driving scenes. DrivingGaussian++ models the static background using incremental 3D Gaussians and reconstructs moving objects with a composite dynamic Gaussian graph, ensuring accurate positions and occlusions. By integrating a LiDAR prior, it achieves detailed and consistent scene reconstruction, outperforming existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. DrivingGaussian++ supports training-free controllable editing for dynamic driving scenes, including texture modification, weather simulation, and object manipulation, leveraging multi-view images and depth priors. By integrating large language models (LLMs) and controllable editing, our method can automatically generate dynamic object motion trajectories and enhance their realism during the optimization process. DrivingGaussian++ demonstrates consistent and realistic editing results and generates dynamic multi-view driving scenarios, while significantly enhancing scene diversity. More results and code can be found at the project site: https://xiong-creator.github.io/DrivingGaussian_plus.github.io
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation</title>
<link>https://arxiv.org/abs/2508.20987</link>
<guid>https://arxiv.org/abs/2508.20987</guid>
<content:encoded><![CDATA[
arXiv:2508.20987v1 Announce Type: new 
Abstract: Images manipulated using image editing tools can mislead viewers and pose significant risks to social security. However, accurately localizing the manipulated regions within an image remains a challenging problem. One of the main barriers in this area is the high cost of data acquisition and the severe lack of high-quality annotated datasets. To address this challenge, we introduce novel methods that mitigate data scarcity by leveraging readily available web data. We utilize a large collection of manually forged images from the web, as well as automatically generated annotations derived from a simpler auxiliary task, constrained image manipulation localization. Specifically, we introduce a new paradigm CAAAv2, which automatically and accurately annotates manipulated regions at the pixel level. To further improve annotation quality, we propose a novel metric, QES, which filters out unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a large-scale, diverse, and high-quality dataset containing 246,212 manually forged images with pixel-level mask annotations. This is over 120x larger than existing handcrafted datasets like IMD20. Additionally, we introduce Object Jitter, a technique that further enhances model training by generating high-quality manipulation artifacts. Building on these advances, we develop a new model, Web-IML, designed to effectively leverage web-scale supervision for the image manipulation localization task. Extensive experiments demonstrate that our approach substantially alleviates the data scarcity problem and significantly improves the performance of various models on multiple real-world forgery benchmarks. With the proposed web supervision, Web-IML achieves a striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1 average IoU points. The dataset and code will be made publicly available at https://github.com/qcf-568/MIML.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts</title>
<link>https://arxiv.org/abs/2508.20991</link>
<guid>https://arxiv.org/abs/2508.20991</guid>
<content:encoded><![CDATA[
arXiv:2508.20991v1 Announce Type: new 
Abstract: Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title>
<link>https://arxiv.org/abs/2508.21010</link>
<guid>https://arxiv.org/abs/2508.21010</guid>
<content:encoded><![CDATA[
arXiv:2508.21010v1 Announce Type: new 
Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.21019</link>
<guid>https://arxiv.org/abs/2508.21019</guid>
<content:encoded><![CDATA[
arXiv:2508.21019v1 Announce Type: new 
Abstract: The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets</title>
<link>https://arxiv.org/abs/2508.21032</link>
<guid>https://arxiv.org/abs/2508.21032</guid>
<content:encoded><![CDATA[
arXiv:2508.21032v1 Announce Type: new 
Abstract: Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitosis detection in domain shift scenarios: a Mamba-based approach</title>
<link>https://arxiv.org/abs/2508.21033</link>
<guid>https://arxiv.org/abs/2508.21033</guid>
<content:encoded><![CDATA[
arXiv:2508.21033v1 Announce Type: new 
Abstract: Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-task neural network for atypical mitosis recognition under domain shift</title>
<link>https://arxiv.org/abs/2508.21035</link>
<guid>https://arxiv.org/abs/2508.21035</guid>
<content:encoded><![CDATA[
arXiv:2508.21035v1 Announce Type: new 
Abstract: Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator</title>
<link>https://arxiv.org/abs/2508.21040</link>
<guid>https://arxiv.org/abs/2508.21040</guid>
<content:encoded><![CDATA[
arXiv:2508.21040v1 Announce Type: new 
Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs</title>
<link>https://arxiv.org/abs/2508.21044</link>
<guid>https://arxiv.org/abs/2508.21044</guid>
<content:encoded><![CDATA[
arXiv:2508.21044v1 Announce Type: new 
Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their excessive visual tokens pose a significant computational challenge for real-world applications. Current methods aim to enhance inference efficiency by visual token pruning. However, they do not consider the dynamic characteristics and temporal dependencies of video frames, as they perceive video understanding as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel training-free visual token pruning framework that removes redundancy by Maximizing Marginal Gains at both segment-level and token-level. Specifically, we first divide the video into segments based on frame similarity, and then dynamically allocate the token budget for each segment to maximize the marginal gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm that jointly models inter-frame uniqueness and intra-frame diversity, thereby maximizing the marginal gain of each token. By combining both stages, MMG-Vid can maximize the utilization of the limited token budget, significantly improving efficiency while maintaining strong performance. Extensive experiments demonstrate that MMG-Vid can maintain over 99.5% of the original performance, while effectively reducing 75% visual tokens and accelerating the prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</title>
<link>https://arxiv.org/abs/2508.21046</link>
<guid>https://arxiv.org/abs/2508.21046</guid>
<content:encoded><![CDATA[
arXiv:2508.21046v1 Announce Type: new 
Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning</title>
<link>https://arxiv.org/abs/2508.21048</link>
<guid>https://arxiv.org/abs/2508.21048</guid>
<content:encoded><![CDATA[
arXiv:2508.21048v1 Announce Type: new 
Abstract: Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as "planning" and "self-reflection" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FakeParts: a New Family of AI-Generated DeepFakes</title>
<link>https://arxiv.org/abs/2508.21052</link>
<guid>https://arxiv.org/abs/2508.21052</guid>
<content:encoded><![CDATA[
arXiv:2508.21052v1 Announce Type: new 
Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-View 3D Point Tracking</title>
<link>https://arxiv.org/abs/2508.21060</link>
<guid>https://arxiv.org/abs/2508.21060</guid>
<content:encoded><![CDATA[
arXiv:2508.21060v1 Announce Type: new 
Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</title>
<link>https://arxiv.org/abs/2508.21066</link>
<guid>https://arxiv.org/abs/2508.21066</guid>
<content:encoded><![CDATA[
arXiv:2508.21066v1 Announce Type: new 
Abstract: In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dress&amp;Dance: Dress up and Dance as You Like It - Technical Preview</title>
<link>https://arxiv.org/abs/2508.21070</link>
<guid>https://arxiv.org/abs/2508.21070</guid>
<content:encoded><![CDATA[
arXiv:2508.21070v1 Announce Type: new 
Abstract: We present Dress&amp;Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&amp;Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge</title>
<link>https://arxiv.org/abs/2508.21072</link>
<guid>https://arxiv.org/abs/2508.21072</guid>
<content:encoded><![CDATA[
arXiv:2508.21072v1 Announce Type: new 
Abstract: Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty</title>
<link>https://arxiv.org/abs/2503.18314</link>
<guid>https://arxiv.org/abs/2503.18314</guid>
<content:encoded><![CDATA[
arXiv:2503.18314v4 Announce Type: cross 
Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model up to an information-theoretic bound, mitigating its over-confidence stemming from data memorization. We evaluate LoTUS on Transformer and ResNet18 models against eight baselines across five public datasets. Beyond established MU benchmarks, we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining is impractical, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. The experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: https://github.com/cspartalis/LoTUS.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms</title>
<link>https://arxiv.org/abs/2508.20125</link>
<guid>https://arxiv.org/abs/2508.20125</guid>
<content:encoded><![CDATA[
arXiv:2508.20125v1 Announce Type: cross 
Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as energy-efficient, biologically plausible alternatives to conventional deep learning models. Their application in high-stakes biomedical imaging remains almost entirely unexplored. Methods: This study introduces SNNDeep, the first tailored SNN specifically optimized for binary classification of liver health status from computed tomography (CT) features. To ensure clinical relevance and broad generalizability, the model was developed and evaluated using the Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a standardized benchmark widely used for assessing performance across diverse medical imaging tasks. We benchmark three fundamentally different learning algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and Bio-Inspired Active Learning across three architectural variants: a fully customized low-level model built from scratch, and two implementations using leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter optimization was performed using Optuna. Results: Our results demonstrate that the custom-built SNNDeep consistently outperforms framework-based implementations, achieving a maximum validation accuracy of 98.35%, superior adaptability across learning rules, and significantly reduced training overhead. Conclusion:This study provides the first empirical evidence that low-level, highly tunable SNNs can surpass standard frameworks in medical imaging, especially in data-limited, temporally constrained diagnostic settings, thereby opening a new pathway for neuro-inspired AI in precision medicine.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Volumetric Computations of Solid Pulmonary Nodules</title>
<link>https://arxiv.org/abs/2508.20127</link>
<guid>https://arxiv.org/abs/2508.20127</guid>
<content:encoded><![CDATA[
arXiv:2508.20127v1 Announce Type: cross 
Abstract: Early detection of lung cancer is crucial for effective treatment and relies on accurate volumetric assessment of pulmonary nodules in CT scans. Traditional methods, such as consolidation-to-tumor ratio (CTR) and spherical approximation, are limited by inconsistent estimates due to variability in nodule shape and density. We propose an advanced framework that combines a multi-scale 3D convolutional neural network (CNN) with subtype-specific bias correction for precise volume estimation. The model was trained and evaluated on a dataset of 364 cases from Shanghai Chest Hospital. Our approach achieved a mean absolute deviation of 8.0 percent compared to manual nonlinear regression, with inference times under 20 seconds per scan. This method outperforms existing deep learning and semi-automated pipelines, which typically have errors of 25 to 30 percent and require over 60 seconds for processing. Our results show a reduction in error by over 17 percentage points and a threefold acceleration in processing speed. These advancements offer a highly accurate, efficient, and scalable tool for clinical lung nodule screening and monitoring, with promising potential for improving early lung cancer detection.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is the medical image segmentation problem solved? A survey of current developments and future directions</title>
<link>https://arxiv.org/abs/2508.20139</link>
<guid>https://arxiv.org/abs/2508.20139</guid>
<content:encoded><![CDATA[
arXiv:2508.20139v1 Announce Type: cross 
Abstract: Medical image segmentation has advanced rapidly over the past two decades, largely driven by deep learning, which has enabled accurate and efficient delineation of cells, tissues, organs, and pathologies across diverse imaging modalities. This progress raises a fundamental question: to what extent have current models overcome persistent challenges, and what gaps remain? In this work, we provide an in-depth review of medical image segmentation, tracing its progress and key developments over the past decade. We examine core principles, including multiscale analysis, attention mechanisms, and the integration of prior knowledge, across the encoder, bottleneck, skip connections, and decoder components of segmentation networks. Our discussion is organized around seven key dimensions: (1) the shift from supervised to semi-/unsupervised learning, (2) the transition from organ segmentation to lesion-focused tasks, (3) advances in multi-modality integration and domain adaptation, (4) the role of foundation models and transfer learning, (5) the move from deterministic to probabilistic segmentation, (6) the progression from 2D to 3D and 4D segmentation, and (7) the trend from model invocation to segmentation agents. Together, these perspectives provide a holistic overview of the trajectory of deep learning-based medical image segmentation and aim to inspire future innovation. To support ongoing research, we maintain a continually updated repository of relevant literature and open-source resources at https://github.com/apple1986/medicalSegReview
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases</title>
<link>https://arxiv.org/abs/2508.20141</link>
<guid>https://arxiv.org/abs/2508.20141</guid>
<content:encoded><![CDATA[
arXiv:2508.20141v1 Announce Type: cross 
Abstract: Ear diseases affect billions of people worldwide, leading to substantial health and socioeconomic burdens. Computed tomography (CT) plays a pivotal role in accurate diagnosis, treatment planning, and outcome evaluation. The objective of this study is to present the establishment and design of UltraEar Database, a large-scale, multicentric repository of isotropic 0.1 mm ultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated to ear diseases. UltraEar recruits patients from 11 tertiary hospitals between October 2020 and October 2035, integrating U-HRCT images, structured CT reports, and comprehensive clinical information, including demographics, audiometric profiles, surgical records, and pathological findings. A broad spectrum of otologic disorders is covered, such as otitis media, cholesteatoma, ossicular chain malformation, temporal bone fracture, inner ear malformation, cochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus bony deficiency. Standardized preprocessing pipelines have been developed for geometric calibration, image annotation, and multi-structure segmentation. All personal identifiers in DICOM headers and metadata are removed or anonymized to ensure compliance with data privacy regulation. Data collection and curation are coordinated through monthly expert panel meetings, with secure storage on an offline cloud system. UltraEar provides an unprecedented ultra-high-resolution reference atlas with both technical fidelity and clinical relevance. This resource has significant potential to advance radiological research, enable development and validation of AI algorithms, serve as an educational tool for training in otologic imaging, and support multi-institutional collaborative studies. UltraEar will be continuously updated and expanded, ensuring long-term accessibility and usability for the global otologic research community.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Teacher Calibration in Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.20224</link>
<guid>https://arxiv.org/abs/2508.20224</guid>
<content:encoded><![CDATA[
arXiv:2508.20224v1 Announce Type: cross 
Abstract: Knowledge Distillation (KD) has emerged as an effective model compression technique in deep learning, enabling the transfer of knowledge from a large teacher model to a compact student model. While KD has demonstrated significant success, it is not yet fully understood which factors contribute to improving the student's performance. In this paper, we reveal a strong correlation between the teacher's calibration error and the student's accuracy. Therefore, we claim that the calibration of the teacher model is an important factor for effective KD. Furthermore, we demonstrate that the performance of KD can be improved by simply employing a calibration method that reduces the teacher's calibration error. Our algorithm is versatile, demonstrating effectiveness across various tasks from classification to detection. Moreover, it can be easily integrated with existing state-of-the-art methods, consistently achieving superior performance.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR</title>
<link>https://arxiv.org/abs/2508.20250</link>
<guid>https://arxiv.org/abs/2508.20250</guid>
<content:encoded><![CDATA[
arXiv:2508.20250v1 Announce Type: cross 
Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile devices can be used as a replacement for traditional background removal and compositing techniques. Unlike approaches such as chroma keying and trained AI models, LiDAR's depth information is independent of subject lighting, and performs equally well in low-light and well-lit environments. We integrate the LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image processing. We use Apple's SwiftUI and Swift frameworks for user interface and backend development, and Metal Shader Language (MSL) for realtime image enhancement at the standard iPhone streaming frame rate of 60 frames per second. The only meaningful limitations of the technology are the streaming bandwidth of the depth data, which currently reduces the depth map resolution to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect accurate depth from some materials. If the LiDAR resolution on a mobile device like the iPhone can be improved to match the color image resolution, LiDAR could feasibly become the preeminent method of background removal for video applications and photography.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title>
<link>https://arxiv.org/abs/2508.20325</link>
<guid>https://arxiv.org/abs/2508.20325</guid>
<content:encoded><![CDATA[
arXiv:2508.20325v1 Announce Type: cross 
Abstract: As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline \textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Large Models in Medical Imaging: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2508.20414</link>
<guid>https://arxiv.org/abs/2508.20414</guid>
<content:encoded><![CDATA[
arXiv:2508.20414v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the realm of medical imaging. However, the development of high-performance AI models typically necessitates training on large-scale, centralized datasets. This approach is confronted with significant challenges due to strict patient privacy regulations and legal restrictions on data sharing and utilization. These limitations hinder the development of large-scale models in medical domains and impede continuous updates and training with new data. Federated Learning (FL), a privacy-preserving distributed training framework, offers a new solution by enabling collaborative model development across fragmented medical datasets. In this survey, we review FL's contributions at two stages of the full-stack medical analysis pipeline. First, in upstream tasks such as CT or MRI reconstruction, FL enables joint training of robust reconstruction networks on diverse, multi-institutional datasets, alleviating data scarcity while preserving confidentiality. Second, in downstream clinical tasks like tumor diagnosis and segmentation, FL supports continuous model updating by allowing local fine-tuning on new data without centralizing sensitive images. We comprehensively analyze FL implementations across the medical imaging pipeline, from physics-informed reconstruction networks to diagnostic AI systems, highlighting innovations that improve communication efficiency, align heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this paper provides an outlook on future research directions, aiming to serve as a valuable reference for the field's development.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network</title>
<link>https://arxiv.org/abs/2508.20469</link>
<guid>https://arxiv.org/abs/2508.20469</guid>
<content:encoded><![CDATA[
arXiv:2508.20469v1 Announce Type: cross 
Abstract: Metastasis remains the major challenge in the clinical management of head and neck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction of metastatic risk is crucial for optimizing treatment strategies and prognosis. This study develops a deep learning-based multimodal framework to predict metastasis risk in HNSCC patients by integrating computed tomography (CT) images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumor and organ masks were derived from pretreatment CT images. A 3D Swin Transformer extracted deep features from tumor regions. Meanwhile, 1562 radiomics features were obtained using PyRadiomics, followed by correlation filtering and random forest selection, leaving 36 features. Clinical variables including age, sex, smoking, and alcohol status were encoded and fused with imaging-derived features. Multimodal features were fed into a fully connected network to predict metastasis risk. Performance was evaluated using five-fold cross-validation with area under the curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The proposed fusion model outperformed single-modality models. The 3D deep learning module alone achieved an AUC of 0.715, and when combined with radiomics and clinical features, predictive performance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analysis showed generalizability across tumor subtypes. Ablation studies indicated complementary information from different modalities. Evaluation showed the 3D Swin Transformer provided more robust representation learning than conventional networks. This multimodal fusion model demonstrated high accuracy and robustness in predicting metastasis risk in HNSCC, offering a comprehensive representation of tumor biology. The interpretable model has potential as a clinical decision-support tool for personalized treatment planning.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes</title>
<link>https://arxiv.org/abs/2508.20547</link>
<guid>https://arxiv.org/abs/2508.20547</guid>
<content:encoded><![CDATA[
arXiv:2508.20547v1 Announce Type: cross 
Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction</title>
<link>https://arxiv.org/abs/2508.20600</link>
<guid>https://arxiv.org/abs/2508.20600</guid>
<content:encoded><![CDATA[
arXiv:2508.20600v1 Announce Type: cross 
Abstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications</title>
<link>https://arxiv.org/abs/2508.20622</link>
<guid>https://arxiv.org/abs/2508.20622</guid>
<content:encoded><![CDATA[
arXiv:2508.20622v1 Announce Type: cross 
Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs) with Vision Transformer (ViT) architectures for self-supervised representation learning on one-dimensional (1D) ultrasound signals. Although MAEs have demonstrated significant success in computer vision and other domains, their use for 1D signal analysis, especially for raw ultrasound data, remains largely unexplored. Ultrasound signals are vital in industrial applications such as non-destructive testing (NDT) and structural health monitoring (SHM), where labeled data are often scarce and signal processing is highly task-specific. We propose an approach that leverages MAE to pre-train on unlabeled synthetic ultrasound signals, enabling the model to learn robust representations that enhance performance in downstream tasks, such as time-of-flight (ToF) classification. This study systematically investigated the impact of model size, patch size, and masking ratio on pre-training efficiency and downstream accuracy. Our results show that pre-trained models significantly outperform models trained from scratch and strong convolutional neural network (CNN) baselines optimized for the downstream task. Additionally, pre-training on synthetic data demonstrates superior transferability to real-world measured signals compared with training solely on limited real datasets. This study underscores the potential of MAEs for advancing ultrasound signal analysis through scalable, self-supervised learning.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI</title>
<link>https://arxiv.org/abs/2508.20773</link>
<guid>https://arxiv.org/abs/2508.20773</guid>
<content:encoded><![CDATA[
arXiv:2508.20773v1 Announce Type: cross 
Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActLoc: Learning to Localize on the Move via Active Viewpoint Selection</title>
<link>https://arxiv.org/abs/2508.20981</link>
<guid>https://arxiv.org/abs/2508.20981</guid>
<content:encoded><![CDATA[
arXiv:2508.20981v1 Announce Type: cross 
Abstract: Reliable localization is critical for robot navigation, yet most existing systems implicitly assume that all viewing directions at a location are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At its core, ActLoc employs a largescale trained attention-based model for viewpoint selection. The model encodes a metric map and the camera poses used during map construction, and predicts localization accuracy across yaw and pitch directions at arbitrary 3D locations. These per-point accuracy distributions are incorporated into a path planner, enabling the robot to actively select camera orientations that maximize localization robustness while respecting task and motion constraints. ActLoc achieves stateof-the-art results on single-viewpoint selection and generalizes effectively to fulltrajectory planning. Its modular design makes it readily applicable to diverse robot navigation and inspection tasks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025</title>
<link>https://arxiv.org/abs/2508.21041</link>
<guid>https://arxiv.org/abs/2508.21041</guid>
<content:encoded><![CDATA[
arXiv:2508.21041v1 Announce Type: cross 
Abstract: Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Contexts for Long Video Generation</title>
<link>https://arxiv.org/abs/2508.21058</link>
<guid>https://arxiv.org/abs/2508.21058</guid>
<content:encoded><![CDATA[
arXiv:2508.21058v1 Announce Type: cross 
Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2308.09388</link>
<guid>https://arxiv.org/abs/2308.09388</guid>
<content:encoded><![CDATA[
arXiv:2308.09388v2 Announce Type: replace 
Abstract: Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, "whether diffusion model can boost image restoration". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2312.05407</link>
<guid>https://arxiv.org/abs/2312.05407</guid>
<content:encoded><![CDATA[
arXiv:2312.05407v4 Announce Type: replace 
Abstract: Unsupervised domain adaptive segmentation typically relies on self-training using pseudo labels predicted by a pre-trained network on an unlabeled target dataset. However, the noisy nature of such pseudo-labels presents a major bottleneck in adapting a network to the distribution shift between source and target datasets. This challenge is exaggerated when the network encounters an incoming data stream in online fashion, where the network is constrained to adapt to incoming streams of target domain data in exactly one round of forward and backward passes. In this scenario, relying solely on inaccurate pseudo-labels can lead to low-quality segmentation, which is detrimental to medical image analysis where accuracy and precision are of utmost priority. We hypothesize that a small amount of pixel-level annotation obtained from an expert can address this problem, thereby enhancing the performance of domain adaptation of online streaming data, even in the absence of dedicated training data. We call our method ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation that adapts to each incoming data batch in an online setup, incorporating feedback from an expert through active learning. Through active learning, the most informative pixels in each image can be selected for expert annotation. However, the acquisition of pixel-level annotations across all images in a batch often leads to redundant information while increasing temporal overhead in online learning. To reduce the annotation acquisition time and make the adaptation process more online-friendly, we further propose a novel image-pruning strategy that selects the most useful subset of images from the current batch for active learning. Our proposed approach outperforms existing online adaptation approaches and produces competitive results compared to offline domain adaptive active learning methods.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models</title>
<link>https://arxiv.org/abs/2407.11691</link>
<guid>https://arxiv.org/abs/2407.11691</guid>
<content:encoded><![CDATA[
arXiv:2407.11691v4 Announce Type: replace 
Abstract: We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</title>
<link>https://arxiv.org/abs/2408.04631</link>
<guid>https://arxiv.org/abs/2408.04631</guid>
<content:encoded><![CDATA[
arXiv:2408.04631v2 Announce Type: replace 
Abstract: We introduce Puppet-Master, an interactive video generator that captures the internal, part-level motion of objects, serving as a proxy for modeling object dynamics universally. Given an image of an object and a set of "drags" specifying the trajectory of a few points on the object, the model synthesizes a video where the object's parts move accordingly. To build Puppet-Master, we extend a pre-trained image-to-video generator to encode the input drags. We also propose all-to-first attention, an alternative to conventional spatial attention that mitigates artifacts caused by fine-tuning a video generator on out-of-domain data. The model is fine-tuned on Objaverse-Animation-HQ, a new dataset of curated part-level motion clips obtained by rendering synthetic 3D animations. Unlike real videos, these synthetic clips avoid confounding part-level motion with overall object and camera motion. We extensively filter sub-optimal animations and augment the synthetic renderings with meaningful drags that emphasize the internal dynamics of objects. We demonstrate that Puppet-Master learns to generate part-level motions, unlike other motion-conditioned video generators that primarily move the object as a whole. Moreover, Puppet-Master generalizes well to out-of-domain real images, outperforming existing methods on real-world benchmarks in a zero-shot manner.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See then Tell: Enhancing Key Information Extraction with Vision Grounding</title>
<link>https://arxiv.org/abs/2409.19573</link>
<guid>https://arxiv.org/abs/2409.19573</guid>
<content:encoded><![CDATA[
arXiv:2409.19573v2 Announce Type: replace 
Abstract: In the digital era, the ability to understand visually rich documents that integrate text, complex layouts, and imagery is critical. Traditional Key Information Extraction (KIE) methods primarily rely on Optical Character Recognition (OCR), which often introduces significant latency, computational overhead, and errors. Current advanced image-to-text approaches, which bypass OCR, typically yield plain text outputs without corresponding vision grounding. In this paper, we introduce STNet (See then Tell Net), a novel end-to-end model designed to deliver precise answers with relevant vision grounding. Distinctively, STNet utilizes a unique  token to observe pertinent image areas, aided by a decoder that interprets physical coordinates linked to this token. Positioned at the outset of the answer text, the  token allows the model to first see-observing the regions of the image related to the input question-and then tell-providing articulated textual responses. To enhance the model's seeing capabilities, we collect extensive structured table recognition datasets. Leveraging the advanced text processing prowess of GPT-4, we develop the TVG (TableQA with Vision Grounding) dataset, which not only provides text-based Question Answering (QA) pairs but also incorporates precise vision grounding for these pairs. Our approach demonstrates substantial advancements in KIE performance, achieving state-of-the-art results on publicly available datasets such as CORD, SROIE, and DocVQA. The code will also be made publicly available.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models</title>
<link>https://arxiv.org/abs/2410.01262</link>
<guid>https://arxiv.org/abs/2410.01262</guid>
<content:encoded><![CDATA[
arXiv:2410.01262v4 Announce Type: replace 
Abstract: While many diffusion models perform well when controlling particular aspects such as style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel training-free algorithm, independent of denoising network architectures, for fine-grained generation, called Aggregation of Multiple Diffusion Models (AMDM). The algorithm integrates features from multiple diffusion models into a specified model to activate particular features and enable fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional generation in diffusion models. Specifically, it allows us to fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZIM: Zero-Shot Image Matting for Anything</title>
<link>https://arxiv.org/abs/2411.00626</link>
<guid>https://arxiv.org/abs/2411.00626</guid>
<content:encoded><![CDATA[
arXiv:2411.00626v2 Announce Type: replace 
Abstract: The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://github.com/naver-ai/ZIM.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs</title>
<link>https://arxiv.org/abs/2412.10439</link>
<guid>https://arxiv.org/abs/2412.10439</guid>
<content:encoded><![CDATA[
arXiv:2412.10439v3 Announce Type: replace 
Abstract: Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-Stars-Poster: A Framework for Product-Centric Advertising Image Design</title>
<link>https://arxiv.org/abs/2501.14316</link>
<guid>https://arxiv.org/abs/2501.14316</guid>
<content:encoded><![CDATA[
arXiv:2501.14316v3 Announce Type: replace 
Abstract: Creating advertising images is often a labor-intensive and time-consuming process. Can we automatically generate such images using basic product information like a product foreground image, taglines, and a target size? Existing methods mainly focus on parts of the problem and lack a comprehensive solution. To bridge this gap, we propose a novel product-centric framework for advertising image design called T-Stars-Poster. It consists of four sequential stages to highlight product foregrounds and taglines while achieving overall image aesthetics: prompt generation, layout generation, background image generation, and graphics rendering. Different expert models are designed and trained for the first three stages: First, a visual language model (VLM) generates background prompts that match the products. Next, a VLM-based layout generation model arranges the placement of product foregrounds, graphic elements (taglines and decorative underlays), and various nongraphic elements (objects from the background prompt). Following this, an SDXL-based model can simultaneously accept prompts, layouts, and foreground controls to generate images. To support T-Stars-Poster, we create two corresponding datasets with over 50,000 labeled images. Extensive experiments and online A/B tests demonstrate that T-Stars-Poster can produce more visually appealing advertising images.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-to-Space Programming for Training-Free 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2502.01401</link>
<guid>https://arxiv.org/abs/2502.01401</guid>
<content:encoded><![CDATA[
arXiv:2502.01401v4 Announce Type: replace 
Abstract: 3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high annotation costs of 3D vision-language datasets. Training-free approaches based on LLMs/VLMs eliminate the need for large-scale training data, but they either incur prohibitive grounding time and token costs or have unsatisfactory accuracy. To address the challenges, we introduce a novel method for training-free 3D visual grounding, namely Language-to-Space Programming (LaSP). LaSP introduces LLM-generated codes to analyze 3D spatial relations among objects, along with a pipeline that evaluates and optimizes the codes automatically. Experimental results demonstrate that LaSP achieves 52.9% accuracy on the Nr3D benchmark, ranking among the best training-free methods. Moreover, it substantially reduces the grounding time and token costs, offering a balanced trade-off between performance and efficiency.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2503.02549</link>
<guid>https://arxiv.org/abs/2503.02549</guid>
<content:encoded><![CDATA[
arXiv:2503.02549v2 Announce Type: replace 
Abstract: The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the collected data is stored in the same location where nnU-Net is trained. This centralized approach has various limitations, such as potential leakage of sensitive patient information and violation of patient privacy. Federated learning has emerged as a key approach for training segmentation models in a decentralized manner, enabling collaborative development while prioritising patient privacy. In this paper, we propose FednnU-Net, a plug-and-play, federated learning extension of the nnU-Net framework. To this end, we contribute two federated methodologies to unlock decentralized training of nnU-Net, namely, Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a comprehensive set of experiments demonstrating high and consistent performance of our methods for breast, cardiac and fetal segmentation based on a multi-modal collection of 6 datasets representing samples from 18 different institutions. To democratize research as well as real-world deployments of decentralized training in clinical centres, we publicly share our framework at https://github.com/faildeny/FednnUNet .
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.08751</link>
<guid>https://arxiv.org/abs/2503.08751</guid>
<content:encoded><![CDATA[
arXiv:2503.08751v2 Announce Type: replace 
Abstract: Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2503.12232</link>
<guid>https://arxiv.org/abs/2503.12232</guid>
<content:encoded><![CDATA[
arXiv:2503.12232v2 Announce Type: replace 
Abstract: Visible-infrared person re-identification (VI-ReID) is a challenging task that aims to match pedestrian images captured under varying lighting conditions, which has drawn intensive research attention and achieved promising results. However, existing methods adopt the centralized training, ignoring the potential privacy concerns as the data is distributed across multiple devices or entities in reality. In this paper, we propose L2RW+, a benchmark that brings VI-ReID closer to real-world applications. The core rationale behind L2RW+ is that incorporating decentralized training into VI-ReID can address privacy concerns in scenarios with limited data-sharing constrains. Specifically, we design protocols and corresponding algorithms for different privacy sensitivity levels. In our new benchmark, we simulate the training under real-world data conditions that: 1) data from each camera is completely isolated, or 2) different data entities (e.g., data controllers of a certain region) can selectively share the data. In this way, we simulate scenarios with strict privacy restrictions, which is closer to real-world conditions. Comprehensive experiments show the feasibility and potential of decentralized VI-ReID training at both image and video levels. In particular, with increasing data scales, the performance gap between decentralized and centralized training decreases, especially in video-level VI-ReID. In unseen domains, decentralized training even achieves performance comparable to SOTA centralized methods. This work offers a novel research entry for deploying VI-ReID into real-world scenarios and can benefit the community. Code is available at: https://github.com/Joey623/L2RW.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
arXiv:2503.19065v2 Announce Type: replace 
Abstract: Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. Our code and examples are available at https://wikiautogen.github.io/ .
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</title>
<link>https://arxiv.org/abs/2503.22677</link>
<guid>https://arxiv.org/abs/2503.22677</guid>
<content:encoded><![CDATA[
arXiv:2503.22677v2 Announce Type: replace 
Abstract: Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO) - a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoTexBuild: 3D Building Model Generation from Map Footprints</title>
<link>https://arxiv.org/abs/2504.08419</link>
<guid>https://arxiv.org/abs/2504.08419</guid>
<content:encoded><![CDATA[
arXiv:2504.08419v2 Announce Type: replace 
Abstract: We introduce GeoTexBuild, a modular generative framework for creating 3D building models from footprints derived from site planning or map designs. The system is designed for architects and city planners, offering a seamless solution that directly converts map features into 3D buildings. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with detailed geometry and appearance attributes. By integrating customized ControlNet, Neural style field (NSF), and Multi-view diffusion model, we explore effective methods for controlling both geometric and visual attributes during the generation process. Our approach eliminates the problem of structural variations in a single facade image in existing 3D generation techniques for buildings. Experimental results at each stage validate the capability of GeoTexBuild to generate detailed and accurate building models from footprints.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation</title>
<link>https://arxiv.org/abs/2504.17991</link>
<guid>https://arxiv.org/abs/2504.17991</guid>
<content:encoded><![CDATA[
arXiv:2504.17991v2 Announce Type: replace 
Abstract: Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the "user-matched goal" setting, highlighting its potential for real-world applications.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems</title>
<link>https://arxiv.org/abs/2504.20376</link>
<guid>https://arxiv.org/abs/2504.20376</guid>
<content:encoded><![CDATA[
arXiv:2504.20376v2 Announce Type: replace 
Abstract: Modern text-to-image (T2I) generation systems (e.g., DALL$\cdot$E 3) exploit the memory mechanism, which captures key information in multi-turn interactions for faithful generation. Despite its practicality, the security analyses of this mechanism have fallen far behind. In this paper, we reveal that it can exacerbate the risk of jailbreak attacks. Previous attacks fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or lead to the generation of non-unsafe images due to under- or over-detoxification. In contrast, we propose embedding the malice at the inception of the chat session in memory, addressing the above limitations.
  Specifically, we propose Inception, the first multi-turn jailbreak attack against real-world text-to-image generation systems that explicitly exploits their memory mechanisms. Inception is composed of two key modules: segmentation and recursion. We introduce Segmentation, a semantic-preserving method that generates multi-round prompts. By leveraging NLP analysis techniques, we design policies to decompose a prompt, together with its malicious intent, according to sentence structure, thereby evading safety filters. Recursion further addresses the challenge posed by unsafe sub-prompts that cannot be separated through simple segmentation. It firstly expands the sub-prompt, then invokes segmentation recursively. To facilitate multi-turn adversarial prompts crafting, we build VisionFlow, an emulation T2I system that integrates two-stage safety filters and industrial-grade memory mechanisms. The experiment results show that Inception successfully allures unsafe image generation, surpassing the SOTA by a 20.0\% margin in attack success rate. We also conduct experiments on the real-world commercial T2I generation platforms, further validating the threats of Inception in practice.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
arXiv:2505.07818v4 Announce Type: replace 
Abstract: Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPO's inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image/video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181\% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10583</link>
<guid>https://arxiv.org/abs/2505.10583</guid>
<content:encoded><![CDATA[
arXiv:2505.10583v2 Announce Type: replace 
Abstract: Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to a similar area in the latent space as a textual description of the strokes that form the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper, we evaluate the complexity of teaching vision-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.15576</link>
<guid>https://arxiv.org/abs/2505.15576</guid>
<content:encoded><![CDATA[
arXiv:2505.15576v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model's discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs' performance on complex CR tasks. The source code is available at https://github.com/nynu-BDAI/AHNPL.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leadership Assessment in Pediatric Intensive Care Unit Team Training</title>
<link>https://arxiv.org/abs/2505.24389</link>
<guid>https://arxiv.org/abs/2505.24389</guid>
<content:encoded><![CDATA[
arXiv:2505.24389v2 Announce Type: replace 
Abstract: This paper addresses the task of assessing PICU team's leadership skills by developing an automated analysis framework based on egocentric vision. We identify key behavioral cues, including fixation object, eye contact, and conversation patterns, as essential indicators of leadership assessment. In order to capture these multimodal signals, we employ Aria Glasses to record egocentric video, audio, gaze, and head movement data. We collect one-hour videos of four simulated sessions involving doctors with different roles and levels. To automate data processing, we propose a method leveraging REMoDNaV, SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection, and conversation classification. In the experiments, significant correlations are observed between leadership skills and behavioral metrics, i.e., the output of our proposed methods, such as fixation time, transition patterns, and direct orders in speech. These results indicate that our proposed data collection and analysis framework can effectively solve skill assessment for training PICU teams.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust ID-Specific Face Restoration via Alignment Learning</title>
<link>https://arxiv.org/abs/2507.10943</link>
<guid>https://arxiv.org/abs/2507.10943</guid>
<content:encoded><![CDATA[
arXiv:2507.10943v2 Announce Type: replace 
Abstract: The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct-Video: Reasoning-Rich Video QA for Urban Traffic</title>
<link>https://arxiv.org/abs/2507.14743</link>
<guid>https://arxiv.org/abs/2507.14743</guid>
<content:encoded><![CDATA[
arXiv:2507.14743v3 Announce Type: replace 
Abstract: Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems (ITS). Deep learning has advanced video-based traffic monitoring through video question answering (VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions. To address these challenges, this paper introduces \textbf{InterAct VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes. State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems. GitHub Repo: https://github.com/joe-rabbit/InterAct_VideoQA
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios</title>
<link>https://arxiv.org/abs/2507.20198</link>
<guid>https://arxiv.org/abs/2507.20198</guid>
<content:encoded><![CDATA[
arXiv:2507.20198v4 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have made remarkable strides, largely driven by their ability to process increasingly long and complex contexts, such as high-resolution images, extended video sequences, and lengthy audio input. While this ability significantly enhances MLLM capabilities, it introduces substantial computational challenges, primarily due to the quadratic complexity of self-attention mechanisms with numerous input tokens. To mitigate these bottlenecks, token compression has emerged as an auspicious and critical approach, efficiently reducing the number of tokens during both training and inference. In this paper, we present the first systematic survey and synthesis of the burgeoning field of multimodal long context token compression. Recognizing that effective compression strategies are deeply tied to the unique characteristics and redundancies of each modality, we categorize existing approaches by their primary data focus, enabling researchers to quickly access and learn methods tailored to their specific area of interest: (1) image-centric compression, which addresses spatial redundancy in visual data; (2) video-centric compression, which tackles spatio-temporal redundancy in dynamic sequences; and (3) audio-centric compression, which handles temporal and spectral redundancy in acoustic signals. Beyond this modality-driven categorization, we further dissect methods based on their underlying mechanisms, including transformation-based, similarity-based, attention-based, and query-based approaches. By providing a comprehensive and structured overview, this survey aims to consolidate current progress, identify key challenges, and inspire future research directions in this rapidly evolving domain. We also maintain a public repository to continuously track and update the latest advances in this promising area.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement</title>
<link>https://arxiv.org/abs/2508.10950</link>
<guid>https://arxiv.org/abs/2508.10950</guid>
<content:encoded><![CDATA[
arXiv:2508.10950v2 Announce Type: replace 
Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling technique that represents complex white matter fiber configurations, and a key step for subsequent brain tractography and connectome analysis. Its reliability and accuracy, however, heavily rely on the quality of the MRI acquisition and the subsequent estimation of the FODs at each voxel. Generating reliable FODs from widely available clinical protocols with single-shell and low-angular-resolution acquisitions remains challenging but could potentially be addressed with recent advances in deep learning-based enhancement techniques. Despite advancements, existing methods have predominantly been assessed on healthy subjects, which have proved to be a major hurdle for their clinical adoption. In this work, we validate a newly optimized enhancement framework, FastFOD-Net, across healthy controls and six neurological disorders. This accelerated end-to-end deep learning framework enhancing FODs with superior performance and delivering training/inference efficiency for clinical use ($60\times$ faster comparing to its predecessor). With the most comprehensive clinical evaluation to date, our work demonstrates the potential of FastFOD-Net in accelerating clinical neuroscience research, empowering diffusion MRI analysis for disease differentiation, improving interpretability in connectome applications, and reducing measurement errors to lower sample size requirements. Critically, this work will facilitate the more widespread adoption of, and build clinical trust in, deep learning based methods for diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of real-world, clinical diffusion MRI data, comparable to that achievable with high-quality research acquisitions.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation</title>
<link>https://arxiv.org/abs/2508.11803</link>
<guid>https://arxiv.org/abs/2508.11803</guid>
<content:encoded><![CDATA[
arXiv:2508.11803v3 Announce Type: replace 
Abstract: This study investigates whether second-order geometric cues - planar curvature magnitude, curvature sign, and gradient orientation - are sufficient on their own to drive a multilayer perceptron (MLP) classifier for handwritten character recognition (HCR), offering an alternative to convolutional neural networks (CNNs). Using these three handcrafted feature maps as inputs, our curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89 percent on EMNIST letters. These results underscore the discriminative power of curvature-based representations for handwritten character images and demonstrate that the advantages of deep learning can be realized even with interpretable, hand-engineered features.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sobel-Gradient MLP Baseline for Handwritten Character Recognition</title>
<link>https://arxiv.org/abs/2508.11902</link>
<guid>https://arxiv.org/abs/2508.11902</guid>
<content:encoded><![CDATA[
arXiv:2508.11902v3 Announce Type: replace 
Abstract: We revisit the classical Sobel operator to ask a simple question: Are first-order edge maps sufficient to drive an all-dense multilayer perceptron (MLP) for handwritten character recognition (HCR), as an alternative to convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory footprint and transparent features. Our findings highlight that much of the class-discriminative information in handwritten character images is already captured by first-order gradients, making edge-aware MLPs a compelling option for HCR.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset</title>
<link>https://arxiv.org/abs/2301.06375</link>
<guid>https://arxiv.org/abs/2301.06375</guid>
<content:encoded><![CDATA[
arXiv:2301.06375v2 Announce Type: replace-cross 
Abstract: Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Concepts from Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2407.14209</link>
<guid>https://arxiv.org/abs/2407.14209</guid>
<content:encoded><![CDATA[
arXiv:2407.14209v2 Announce Type: replace-cross 
Abstract: With the advancement of computer vision and natural language processing, text-to-video generation, enabled by text-to-video diffusion models, has become more prevalent. These models are trained using a large amount of data from the internet. However, the training data often contain copyrighted content, including cartoon character icons and artist styles, private portraits, and unsafe videos. Since filtering the data and retraining the model is challenging, methods for unlearning specific concepts from text-to-video diffusion models have been investigated. However, due to the high computational complexity and relative large optimization scale, there is little work on unlearning methods for text-to-video diffusion models. We propose a novel concept-unlearning method by transferring the unlearning capability of the text encoder of text-to-image diffusion models to text-to-video diffusion models. Specifically, the method optimizes the text encoder using few-shot unlearning, where several generated images are used. We then use the optimized text encoder in text-to-video diffusion models to generate videos. Our method costs low computation resources and has small optimization scale. We discuss the generated videos after unlearning a concept. The experiments demonstrates that our method can unlearn copyrighted cartoon characters, artist styles, objects and people's facial characteristics. Our method can unlearn a concept within about 100 seconds on an RTX 3070. Since there was no concept unlearning method for text-to-video diffusion models before, we make concept unlearning feasible and more accessible in the text-to-video domain.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking</title>
<link>https://arxiv.org/abs/2412.08321</link>
<guid>https://arxiv.org/abs/2412.08321</guid>
<content:encoded><![CDATA[
arXiv:2412.08321v3 Announce Type: replace-cross 
Abstract: Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multimodal dataset for understanding the impact of mobile phones on remote online virtual education</title>
<link>https://arxiv.org/abs/2412.14195</link>
<guid>https://arxiv.org/abs/2412.14195</guid>
<content:encoded><![CDATA[
arXiv:2412.14195v3 Announce Type: replace-cross 
Abstract: This work presents the IMPROVE dataset, a multimodal resource designed to evaluate the effects of mobile phone usage on learners during online education. It includes behavioral, biometric, physiological, and academic performance data collected from 120 learners divided into three groups with different levels of phone interaction, enabling the analysis of the impact of mobile phone usage and related phenomena such as nomophobia. A setup involving 16 synchronized sensors-including EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics-was used to monitor learner activity during 30-minute sessions involving educational videos, document reading, and multiple-choice tests. Mobile phone usage events, including both controlled interventions and uncontrolled interactions, were labeled by supervisors and refined through a semi-supervised re-labeling process. Technical validation confirmed signal quality, and statistical analyses revealed biometric changes associated with phone usage. The dataset is publicly available for research through GitHub and Science Data Bank, with synchronized recordings from three platforms (edBB, edX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and accompanied by a detailed guide.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality</title>
<link>https://arxiv.org/abs/2501.10977</link>
<guid>https://arxiv.org/abs/2501.10977</guid>
<content:encoded><![CDATA[
arXiv:2501.10977v3 Announce Type: replace-cross 
Abstract: This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (for example, textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application</title>
<link>https://arxiv.org/abs/2503.06989</link>
<guid>https://arxiv.org/abs/2503.06989</guid>
<content:encoded><![CDATA[
arXiv:2503.06989v4 Announce Type: replace-cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal content. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on input image to maximize jailbreak probability, and further enhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To counteract attacks, we also propose Jailbreak-Probability-based Finetuning (JPF), which minimizes jailbreak probability through MLLM parameter updates. Extensive experiments show that (1) (M)JPA yields significant improvements when attacking a wide range of models under both white and black box settings. (2) JPF vastly reduces jailbreaks by at most over 60\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation</title>
<link>https://arxiv.org/abs/2503.16848</link>
<guid>https://arxiv.org/abs/2503.16848</guid>
<content:encoded><![CDATA[
arXiv:2503.16848v2 Announce Type: replace-cross 
Abstract: Despite advances in indoor 3D scene layout generation, synthesizing scenes with dense object arrangements remains challenging. Existing methods focus on large furniture while neglecting smaller objects, resulting in unrealistically empty scenes. Those that place small objects typically do not honor arrangement specifications, resulting in largely random placement not following the text description. We present Hierarchical Scene Motifs (HSM): a hierarchical framework for indoor scene generation with dense object arrangements across spatial scales. Indoor scenes are inherently hierarchical, with surfaces supporting objects at different scales, from large furniture on floors to smaller objects on tables and shelves. HSM embraces this hierarchy and exploits recurring cross-scale spatial patterns to generate complex and realistic scenes in a unified manner. Our experiments show that HSM outperforms existing methods by generating scenes that better conform to user input across room types and spatial configurations. Project website is available at https://3dlg-hcvc.github.io/hsm .
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Motion as Universal Representation for Robot Control</title>
<link>https://arxiv.org/abs/2505.07817</link>
<guid>https://arxiv.org/abs/2505.07817</guid>
<content:encoded><![CDATA[
arXiv:2505.07817v2 Announce Type: replace-cross 
Abstract: We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity</title>
<link>https://arxiv.org/abs/2506.23484</link>
<guid>https://arxiv.org/abs/2506.23484</guid>
<content:encoded><![CDATA[
arXiv:2506.23484v2 Announce Type: replace-cross 
Abstract: AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Escaping Plato's Cave: JAM for Aligning Independently Trained Vision and Language Models</title>
<link>https://arxiv.org/abs/2507.01201</link>
<guid>https://arxiv.org/abs/2507.01201</guid>
<content:encoded><![CDATA[
arXiv:2507.01201v5 Announce Type: replace-cross 
Abstract: Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, objectives, and architectures. The Platonic Representation Hypothesis (PRH) suggests these models may nonetheless converge toward a shared statistical model of reality. This raises a fundamental question: can we move beyond post-hoc detection of such alignment and explicitly optimize for it? We argue this challenge is most critical in fine-grained contextual distinctions-where multiple descriptions share global semantics but differ in subtle compositional details. We address this with the Joint Autoencoder Modulator (JAM), which aligns frozen unimodal models by jointly training modality-specific autoencoders with coordinated reconstruction and cross-modal alignment objectives. We systematically evaluate JAM across three design axes: (i) alignment objectives, introducing our multimodal Spread Loss that outperforms classic contrastive methods; (ii) the layer depth at which alignment is most effective; and (iii) the role of foundation model scale in representational convergence. Our findings show that JAM reliably induces alignment even across independently trained representations, offering both theoretical insight into the structure of shared semantics and practical guidance for transforming generalist unimodal foundations into specialist multimodal models.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation</title>
<link>https://arxiv.org/abs/2507.04671</link>
<guid>https://arxiv.org/abs/2507.04671</guid>
<content:encoded><![CDATA[
arXiv:2507.04671v2 Announce Type: replace-cross 
Abstract: Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at https://github.com/Applied-Machine-Learning-Lab/DANCE.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Algorithms (FakeIDet2)</title>
<link>https://arxiv.org/abs/2508.11716</link>
<guid>https://arxiv.org/abs/2508.11716</guid>
<content:encoded><![CDATA[
arXiv:2508.11716v2 Announce Type: replace-cross 
Abstract: Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations</title>
<link>https://arxiv.org/abs/2508.13647</link>
<guid>https://arxiv.org/abs/2508.13647</guid>
<content:encoded><![CDATA[
arXiv:2508.13647v2 Announce Type: replace-cross 
Abstract: This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</title>
<link>https://arxiv.org/abs/2508.14036</link>
<guid>https://arxiv.org/abs/2508.14036</guid>
<content:encoded><![CDATA[
<div> Keywords: GeoSAM2, 3D part segmentation, multi-view, 2D mask prediction, controllability <br />
Summary: <br />
GeoSAM2 is a novel framework for 3D part segmentation that uses multi-view 2D mask prediction and simple 2D prompts to guide part selection. By leveraging a shared backbone with added features for view-specific reasoning, the method achieves state-of-the-art performance without the need for text prompts or full 3D labels. The approach allows for fine-grained, part-specific control and precision in object-level part understanding. By aligning the paradigm of 3D segmentation with SAM2, the framework enables controllability and interpretable results, surpassing both slow optimization-based pipelines and fast feedforward approaches. The method showcases how interactive 2D inputs can enhance the accuracy and usability of 3D segmentation tasks. <br /> <div>
arXiv:2508.14036v2 Announce Type: replace 
Abstract: We introduce GeoSAM2, a prompt-controllable framework for 3D part segmentation that casts the task as multi-view 2D mask prediction. Given a textureless object, we render normal and point maps from predefined viewpoints and accept simple 2D prompts - clicks or boxes - to guide part selection. These prompts are processed by a shared SAM2 backbone augmented with LoRA and residual geometry fusion, enabling view-specific reasoning while preserving pretrained priors. The predicted masks are back-projected to the object and aggregated across views. Our method enables fine-grained, part-specific control without requiring text prompts, per-shape optimization, or full 3D labels. In contrast to global clustering or scale-based methods, prompts are explicit, spatially grounded, and interpretable. We achieve state-of-the-art class-agnostic performance on PartObjaverse-Tiny and PartNetE, outperforming both slow optimization-based pipelines and fast but coarse feedforward approaches. Our results highlight a new paradigm: aligning the paradigm of 3D segmentation with SAM2, leveraging interactive 2D inputs to unlock controllability and precision in object-level part understanding.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration</title>
<link>https://arxiv.org/abs/2508.19254</link>
<guid>https://arxiv.org/abs/2508.19254</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time generative drawing system, formal intent, contextual intent, vision-language models, multi-user collaboration

Summary: 
This paper introduces a novel real-time generative drawing system that combines formal intent, which encompasses the structural and stylistic attributes of a sketch, with contextual intent derived from semantic visual content analysis. Unlike traditional generative systems, this approach considers both intuitive geometric features and high-level semantic cues for unified transformation. The system utilizes a touchscreen interface and distributed inference architecture to enable low-latency, multi-stage transformations and support collaborative drawing on shared canvases. By integrating contour-preserving structural control and style- and content-aware image synthesis, the platform facilitates synchronous, co-authored visual creation among participants of varying artistic backgrounds. This redefines human-AI interaction by emphasizing co-creation and mutual enhancement in the creative process.<br /><br />Summary: <div>
arXiv:2508.19254v1 Announce Type: new 
Abstract: This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2508.19257</link>
<guid>https://arxiv.org/abs/2508.19257</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Temporal Token Fusion, robotic manipulation tasks, selective Query matrix reuse, model-agnostic.

Summary: 
Temporal Token Fusion (TTF) is introduced as a training-free technique for enhancing Vision-Language-Action (VLA) models by integrating historical and current visual representations in robotic manipulation tasks. TTF utilizes dual-dimension detection to combine grayscale pixel difference analysis and attention-based semantic relevance assessment for selective temporal token fusion. This approach improves inference quality by preventing error accumulation and exploiting coherence between consecutive frames in manipulation sequences. Experimental results on LIBERO, SimplerEnv, and real robot tasks demonstrate consistent performance enhancements, with an average increase of 4.0 percentage points on LIBERO, a relative improvement of 4.8% on SimplerEnv, and a relative improvement of 8.7% on real robot tasks. TTF is shown to be model-agnostic, working effectively across various architectures such as OpenVLA and VLA-Cache. Furthermore, the technique highlights the potential benefits of selective Query matrix reuse in attention mechanisms for computational acceleration and improved task success rates.<br /><br />Summary: <div>
arXiv:2508.19257v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation</title>
<link>https://arxiv.org/abs/2508.19289</link>
<guid>https://arxiv.org/abs/2508.19289</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised slide-quality assessment, visual-design metrics, CLIP-ViT embeddings, Isolation Forest, presentation slides 

Summary: 
The article introduces an unsupervised pipeline for assessing the quality of presentation slides by combining expert-inspired visual-design metrics with CLIP-ViT embeddings. The pipeline utilizes Isolation Forest-based anomaly scoring to evaluate slides, achieving strong correlations with human visual-quality ratings. Compared to leading vision-language models, the method outperforms in accuracy. The study demonstrates the validity of the assessment method through convergent and discriminant validity, as well as alignment with overall impressions. By incorporating low-level design cues with multimodal embeddings, the pipeline offers scalable and objective feedback on slide quality in real time. Overall, the approach provides a promising solution for automated slide-quality assessment in academic and professional settings. 

<br /><br />Summary: <div>
arXiv:2508.19289v1 Announce Type: new 
Abstract: We present an unsupervised slide-quality assessment pipeline that combines seven expert-inspired visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate presentation slides. Trained on 12k professional lecture slides and evaluated on six academic talks (115 slides), our method achieved Pearson correlations up to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual ratings, discriminant validity against speaker-delivery scores, and exploratory alignment with overall impressions. Our results show that augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling scalable, objective feedback in real time.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation</title>
<link>https://arxiv.org/abs/2508.19290</link>
<guid>https://arxiv.org/abs/2508.19290</guid>
<content:encoded><![CDATA[
<div> Range-view LiDAR segmentation, adversarial defense, lightweight, purification framework, autonomous vehicles  
Summary:  
- The article introduces an efficient model-based purification framework designed specifically for defending against adversarial attacks in 2D range-view LiDAR segmentation.  
- The framework focuses on the range-view domain and utilizes a mathematical optimization problem to create an explainable purification network.  
- It achieves strong adversarial resilience with minimal computational overhead, outperforming existing generative and adversarial training approaches.  
- Competitive performance on benchmarks demonstrates the effectiveness of the proposed method.  
- Real-world deployment on a demo vehicle shows accurate operation in practical autonomous driving scenarios.  

<br /><br />Summary: <div>
arXiv:2508.19290v1 Announce Type: new 
Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous vehicles, yet modern segmentation networks are highly susceptible to adversarial attacks that can compromise safety. Most existing defenses are designed for networks operating directly on raw 3D point clouds and rely on large, computationally intensive generative models. However, many state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D range view representations. Despite their widespread adoption, dedicated lightweight adversarial defenses for this domain remain largely unexplored. We introduce an efficient model-based purification framework tailored for adversarial defense in 2D range-view LiDAR segmentation. We propose a direct attack formulation in the range-view domain and develop an explainable purification network based on a mathematical justified optimization problem, achieving strong adversarial resilience with minimal computational overhead. Our method achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. More importantly, real-world deployment on a demo vehicle demonstrates the framework's ability to deliver accurate operation in practical autonomous driving scenarios.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</title>
<link>https://arxiv.org/abs/2508.19294</link>
<guid>https://arxiv.org/abs/2508.19294</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, object detection, contextual understanding, integration, limitations <br />
Summary: Vision-language models (VLMs) have significantly improved object detection through the fusion of natural language processing and computer vision techniques. Recent developments in large vision-language models (LVLMs) have advanced contextual reasoning, adaptability, and generalization in object detection. These models integrate visual and textual information, leading to more sophisticated object detection strategies. With their enhanced performance, LVLMs are expected to surpass traditional methods in object detection. Despite their progress, there are limitations that need to be addressed, such as complexity and adaptability issues. The review provides visualizations of LVLM effectiveness in scenarios like localization and segmentation and compares their real-time performance to traditional deep learning systems. Looking ahead, LVLMs are poised to have a transformative impact on object detection and robotic applications, with ongoing advancements and solutions proposed for future progress. <br /><br />Summary: <div>
arXiv:2508.19294v1 Announce Type: new 
Abstract: The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolutionize object detection and localization. We then explain the architectural innovations, training paradigms, and output flexibility of recent LVLMs for object detection, highlighting how they achieve advanced contextual understanding for object detection. The review thoroughly examines the approaches used in integration of visual and textual information, demonstrating the progress made in object detection using VLMs that facilitate more sophisticated object detection and localization strategies. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. Based on the review, its is expected that LVLMs will soon meet or surpass the performance of conventional methods in object detection. The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field. We conclude, based on this study, that the recent advancement in LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large VLM-based Stylized Sports Captioning</title>
<link>https://arxiv.org/abs/2508.19295</link>
<guid>https://arxiv.org/abs/2508.19295</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sports captioning, fine-tuning, Super Bowl LIX, professional sports journalism

Summary: 
A new study addresses the limitations of existing large language models (LLMs) in generating accurate and natural sports captions. Existing LLMs lack domain-specific sports jargon required for creating human-like descriptions of sports activities. The study proposes a two-level fine-tuned LVLM pipeline to enhance sports captioning. This pipeline improves F1 and BERT scores by over 8-10% and 2-10%, respectively, compared to alternative approaches. It also boasts a small runtime memory footprint and fast execution time. During Super Bowl LIX, the pipeline demonstrated practical application by generating highly accurate and stylized captions at a rapid rate of 6 images per 3-5 seconds for over 1000 images during the game. The successful implementation highlights the potential of LVLMs for enhancing sports journalism and content generation. 

<br /><br />Summary: <div>
arXiv:2508.19295v1 Announce Type: new 
Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a deluge of automated human-like systems in several domains including social media content generation, search and recommendation, healthcare prognosis, AI assistants for cognitive tasks etc. Although these systems have been successfully integrated in production; very little focus has been placed on sports, particularly accurate identification and natural language description of the game play. Most existing LLM/LVLMs can explain generic sports activities, but lack sufficient domain-centric sports' jargon to create natural (human-like) descriptions. This work highlights the limitations of existing SoTA LLM/LVLMs for generating production-grade sports captions from images in a desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to address that. The proposed pipeline yields an improvement > 8-10% in the F1, and > 2-10% in BERT score compared to alternative approaches. In addition, it has a small runtime memory footprint and fast execution time. During Super Bowl LIX the pipeline proved its practical application for live professional sports journalism; generating highly accurate and stylized captions at the rate of 6 images per 3-5 seconds for over 1000 images during the game play.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.19298</link>
<guid>https://arxiv.org/abs/2508.19298</guid>
<content:encoded><![CDATA[
<div> biometric face recognition, demographic biases, Large Vision Language Models, LVLMs, fairness

Summary: 
The article investigates demographic biases in Large Vision Language Models (LVLMs) for biometric face recognition tasks with textual token generation. Three LVLMs, LLaVA, BLIP-2, and PaliGemma, were fine-tuned and evaluated on a demographic-balanced dataset. The study revealed demographic biases in LVLMs, with PaliGemma and LLaVA showing higher disparities for Hispanic/Latino, Caucasian, and South Asian groups compared to BLIP-2. Evaluation metrics like group-specific BERTScores and the Fairness Discrepancy Rate were used to quantify performance disparities. The experimental results provide insights into the fairness and reliability of LVLMs across diverse demographic groups. The findings highlight the need for addressing demographic biases in LVLMs to ensure equitable performance in biometric face recognition tasks. 

<br /><br />Summary: <div>
arXiv:2508.19298v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities across various downstream tasks, including biometric face recognition (FR) with description. However, demographic biases remain a critical concern in FR, as these foundation models often fail to perform equitably across diverse demographic groups, considering ethnicity/race, gender, and age. Therefore, through our work DemoBias, we conduct an empirical evaluation to investigate the extent of demographic biases in LVLMs for biometric FR with textual token generation tasks. We fine-tuned and evaluated three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own generated demographic-balanced dataset. We utilize several evaluation metrics, like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify and trace the performance disparities. The experimental results deliver compelling insights into the fairness and reliability of LVLMs across diverse demographic groups. Our empirical study uncovered demographic biases in LVLMs, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably consistent. Repository: https://github.com/Sufianlab/DemoBias.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities</title>
<link>https://arxiv.org/abs/2508.19305</link>
<guid>https://arxiv.org/abs/2508.19305</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial representation learning, GeoAI, Geo2Vec, signed distance fields (SDF), neural network

Summary:
Geo2Vec is a novel spatial representation learning method for GeoAI applications, focusing on capturing shapes, locations, and spatial relationships of various geo-entities. Unlike existing methods, Geo2Vec operates directly in the original space, adaptively sampling points and encoding their signed distances to create geometry-aware representations without decomposition. A neural network trained to approximate the signed distance fields produces compact and unified representations for all geo-entity types. The method also utilizes rotation-invariant positional encoding to model high-frequency spatial variations, creating a robust embedding space for GeoAI models. Empirical results demonstrate that Geo2Vec outperforms existing methods in representing shape and location, capturing topological and distance relationships accurately, and achieving greater efficiency in real-world GeoAI applications.<br /><br />Summary: <div>
arXiv:2508.19305v1 Announce Type: new 
Abstract: Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: https://github.com/chuchen2017/GeoNeuralRepresentation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Crop Analysis through Deep Learning and Explainable AI</title>
<link>https://arxiv.org/abs/2508.19307</link>
<guid>https://arxiv.org/abs/2508.19307</guid>
<content:encoded><![CDATA[
<div> Varieties, Convolutional Neural Networks, Deep learning, Explainable artificial intelligence, Agricultural applications<br />
Summary:
Deep learning models, specifically Convolutional Neural Networks (CNN), were used to classify five rice grain varieties and diagnose rice leaf diseases in an automated approach. The models showed high accuracy in classification and disease diagnosis, with minimal misclassifications. Explainable artificial intelligence techniques like SHAP and LIME were employed to enhance model transparency by showing the influence of specific features on predictions. This study highlights the potential of deep learning in agriculture for automated crop quality inspection and disease diagnosis, offering benefits to farmers, consumers, and the agricultural economy.<br /><br /> <div>
arXiv:2508.19307v1 Announce Type: new 
Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and economic growth. Among Asian nations such as China, India, Pakistan, Thailand, Vietnam and Indonesia are leading producers of both long and short grain varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To ensure consumer satisfaction and strengthen national reputations, monitoring rice crops and grain quality is essential. Manual inspection, however, is labour intensive, time consuming and error prone, highlighting the need for automated solutions for quality control and yield improvement. This study proposes an automated approach to classify five rice grain varieties using Convolutional Neural Networks (CNN). A publicly available dataset of 75000 images was used for training and testing. Model evaluation employed accuracy, recall, precision, F1-score, ROC curves, and confusion matrices. Results demonstrated high classification accuracy with minimal misclassifications, confirming the model effectiveness in distinguishing rice varieties. In addition, an accurate diagnostic method for rice leaf diseases such as Brown Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined explainable artificial intelligence (XAI) with deep learning models including CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) revealed how specific grain and leaf features influenced predictions, enhancing model transparency and reliability. The findings demonstrate the strong potential of deep learning in agricultural applications, paving the way for robust, interpretable systems that can support automated crop quality inspection and disease diagnosis, ultimately benefiting farmers, consumers, and the agricultural economy.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax</title>
<link>https://arxiv.org/abs/2508.19312</link>
<guid>https://arxiv.org/abs/2508.19312</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial recognition, Artificial Intelligence, Privacy, Identity management, Federated learning

Summary:
Facial recognition powered by Artificial Intelligence has shown high accuracy in specific scenarios but faces challenges in privacy and identity management, especially in open-set scenarios with unknown individuals. This study presents a facial recognition system implemented in a federated learning framework tailored to open-set scenarios. The system integrates the OpenMax algorithm into federated learning to distinguish between known and unknown subjects by exchanging mean activation vectors and local distance measures. Experimental results confirm the effectiveness of the proposed solution in enhancing privacy-aware and robust facial recognition in distributed environments. With a focus on privacy and accuracy, this approach shows promise in addressing the challenges of identity management and improving the reliability of facial recognition systems. 

<br /><br />Summary: 
Facial recognition using AI has achieved high accuracy but faces challenges in privacy and handling unknown identities. This study introduces a facial recognition system in a federated learning setup for open-set scenarios. By incorporating the OpenMax algorithm, the system can differentiate between known and unknown subjects through the exchange of activation vectors and distance measures. Experimental results support the system's effectiveness in enhancing privacy and robustness in distributed environments. This approach offers a solution to privacy concerns and identity management challenges, ultimately improving the reliability of facial recognition technology. <div>
arXiv:2508.19312v1 Announce Type: new 
Abstract: Facial recognition powered by Artificial Intelligence has achieved high accuracy in specific scenarios and applications. Nevertheless, it faces significant challenges regarding privacy and identity management, particularly when unknown individuals appear in the operational context. This paper presents the design, implementation, and evaluation of a facial recognition system within a federated learning framework tailored to open-set scenarios. The proposed approach integrates the OpenMax algorithm into federated learning, leveraging the exchange of mean activation vectors and local distance measures to reliably distinguish between known and unknown subjects. Experimental results validate the effectiveness of the proposed solution, demonstrating its potential for enhancing privacy-aware and robust facial recognition in distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo, presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de personas, especialmente considerando que pueden aparecer sujetos desconocidos para el sistema que lo implementa. En este trabajo, se propone el dise\~no, implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un escenario de aprendizaje federado, orientado a conjuntos abiertos. Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para escenarios de aprendizaje federado. La propuesta emplea el intercambio de los vectores de activaci\'on promedio y distancias locales para identificar de manera eficaz tanto personas conocidas como desconocidas. Los experimentos realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated classification of natural habitats using ground-level imagery</title>
<link>https://arxiv.org/abs/2508.19314</link>
<guid>https://arxiv.org/abs/2508.19314</guid>
<content:encoded><![CDATA[
<div> Classification, terrestrial habitats, ground-level imagery, deep learning, habitat classification <br />
Summary:
This study proposes a methodology for classifying terrestrial habitats using ground-level imagery, eliminating the need for satellite data. Collaborating with Natural England, the researchers developed a deep learning model that accurately classifies 18 habitat classes defined by the 'Living England' framework. The model achieved strong performance overall, with an average F1-score of 0.61 across all classes. Distinct habitats like Bare Soil, Silt and Peat (BSSP) and Bare Sand (BS) had high F1-scores above 0.90. The model's success suggests potential for ecological monitoring and land-use planning applications. With the ease of obtaining ground-level imagery, this approach could be valuable for biodiversity conservation efforts. Additionally, the researchers created a user-friendly web application for practitioners to classify images using the model. <div>
arXiv:2508.19314v1 Announce Type: new 
Abstract: Accurate classification of terrestrial habitats is critical for biodiversity conservation, ecological monitoring, and land-use planning. Several habitat classification schemes are in use, typically based on analysis of satellite imagery with validation by field ecologists. Here we present a methodology for classification of habitats based solely on ground-level imagery (photographs), offering improved validation and the ability to classify habitats at scale (for example using citizen-science imagery). In collaboration with Natural England, a public sector organisation responsible for nature conservation in England, this study develops a classification system that applies deep learning to ground-level habitat photographs, categorising each image into one of 18 classes defined by the 'Living England' framework. Images were pre-processed using resizing, normalisation, and augmentation; re-sampling was used to balance classes in the training data and enhance model robustness. We developed and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label to each photograph. Using five-fold cross-validation, the model demonstrated strong overall performance across 18 habitat classes, with accuracy and F1-scores varying between classes. Across all folds, the model achieved a mean F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or ambiguous classes scoring lower. These findings demonstrate the potential of this approach for ecological monitoring. Ground-level imagery is readily obtained, and accurate computational methods for habitat classification based on such data have many potential applications. To support use by practitioners, we also provide a simple web application that classifies uploaded images using our model.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation</title>
<link>https://arxiv.org/abs/2508.19320</link>
<guid>https://arxiv.org/abs/2508.19320</guid>
<content:encoded><![CDATA[
<div> Autoregressive Video Generation, Interactive Control, Low Latency, Multimodal, Deep Compression
Summary:<br /><br />
The article introduces an autoregressive video generation framework that allows for interactive multimodal control and low-latency extrapolation in a streaming manner. By incorporating multimodal condition encodings such as audio, pose, and text, the framework can produce spatially and semantically coherent representations to guide the denoising process of a diffusion head. A large-scale dialogue dataset of approximately 20,000 hours is utilized for training, offering rich conversational scenarios. Additionally, a deep compression autoencoder with a significant reduction ratio assists in reducing the long-horizon inference burden of the autoregressive model. Extensive experiments demonstrate the advantages of the approach in terms of low latency, high efficiency, and fine-grained multimodal controllability in various scenarios including duplex conversation, multilingual human synthesis, and interactive world modeling. <div>
arXiv:2508.19320v1 Announce Type: new 
Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Data Hiding for ICAO-Compliant Face Images: A Survey</title>
<link>https://arxiv.org/abs/2508.19324</link>
<guid>https://arxiv.org/abs/2508.19324</guid>
<content:encoded><![CDATA[
<div> Keywords: ICAO-compliant facial images, digital watermarking, steganography, tamper-evident signals, identity verification<br />
Summary:<br />
This survey paper explores the use of digital watermarking and steganography as solutions for enhancing the security of ICAO-compliant facial images used for identity verification. These techniques allow for the embedding of tamper-evident signals directly into the image, enabling persistent verification without compromising compliance with ICAO standards. The paper provides a comprehensive analysis of state-of-the-art techniques in these areas, considering their potential benefits and limitations for applications involving ICAO-compliant images. By highlighting key trade-offs, the paper offers guidance for the secure deployment of these techniques in real-world identity systems. This research addresses the increasing concern over practices like morphing and deepfakes, which can lead to identity theft and illegal sharing of documents, by presenting alternative approaches for enhancing the security of biometric data. <div>
arXiv:2508.19324v1 Announce Type: new 
Abstract: ICAO-compliant facial images, initially designed for secure biometric passports, are increasingly becoming central to identity verification in a wide range of application contexts, including border control, digital travel credentials, and financial services. While their standardization enables global interoperability, it also facilitates practices such as morphing and deepfakes, which can be exploited for harmful purposes like identity theft and illegal sharing of identity documents. Traditional countermeasures like Presentation Attack Detection (PAD) are limited to real-time capture and offer no post-capture protection. This survey paper investigates digital watermarking and steganography as complementary solutions that embed tamper-evident signals directly into the image, enabling persistent verification without compromising ICAO compliance. We provide the first comprehensive analysis of state-of-the-art techniques to evaluate the potential and drawbacks of the underlying approaches concerning the applications involving ICAO-compliant images and their suitability under standard constraints. We highlight key trade-offs, offering guidance for secure deployment in real-world identity systems.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI</title>
<link>https://arxiv.org/abs/2508.19325</link>
<guid>https://arxiv.org/abs/2508.19325</guid>
<content:encoded><![CDATA[
<div> Keywords: MACE prediction, cardiovascular prognosis, cardiac MRI, survival analysis, deep learning

Summary:
PRISM is a novel framework that integrates non-contrast cardiac MRI images and electronic health records for accurate major adverse cardiac event (MACE) prediction. By extracting synchronized imaging features and using medically informed prompts, PRISM outperforms traditional prediction models and deep learning baselines in survival analysis. Three distinct imaging signatures linked to increased MACE risk are identified, along with key clinical factors such as hypertension, diabetes, and smoking. The combined imaging and EHR representations offer valuable insights into cardiac risk across diverse patient cohorts. This approach sheds light on specific cardiac abnormalities, such as lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole, while attributing risk to common clinical conditions. <div>
arXiv:2508.19325v1 Announce Type: new 
Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis. We present PRISM (Prompt-guided Representation Integration for Survival Modeling), a self-supervised framework that integrates visual representations from non-contrast cardiac cine magnetic resonance imaging with structured electronic health records (EHRs) for survival analysis. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and modulates them using medically informed textual prompts to enable fine-grained risk prediction. Across four independent clinical cohorts, PRISM consistently surpasses classical survival prediction models and state-of-the-art (SOTA) deep learning baselines under internal and external validation. Further clinical findings demonstrate that the combined imaging and EHR representations derived from PRISM provide valuable insights into cardiac risk across diverse cohorts. Three distinct imaging signatures associated with elevated MACE risk are uncovered, including lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole. Prompt-guided attribution further identifies hypertension, diabetes, and smoking as dominant contributors among clinical and physiological EHR factors.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2508.19349</link>
<guid>https://arxiv.org/abs/2508.19349</guid>
<content:encoded><![CDATA[
<div> MRI, Alzheimer's Disease, Mild Cognitive Impairment, Convolutional Neural Network, Vision Transformer

Summary:
EffNetViTLoRA is a model designed for Alzheimer's disease (AD) diagnosis using MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. It combines a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture local and global features from MRI images for accurate diagnosis of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN) individuals. The model is trained on the full T1-weighted MRI dataset from ADNI, enhancing its clinical reliability. Low-Rank Adaptation (LoRA) is used to adapt the pretrained ViT model to the target domain, improving knowledge transfer and reducing overfitting. The model achieves high classification accuracy and F1-score across the three diagnostic categories, making it a robust tool for early diagnosis and management of AD progression. 

<br /><br />Summary: <div>
arXiv:2508.19349v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage</title>
<link>https://arxiv.org/abs/2508.19477</link>
<guid>https://arxiv.org/abs/2508.19477</guid>
<content:encoded><![CDATA[
<div> computer-vision, artificial intelligence, player tracking software, accuracy, resolution
Summary:<br /><br />This study assessed the accuracy of commercially available computer-vision and artificial intelligence player tracking software using broadcast footage from a match at the 2022 FIFA World Cup. Three providers were compared to a high-definition multi-camera tracking system, evaluating player position, speed, and distance. Results showed position RMSE ranging from 1.68 to 16.39 m and speed RMSE ranging from 0.34 to 2.38 m/s. Mean bias for total match distance varied from -21.8% to 24.3% across providers. The study found that software accuracy depends on player detection, recommending the use of a tactical feed for tracking. It also suggests that both 720p and 1080p resolutions are suitable for accurate tracking when employing appropriate computer-vision and AI models. Overall, the software offers the potential for precise player tracking when players are correctly detected by the system. <br /><br /> <div>
arXiv:2508.19477v1 Announce Type: new 
Abstract: This study aimed to: (1) understand whether commercially available computer-vision and artificial intelligence (AI) player tracking software can accurately measure player position, speed and distance using broadcast footage and (2) determine the impact of camera feed and resolution on accuracy. Data were obtained from one match at the 2022 Qatar Federation Internationale de Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds were used. Three commercial tracking providers that use computer-vision and AI participated. Providers analysed instantaneous position (x, y coordinates) and speed (m\,s^{-1}) of each player. Their data were compared with a high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to 16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across providers. Computer-vision and AI player tracking software offer the ability to track players with fair precision when players are detected by the software. Providers should use a tactical feed when tracking position and speed, which will maximise player detection, improving accuracy. Both 720p and 1080p resolutions are suitable, assuming appropriate computer-vision and AI models are implemented.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JVLGS: Joint Vision-Language Gas Leak Segmentation</title>
<link>https://arxiv.org/abs/2508.19485</link>
<guid>https://arxiv.org/abs/2508.19485</guid>
<content:encoded><![CDATA[
<div> Keywords: Gas leak detection, vision-based techniques, Joint Vision-Language Gas leak Segmentation, post-processing, supervised and few-shot learning.

Summary:
Gas leaks are a serious threat to human health and the environment, but current detection methods are ineffective. The Joint Vision-Language Gas leak Segmentation (JVLGS) framework combines visual and textual modalities to improve gas leak representation and segmentation. By addressing the sporadic nature of gas leaks and implementing a post-processing step to reduce false positives, JVLGS outperforms existing gas leak segmentation methods. The model performs well in both supervised and few-shot learning settings, showcasing consistent strong performance. The code for JVLGS is publicly available for further experimentation and development. <div>
arXiv:2508.19485v1 Announce Type: new 
Abstract: Gas leaks pose serious threats to human health and contribute significantly to atmospheric pollution, drawing increasing public concern. However, the lack of effective detection methods hampers timely and accurate identification of gas leaks. While some vision-based techniques leverage infrared videos for leak detection, the blurry and non-rigid nature of gas clouds often limits their effectiveness. To address these challenges, we propose a novel framework called Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the complementary strengths of visual and textual modalities to enhance gas leak representation and segmentation. Recognizing that gas leaks are sporadic and many video frames may contain no leak at all, our method incorporates a post-processing step to reduce false positives caused by noise and non-target objects, an issue that affects many existing approaches. Extensive experiments conducted across diverse scenarios show that JVLGS significantly outperforms state-of-the-art gas leak segmentation methods. We evaluate our model under both supervised and few-shot learning settings, and it consistently achieves strong performance in both, whereas competing methods tend to perform well in only one setting or poorly in both. Code available at: https://github.com/GeekEagle/JVLGS
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models</title>
<link>https://arxiv.org/abs/2508.19498</link>
<guid>https://arxiv.org/abs/2508.19498</guid>
<content:encoded><![CDATA[
<div> Framework, Knowledge transfer, Deep learning, Pre-trained models, Object recognition

Summary: 
The paper introduces UNIFORM, a framework for transferring knowledge from a diverse set of pre-trained models into a single student model without constraints. It utilizes a voting mechanism to capture knowledge consensus at both the logit and feature levels, enhancing unsupervised object recognition performance. UNIFORM outperforms existing methods and is highly scalable, benefiting from over one hundred teacher models compared to smaller scales. The framework allows for effective integration of diverse model interpretations, resulting in universal and generalizable knowledge transfer for improved recognition tasks. <div>
arXiv:2508.19498v1 Announce Type: new 
Abstract: In the era of deep learning, the increasing number of pre-trained models available online presents a wealth of knowledge. These models, developed with diverse architectures and trained on varied datasets for different tasks, provide unique interpretations of the real world. Their collective consensus is likely universal and generalizable to unseen data. However, effectively harnessing this collective knowledge poses a fundamental challenge due to the heterogeneity of pre-trained models. Existing knowledge integration solutions typically rely on strong assumptions about training data distributions and network architectures, limiting them to learning only from specific types of models and resulting in data and/or inductive biases. In this work, we introduce a novel framework, namely UNIFORM, for knowledge transfer from a diverse set of off-the-shelf models into one student model without such constraints. Specifically, we propose a dedicated voting mechanism to capture the consensus of knowledge both at the logit level -- incorporating teacher models that are capable of predicting target classes of interest -- and at the feature level, utilizing visual representations learned on arbitrary label spaces. Extensive experiments demonstrate that UNIFORM effectively enhances unsupervised object recognition performance compared to strong knowledge transfer baselines. Notably, it exhibits remarkable scalability by benefiting from over one hundred teachers, while existing methods saturate at a much smaller scale.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery</title>
<link>https://arxiv.org/abs/2508.19499</link>
<guid>https://arxiv.org/abs/2508.19499</guid>
<content:encoded><![CDATA[
<div> Keywords: urban mobility analysis, OD flow matrices, satellite imagery, diffusion-based framework, structural coherence<br />
Summary:<br />
The article introduces Sat2Flow, a novel framework for generating structurally coherent Origin-Destination (OD) flow matrices using only satellite imagery. This approach addresses two major limitations in existing methods: reliance on costly auxiliary features and sensitivity to spatial topology. Sat2Flow utilizes a multi-kernel encoder to capture regional interactions and a permutation-aware diffusion process to align latent representations across different regional orderings, ensuring topological robustness. By leveraging a joint contrastive training objective and equivariant diffusion training, the framework outperforms physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures. Sat2Flow offers a scalable solution for OD flow generation in data-scarce urban environments, eliminating the need for region-specific auxiliary data and maintaining structural invariance for robust mobility modeling.<br /> 
Summary: <div>
arXiv:2508.19499v1 Announce Type: new 
Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility analysis, underpinning applications in traffic forecasting, infrastructure planning, and policy design. However, existing methods suffer from two critical limitations: (1) reliance on auxiliary features (e.g., Points of Interest, socioeconomic statistics) that are costly to collect and have limited spatial coverage; and (2) sensitivity to spatial topology, where minor index reordering of urban regions (e.g., census tract relabeling) disrupts structural coherence in generated flows. To address these challenges, we propose Sat2Flow, a latent structure-aware diffusion-based framework that generates structurally coherent OD flows using solely satellite imagery as input. Our approach introduces a multi-kernel encoder to capture diverse regional interactions and employs a permutation-aware diffusion process that aligns latent representations across different regional orderings. Through a joint contrastive training objective that bridges satellite-derived features with OD patterns, combined with equivariant diffusion training that enforces structural consistency, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experimental results on real-world urban datasets demonstrate that Sat2Flow outperforms both physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce urban environments, eliminating region-specific auxiliary data dependencies while maintaining structural invariance for robust mobility modeling.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity</title>
<link>https://arxiv.org/abs/2508.19511</link>
<guid>https://arxiv.org/abs/2508.19511</guid>
<content:encoded><![CDATA[
<div> Keywords: invasive weeds, deep learning, semi-supervised framework, diagnostic-driven, precision agriculture

Summary: 
This study addresses the challenges faced by deep learning models in managing invasive weeds in agriculture due to difficult environmental conditions and high data annotation costs. By analyzing a dataset of labeled and unlabeled images of Guinea Grass in sugarcane fields, the study establishes strong supervised models for classification and detection, uncovering a pervasive "shadow bias" where models incorrectly identify shadows as vegetation. Leveraging this insight, the study develops a semi-supervised pipeline that uses unlabeled data to enhance model robustness and reduce the shadow bias, resulting in improved recall rates critical for automated weed management systems. Validation on a public crop-weed benchmark demonstrates the effectiveness of the methodology in a low-data setting, providing a field-tested framework for developing and improving computer vision systems for precision agriculture.<br /><br />Summary: <div>
arXiv:2508.19511v1 Announce Type: new 
Abstract: The automated management of invasive weeds is critical for sustainable agriculture, yet the performance of deep learning models in real-world fields is often compromised by two factors: challenging environmental conditions and the high cost of data annotation. This study tackles both issues through a diagnostic-driven, semi-supervised framework. Using a unique dataset of approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in sugarcane, we first establish strong supervised baselines for classification (ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by interpretability tools, uncovered a pervasive "shadow bias," where models learned to misidentify shadows as vegetation. This diagnostic insight motivated our primary contribution: a semi-supervised pipeline that leverages unlabeled data to enhance model robustness. By training models on a more diverse set of visual information through pseudo-labeling, this framework not only helps mitigate the shadow bias but also provides a tangible boost in recall, a critical metric for minimizing weed escapes in automated spraying systems. To validate our methodology, we demonstrate its effectiveness in a low-data regime on a public crop-weed benchmark. Our work provides a clear and field-tested framework for developing, diagnosing, and improving robust computer vision systems for the complex realities of precision agriculture.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment</title>
<link>https://arxiv.org/abs/2508.19527</link>
<guid>https://arxiv.org/abs/2508.19527</guid>
<content:encoded><![CDATA[
<div> Keywords: Motion generation, Aligned Preference Optimization, MotionFLUX, real-time synthesis, semantic consistency

Summary: 
TAPO introduces a novel framework for motion generation that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments for semantic grounding. MotionFLUX is a high-speed generation framework that utilizes deterministic rectified flow matching to facilitate real-time synthesis by constructing optimal transport paths between noise distributions and motion spaces. This approach accelerates inference time without compromising motion quality, as it reduces the need for multi-step sampling. Experimental results show that the unified system of TAPO and MotionFLUX surpasses existing methods in both semantic consistency and motion quality while significantly improving generation speed. The code and pretrained models will be made available for further research and development.<br /><br />Summary: <div>
arXiv:2508.19527v1 Announce Type: new 
Abstract: Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2508.19542</link>
<guid>https://arxiv.org/abs/2508.19542</guid>
<content:encoded><![CDATA[
<div> Benchmark, Multimodal large language models, Cross-video relational reasoning, Evaluation, Architectural insights <br />
Summary: <br />
The paper introduces CVBench, a benchmark designed to evaluate cross-video relational reasoning in multimodal large language models (MLLMs). The benchmark consists of 1,000 question-answer pairs across three tiers: cross-video object association, cross-video event association, and cross-video complex reasoning. It challenges models to synthesize information across different video contexts. The evaluation of 10+ leading MLLMs reveals performance gaps, especially in causal reasoning tasks where even top models fall short of human performance. The analysis uncovers deficiencies in inter-video context retention and entity disambiguation in current MLLM architectures. The benchmark provides a framework for diagnosing and improving multi-video reasoning, offering insights for next-generation MLLMs. The data and evaluation code are available on GitHub for further research and development. <br /> <div>
arXiv:2508.19542v1 Announce Type: new 
Abstract: While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs.The data and evaluation code are available at https://github.com/Hokhim2/CVBench.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization</title>
<link>https://arxiv.org/abs/2508.19544</link>
<guid>https://arxiv.org/abs/2508.19544</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, gaze estimation, web-based, lightweight models, real-time

Summary:
WebEyeTrack is a new framework that addresses the gap between AI-based gaze estimation methods and commercial eye-tracking solutions by integrating lightweight SOTA models directly in the browser. This framework includes model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples. WebEyeTrack can adapt to new users and achieve SOTA performance with an error margin of 2.32 cm on GazeCapture. Additionally, it offers real-time inference speeds of 2.4 milliseconds on an iPhone 14. By providing open-source code, WebEyeTrack aims to bridge the gap in accuracy and efficiency between existing gaze estimation methods and commercial eye-tracking solutions. The framework focuses on addressing factors like model size, inference time, and privacy concerns, making it a promising solution for web-based gaze estimation applications. 

<br /><br />Summary: <div>
arXiv:2508.19544v1 Announce Type: new 
Abstract: With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at https://github.com/RedForestAi/WebEyeTrack.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery</title>
<link>https://arxiv.org/abs/2508.19555</link>
<guid>https://arxiv.org/abs/2508.19555</guid>
<content:encoded><![CDATA[
<div> 2.5D reliefs, MonoRelief V2, real data, pseudo real images, depth predictions <br />
<br />
Summary: 
MonoRelief V2 is an end-to-end model that directly recovers 2.5D reliefs from single images, incorporating real data to improve robustness, accuracy, and efficiency compared to its predecessor. To address the challenge of acquiring a large-scale real-world dataset, the model uses approximately 15,000 pseudo real images generated through a text-to-image generative model and derives depth pseudo-labels by fusing depth and normal predictions. A small-scale real-world dataset of 800 samples is created through multi-view reconstruction and detail refinement. MonoRelief V2 is trained progressively on both the pseudo-real and real-world datasets and exhibits state-of-the-art performance in depth and normal predictions, showcasing its potential for various downstream applications. The code for MonoRelief V2 is available at https://github.com/glp1001/MonoreliefV2. <div>
arXiv:2508.19555v1 Announce Type: new 
Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly recovering 2.5D reliefs from single images under complex material and illumination variations. In contrast to its predecessor, MonoRelief V1 [1], which was solely trained on synthetic data, MonoRelief V2 incorporates real data to achieve improved robustness, accuracy and efficiency. To overcome the challenge of acquiring large-scale real-world dataset, we generate approximately 15,000 pseudo real images using a text-to-image generative model, and derive corresponding depth pseudo-labels through fusion of depth and normal predictions. Furthermore, we construct a small-scale real-world dataset (800 samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is then progressively trained on the pseudo-real and real-world datasets. Comprehensive experiments demonstrate its state-of-the-art performance both in depth and normal predictions, highlighting its strong potential for a range of downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection</title>
<link>https://arxiv.org/abs/2508.19565</link>
<guid>https://arxiv.org/abs/2508.19565</guid>
<content:encoded><![CDATA[
<div> Keywords: End-to-end object detectors, FlowDet, Geometric Deformable Unit, Scale-Aware Attention, Intersection-Flow-5k dataset <br />
Summary:
FlowDet is introduced as a high-speed object detector using a decoupled encoder optimization strategy within the DETR architecture. It incorporates a Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to handle extreme scale variations. The Intersection-Flow-5k dataset was created to evaluate the model in scenarios with severe occlusion and high object density. FlowDet outperforms the RT-DETR baseline on this dataset, improving AP(test) by 1.5% and AP50(test) by 1.6% while reducing computational complexity by 63.2% and increasing inference speed by 16.2%. This work showcases a promising approach to developing efficient and accurate object detectors for real-world perception systems. The Intersection-Flow-5k dataset is publicly available for research purposes. <br /><br />Summary: <div>
arXiv:2508.19565v1 Announce Type: new 
Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time applications, yet their high computational cost remains a significant barrier, particularly for complex scenarios like intersection traffic monitoring. To address this challenge, we propose FlowDet, a high-speed detector featuring a decoupled encoder optimization strategy applied to the DETR architecture. Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to maintain high representational power across extreme scale variations. To rigorously evaluate the model's performance in environments with severe occlusion and high object density, we collected the Intersection-Flow-5k dataset, a new challenging scene for this task. Evaluated on Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by 1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference speed by 16.2%. Our work demonstrates a new path towards building highly efficient and accurate detectors for demanding, real-world perception systems. The Intersection-Flow-5k dataset is available at https://github.com/AstronZh/Intersection-Flow-5K.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.19573</link>
<guid>https://arxiv.org/abs/2508.19573</guid>
<content:encoded><![CDATA[
<div> trainable encoder, prototype-guided reconstruction, diversity-aware alignment loss, anomaly detection, medical images

Summary:
The article introduces a new framework for anomaly detection in medical images, addressing challenges such as limited annotations and domain gaps. The proposed framework combines a trainable encoder with prototype-guided reconstruction and a Diversity-Aware Alignment Loss. The trainable encoder, with a momentum branch, allows for stable domain-adaptive feature learning. A Prototype Extractor mines normal prototypes to guide reconstruction, enhancing localization accuracy. The novel loss function prevents prototype collapse by enforcing balanced use and diversity constraints. Experimental results on medical imaging benchmarks demonstrate improved representation quality and anomaly localization compared to existing methods. Visualizations and prototype assignment analyses confirm the effectiveness of the anti-collapse mechanism and enhanced interpretability in the framework. 

<br /><br />Summary: <div>
arXiv:2508.19573v1 Announce Type: new 
Abstract: Anomaly detection in medical images is challenging due to limited annotations and a domain gap compared to natural images. Existing reconstruction methods often rely on frozen pre-trained encoders, which limits adaptation to domain-specific features and reduces localization accuracy. Prototype-based learning offers interpretability and clustering benefits but suffers from prototype collapse, where few prototypes dominate training, harming diversity and generalization. To address this, we propose a unified framework combining a trainable encoder with prototype-guided reconstruction and a novel Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum branch, enables stable domain-adaptive feature learning. A lightweight Prototype Extractor mines informative normal prototypes to guide the decoder via attention for precise reconstruction. Our loss enforces balanced prototype use through diversity constraints and per-prototype normalization, effectively preventing collapse. Experiments on multiple medical imaging benchmarks show significant improvements in representation quality and anomaly localization, outperforming prior methods. Visualizations and prototype assignment analyses further validate the effectiveness of our anti-collapse mechanism and enhanced interpretability.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</title>
<link>https://arxiv.org/abs/2508.19574</link>
<guid>https://arxiv.org/abs/2508.19574</guid>
<content:encoded><![CDATA[
<div> Keywords: pathological image segmentation, multimodal prototype-guided supervision, semi-supervised learning, deep learning, contrastive learning

Summary:
MPAMatch is a novel segmentation framework for pathological image segmentation that utilizes pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. It introduces a dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, enhancing both structural and semantic modeling. By incorporating text prototype supervision into segmentation, MPAMatch significantly improves semantic boundary modeling, addressing the limitations of perturbation-based consistency methods. The framework reconstructs the TransUNet architecture by replacing its ViT backbone with a pathology-pretrained foundation model, enabling more effective feature extraction for pathology images. Extensive experiments on various datasets demonstrate MPAMatch's superiority over state-of-the-art methods in structural and semantic modeling, highlighting its ability to improve segmentation accuracy and capture high-level semantic priors.<br /><br />Summary: <div>
arXiv:2508.19574v1 Announce Type: new 
Abstract: Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interact-Custom: Customized Human Object Interaction Image Generation</title>
<link>https://arxiv.org/abs/2508.19575</link>
<guid>https://arxiv.org/abs/2508.19575</guid>
<content:encoded><![CDATA[
<div> customized image generation, human object interaction, interaction control, identity preservation, spatial configuration<br />
<br />
Summary: <br />
The article introduces the task of Customized Human Object Interaction Image Generation (CHOI), focusing on preserving the identity of the target human object while controlling the interaction semantics between them. The challenges of this task include decomposing human objects into identity and interaction features and ensuring appropriate spatial configuration. The proposed Interact-Custom model addresses these challenges by first generating a foreground mask to depict interaction behavior and then generating the target human object with preserved identity features. Users can also specify background images and target locations for added content control. Experimental results demonstrate the effectiveness of the approach in achieving compositional customized image generation with fine-grained interaction control. <div>
arXiv:2508.19575v1 Announce Type: new 
Abstract: Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application.Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities.To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them.Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics.To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses.Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features.Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Speed FHD Full-Color Video Computer-Generated Holography</title>
<link>https://arxiv.org/abs/2508.19579</link>
<guid>https://arxiv.org/abs/2508.19579</guid>
<content:encoded><![CDATA[
<div> Learning-based models, Computer-generated holography, Holographic video, Spectrum-guided depth division multiplexing, Spatial-temporal correlations<br />
<br />
Summary: Computer-generated holography (CGH) for high-speed, high-quality holographic video faces challenges of phase smoothing and computational efficiency. The proposed solution addresses these issues through Spectrum-Guided Depth Division Multiplexing (SGDDM) technique which optimizes phase distributions using frequency modulation for high-fidelity full-color display at high frame rates. Additionally, a lightweight asymmetric Mamba-Unet architecture named HoloMamba is introduced to enhance reconstruction quality and computational efficiency by explicitly modeling spatial-temporal correlations in video sequences. SGDDM achieves high-fidelity full-color display without compromising frame rate, while HoloMamba generates FHD full-color holographic video at over 260 FPS, surpassing the previous state-of-the-art strategy by 2.6 times. <div>
arXiv:2508.19579v1 Announce Type: new 
Abstract: Computer-generated holography (CGH) is a promising technology for next-generation displays. However, generating high-speed, high-quality holographic video requires both high frame rate display and efficient computation, but is constrained by two key limitations: ($i$) Learning-based models often produce over-smoothed phases with narrow angular spectra, causing severe color crosstalk in high frame rate full-color displays such as depth-division multiplexing and thus resulting in a trade-off between frame rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods typically optimize frames independently, neglecting spatial-temporal correlations between consecutive frames and leading to computationally inefficient solutions. To overcome these challenges, in this paper, we propose a novel high-speed full-color video CGH generation scheme. First, we introduce Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase distributions via frequency modulation, enabling high-fidelity full-color display at high frame rates. Second, we present HoloMamba, a lightweight asymmetric Mamba-Unet architecture that explicitly models spatial-temporal correlations across video sequences to enhance reconstruction quality and computational efficiency. Extensive simulated and real-world experiments demonstrate that SGDDM achieves high-fidelity full-color display without compromise in frame rate, while HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior state-of-the-art Divide-Conquer-and-Merge Strategy.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction</title>
<link>https://arxiv.org/abs/2508.19581</link>
<guid>https://arxiv.org/abs/2508.19581</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, images, videos, noise detection, generative capabilities

Summary:<br /><br />
Diffusion models are popular for image and video synthesis, but datasets may contain labeling errors. This study introduces Score-based Discriminator Correction (SBDC) to align noisy pre-trained diffusion models by using discriminator training with adversarial loss. The method assesses sample authenticity using noise detection techniques and is applied in the early generation phase for improved performance. SBDC is computationally efficient, has minimal impact on inference time, and does not require model retraining. Experiments show superior results compared to existing methods on various noise settings. <div>
arXiv:2508.19581v1 Announce Type: new 
Abstract: Diffusion models have gained prominence as state-of-the-art techniques for synthesizing images and videos, particularly due to their ability to scale effectively with large datasets. Recent studies have uncovered that these extensive datasets often contain mistakes from manual labeling processes. However, the extent to which such errors compromise the generative capabilities and controllability of diffusion models is not well studied. This paper introduces Score-based Discriminator Correction (SBDC), a guidance technique for aligning noisy pre-trained conditional diffusion models. The guidance is built on discriminator training using adversarial loss, drawing on prior noise detection techniques to assess the authenticity of each sample. We further show that limiting the usage of our guidance to the early phase of the generation process leads to better performance. Our method is computationally efficient, only marginally increases inference time, and does not require retraining diffusion models. Experiments on different noise settings demonstrate the superiority of our method over previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.19593</link>
<guid>https://arxiv.org/abs/2508.19593</guid>
<content:encoded><![CDATA[
<div> Keywords: Monocular 3D object detection, occlusion robustness, generalization, segmentation-based approach, extrapolation 

Summary: 
Monocular 3D object detection (Mono3D) is crucial for various computer vision applications like autonomous driving and robotics. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios by proposing innovative solutions. To enhance occlusion robustness, a mathematically differentiable NMS (GrooMeD-NMS) is introduced. Depth equivariant (DEVIANT) backbones are explored to improve generalization to new datasets. The issue of large object detection is addressed by introducing a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Additionally, the thesis presents a mathematical analysis of the extrapolation of Mono3D models to unseen camera heights and proposes methods to improve Mono3D generalization in out-of-distribution settings. <div>
arXiv:2508.19593v1 Announce Type: new 
Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Robustness to Input Degradations for Object Detection</title>
<link>https://arxiv.org/abs/2508.19600</link>
<guid>https://arxiv.org/abs/2508.19600</guid>
<content:encoded><![CDATA[
<div> quantization, object detection, YOLO, model robustness, calibration <br />
Summary:<br />
Post-training quantization (PTQ) is essential for deploying efficient object detection models like YOLO on resource-constrained devices. This study evaluated the robustness of YOLO models across various precision formats and degradation conditions. While Static INT8 TensorRT engines offer significant speedups with a moderate accuracy drop on clean data, the proposed degradation-aware calibration did not consistently improve robustness. Larger model scales showed potential improvements under specific noise conditions, indicating the influence of model capacity on the calibration approach effectiveness. These findings underscore the challenges in enhancing PTQ robustness and provide valuable insights for deploying quantized detectors in uncontrolled environments.<br /> <div>
arXiv:2508.19600v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at https://github.com/AllanK24/QRID.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2508.19604</link>
<guid>https://arxiv.org/abs/2508.19604</guid>
<content:encoded><![CDATA[
<div> Diffusion models, semantic segmentation, domain generalization, inverse evolution layers, data augmentation<br />
<br />
Summary: Domain Generalized Semantic Segmentation (DGSS) aims to improve model generalization by training on source domain data and generalizing to unseen domains. The article introduces inverse evolution layers (IELs) to enhance the generative process, reducing defects in synthetic data used for training. They propose IELDM, a data augmentation framework incorporating IELs for higher-quality image generation. By embedding IELs in the segmentation network, IELFormer improves generalization in cross-domain scenarios and suppresses artifact propagation. Additionally, IELFormer utilizes a multi-scale frequency fusion (MFF) module to enhance semantic consistency across scales through frequency-domain analysis. Extensive experiments demonstrate superior generalization performance compared to existing methods. <div>
arXiv:2508.19604v1 Announce Type: new 
Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model using labeled data from a source domain, with the goal of achieving robust generalization to unseen target domains during inference. A common approach to improve generalization is to augment the source domain with synthetic data generated by diffusion models (DMs). However, the generated images often contain structural or semantic defects due to training imperfections. Training segmentation models with such flawed data can lead to performance degradation and error accumulation. To address this issue, we propose to integrate inverse evolution layers (IELs) into the generative process. IELs are designed to highlight spatial discontinuities and semantic inconsistencies using Laplacian-based priors, enabling more effective filtering of undesirable generative patterns. Based on this mechanism, we introduce IELDM, an enhanced diffusion-based data augmentation framework that can produce higher-quality images. Furthermore, we observe that the defect-suppression capability of IELs can also benefit the segmentation network by suppressing artifact propagation. Based on this insight, we embed IELs into the decoder of the DGSS model and propose IELFormer to strengthen generalization capability in cross-domain scenarios. To further strengthen the model's semantic consistency across scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module, which performs frequency-domain analysis to achieve structured integration of multi-resolution features, thereby improving cross-scale coherence. Extensive experiments on benchmark datasets demonstrate that our approach achieves superior generalization performance compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model</title>
<link>https://arxiv.org/abs/2508.19626</link>
<guid>https://arxiv.org/abs/2508.19626</guid>
<content:encoded><![CDATA[
<div> Lesion measurement, lesion type labels, skin image synthesis, deep-learning models, LF-VAR <br />
Summary: <br />
The article introduces LF-VAR, a model that uses lesion measurement scores and lesion type labels to generate high-quality synthetic skin images for deep-learning model training. By combining a VQVAE for structured tokenization and a VAR Transformer for image synthesis, LF-VAR allows for controlled generation of skin images based on specific lesion characteristics guided by language prompts. The integration of lesion measurement and type as conditional embeddings enhances the fidelity of the synthesis process. The model outperforms the previous state-of-the-art in generating synthetic skin images with specific lesion types, achieving the best overall FID score. The study demonstrates the effectiveness of the LF-VAR framework in producing clinically relevant and controllable synthetic skin images, thus addressing the limitations of existing skin image synthesis methods. Code for the framework is available on GitHub for further exploration. <br /> <div>
arXiv:2508.19626v1 Announce Type: new 
Abstract: Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and types as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition</title>
<link>https://arxiv.org/abs/2508.19630</link>
<guid>https://arxiv.org/abs/2508.19630</guid>
<content:encoded><![CDATA[
<div> Keywords: long-tailed visual recognition, difficulty-aware optimization, dynamic expert collaboration, mixture-of-experts design, decentralized expert routing

Summary:
DQRoute is a novel framework designed to address the challenges of long-tailed visual recognition. By incorporating difficulty-aware optimization and dynamic expert collaboration, DQRoute can effectively handle varying classification difficulties across categories. The framework first estimates class-wise difficulty using prediction uncertainty and historical performance, guiding training with adaptive loss weighting. Utilizing a mixture-of-experts architecture, each expert specializes in a different region of the class distribution. During inference, expert predictions are weighted based on confidence scores from expert-specific out-of-distribution detectors, enabling input-adaptive routing without a centralized router. By jointly training all components in an end-to-end manner, DQRoute significantly improves performance, particularly on rare and difficult classes. This integration of difficulty modeling with decentralized expert routing showcases the benefits of DQRoute in improving long-tailed visual recognition tasks. 

<br /><br />Summary: <div>
arXiv:2508.19630v1 Announce Type: new 
Abstract: Long-tailed visual recognition is challenging not only due to class imbalance but also because of varying classification difficulty across categories. Simply reweighting classes by frequency often overlooks those that are intrinsically hard to learn. To address this, we propose \textbf{DQRoute}, a modular framework that combines difficulty-aware optimization with dynamic expert collaboration. DQRoute first estimates class-wise difficulty based on prediction uncertainty and historical performance, and uses this signal to guide training with adaptive loss weighting. On the architectural side, DQRoute employs a mixture-of-experts design, where each expert specializes in a different region of the class distribution. At inference time, expert predictions are weighted by confidence scores derived from expert-specific OOD detectors, enabling input-adaptive routing without the need for a centralized router. All components are trained jointly in an end-to-end manner. Experiments on standard long-tailed benchmarks demonstrate that DQRoute significantly improves performance, particularly on rare and difficult classes, highlighting the benefit of integrating difficulty modeling with decentralized expert routing.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</title>
<link>https://arxiv.org/abs/2508.19638</link>
<guid>https://arxiv.org/abs/2508.19638</guid>
<content:encoded><![CDATA[
<div> Point-level tokens, Collaborative perception, Point-Level Optimized Tokens, token reordering, semantic-aware token reordering module<br />
<br />
Summary: 
The article introduces a novel framework called CoPLOT for collaborative perception using point-level tokens. CoPLOT addresses the challenge of preserving detailed structural information in point-cloud data by incorporating a point-native processing pipeline. It includes a semantic-aware token reordering module, a frequency-enhanced state space model, and a neighbor-to-ego alignment module. These components work together to generate adaptive 1D reorderings, capture long-range sequence dependencies, and mitigate localization noise. Extensive experiments demonstrate that CoPLOT outperforms existing models with lower communication and computation overhead. The code for CoPLOT will be made available on GitHub for further exploration and implementation. <div>
arXiv:2508.19638v1 Announce Type: new 
Abstract: Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at https://github.com/CheeryLeeyy/CoPLOT.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2508.19647</link>
<guid>https://arxiv.org/abs/2508.19647</guid>
<content:encoded><![CDATA[
<div> Keywords: action localization, unsupervised learning, spatio-temporal graph neural network, motion dynamics, real-time analysis

Summary: 
The article introduces a lightweight and unsupervised skeleton-based action localization method for untrimmed sports videos. It utilizes an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) trained on a pose-sequence denoising task to learn motion dynamics without manual labeling. A novel Action Dynamics Metric (ADM) is defined for inferring motion boundaries by analyzing the curvature profile of ASTGCN embeddings. The method achieves a mean Average Precision (mAP) of 82.66% and an average localization latency of 29.09 ms on the DSV Diving dataset, comparable to supervised models. It demonstrates robust generalization to unseen diving footage without retraining, making it suitable for real-time action analysis in embedded or dynamic environments.<br /><br />Summary: <div>
arXiv:2508.19647v1 Announce Type: new 
Abstract: Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising</title>
<link>https://arxiv.org/abs/2508.19649</link>
<guid>https://arxiv.org/abs/2508.19649</guid>
<content:encoded><![CDATA[
<div> Keywords: image denoising, deep learning, dynamically generated kernels, noise types, noise levels

Summary:
Our study focuses on image denoising, a crucial task in computer vision, particularly in fields like photography and medical imaging. While deep learning methods have shown significant success in this area, they often struggle to generalize to unseen noise types and levels. To address this challenge, we propose a novel approach that utilizes dynamically generated kernels through efficient operations. By incorporating a Feature Extraction Module, Global Statistics, Local Correlation Modules, and Kernel Prediction Module, our method can generate pixel-wise varying kernels adapted to local image structures for denoising purposes. Despite being trained on Gaussian noise, our compact model excels in denoising images affected by various noise types and levels. This study demonstrates the efficacy of iterative dynamic filtering for practical image denoising tasks.<br /><br />Summary: <div>
arXiv:2508.19649v1 Announce Type: new 
Abstract: Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
<link>https://arxiv.org/abs/2508.19650</link>
<guid>https://arxiv.org/abs/2508.19650</guid>
<content:encoded><![CDATA[
<div> video language models, positional bias, evaluation benchmarks, Video-LevelGauge,  model performance 
Summary:
The study introduces a new benchmark called Video-LevelGauge to assess positional bias in large video language models (LVLMs). It employs standardized probes and custom contextual setups to simulate real-world scenarios, revealing significant biases in leading open-source LVLMs towards head or neighbor content preferences. Commercial models like Gemini2.5-Pro demonstrate consistent performance across entire video sequences. The benchmark consists of 438 videos with multiple-choice and open-ended questions validated to expose bias. Analysis on context length, variation, and model scale offers insights for mitigating bias and enhancing model performance. The study evaluates 27 LVLMs, highlighting the need for addressing positional bias to improve overall video understanding capabilities. <br /><br />Summary: <div>
arXiv:2508.19650v1 Announce Type: new 
Abstract: Large video language models (LVLMs) have made notable progress in video understanding, spurring the development of corresponding evaluation benchmarks. However, existing benchmarks generally assess overall performance across entire video sequences, overlooking nuanced behaviors such as contextual positional bias, a critical yet under-explored aspect of LVLM performance. We present Video-LevelGauge, a dedicated benchmark designed to systematically assess positional bias in LVLMs. We employ standardized probes and customized contextual setups, allowing flexible control over context length, probe position, and contextual types to simulate diverse real-world scenarios. In addition, we introduce a comprehensive analysis method that combines statistical measures with morphological pattern recognition to characterize bias. Our benchmark comprises 438 manually curated videos spanning multiple types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended questions, validated for their effectiveness in exposing positional bias. Based on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and open-source models. Our findings reveal significant positional biases in many leading open-source models, typically exhibiting head or neighbor-content preferences. In contrast, commercial models such as Gemini2.5-Pro show impressive, consistent performance across entire video sequences. Further analyses on context length, context variation, and model scale provide actionable insights for mitigating bias and guiding model enhancement.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Object Detection in the Car Interior With Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.19651</link>
<guid>https://arxiv.org/abs/2508.19651</guid>
<content:encoded><![CDATA[
<div> Object Detection, Localization, Interior Scene Understanding, Computational Resources, Distributed Architecture  
Summary:  
The article introduces the ODAL framework for interior scene understanding in car environments, focusing on object detection and localization. Due to limited computational resources in vehicles, the framework splits tasks between onboard and cloud systems, using vision foundation models. A new benchmark metric, ODALbench, is proposed for comprehensive assessment. Comparing the GPT-4o model with the lightweight LLaVA 1.5 7B model, ODAL-LLaVA achieves an 89% ODAL score, a 71% improvement over baseline, and outperforms GPT-4o by 20%. Fine-tuning enhances the lightweight model's performance, maintaining high detection accuracy while reducing hallucinations. The fine-tuned model achieves an ODAL signal-to-noise ratio three times higher than GPT-4o, demonstrating potential for new standards in interior scene understanding for AI applications in vehicles.  
Summary: <div>
arXiv:2508.19651v1 Announce Type: new 
Abstract: AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Rewarding Vision-Language Model via Reasoning Decomposition</title>
<link>https://arxiv.org/abs/2508.19652</link>
<guid>https://arxiv.org/abs/2508.19652</guid>
<content:encoded><![CDATA[
<div> supervised reinforcement learning, visual reasoning, vision-language models, visual perception, language reasoning
Summary:
- The paper introduces Vision-SR1, a self-rewarding method for improving visual reasoning in Vision-Language Models (VLMs) without relying on external visual supervision.
- Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning, prompting the model to generate self-contained visual perceptions before performing language reasoning.
- This self-rewarding method combines supervision on final outputs with self-contained visual perceptions, providing a balanced training signal that strengthens both visual perception and language reasoning.
- Experiments show that Vision-SR1 enhances visual reasoning, reduces visual hallucinations, and decreases dependence on language shortcuts across various vision-language tasks. 

<br /><br />Summary: <div>
arXiv:2508.19652v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications</title>
<link>https://arxiv.org/abs/2508.19654</link>
<guid>https://arxiv.org/abs/2508.19654</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, multi-output regression, energy efficiency, hardware-aware analysis, satellite image processing

Summary:
Spiking Neural Networks (SNNs) are explored for multi-output regression tasks such as satellite position estimation from images. The SNN model, utilizing Leaky Integrate-and-Fire neurons, achieves comparable performance to Convolutional Neural Networks (CNNs) on a satellite dataset. Energy analysis reveals that while hardware-agnostic methods suggest a consistent energy advantage for SNNs over CNNs, hardware-aware analysis shows significant energy savings only on neuromorphic hardware with high input sparsity. The impact of data characteristics, specifically dark pixel ratio, on energy consumption is quantified. Transparent evaluation methods and disclosure of underlying assumptions are crucial for fair comparisons of neural network energy efficiency.<br /><br />Summary: Spiking Neural Networks are investigated for satellite image processing, achieving performance comparable to Convolutional Neural Networks. Energy analysis indicates that while SNNs are more energy-efficient according to hardware-agnostic methods, significant savings are observed only on specific hardware and with high input sparsity. The influence of data characteristics on energy consumption is emphasized, underscoring the importance of transparent evaluation methods. <div>
arXiv:2508.19654v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have long been considered inherently energy-efficient, making them attractive for resource-constrained domains such as space applications. However, recent comparative studies with conventional Artificial Neural Networks (ANNs) have begun to question this reputation, especially for digital implementations. This work investigates SNNs for multi-output regression, specifically 3-D satellite position estimation from monocular images, and compares hardware-aware and hardware-agnostic energy estimation methods. The proposed SNN, trained using the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the final layer, achieves comparable Mean Squared Error (MSE) to a reference Convolutional Neural Network (CNN) on a photorealistic satellite dataset. Energy analysis shows that while hardware-agnostic methods predict a consistent 50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals that significant energy savings are realized only on neuromorphic hardware and with high input sparsity. The influence of dark pixel ratio on energy consumption is quantified, emphasizing the impact of data characteristics and hardware assumptions. These findings highlight the need for transparent evaluation methods and explicit disclosure of underlying assumptions to ensure fair comparisons of neural network energy efficiency.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement</title>
<link>https://arxiv.org/abs/2508.19664</link>
<guid>https://arxiv.org/abs/2508.19664</guid>
<content:encoded><![CDATA[
<div> frequency-aware self-supervised learning, UWF image enhancement, image deblurring, Retinex-guided illumination compensation, color preservation unit <br />
Summary: <br />
This paper introduces a novel frequency-aware self-supervised learning approach for enhancing Ultra-Wide-Field (UWF) retinal images. The method combines image deblurring and Retinex-guided illumination compensation modules to preserve fine details and correct uneven intensity. An asymmetric channel integration operation enables the integration of high- and low-frequency information, ensuring the preservation of both local and global structural details. Additionally, a color preservation unit helps in accurate illumination estimation and correction by providing multi-scale spatial and frequency information. Experimental results show that the proposed method enhances visualization quality and improves disease diagnosis by restoring fine local details and correcting uneven intensity. This work is the first of its kind in UWF image enhancement, offering a robust tool for enhancing retinal disease management. <br /> <div>
arXiv:2508.19664v1 Announce Type: new 
Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware self-supervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high- and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction</title>
<link>https://arxiv.org/abs/2508.19688</link>
<guid>https://arxiv.org/abs/2508.19688</guid>
<content:encoded><![CDATA[
<div> Keywords: Monocular texture 3D human reconstruction, geometric ambiguity, prior geometric estimation networks, Supervisor Feature Regularization module, Online Animation Augmentation module

Summary:<br /><br />
The article presents a new framework, SAT, for monocular texture 3D human reconstruction from a single RGB image. One of the key challenges in this field is the geometric ambiguity in 2D images and the lack of 3D training data. To address this, SAT utilizes prior geometric estimation networks to integrate various human geometries effectively, minimizing view inconsistencies. A Supervisor Feature Regularization module aids in better fusing varied geometric priors. An Online Animation Augmentation module enhances reconstruction quality by creating additional training samples from original 3D human data. Experimental results on benchmark datasets demonstrate the superior performance of SAT compared to existing methods. <div>
arXiv:2508.19688v1 Announce Type: new 
Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D digital avatar from just a single front-view human RGB image. However, the geometric ambiguity inherent in a single 2D image and the scarcity of 3D human training data are the main obstacles limiting progress in this field. To address these issues, current methods employ prior geometric estimation networks to derive various human geometric forms, such as the SMPL model and normal maps. However, they struggle to integrate these modalities effectively, leading to view inconsistencies, such as facial distortions. To this end, we propose a two-process 3D human reconstruction framework, SAT, which seamlessly learns various prior geometries in a unified manner and reconstructs high-quality textured 3D avatars as the final output. To further facilitate geometry learning, we introduce a Supervisor Feature Regularization module. By employing a multi-view network with the same structure to provide intermediate features as training supervision, these varied geometric priors can be better fused. To tackle data scarcity and further improve reconstruction quality, we also propose an Online Animation Augmentation module. By building a one-feed-forward animation network, we augment a massive number of samples from the original 3D human data online for model training. Extensive experiments on two benchmarks show the superiority of our approach compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators</title>
<link>https://arxiv.org/abs/2508.19698</link>
<guid>https://arxiv.org/abs/2508.19698</guid>
<content:encoded><![CDATA[
<div> physics-inspired, model-agnostic detector, synthetic-image identification, community-detection problem, Multi-Edge Type QC-LDPC graph, Random Bond Ising Model, Bethe-Hessian spectrum <br />
<br />
Summary: 
A new physics-inspired, model-agnostic detector for identifying synthetic images treats the problem as a community-detection issue on a graph. Using pretrained CNNs, image features are reduced and transformed into a Multi-Edge Type QC-LDPC graph. Edges calibrated at the Nishimori temperature create a Random Bond Ising Model (RBIM) that shows characteristic gaps for genuine images. This approach was validated on binary tasks and achieved over 94% accuracy without using labeled synthetic data or retraining. The detector's spectral analysis revealed separated gaps for real images and a collapsed spectrum for generated ones. The contributions include a novel LDPC graph construction embedding deep image features, an analytical link between RBIM and the Bethe-Hessian spectrum for optimal detection, and a robust, unsupervised detector for synthetic images. Future work will extend this framework to video streams and multi-class anomaly detection. <br /> <div>
arXiv:2508.19698v1 Announce Type: new 
Abstract: The rapid advance of deep generative models such as GANs and diffusion networks now produces images that are virtually indistinguishable from genuine photographs, undermining media forensics and biometric security. Supervised detectors quickly lose effectiveness on unseen generators or after adversarial post-processing, while existing unsupervised methods that rely on low-level statistical cues remain fragile. We introduce a physics-inspired, model-agnostic detector that treats synthetic-image identification as a community-detection problem on a sparse weighted graph. Image features are first extracted with pretrained CNNs and reduced to 32 dimensions, each feature vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities are transformed into edge couplings calibrated at the Nishimori temperature, producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum exhibits a characteristic gap when genuine community structure (real images) is present. Synthetic images violate the Nishimori symmetry and therefore lack such gaps. We validate the approach on binary tasks cat versus dog and male versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic counterparts generated by GANs and diffusion models. Without any labeled synthetic data or retraining of the feature extractor, the detector achieves over 94% accuracy. Spectral analysis shows multiple well separated gaps for real image sets and a collapsed spectrum for generated ones. Our contributions are threefold: a novel LDPC graph construction that embeds deep image features, an analytical link between Nishimori temperature RBIM and the Bethe-Hessian spectrum providing a Bayes optimal detection criterion; and a practical, unsupervised synthetic image detector robust to new generative architectures. Future work will extend the framework to video streams and multi-class anomaly detection.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation</title>
<link>https://arxiv.org/abs/2508.19699</link>
<guid>https://arxiv.org/abs/2508.19699</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D scenes, Label-aware, segmentation, optimization <br />
Summary: <br />
The paper introduces Label-aware 3D Gaussian Splatting (LabelGS), a method that enhances 3D Gaussian representation with object labels for improved 3D scene segmentation. LabelGS incorporates semantic masks, an Occlusion Analysis Model, Main Gaussian Labeling model, and Gaussian Projection Filter to enhance the segmentation process. The method effectively decouples Gaussian representations and refines the optimization process through a random region sampling strategy, significantly boosting efficiency. LabelGS outperforms previous methods like Feature-3DGS in 3D scene segmentation, achieving a remarkable 22X speedup in training at a resolution of 1440X1080. The proposed approach addresses the limitation in 3DGS by incorporating object labels and demonstrates superior performance in scene understanding tasks. The code implementation of LabelGS will be available on GitHub, providing a valuable resource for the research community. <br /> <div>
arXiv:2508.19699v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation</title>
<link>https://arxiv.org/abs/2508.19705</link>
<guid>https://arxiv.org/abs/2508.19705</guid>
<content:encoded><![CDATA[
<div> Keywords: video polyp segmentation, track-by-detect paradigm, spatial contexts, temporal modeling, clinical analysis 

Summary: 
Existing video polyp segmentation methods face challenges in balancing spatiotemporal modeling and domain generalization. To address this, a new approach called FreeVPS combines spatial information from image polyp segmentation with temporal modeling from segment anything model 2 (SAM2) to track polyps in colonoscopy videos. SAM2, however, suffers from error accumulation over time, which compromises segmentation stability. FreeVPS introduces two training-free modules to stabilize SAM2 - the intra-association filtering module reduces false positives by eliminating spatial inaccuracies, and the inter-association refinement module updates the memory bank to enhance temporal coherence and prevent error propagation. The synergistic operation of both modules enhances the performance of FreeVPS in both in-domain and out-of-domain scenarios. The robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos show promise for reliable clinical analysis. 

<br /><br />Summary: <div>
arXiv:2508.19705v1 Announce Type: new 
Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance between spatiotemporal modeling and domain generalization, limiting their applicability in real clinical scenarios. To embrace this challenge, we recast the VPS task as a track-by-detect paradigm that leverages the spatial contexts captured by the image polyp segmentation (IPS) model while integrating the temporal modeling capabilities of segment anything model 2 (SAM2). However, during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error accumulation, resulting in a snowball effect that compromises segmentation stability. We mitigate this issue by repurposing SAM2 as a video polyp segmenter with two training-free modules. In particular, the intra-association filtering module eliminates spatial inaccuracies originating from the detecting stage, reducing false positives. The inter-association refinement module adaptively updates the memory bank to prevent error propagation over time, enhancing temporal coherence. Both modules work synergistically to stabilize SAM2, achieving cutting-edge performance in both in-domain and out-of-domain scenarios. Furthermore, we demonstrate the robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential reliable clinical analysis.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning</title>
<link>https://arxiv.org/abs/2508.19730</link>
<guid>https://arxiv.org/abs/2508.19730</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, video, face foundation models, generalization, self-supervised model <br />
<br />
Summary: 
The article discusses a new robust video deepfake detection framework that focuses on generalization and utilizes facial representations learned by face foundation models. The method combines a self-supervised model (FSFM) trained on real face data with fine-tuning using deepfake datasets. Triplet loss variants are incorporated during training to enhance discriminative power and produce more separable embeddings between real and fake samples. Attribution-based supervision schemes are explored to categorize deepfakes by manipulation type or source dataset and assess their impact on generalization. Extensive experiments across diverse evaluation benchmarks showcase the approach's effectiveness, particularly in challenging real-world scenarios. The framework shows strong generalization beyond training distributions and provides a promising solution to combat the rising concerns surrounding deepfake authenticity and information integrity. <br /> <div>
arXiv:2508.19730v1 Announce Type: new 
Abstract: The increasing realism and accessibility of deepfakes have raised critical concerns about media authenticity and information integrity. Despite recent advances, deepfake detection models often struggle to generalize beyond their training distributions, particularly when applied to media content found in the wild. In this work, we present a robust video deepfake detection framework with strong generalization that takes advantage of the rich facial representations learned by face foundation models. Our method is built on top of FSFM, a self-supervised model trained on real face data, and is further fine-tuned using an ensemble of deepfake datasets spanning both face-swapping and face-reenactment manipulations. To enhance discriminative power, we incorporate triplet loss variants during training, guiding the model to produce more separable embeddings between real and fake samples. Additionally, we explore attribution-based supervision schemes, where deepfakes are categorized by manipulation type or source dataset, to assess their impact on generalization. Extensive experiments across diverse evaluation benchmarks demonstrate the effectiveness of our approach, especially in challenging real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection</title>
<link>https://arxiv.org/abs/2508.19742</link>
<guid>https://arxiv.org/abs/2508.19742</guid>
<content:encoded><![CDATA[
<div> line segment detection, images, deep learning, edge detector, POEv2 <br />
Summary: <br />
This study introduces a novel approach, POEv2, for line segment detection in images. Traditional methods are categorized as generic or wireframe detectors, with deep learning techniques usually falling into the latter. The proposed POEv2 framework is versatile, capable of detecting line segments for both generic and wireframe purposes. By enhancing the Pixel Orientation Estimation method, POEv2 utilizes edge strength maps and can be integrated with any edge detector. Experimental results demonstrate its superior performance on three public datasets, surpassing existing methods. The flexibility of POEv2 makes it a valuable tool for various line segment detection tasks in image analysis. <br /> <div>
arXiv:2508.19742v1 Announce Type: new 
Abstract: Line segment detection in images has been studied for several decades. Existing line segment detectors can be roughly divided into two categories: generic line segment detectors and wireframe line segment detectors. Generic line segment detectors aim to detect all meaningful line segments in images and traditional approaches usually fall into this category. Recent deep learning based approaches are mostly wireframe line segment detectors. They detect only line segments that are geometrically meaningful and have large spatial support. Due to the difference in the aim of design, the performance of generic line segment detectors for the task of wireframe line segment detection won't be satisfactory, and vice versa. In this work, we propose a robust framework that can be used for both generic line segment detection and wireframe line segment detection. The proposed method is an improved version of the Pixel Orientation Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments from edge strength maps, and can be combined with any edge detector. We show in our experiments that by combining the proposed POEv2 with an efficient edge detector, it achieves state-of-the-art performance on three publicly available datasets.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection</title>
<link>https://arxiv.org/abs/2508.19746</link>
<guid>https://arxiv.org/abs/2508.19746</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, light field salient object detection, multi-scale feature embedding block, frequency-domain information, code availability

Summary: 
The paper introduces a new model called SPLF-SAM for light field salient object detection, addressing the issue of prompt information extraction. This model incorporates a Unified Multi-scale Feature Embedding Block (UMFEB) and a Multi-scale Adaptive Filtering Adapter (MAFA) to improve performance. UMFEB allows for the identification of objects of varying sizes, while MAFA focuses on learning frequency features to prevent small objects from being obscured by noise. Extensive experiments have shown that SPLF-SAM outperforms ten existing state-of-the-art LF SOD methods. The code for this model will be made available on GitHub for further study and implementation. <div>
arXiv:2508.19746v1 Announce Type: new 
Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in solving light field salient object detection (LF SOD). However, most existing models tend to neglect the extraction of prompt information under this task. Meanwhile, traditional models ignore the analysis of frequency-domain information, which leads to small objects being overwhelmed by noise. In this paper, we put forward a novel model called self-prompting light field segment anything model (SPLF-SAM), equipped with unified multi-scale feature embedding block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is capable of identifying multiple objects of varying sizes, while MAFA, by learning frequency features, effectively prevents small objects from being overwhelmed by noise. Extensive experiments have demonstrated the superiority of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be available at https://github.com/XucherCH/splfsam.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers</title>
<link>https://arxiv.org/abs/2508.19754</link>
<guid>https://arxiv.org/abs/2508.19754</guid>
<content:encoded><![CDATA[
arXiv:2508.19754v1 Announce Type: new 
Abstract: Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions</title>
<link>https://arxiv.org/abs/2508.19762</link>
<guid>https://arxiv.org/abs/2508.19762</guid>
<content:encoded><![CDATA[
arXiv:2508.19762v1 Announce Type: new 
Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food production and ecosystem stability, yet their populations are declining due to increasing anthropogenic and environmental stressors. To support scalable, automated pollinator monitoring, we introduce BuzzSet, a new large-scale dataset of high-resolution pollinator images collected in real agricultural field conditions. BuzzSet contains 7856 manually verified and labeled images, with over 8000 annotated instances across three classes: honeybees, bumblebees, and unidentified insects. Initial annotations were generated using a YOLOv12 model trained on external data and refined via human verification using open-source labeling tools. All images were preprocessed into 256~$\times$~256 tiles to improve the detection of small insects. We provide strong baselines using the RF-DETR transformer-based object detector. The model achieves high F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively, with confusion matrix results showing minimal misclassification between these categories. The unidentified class remains more challenging due to label ambiguity and lower sample frequency, yet still contributes useful insights for robustness evaluation. Overall detection quality is strong, with a best mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning</title>
<link>https://arxiv.org/abs/2508.19769</link>
<guid>https://arxiv.org/abs/2508.19769</guid>
<content:encoded><![CDATA[
arXiv:2508.19769v1 Announce Type: new 
Abstract: Multimodal learning has significantly enhanced machine learning performance but still faces numerous challenges and limitations. Imbalanced multimodal learning is one of the problems extensively studied in recent works and is typically mitigated by modulating the learning of each modality. However, we find that these methods typically hinder the dominant modality's learning to promote weaker modalities, which affects overall multimodal performance. We analyze the cause of this issue and highlight a commonly overlooked problem: optimization bias within networks. To address this, we propose Adaptive Intra-Network Modulation (AIM) to improve balanced modality learning. AIM accounts for differences in optimization state across parameters and depths within the network during modulation, achieving balanced multimodal learning without hindering either dominant or weak modalities for the first time. Specifically, AIM decouples the dominant modality's under-optimized parameters into Auxiliary Blocks and encourages reliance on these performance-degraded blocks for joint training with weaker modalities. This approach effectively prevents suppression of weaker modalities while enabling targeted optimization of under-optimized parameters to improve the dominant modality. Additionally, AIM assesses modality imbalance level across network depths and adaptively adjusts modulation strength at each depth. Experimental results demonstrate that AIM outperforms state-of-the-art imbalanced modality learning methods across multiple benchmarks and exhibits strong generalizability across different backbones, fusion strategies, and optimizers.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Return of Structural Handwritten Mathematical Expression Recognition</title>
<link>https://arxiv.org/abs/2508.19773</link>
<guid>https://arxiv.org/abs/2508.19773</guid>
<content:encoded><![CDATA[
arXiv:2508.19773v1 Announce Type: new 
Abstract: Handwritten Mathematical Expression Recognition is foundational for educational technologies, enabling applications like digital note-taking and automated grading. While modern encoder-decoder architectures with large language models excel at LaTeX generation, they lack explicit symbol-to-trace alignment, a critical limitation for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates. This paper introduces a structural recognition approach with two innovations: 1 an automatic annotation system that uses a neural network to map LaTeX equations to raw traces, automatically generating annotations for symbol segmentation, classification, and spatial relations, and 2 a modular structural recognition system that independently optimizes segmentation, classification, and relation prediction. By leveraging a dataset enriched with structural annotations from our auto-labeling system, the proposed recognition system combines graph-based trace sorting, a hybrid convolutional-recurrent network, and transformer-based correction to achieve competitive performance on the CROHME-2023 benchmark. Crucially, our structural recognition system generates a complete graph structure that directly links handwritten traces to predicted symbols, enabling transparent error analysis and interpretable outputs.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2508.19786</link>
<guid>https://arxiv.org/abs/2508.19786</guid>
<content:encoded><![CDATA[
arXiv:2508.19786v1 Announce Type: new 
Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation</title>
<link>https://arxiv.org/abs/2508.19789</link>
<guid>https://arxiv.org/abs/2508.19789</guid>
<content:encoded><![CDATA[
arXiv:2508.19789v1 Announce Type: new 
Abstract: Recovering material information from images has been extensively studied in computer graphics and vision. Recent works in material estimation leverage diffusion model showing promising results. However, these diffusion-based methods adopt a multi-step denoising strategy, which is time-consuming for each estimation. Such stochastic inference also conflicts with the deterministic material estimation task, leading to a high variance estimated results. In this paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view material estimation that can produce high-quality material parameters with low variance. To address the overly-smoothing problem in one-step diffusion, StableIntrinsic applies losses in pixel space, with each loss designed based on the properties of the material. Additionally, StableIntrinsic introduces a Detail Injection Network (DIN) to eliminate the detail loss caused by VAE encoding, while further enhancing the sharpness of material prediction results. The experimental results indicate that our method surpasses the current state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error (MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.19791</link>
<guid>https://arxiv.org/abs/2508.19791</guid>
<content:encoded><![CDATA[
arXiv:2508.19791v1 Announce Type: new 
Abstract: Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization</title>
<link>https://arxiv.org/abs/2508.19798</link>
<guid>https://arxiv.org/abs/2508.19798</guid>
<content:encoded><![CDATA[
arXiv:2508.19798v1 Announce Type: new 
Abstract: In the realm of waste management, automating the sorting process for non-biodegradable materials presents considerable challenges due to the complexity and variability of waste streams. To address these challenges, we introduce an enhanced neural architecture that builds upon an existing Encoder-Decoder structure to improve the accuracy and efficiency of waste sorting systems. Our model integrates several key innovations: a Comprehensive Attention Block within the decoder, which refines feature representations by combining convolutional and upsampling operations. In parallel, we utilize attention through the Mamba architecture, providing an additional performance boost. We also introduce a Data Fusion Block that fuses images with more than three channels. To achieve this, we apply PCA transformation to reduce the dimensionality while retaining the maximum variance and essential information across three dimensions, which are then used for further processing. We evaluated the model on RGB, hyperspectral, multispectral, and a combination of RGB and hyperspectral data. The results demonstrate that our approach outperforms existing methods by a significant margin.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A bag of tricks for real-time Mitotic Figure detection</title>
<link>https://arxiv.org/abs/2508.19804</link>
<guid>https://arxiv.org/abs/2508.19804</guid>
<content:encoded><![CDATA[
arXiv:2508.19804v1 Announce Type: new 
Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-aware Sparse Spatiotemporal Learning for Event-based Vision</title>
<link>https://arxiv.org/abs/2508.19806</link>
<guid>https://arxiv.org/abs/2508.19806</guid>
<content:encoded><![CDATA[
arXiv:2508.19806v1 Announce Type: new 
Abstract: Event-based camera has emerged as a promising paradigm for robot perception, offering advantages with high temporal resolution, high dynamic range, and robustness to motion blur. However, existing deep learning-based event processing methods often fail to fully leverage the sparse nature of event data, complicating their integration into resource-constrained edge applications. While neuromorphic computing provides an energy-efficient alternative, spiking neural networks struggle to match of performance of state-of-the-art models in complex event-based vision tasks, like object detection and optical flow. Moreover, achieving high activation sparsity in neural networks is still difficult and often demands careful manual tuning of sparsity-inducing loss terms. Here, we propose Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints. Applied to event-based object detection and optical flow estimation, CSSL achieves comparable or superior performance to state-of-the-art methods while maintaining extremely high neuronal sparsity. Our experimental results highlight CSSL's crucial role in enabling efficient event-based vision for neuromorphic processing.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment</title>
<link>https://arxiv.org/abs/2508.19808</link>
<guid>https://arxiv.org/abs/2508.19808</guid>
<content:encoded><![CDATA[
arXiv:2508.19808v1 Announce Type: new 
Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at https://github.com/wcbup/AutoQ-VIS.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images</title>
<link>https://arxiv.org/abs/2508.19815</link>
<guid>https://arxiv.org/abs/2508.19815</guid>
<content:encoded><![CDATA[
arXiv:2508.19815v1 Announce Type: new 
Abstract: Automated segmentation of the fetal head in ultrasound images is critical for prenatal monitoring. However, achieving robust segmentation remains challenging due to the poor quality of ultrasound images and the lack of annotated data. Semi-supervised methods alleviate the lack of annotated data but struggle with the unique characteristics of fetal head ultrasound images, making it challenging to generate reliable pseudo-labels and enforce effective consistency regularization constraints. To address this issue, we propose a novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation. Our framework consists of the dual-scoring adaptive filtering strategy, the ellipse-constrained pseudo-label refinement, and the symmetry-based multiple consistency regularization. The dual-scoring adaptive filtering strategy uses boundary consistency and contour regularity criteria to evaluate and filter teacher outputs. The ellipse-constrained pseudo-label refinement refines these filtered outputs by fitting least-squares ellipses, which strengthens pixels near the center of the fitted ellipse and suppresses noise simultaneously. The symmetry-based multiple consistency regularization enforces multi-level consistency across perturbed images, symmetric regions, and between original predictions and pseudo-labels, enabling the model to capture robust and stable shape representations. Our method achieves state-of-the-art performance on two benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36% with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores are 91.68% and 93.70% under the same settings.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Rectification for Robust Calibration under Distribution Shift</title>
<link>https://arxiv.org/abs/2508.19830</link>
<guid>https://arxiv.org/abs/2508.19830</guid>
<content:encoded><![CDATA[
arXiv:2508.19830v1 Announce Type: new 
Abstract: Deep neural networks often produce overconfident predictions, undermining their reliability in safety-critical applications. This miscalibration is further exacerbated under distribution shift, where test data deviates from the training distribution due to environmental or acquisition changes. While existing approaches improve calibration through training-time regularization or post-hoc adjustment, their reliance on access to or simulation of target domains limits their practicality in real-world scenarios. In this paper, we propose a novel calibration framework that operates without access to target domain information. From a frequency-domain perspective, we identify that distribution shifts often distort high-frequency visual cues exploited by deep models, and introduce a low-frequency filtering strategy to encourage reliance on domain-invariant features. However, such information loss may degrade In-Distribution (ID) calibration performance. Therefore, we further propose a gradient-based rectification mechanism that enforces ID calibration as a hard constraint during optimization. Experiments on synthetic and real-world shifted datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method significantly improves calibration under distribution shift while maintaining strong in-distribution performance.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models</title>
<link>https://arxiv.org/abs/2508.19850</link>
<guid>https://arxiv.org/abs/2508.19850</guid>
<content:encoded><![CDATA[
arXiv:2508.19850v1 Announce Type: new 
Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance degradation under adverse visual conditions. To address this, we propose a machine-centric image quality assessment (MIQA) framework that quantifies the impact of image degradations on MVS performance. We establish an MIQA paradigm encompassing the end-to-end assessment workflow. To support this, we construct a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million samples that capture distinctive degradation responses in both consistency and accuracy metrics, spanning 75 vision models, 250 degradation types, and three representative vision tasks. We further propose a region-aware MIQA (RA-MIQA) model to evaluate MVS visual quality through fine-grained spatial degradation analysis. Extensive experiments benchmark the proposed RA-MIQA against seven human visual system (HVS)-based IQA metrics and five retrained classical backbones. Results demonstrate RA-MIQA's superior performance in multiple dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, while also revealing task-specific degradation sensitivities. Critically, HVS-based metrics prove inadequate for MVS quality prediction, while even specialized MIQA models struggle with background degradations, accuracy-oriented estimation, and subtle distortions. This study can advance MVS reliability and establish foundations for machine-centric image processing and optimization. The model and code are available at: https://github.com/XiaoqiWang/MIQA.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ego-centric Predictive Model Conditioned on Hand Trajectories</title>
<link>https://arxiv.org/abs/2508.19852</link>
<guid>https://arxiv.org/abs/2508.19852</guid>
<content:encoded><![CDATA[
arXiv:2508.19852v1 Announce Type: new 
Abstract: In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction</title>
<link>https://arxiv.org/abs/2508.19862</link>
<guid>https://arxiv.org/abs/2508.19862</guid>
<content:encoded><![CDATA[
arXiv:2508.19862v1 Announce Type: new 
Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential for timely intervention but remains challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN introduces a dual-branch architecture combining a novel local KNN-based convolutional network (KCN) to preserve fine-grained geometric details and a global graph convolutional network (GCN) to capture long-range structural context, overcoming the over-smoothing limitations of deep GCNs. A dedicated condition branch encodes clinical attributes (age, sex) and the target time interval to generate anatomically plausible, temporally controlled predictions, enabling retrospective and prospective modeling. We curated TAAMesh, a new longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive experiments demonstrate that MCMeshGAN consistently outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation. This framework offers a robust step toward clinically deployable, personalized 3D disease trajectory modeling. The source code for MCMeshGAN and the baseline methods is publicly available at https://github.com/ImperialCollegeLondon/MCMeshGAN.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised structured object representation learning</title>
<link>https://arxiv.org/abs/2508.19864</link>
<guid>https://arxiv.org/abs/2508.19864</guid>
<content:encoded><![CDATA[
arXiv:2508.19864v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for learning visual representations. While recent SSL approaches achieve strong results in global image understanding, they are limited in capturing the structured representation in scenes. In this work, we propose a self-supervised approach that progressively builds structured visual representations by combining semantic grouping, instance level separation, and hierarchical structuring. Our approach, based on a novel ProtoScale module, captures visual elements across multiple spatial scales. Unlike common strategies like DINO that rely on random cropping and global embeddings, we preserve full scene context across augmented views to improve performance in dense prediction tasks. We validate our method on downstream object detection tasks using a combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results show that our method learns object centric representations that enhance supervised object detection and outperform the state-of-the-art methods, even when trained with limited annotated data and fewer fine-tuning epochs.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations</title>
<link>https://arxiv.org/abs/2508.19866</link>
<guid>https://arxiv.org/abs/2508.19866</guid>
<content:encoded><![CDATA[
arXiv:2508.19866v1 Announce Type: new 
Abstract: With the introduction of vehicles with autonomous capabilities on public roads, predicting pedestrian crossing intention has emerged as an active area of research. The task of predicting pedestrian crossing intention involves determining whether pedestrians in the scene are likely to cross the road or not. In this work, we propose TrajFusionNet, a novel transformer-based model that combines future pedestrian trajectory and vehicle speed predictions as priors for predicting crossing intention. TrajFusionNet comprises two branches: a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM branch learns from a sequential representation of the observed and predicted pedestrian trajectory and vehicle speed. Complementarily, the VAM branch enables learning from a visual representation of the predicted pedestrian trajectory by overlaying predicted pedestrian bounding boxes onto scene images. By utilizing a small number of lightweight modalities, TrajFusionNet achieves the lowest total inference time (including model runtime and data preprocessing) among current state-of-the-art approaches. In terms of performance, it achieves state-of-the-art results across the three most commonly used datasets for pedestrian crossing intention prediction.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network</title>
<link>https://arxiv.org/abs/2508.19875</link>
<guid>https://arxiv.org/abs/2508.19875</guid>
<content:encoded><![CDATA[
arXiv:2508.19875v1 Announce Type: new 
Abstract: Sky background subtraction is a critical step in Multi-objective Fiber spectra process. However, current subtraction relies mainly on sky fiber spectra to build Super Sky. These average spectra are lacking in the modeling of the environment surrounding the objects. To address this issue, a sky background estimation model: Sky background building based on Mutual Information (SMI) is proposed. SMI based on mutual information and incremental training approach. It utilizes spectra from all fibers in the plate to estimate the sky background. SMI contains two main networks, the first network applies a wavelength calibration module to extract sky features from spectra, and can effectively solve the feature shift problem according to the corresponding emission position. The second network employs an incremental training approach to maximize mutual information between representations of different spectra to capturing the common component. Then, it minimizes the mutual information between adjoining spectra representations to obtain individual components. This network yields an individual sky background at each location of the object. To verify the effectiveness of the method in this paper, we conducted experiments on the spectra of LAMOST. Results show that SMI can obtain a better object sky background during the observation, especially in the blue end.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multispectral LiDAR data for extracting tree points in urban and suburban areas</title>
<link>https://arxiv.org/abs/2508.19881</link>
<guid>https://arxiv.org/abs/2508.19881</guid>
<content:encoded><![CDATA[
arXiv:2508.19881v1 Announce Type: new 
Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and reducing risks to electrical infrastructure. Airborne laser scanning has advanced large-scale tree management, but challenges remain due to complex urban environments and tree variability. Multispectral (MS) light detection and ranging (LiDAR) improves this by capturing both 3D spatial and spectral data, enabling detailed mapping. This study explores tree point extraction using MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of SPT, with a mean intersection over union (mIoU) of 85.28%. The highest detection accuracy is achieved by incorporating pseudo normalized difference vegetation index (pNDVI) with spatial data, reducing error rate by 10.61 percentage points (pp) compared to using spatial information alone. These findings highlight the potential of MS-LiDAR and DL to improve tree extraction and further tree inventories.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos</title>
<link>https://arxiv.org/abs/2508.19895</link>
<guid>https://arxiv.org/abs/2508.19895</guid>
<content:encoded><![CDATA[
arXiv:2508.19895v1 Announce Type: new 
Abstract: Recent advances in motion generation show remarkable progress. However, several limitations remain: (1) Existing pose-guided character motion transfer methods merely replicate motion without learning its style characteristics, resulting in inexpressive characters. (2) Motion style transfer methods rely heavily on motion capture data, which is difficult to obtain. (3) Generated motions sometimes violate physical laws. To address these challenges, this paper pioneers a new task: Video-to-Video Motion Personalization. We propose a novel framework, PersonaAnimator, which learns personalized motion patterns directly from unconstrained videos. This enables personalized motion transfer. To support this task, we introduce PersonaVid, the first video-based personalized motion dataset. It contains 20 motion content categories and 120 motion style categories. We further propose a Physics-aware Motion Style Regularization mechanism to enforce physical plausibility in the generated motions. Extensive experiments show that PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for the Video-to-Video Motion Personalization task.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities</title>
<link>https://arxiv.org/abs/2508.19905</link>
<guid>https://arxiv.org/abs/2508.19905</guid>
<content:encoded><![CDATA[
arXiv:2508.19905v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD) applications, enabling material-level scene understanding through fine spectral resolution beyond the capabilities of traditional RGB imaging. This paper presents the first comprehensive review of HSI for automotive applications, examining the strengths, limitations, and suitability of current HSI technologies in the context of ADAS/AD. In addition to this qualitative review, we analyze 216 commercially available HSI and multispectral imaging cameras, benchmarking them against key automotive criteria: frame rate, spatial resolution, spectral dimensionality, and compliance with AEC-Q100 temperature standards. Our analysis reveals a significant gap between HSI's demonstrated research potential and its commercial readiness. Only four cameras meet the defined performance thresholds, and none comply with AEC-Q100 requirements. In addition, the paper reviews recent HSI datasets and applications, including semantic segmentation for road surface classification, pedestrian separability, and adverse weather perception. Our review shows that current HSI datasets are limited in terms of scale, spectral consistency, the number of spectral channels, and environmental diversity, posing challenges for the development of perception algorithms and the adequate validation of HSI's true potential in ADAS/AD applications. This review paper establishes the current state of HSI in automotive contexts as of 2025 and outlines key research directions toward practical integration of spectral imaging in ADAS and autonomous systems.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining the Development of Active Learning Methods in Real-World Object Detection</title>
<link>https://arxiv.org/abs/2508.19906</link>
<guid>https://arxiv.org/abs/2508.19906</guid>
<content:encoded><![CDATA[
arXiv:2508.19906v1 Announce Type: new 
Abstract: Active learning (AL) for real-world object detection faces computational and reliability challenges that limit practical deployment. Developing new AL methods requires training multiple detectors across iterations to compare against existing approaches. This creates high costs for autonomous driving datasets where the training of one detector requires up to 282 GPU hours. Additionally, AL method rankings vary substantially across validation sets, compromising reliability in safety-critical transportation systems. We introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features. This enables the elimination of ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables the selection of representative validation sets for robust evaluation. We validate our similarity-based approach on three autonomous driving datasets (KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with two detector architectures (EfficientDet, YOLOv3). This work is the first to unify AL training and evaluation strategies in object detection based on object similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines. This provides a practical framework for deploying AL in real-world applications where computational efficiency and evaluation reliability are critical. Code is available at https://mos-ks.github.io/publications/.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</title>
<link>https://arxiv.org/abs/2508.19909</link>
<guid>https://arxiv.org/abs/2508.19909</guid>
<content:encoded><![CDATA[
arXiv:2508.19909v1 Announce Type: new 
Abstract: Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.19927</link>
<guid>https://arxiv.org/abs/2508.19927</guid>
<content:encoded><![CDATA[
arXiv:2508.19927v1 Announce Type: new 
Abstract: Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</title>
<link>https://arxiv.org/abs/2508.19944</link>
<guid>https://arxiv.org/abs/2508.19944</guid>
<content:encoded><![CDATA[
arXiv:2508.19944v1 Announce Type: new 
Abstract: Understanding and reasoning over text within visual contexts poses a significant challenge for Vision-Language Models (VLMs), given the complexity and diversity of real-world scenarios. To address this challenge, text-rich Visual Question Answering (VQA) datasets and benchmarks have emerged for high-resource languages like English. However, a critical gap persists for low-resource languages such as Korean, where the lack of comprehensive benchmarks hinders robust model evaluation and comparison. To bridge this gap, we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth evaluation of both visual text understanding and reasoning capabilities, while also supporting a multifaceted assessment across 15 domains and 26 image types. Additionally, we introduce a semi-automated VQA generation pipeline specifically optimized for text-rich settings, leveraging refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. While KRETA is tailored for Korean, we hope our adaptable and extensible pipeline will facilitate the development of similar benchmarks in other languages, thereby accelerating multilingual VLM research. The code and dataset for KRETA are available at https://github.com/tabtoyou/KRETA.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework</title>
<link>https://arxiv.org/abs/2508.19946</link>
<guid>https://arxiv.org/abs/2508.19946</guid>
<content:encoded><![CDATA[
arXiv:2508.19946v1 Announce Type: new 
Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese algorithm for image segmentation. We employ a discretized scheme derived from the empirical study of the Chan-Vese model's functional energy and its partial differential equation based on its level set function. We provide a proof of the results and an implementation using MATLAB. Leveraging modern computer vision methodologies, we propose a functional segmentation loss based on active contours, utilizing pytorch.nn.ModuleLoss and a level set based on the Chan-Vese algorithm. We compare our results with common computer vision segmentation datasets and evaluate the performance of classical loss functions against our proposed method. All code and materials used are available at https://github.com/gguzzy/chan_vese_functional_loss.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.19967</link>
<guid>https://arxiv.org/abs/2508.19967</guid>
<content:encoded><![CDATA[
arXiv:2508.19967v1 Announce Type: new 
Abstract: Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is little work on systematically evaluating the geolocation precision of Generative VLMs, their limits and potential for unintended inferences. To bridge this gap, we conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments. Our results offer insight into the internal reasoning of VLMs and highlight their strengths, limitations, and potential societal risks. Our findings indicate that current VLMs perform poorly on generic street-level images yet achieve notably high accuracy (61\%) on images resembling social media content, raising significant and urgent privacy concerns.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity</title>
<link>https://arxiv.org/abs/2508.19972</link>
<guid>https://arxiv.org/abs/2508.19972</guid>
<content:encoded><![CDATA[
arXiv:2508.19972v1 Announce Type: new 
Abstract: Object hallucination in large vision-language models presents a significant challenge to their safe deployment in real-world applications. Recent works have proposed object-level hallucination scores to estimate the likelihood of object hallucination; however, these methods typically adopt either a global or local perspective in isolation, which may limit detection reliability. In this paper, we introduce GLSim, a novel training-free object hallucination detection framework that leverages complementary global and local embedding similarity signals between image and text modalities, enabling more accurate and reliable hallucination detection in diverse scenarios. We comprehensively benchmark existing object hallucination detection methods and demonstrate that GLSim achieves superior detection performance, outperforming competitive baselines by a significant margin.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GS: Generative Segmentation via Label Diffusion</title>
<link>https://arxiv.org/abs/2508.20020</link>
<guid>https://arxiv.org/abs/2508.20020</guid>
<content:encoded><![CDATA[
arXiv:2508.20020v1 Announce Type: new 
Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation Assisted Incremental Test Time Adaptation in an Open World</title>
<link>https://arxiv.org/abs/2508.20029</link>
<guid>https://arxiv.org/abs/2508.20029</guid>
<content:encoded><![CDATA[
arXiv:2508.20029v1 Announce Type: new 
Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often encountered, which challenge the generalization abilities of the deployed trained models. This work addresses Incremental Test Time Adaptation of Vision Language Models, tackling scenarios where unseen classes and unseen domains continuously appear during testing. Unlike traditional Test Time Adaptation approaches, where the test stream comes only from a predefined set of classes, our framework allows models to adapt simultaneously to both covariate and label shifts, actively incorporating new classes as they emerge. Towards this goal, we establish a new benchmark for ITTA, integrating single image TTA methods for VLMs with active labeling techniques that query an oracle for samples potentially representing unseen classes during test time. We propose a segmentation assisted active labeling module, termed SegAssist, which is training free and repurposes the segmentation capabilities of VLMs to refine active sample selection, prioritizing samples likely to belong to unseen classes. Extensive experiments on several benchmark datasets demonstrate the potential of SegAssist to enhance the performance of VLMs in real world scenarios, where continuous adaptation to emerging data is essential. Project-page:https://manogna-s.github.io/segassist/
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title>
<link>https://arxiv.org/abs/2508.20063</link>
<guid>https://arxiv.org/abs/2508.20063</guid>
<content:encoded><![CDATA[
arXiv:2508.20063v1 Announce Type: new 
Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices</title>
<link>https://arxiv.org/abs/2508.20064</link>
<guid>https://arxiv.org/abs/2508.20064</guid>
<content:encoded><![CDATA[
arXiv:2508.20064v1 Announce Type: new 
Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence</title>
<link>https://arxiv.org/abs/2508.20066</link>
<guid>https://arxiv.org/abs/2508.20066</guid>
<content:encoded><![CDATA[
arXiv:2508.20066v1 Announce Type: new 
Abstract: Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title>
<link>https://arxiv.org/abs/2508.20072</link>
<guid>https://arxiv.org/abs/2508.20072</guid>
<content:encoded><![CDATA[
arXiv:2508.20072v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seam360GS: Seamless 360{\deg} Gaussian Splatting from Real-World Omnidirectional Images</title>
<link>https://arxiv.org/abs/2508.20080</link>
<guid>https://arxiv.org/abs/2508.20080</guid>
<content:encoded><![CDATA[
arXiv:2508.20080v1 Announce Type: new 
Abstract: 360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioStory: Generating Long-Form Narrative Audio with Large Language Models</title>
<link>https://arxiv.org/abs/2508.20088</link>
<guid>https://arxiv.org/abs/2508.20088</guid>
<content:encoded><![CDATA[
arXiv:2508.20088v1 Announce Type: new 
Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</title>
<link>https://arxiv.org/abs/2508.20089</link>
<guid>https://arxiv.org/abs/2508.20089</guid>
<content:encoded><![CDATA[
arXiv:2508.20089v1 Announce Type: new 
Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.20096</link>
<guid>https://arxiv.org/abs/2508.20096</guid>
<content:encoded><![CDATA[
arXiv:2508.20096v1 Announce Type: new 
Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saccade crossing avoidance as a visual search strategy</title>
<link>https://arxiv.org/abs/2508.18404</link>
<guid>https://arxiv.org/abs/2508.18404</guid>
<content:encoded><![CDATA[
arXiv:2508.18404v1 Announce Type: cross 
Abstract: Although visual search appears largely random, several oculomotor biases exist such that the likelihoods of saccade directions and lengths depend on the previous scan path. Compared to the most recent fixations, the impact of the longer path history is more difficult to quantify. Using the step-selection framework commonly used in movement ecology, and analyzing data from 45-second viewings of "Where's Waldo"?, we report a new memory-dependent effect that also varies significantly between individuals, which we term self-crossing avoidance. This is a tendency for saccades to avoid crossing those earlier in the scan path, and is most evident when both have small amplitudes. We show this by comparing real data to synthetic data generated from a memoryless approximation of the spatial statistics (i.e. a Markovian nonparametric model with a matching distribution of saccade lengths over time). Maximum likelihood fitting indicates that this effect is strongest when including the last $\approx 7$ seconds of a scan path. The effect size is comparable to well-known forms of history dependence such as inhibition of return. A parametric probabilistic model including a self-crossing penalty term was able to reproduce joint statistics of saccade lengths and self-crossings. We also quantified individual strategic differences, and their consistency over the six images viewed per participant, using mixed-effect regressions. Participants with a higher tendency to avoid crossings displayed smaller saccade lengths and shorter fixation durations on average, but did not display more horizontal, vertical, forward or reverse saccades. Together, these results indicate that the avoidance of crossings is a local orienting strategy that facilitates and complements inhibition of return, and hence exploration of visual scenes.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
arXiv:2508.19026v1 Announce Type: cross 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling spectral filtering effects on color-matching functions: Implications for observer variability</title>
<link>https://arxiv.org/abs/2508.19291</link>
<guid>https://arxiv.org/abs/2508.19291</guid>
<content:encoded><![CDATA[
arXiv:2508.19291v1 Announce Type: cross 
Abstract: This study investigates the impact of spectral filtering on color-matching functions (CMFs) and its implications for observer variability modeling. We conducted color matching experiments with a single observer, both with and without a spectral filter in front of a bipartite field. Using a novel computational approach, we estimated the filter transmittance and transformation matrix necessary to convert unfiltered CMFs to filtered CMFs. Statistical analysis revealed good agreement between estimated and measured filter characteristics, particularly in central wavelength regions. Applying this methodology to compare between Stiles and Burch 1955 (SB1955) mean observer CMFs and our previously published "ICVIO" mean observer CMFs, we identified a "yellow" (short-wavelength suppressing) filter that effectively transforms between these datasets. This finding aligns with our hypothesis that observed differences between the CMF sets are attributable to age-related lens yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955). Our approach enables efficient representation of observer variability through a single filter rather than three separate functions, offering potentially reduced experimental overhead while maintaining accuracy in characterizing individual color vision differences.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy</title>
<link>https://arxiv.org/abs/2508.19300</link>
<guid>https://arxiv.org/abs/2508.19300</guid>
<content:encoded><![CDATA[
arXiv:2508.19300v1 Announce Type: cross 
Abstract: 4D live fluorescence microscopy is often compromised by prolonged high intensity illumination which induces photobleaching and phototoxic effects that generate photo-induced artifacts and severely impair image continuity and detail recovery. To address this challenge, we propose the CellINR framework, a case-specific optimization approach based on implicit neural representation. The method employs blind convolution and structure amplification strategies to map 3D spatial coordinates into the high frequency domain, enabling precise modeling and high-accuracy reconstruction of cellular structures while effectively distinguishing true signals from artifacts. Experimental results demonstrate that CellINR significantly outperforms existing techniques in artifact removal and restoration of structural continuity, and for the first time, a paired 4D live cell imaging dataset is provided for evaluating reconstruction performance, thereby offering a solid foundation for subsequent quantitative analyses and biological research. The code and dataset will be public.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.19303</link>
<guid>https://arxiv.org/abs/2508.19303</guid>
<content:encoded><![CDATA[
arXiv:2508.19303v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to their potential for rupture, which is often asymptomatic but can be fatal. Although maximum diameter is commonly used for risk assessment, diameter alone is insufficient as it does not capture the properties of the underlying material of the vessel wall, which play a critical role in determining the risk of rupture. To overcome this limitation, we propose a deep learning-based framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite element simulations, we generate a diverse dataset of displacement fields with their corresponding modulus distributions. We train a model with U-Net architecture and normalized mean squared error (NMSE) to infer the spatial modulus distribution from the axial and lateral components of the displacement fields. This model is evaluated across three experimental domains: digital phantom data from 3D COMSOL simulations, physical phantom experiments using biomechanically distinct vessel models, and clinical ultrasound exams from AAA patients. Our simulated results demonstrate that the proposed deep learning model is able to reconstruct modulus distributions, achieving an NMSE score of 0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches the expected values, affirming the model's ability to generalize to phantom data. We compare our approach with an iterative method which shows comparable performance but higher computation time. In contrast, the deep learning method can provide quick and effective estimates of tissue stiffness from ultrasound images, which could help assess the risk of AAA rupture without invasive procedures.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction</title>
<link>https://arxiv.org/abs/2508.19319</link>
<guid>https://arxiv.org/abs/2508.19319</guid>
<content:encoded><![CDATA[
arXiv:2508.19319v1 Announce Type: cross 
Abstract: Accurate sarcopenia diagnosis via ultrasound remains challenging due to subtle imaging cues, limited labeled data, and the absence of clinical context in most models. We propose MedVQA-TREE, a multimodal framework that integrates a hierarchical image interpretation module, a gated feature-level fusion mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision module includes anatomical classification, region segmentation, and graph-based spatial reasoning to capture coarse, mid-level, and fine-grained structures. A gated fusion mechanism selectively integrates visual features with textual queries, while clinical knowledge is retrieved through a UMLS-guided pipeline accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA) and a custom sarcopenia ultrasound dataset. The model achieved up to 99% diagnostic accuracy and outperformed previous state-of-the-art methods by over 10%. These results underscore the benefit of combining structured visual understanding with guided knowledge retrieval for effective AI-assisted diagnosis in sarcopenia.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays</title>
<link>https://arxiv.org/abs/2508.19322</link>
<guid>https://arxiv.org/abs/2508.19322</guid>
<content:encoded><![CDATA[
arXiv:2508.19322v1 Announce Type: cross 
Abstract: Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage, where a system decides when to stop, escalate, or defer under real constraints, remains relatively underexplored. To address this gap, we introduce AT-CXR, an uncertainty-aware agent for chest X-rays. The system estimates per-case confidence and distributional fit, then follows a stepwise policy to issue an automated decision or abstain with a suggested label for human intervention. We evaluate two router designs that share the same inputs and actions: a deterministic rule-based router and an LLM-decided router. Across five-fold evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants outperform strong zero-shot vision-language models and state-of-the-art supervised classifiers, achieving higher full-coverage accuracy and superior selective-prediction performance, evidenced by a lower area under the risk-coverage curve (AURC) and a lower error rate at high coverage, while operating with lower latency that meets practical clinical constraints. The two routers provide complementary operating points, enabling deployments to prioritize maximal throughput or maximal accuracy. Our code is available at https://github.com/XLIAaron/uncertainty-aware-cxr-agent.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Technical Review on Comparison and Estimation of Steganographic Tools</title>
<link>https://arxiv.org/abs/2508.19323</link>
<guid>https://arxiv.org/abs/2508.19323</guid>
<content:encoded><![CDATA[
arXiv:2508.19323v1 Announce Type: cross 
Abstract: Steganography is technique of hiding a data under cover media using different steganography tools. Image steganography is hiding of data (Text/Image/Audio/Video) under a cover as Image. This review paper presents classification of image steganography and the comparison of various Image steganography tools using different image formats. Analyzing numerous tools on the basis of Image features and extracting the best one. Some of the tools available in the market were selected based on the frequent use; these tools were tested using the same input on all of them. Specific text was embedded within all host images for each of the six Steganography tools selected. The results of the experiment reveal that all the six tools were relatively performing at the same level, though some software performs better than others through efficiency. And it was based on the image features like size, dimensions, and pixel value and histogram differentiation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Source Knowledge Transfer by Model Merging</title>
<link>https://arxiv.org/abs/2508.19353</link>
<guid>https://arxiv.org/abs/2508.19353</guid>
<content:encoded><![CDATA[
arXiv:2508.19353v1 Announce Type: cross 
Abstract: While transfer learning is an advantageous strategy, it overlooks the opportunity to leverage knowledge from numerous available models online. Addressing this multi-source transfer learning problem is a promising path to boost adaptability and cut re-training costs. However, existing approaches are inherently coarse-grained, lacking the necessary precision for granular knowledge extraction and the aggregation efficiency required to fuse knowledge from either a large number of source models or those with high parameter counts. We address these limitations by leveraging Singular Value Decomposition (SVD) to first decompose each source model into its elementary, rank-one components. A subsequent aggregation stage then selects only the most salient components from all sources, thereby overcoming the previous efficiency and precision limitations. To best preserve and leverage the synthesized knowledge base, our method adapts to the target task by fine-tuning only the principal singular values of the merged matrix. In essence, this process only recalibrates the importance of top SVD components. The proposed framework allows for efficient transfer learning, is robust to perturbations both at the input level and in the parameter space (e.g., noisy or pruned sources), and scales well computationally.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments</title>
<link>https://arxiv.org/abs/2508.19376</link>
<guid>https://arxiv.org/abs/2508.19376</guid>
<content:encoded><![CDATA[
arXiv:2508.19376v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has shown strong potential for multimodal reasoning beyond natural language. In this work, we explore the use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for classifying neutrino interactions from pixelated detector images in high-energy physics (HEP) experiments. We benchmark its performance against an established CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as classification accuracy, precision, recall, and AUC-ROC. Our results show that the VLM not only matches or exceeds CNN performance but also enables richer reasoning and better integration of auxiliary textual or semantic context. These findings suggest that VLMs offer a promising general-purpose backbone for event classification in HEP, paving the way for multimodal approaches in experimental neutrino physics.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents</title>
<link>https://arxiv.org/abs/2508.19493</link>
<guid>https://arxiv.org/abs/2508.19493</guid>
<content:encoded><![CDATA[
arXiv:2508.19493v1 Announce Type: cross 
Abstract: Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View</title>
<link>https://arxiv.org/abs/2508.19508</link>
<guid>https://arxiv.org/abs/2508.19508</guid>
<content:encoded><![CDATA[
arXiv:2508.19508v1 Announce Type: cross 
Abstract: Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by $\sim$360 times, demonstrating strong potential for scalable agricultural digital twin systems.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Texture Transfer for XR Avatars via Barycentric UV Conversion</title>
<link>https://arxiv.org/abs/2508.19518</link>
<guid>https://arxiv.org/abs/2508.19518</guid>
<content:encoded><![CDATA[
arXiv:2508.19518v1 Announce Type: cross 
Abstract: We present a fast and efficient method for transferring facial textures onto SMPL-X-based full-body avatars. Unlike conventional affine-transform methods that are slow and prone to visual artifacts, our method utilizes a barycentric UV conversion technique. Our approach precomputes the entire UV mapping into a single transformation matrix, enabling texture transfer in a single operation. This results in a speedup of over 7000x compared to the baseline, while also significantly improving the final texture quality by eliminating boundary artifacts. Through quantitative and qualitative evaluations, we demonstrate that our method offers a practical solution for personalization in immersive XR applications. The code is available online.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Deepfake Issue in Selfie banking through camera based authentication</title>
<link>https://arxiv.org/abs/2508.19714</link>
<guid>https://arxiv.org/abs/2508.19714</guid>
<content:encoded><![CDATA[
arXiv:2508.19714v1 Announce Type: cross 
Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously, it was just Photoshop, but now deep learning technologies enable us to create highly realistic fake identities, which fraudsters exploit to bypass biometric systems such as facial recognition in online banking. This paper explores the use of an already established forensic recognition system, previously used for picture camera localization, in deepfake detection.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots</title>
<link>https://arxiv.org/abs/2508.19788</link>
<guid>https://arxiv.org/abs/2508.19788</guid>
<content:encoded><![CDATA[
arXiv:2508.19788v1 Announce Type: cross 
Abstract: We present a novel framework for estimating accident-prone regions in everyday indoor scenes, aimed at improving real-time risk awareness in service robots operating in human-centric environments. As robots become integrated into daily life, particularly in homes, the ability to anticipate and respond to environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction. Our approach models object-level risk and context through a semantic graph-based propagation algorithm. Each object is represented as a node with an associated risk score, and risk propagates asymmetrically from high-risk to low-risk objects based on spatial proximity and accident relationship. This enables the robot to infer potential hazards even when they are not explicitly visible or labeled. Designed for interpretability and lightweight onboard deployment, our method is validated on a dataset with human-annotated risk regions, achieving a binary risk detection accuracy of 75%. The system demonstrates strong alignment with human perception, particularly in scenes involving sharp or unstable objects. These results underline the potential of context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces. This framework could serve as a foundation for future systems that make context-driven safety decisions, provide real-time alerts, or autonomously assist users in avoiding or mitigating hazards within home environments.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement</title>
<link>https://arxiv.org/abs/2508.19887</link>
<guid>https://arxiv.org/abs/2508.19887</guid>
<content:encoded><![CDATA[
arXiv:2508.19887v1 Announce Type: cross 
Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question Answering (VQA) Dataset in Bangla, a widely used, low-resource language in multimodal AI research. The majority of existing datasets are either manually annotated with an emphasis on a specific domain, query type, or answer type or are constrained by niche answer formats. In order to mitigate human-induced errors and guarantee lucidity, we implemented a multilingual LLM-assisted translation refinement pipeline. This dataset overcomes the issues of low-quality translations from multilingual sources. The dataset comprises 52,650 question-answer pairs across 4750+ images. Questions are classified into three distinct answer types: nominal (short descriptive), quantitative (numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive open-source, high-quality VQA benchmark in Bangla, aiming to advance research in low-resource multimodal learning and facilitate the development of more inclusive AI systems.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs</title>
<link>https://arxiv.org/abs/2508.19896</link>
<guid>https://arxiv.org/abs/2508.19896</guid>
<content:encoded><![CDATA[
arXiv:2508.19896v1 Announce Type: cross 
Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often rely on purely global, gradient-based optimisation, which can lead to overfitting, redundant filters, and reduced interpretability. To address these limitations, we propose NM-Hebb, a two-phase training framework that integrates neuro-inspired local plasticity with distance-aware supervision. Phase 1 extends standard supervised training by jointly optimising a cross-entropy objective with two biologically inspired mechanisms: (i) a Hebbian regulariser that aligns the spatial mean of activations with the mean of the corresponding convolutional filter weights, encouraging structured, reusable primitives; and (ii) a learnable neuromodulator that gates an elastic-weight-style consolidation loss, preserving beneficial parameters without freezing the network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss, explicitly compressing intra-class distances and enlarging inter-class margins in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2, DenseNet-121), NM-Hebb achieves consistent gains over baseline and other methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp (CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual Information (NMI) increased by up to +0.15. Qualitative visualisations and filter-level analyses further confirm that NM-Hebb produces more structured and selective features, yielding tighter and more interpretable class clusters. Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields CNNs that are not only more accurate but also more interpretable, offering practical benefits for resource-constrained and safety-critical AI deployments.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</title>
<link>https://arxiv.org/abs/2508.20018</link>
<guid>https://arxiv.org/abs/2508.20018</guid>
<content:encoded><![CDATA[
arXiv:2508.20018v1 Announce Type: cross 
Abstract: The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis</title>
<link>https://arxiv.org/abs/2508.20068</link>
<guid>https://arxiv.org/abs/2508.20068</guid>
<content:encoded><![CDATA[
arXiv:2508.20068v1 Announce Type: cross 
Abstract: For human cognitive process, spatial reasoning and perception are closely entangled, yet the nature of this interplay remains underexplored in the evaluation of multimodal large language models (MLLMs). While recent MLLM advancements show impressive performance on reasoning, their capacity for human-like spatial cognition remains an open question. In this work, we introduce a systematic evaluation framework to assess the spatial reasoning abilities of state-of-the-art MLLMs relative to human performance. Central to our work is 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests. 11Plus-Bench also features fine-grained expert annotations of both perceptual complexity and reasoning process, enabling detailed instance-level analysis of model behavior. Through extensive experiments across 14 MLLMs and human evaluation, we find that current MLLMs exhibit early signs of spatial cognition. Despite a large performance gap compared to humans, MLLMs' cognitive profiles resemble those of humans in that cognitive effort correlates strongly with reasoning-related complexity. However, instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity. These findings highlight both emerging capabilities and limitations in current MLLMs' spatial reasoning capabilities and provide actionable insights for advancing model design.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Bayes image restoration with compressive autoencoders</title>
<link>https://arxiv.org/abs/2311.17744</link>
<guid>https://arxiv.org/abs/2311.17744</guid>
<content:encoded><![CDATA[
arXiv:2311.17744v4 Announce Type: replace 
Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play (PnP) methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a second contribution, we introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which performs latent estimation within the framework of variational inference. Thanks to a simple yet efficient parameterization of the variational posterior, VBLE allows for fast and easy (approximate) posterior sampling. Experimental results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar performance as state-of-the-art PnP methods, while being able to quantify uncertainties significantly faster than other existing posterior sampling techniques. The code associated to this paper is available in https://github.com/MaudBqrd/VBLE.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent space configuration for improved generalization in supervised autoencoder neural networks</title>
<link>https://arxiv.org/abs/2402.08441</link>
<guid>https://arxiv.org/abs/2402.08441</guid>
<content:encoded><![CDATA[
arXiv:2402.08441v3 Announce Type: replace 
Abstract: Autoencoders (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS). Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly. In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration. The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration. We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for supervised AE (SAE). Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers. We also show that this leads to more stable and interpretable training. We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to evaluate similarity for unseen classes. We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment</title>
<link>https://arxiv.org/abs/2405.18525</link>
<guid>https://arxiv.org/abs/2405.18525</guid>
<content:encoded><![CDATA[
arXiv:2405.18525v2 Announce Type: replace 
Abstract: Traditional image-to-3D models often struggle with scenes containing multiple objects due to biases and occlusion complexities. To address this challenge, we present REPARO, a novel approach for compositional 3D asset generation from single images. REPARO employs a two-step process: first, it extracts individual objects from the scene and reconstructs their 3D meshes using off-the-shelf image-to-3D models; then, it optimizes the layout of these meshes through differentiable rendering techniques, ensuring coherent scene composition. By integrating optimal transport-based long-range appearance loss term and high-level semantic loss term in the differentiable rendering, REPARO can effectively recover the layout of 3D assets. The proposed method can significantly enhance object independence, detail accuracy, and overall scene coherence. Extensive evaluation of multi-object scenes demonstrates that our REPARO offers a comprehensive approach to address the complexities of multi-object 3D scene generation from single images.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraceNet: Segment one thing efficiently</title>
<link>https://arxiv.org/abs/2406.14874</link>
<guid>https://arxiv.org/abs/2406.14874</guid>
<content:encoded><![CDATA[
arXiv:2406.14874v3 Announce Type: replace 
Abstract: Efficient single instance segmentation is essential for unlocking features in the mobile imaging applications, such as capture or editing. Existing on-the-fly mobile imaging applications scope the segmentation task to portraits or the salient subject due to the computational constraints. Instance segmentation, despite its recent developments towards efficient networks, is still heavy due to the cost of computation on the entire image to identify all instances. To address this, we propose and formulate a one tap driven single instance segmentation task that segments a single instance selected by a user via a positive tap. This task, in contrast to the broader task of segmenting anything as suggested in the Segment Anything Model \cite{sam}, focuses on efficient segmentation of a single instance specified by the user. To solve this problem, we present TraceNet, which explicitly locates the selected instance by way of receptive field tracing. TraceNet identifies image regions that are related to the user tap and heavy computations are only performed on selected regions of the image. Therefore overall computation cost and memory consumption are reduced during inference. We evaluate the performance of TraceNet on instance IoU average over taps and the proportion of the region that a user tap can fall into for a high-quality single-instance mask. Experimental results on MS-COCO and LVIS demonstrate the effectiveness and efficiency of the proposed approach. TraceNet can jointly achieve the efficiency and interactivity, filling in the gap between needs for efficient mobile inference and recent research trend towards multimodal and interactive segmentation models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training with Explanations Alone: A New Paradigm to Prevent Shortcut Learning</title>
<link>https://arxiv.org/abs/2407.09788</link>
<guid>https://arxiv.org/abs/2407.09788</guid>
<content:encoded><![CDATA[
arXiv:2407.09788v2 Announce Type: replace 
Abstract: Application of Artificial Intelligence (AI) in critical domains, like the medical one, is often hampered by shortcut learning, which hinders AI generalization to diverse hospitals and patients. Shortcut learning can be caused, for example, by background biases -- features in image backgrounds that are spuriously correlated to classification labels (e.g., words in X-rays). To mitigate the influence of image background and foreground bias on AI, we introduce a new training paradigm, dubbed Training with Explanations Alone (TEA). TEA trains a classifier (TEA student) only by making its explanation heatmaps match target heatmaps from a larger teacher model. By learning from its explanation heatmaps, the TEA student pays attention to the same image features as the teacher. For example, a teacher uses a large segmenter to remove image backgrounds before classification, thus ignoring background bias. By learning from the teacher's explanation heatmaps, the TEA student learns to also ignore backgrounds -- but it does not need a segmenter. With different teachers, the TEA student can also resist bias in the image foreground. Surprisingly, by training with heatmaps alone the student output naturally matches the teacher output -- with no loss function applied to the student output. We compared the TEA student against 14 state-of-the-art methods in 5 datasets with strong background or foreground bias, including Waterbirds and an X-Ray dataset for COVID-19 and pneumonia classification. The TEA student had better resistance to bias, strongly surpassing state-of-the-art methods, and generalizing better to hospitals not seen in training.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Object Detection and Tracking in Panoramic Videos for Cycling Safety Analysis</title>
<link>https://arxiv.org/abs/2407.15199</link>
<guid>https://arxiv.org/abs/2407.15199</guid>
<content:encoded><![CDATA[
arXiv:2407.15199v2 Announce Type: replace 
Abstract: Cyclists face a disproportionate risk of injury, yet conventional crash records are too limited to reconstruct the circumstances of incidents or to diagnose risk at the finer spatial and temporal detail needed for targeted interventions. Recently, naturalistic studies have gained traction as a way to capture the complex behavioural and infrastructural factors that contribute to crashes. These approaches typically involve the collection and analysis of video data. A video promising format is panoramic video, which can record 360-degree views around a rider. However, its use is limited by severe distortions, large numbers of small objects and boundary continuity. This study addresses these challenges by proposing a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360-degree images into four perspective sub-images, thus reducing distortion; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information for improved tracking consistency; and (3) validating the proposed approach through a real-world application focused on detecting overtaking manoeuvres by vehicles around cyclists. The methodology is evaluated using panoramic videos recorded by cyclists on London's roadways under diverse conditions. Experimental results demonstrate notable improvements over baseline methods, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 3.0% increase in multi-object tracking accuracy and a 4.6% improvement in identification F-score. The overtaking detection task achieves a high F-score of 0.81, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios. The code is available on GitHub (https://github.com/SpaceTimeLab/360_object_tracking) to ensure reproducibility.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffArtist: Towards Structure and Appearance Controllable Image Stylization</title>
<link>https://arxiv.org/abs/2407.15842</link>
<guid>https://arxiv.org/abs/2407.15842</guid>
<content:encoded><![CDATA[
arXiv:2407.15842v4 Announce Type: replace 
Abstract: Artistic styles are defined by both their structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance-level features such as color and texture, often neglecting the equally crucial aspect of structural stylization. To address this gap, we introduce \textbf{DiffArtist}, the first 2D stylization method to offer fine-grained, simultaneous control over both structure and appearance style strength. This dual controllability is achieved by representing structure and appearance generation as separate diffusion processes, necessitating no further tuning or additional adapters. To properly evaluate this new capability of dual stylization, we further propose a Multimodal LLM-based stylization evaluator that aligns significantly better with human preferences than existing metrics. Extensive analysis shows that DiffArtist achieves superior style fidelity and dual-controllability compared to state-of-the-art methods. Its text-driven, training-free design and unprecedented dual controllability make it a powerful and interactive tool for various creative applications. Project homepage: https://diffusionartist.github.io.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2408.06747</link>
<guid>https://arxiv.org/abs/2408.06747</guid>
<content:encoded><![CDATA[
arXiv:2408.06747v3 Announce Type: replace 
Abstract: Recent works utilize CLIP to perform the challenging unsupervised semantic segmentation task where only images without annotations are available. However, we observe that when adopting CLIP to such a pixel-level understanding task, unexpected bias (including class-preference bias and space-preference bias) occurs. Previous works don't explicitly model the bias, which largely constrains the segmentation performance. In this paper, we propose to explicitly model and rectify the bias existing in CLIP to facilitate the unsupervised semantic segmentation task. Specifically, we design a learnable "Reference" prompt to encode class-preference bias and a projection of the positional embedding in the vision transformer to encode space-preference bias respectively. To avoid interference, two kinds of biases are firstly independently encoded into different features, i.e., the Reference feature and the positional feature. Via a matrix multiplication between the Reference feature and the positional feature, a bias logit map is generated to explicitly represent two kinds of biases. Then we rectify the logits of CLIP via a simple element-wise subtraction. To make the rectified results smoother and more contextual, we design a mask decoder which takes the feature of CLIP and the rectified logits as input and outputs a rectified segmentation mask with the help of Gumbel-Softmax operation. A contrastive loss based on the masked visual features and the text features of different classes is imposed, which makes the bias modeling and rectification process meaningful and effective. Extensive experiments on various benchmarks including PASCAL VOC, PASCAL Context, ADE20K, Cityscapes, and COCO Stuff demonstrate that our method performs favorably against previous state-of-the-arts. The implementation is available at: https://github.com/dogehhh/ReCLIP.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models</title>
<link>https://arxiv.org/abs/2412.01824</link>
<guid>https://arxiv.org/abs/2412.01824</guid>
<content:encoded><![CDATA[
arXiv:2412.01824v2 Announce Type: replace 
Abstract: In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LV-CadeNet: A Long-View Feature Convolution-Attention Fusion Encoder-Decoder Network for EEG/MEG Spike Analysis</title>
<link>https://arxiv.org/abs/2412.08896</link>
<guid>https://arxiv.org/abs/2412.08896</guid>
<content:encoded><![CDATA[
arXiv:2412.08896v2 Announce Type: replace 
Abstract: The analysis of interictal epileptiform discharges (IEDs) in magnetoencephalography (MEG) or electroencephalogram (EEG) recordings represents a critical component in the diagnosis of epilepsy. However, manual analysis of these IEDs, which appear as epileptic spikes, from the large amount of MEG/EEG data is labor intensive and requires high expertise. Although automated methods have been developed to address this challenge, current approaches fail to fully emulate clinical experts' diagnostic intelligence in two key aspects: (1) their analysis on the input signals is limited to short temporal windows matching individual spike durations, missing the extended contextual patterns clinicians use to assess significance; and (2) they fail to adequately capture the dipole patterns with simultaneous positive-negative potential distributions across adjacent sensors that serve as clinicians' key diagnostic criterion for IED identification. To bridge this artificial-human intelligence gap, we propose a novel deep learning framework LV-CadeNet that integrates two key innovations: (1) a Long-View morphological feature representation that mimics expert clinicians' comprehensive assessment of both local spike characteristics and long-view contextual information, and (2) a hierarchical Encoder-Decoder NETwork that employs Convolution-Attention blocks for multi-scale spatiotemporal feature learning with progressive abstraction. Extensive evaluations confirm the superior performance of LV-CadeNet, which outperforms six state-of-the-art methods in EEG spike classification on TUEV, the largest public EEG spike dataset. Additionally, LV-CadeNet attains a significant improvement of 13.58% in balanced accuracy over the leading baseline for MEG spike detection on a clinical MEG dataset from Sanbo Brain Hospital, Capital Medical University.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic Temporal-Frequency Representation Learning Approach</title>
<link>https://arxiv.org/abs/2412.11668</link>
<guid>https://arxiv.org/abs/2412.11668</guid>
<content:encoded><![CDATA[
arXiv:2412.11668v2 Announce Type: replace 
Abstract: Currently, the prevalence of online handwriting has spurred a critical need for effective retrieval systems to accurately search relevant handwriting instances from specific writers, known as online writer retrieval. Despite the growing demand, this field suffers from a scarcity of well-established methodologies and public large-scale datasets. This paper tackles these challenges with a focus on Chinese handwritten phrases. First, we propose DOLPHIN, a novel retrieval model designed to enhance handwriting representations through synergistic temporal-frequency analysis. For frequency feature learning, we propose the HFGA block, which performs gated cross-attention between the vanilla temporal handwriting sequence and its high-frequency sub-bands to amplify salient writing details. For temporal feature learning, we propose the CAIR block, tailored to promote channel interaction and reduce channel redundancy. Second, to address data deficit, we introduce OLIWER, a large-scale online writer retrieval dataset encompassing over 670,000 Chinese handwritten phrases from 1,731 individuals. Through extensive evaluations, we demonstrate the superior performance of DOLPHIN over existing methods. In addition, we explore cross-domain writer retrieval and reveal the pivotal role of increasing feature alignment in bridging the distributional gap between different handwriting data. Our findings emphasize the significance of point sampling frequency and pressure features in improving handwriting representation quality and retrieval performance. Code and dataset are available at https://github.com/SCUT-DLVCLab/DOLPHIN.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIMS: Image Matching System Based on Adaptive Graph Construction and Graph Neural Network</title>
<link>https://arxiv.org/abs/2412.18221</link>
<guid>https://arxiv.org/abs/2412.18221</guid>
<content:encoded><![CDATA[
arXiv:2412.18221v2 Announce Type: replace 
Abstract: Feature-based image matching has extensive applications in computer vision. Keypoints detected in images can be naturally represented as graph structures, and Graph Neural Networks (GNNs) have been shown to outperform traditional deep learning techniques. Consequently, the paradigm of image matching via GNNs has gained significant prominence in recent academic research. In this paper, we first introduce an innovative adaptive graph construction method that utilizes a filtering mechanism based on distance and dynamic threshold similarity. This method dynamically adjusts the criteria for incorporating new vertices based on the characteristics of existing vertices, allowing for the construction of more precise and robust graph structures while avoiding redundancy. We further combine the vertex processing capabilities of GNNs with the global awareness capabilities of Transformers to enhance the model's representation of spatial and feature information within graph structures. This hybrid model provides a deeper understanding of the interrelationships between vertices and their contributions to the matching process. Additionally, we employ the Sinkhorn algorithm to iteratively solve for optimal matching results. Finally, we validate our system using extensive image datasets and conduct comprehensive comparative experiments. Experimental results demonstrate that our system achieves an average improvement of 3.8x-40.3x in overall matching performance. Additionally, the number of vertices and edges significantly impacts training efficiency and memory usage; therefore, we employ multi-GPU technology to accelerate the training process. Our code is available at https://github.com/songxf1024/GIMS.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraRay: Introducing Full-Path Ray Tracing in Physics-Based Ultrasound Simulation</title>
<link>https://arxiv.org/abs/2501.05828</link>
<guid>https://arxiv.org/abs/2501.05828</guid>
<content:encoded><![CDATA[
arXiv:2501.05828v3 Announce Type: replace 
Abstract: Traditional ultrasound simulators solve the wave equation to model pressure distribution fields, achieving high accuracy but requiring significant computational time and resources. To address this, ray tracing approaches have been introduced, modeling wave propagation as rays interacting with boundaries and scatterers. However, existing models simplify ray propagation, generating echoes at interaction points without considering return paths to the sensor. This can result in unrealistic artifacts and necessitates careful scene tuning for plausible results. We propose a novel ultrasound simulation pipeline that utilizes a ray tracing algorithm to generate echo data, tracing each ray from the transducer through the scene and back to the sensor. To replicate advanced ultrasound imaging, we introduce a ray emission scheme optimized for plane wave imaging, incorporating delay and steering capabilities. Furthermore, we integrate a standard signal processing pipeline to simulate end-to-end ultrasound image formation. We showcase the efficacy of the proposed pipeline by modeling synthetic scenes featuring highly reflective objects, such as bones. In doing so, our proposed approach, UltraRay, not only enhances the overall visual quality but also improves the realism of the simulated images by accurately capturing secondary reflections and reducing unnatural artifacts. By building on top of a differentiable framework, the proposed pipeline lays the groundwork for a fast and differentiable ultrasound simulation tool necessary for gradient-based optimization, enabling advanced ultrasound beamforming strategies, neural network integration, and accurate inverse scene reconstruction.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP</title>
<link>https://arxiv.org/abs/2501.10913</link>
<guid>https://arxiv.org/abs/2501.10913</guid>
<content:encoded><![CDATA[
arXiv:2501.10913v3 Announce Type: replace 
Abstract: While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation - such as failing to differentiate concepts like "parking" from "no parking" - poses substantial challenges. By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data. To address this, we introduce data generation pipelines that employ a large language model (LLM) and a multimodal LLM to produce negation-inclusive captions. Fine-tuning CLIP with data generated from our pipelines, we develop NegationCLIP, which enhances negation awareness while preserving the generality. Moreover, to enable a comprehensive evaluation of negation understanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs' ability to interpret negation across diverse expressions and positions within a sentence. Experiments on various CLIP architectures validate the effectiveness of our data generation pipelines in enhancing CLIP's ability to perceive negation accurately. Additionally, NegationCLIP's enhanced negation awareness has practical applications across various multimodal tasks, demonstrated by performance gains in text-to-image generation and referring image segmentation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problems using Diffusion with Iterative Colored Renoising</title>
<link>https://arxiv.org/abs/2501.17468</link>
<guid>https://arxiv.org/abs/2501.17468</guid>
<content:encoded><![CDATA[
arXiv:2501.17468v4 Announce Type: replace 
Abstract: Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and "renoises" the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting "DDfire" offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at https://github.com/matt-bendel/DDfire
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging</title>
<link>https://arxiv.org/abs/2502.02171</link>
<guid>https://arxiv.org/abs/2502.02171</guid>
<content:encoded><![CDATA[
arXiv:2502.02171v4 Announce Type: replace 
Abstract: Access to below-canopy volumetric vegetation data is crucial for understanding ecosystem dynamics. We address the long-standing limitation of remote sensing to penetrate deep into dense canopy layers. LiDAR and radar are currently considered the primary options for measuring 3D vegetation structures, while cameras can only extract the reflectance and depth of top layers. Using conventional, high-resolution aerial images, our approach allows sensing deep into self-occluding vegetation volumes, such as forests. It is similar in spirit to the imaging process of wide-field microscopy, but can handle much larger scales and strong occlusion. We scan focal stacks by synthetic-aperture imaging with drones and reduce out-of-focus signal contributions using pre-trained 3D convolutional neural networks with mean squared error (MSE) as the loss function. The resulting volumetric reflectance stacks contain low-frequency representations of the vegetation volume. Combining multiple reflectance stacks from various spectral channels provides insights into plant health, growth, and environmental conditions throughout the entire vegetation volume. Compared with simulated ground truth, our correction leads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of 220 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of 0.05 when comparing with the top-vegetation layer that was measured with classical multispectral aerial imaging.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore</title>
<link>https://arxiv.org/abs/2502.20034</link>
<guid>https://arxiv.org/abs/2502.20034</guid>
<content:encoded><![CDATA[
arXiv:2502.20034v3 Announce Type: replace 
Abstract: Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination. This study revisits the previous claim that the cause of such hallucinations lies in the limited representational capacity of the vision encoder. Our analysis implies that the capacity of the vision encoder is not necessarily a major limiting factor in detecting object hallucination. Based on this insight, we propose Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun level. Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of \textbf{39.6\%} without additional training. We further demonstrate that F-CLIPScore-based data filtering reduces object hallucination in LVLM (4.9\% in POPE).
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation</title>
<link>https://arxiv.org/abs/2503.03453</link>
<guid>https://arxiv.org/abs/2503.03453</guid>
<content:encoded><![CDATA[
arXiv:2503.03453v2 Announce Type: replace 
Abstract: Hemodynamic parameters such as pressure and wall shear stress play an important role in diagnosis, prognosis, and treatment planning in cardiovascular diseases. These parameters can be accurately computed using computational fluid dynamics (CFD), but CFD is computationally intensive. Hence, deep learning methods have been adopted as a surrogate to rapidly estimate CFD outcomes. A drawback of such data-driven models is the need for time-consuming reference CFD simulations for training. In this work, we introduce an active learning framework to reduce the number of CFD simulations required for the training of surrogate models, lowering the barriers to their deployment in new applications. We propose three distinct querying strategies to determine for which unlabeled samples CFD simulations should be obtained. These querying strategies are based on geometrical variance, ensemble uncertainty, and adherence to the physics governing fluid dynamics. We benchmark these methods on velocity field estimation in synthetic coronary artery bifurcations and find that they allow for substantial reductions in annotation cost. Notably, we find that our strategies reduce the number of samples required by up to 50% and make the trained models more robust to difficult cases. Our results show that active learning is a feasible strategy to increase the potential of deep learning-based CFD surrogates.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Action Segmentation Transformer</title>
<link>https://arxiv.org/abs/2503.06316</link>
<guid>https://arxiv.org/abs/2503.06316</guid>
<content:encoded><![CDATA[
arXiv:2503.06316v3 Announce Type: replace 
Abstract: Most recent work on action segmentation relies on pre-computed frame features from models trained on other tasks and typically focuses on framewise encoding and labeling without explicitly modeling action segments. To overcome these limitations, we introduce the End-to-End Action Segmentation Transformer (EAST), which processes raw video frames directly -- eliminating the need for pre-extracted features and enabling true end-to-end training. Our contributions are as follows: (1) a lightweight adapter design for effective fine-tuning of large backbones; (2) an efficient segmentation-by-detection framework for leveraging action proposals predicted over a coarsely downsampled video; and (3) a novel action-proposal-based data augmentation strategy. EAST achieves SOTA performance on standard benchmarks, including GTEA, 50Salads, Breakfast, and Assembly-101.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title>
<link>https://arxiv.org/abs/2503.11519</link>
<guid>https://arxiv.org/abs/2503.11519</guid>
<content:encoded><![CDATA[
arXiv:2503.11519v2 Announce Type: replace 
Abstract: Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Text-to-Image and Text-to-Video Synthesis with a Conditional Fr\'{e}chet Distance</title>
<link>https://arxiv.org/abs/2503.21721</link>
<guid>https://arxiv.org/abs/2503.21721</guid>
<content:encoded><![CDATA[
arXiv:2503.21721v2 Announce Type: replace 
Abstract: Evaluating text-to-image and text-to-video models is challenging due to a fundamental disconnect: established metrics fail to jointly measure visual quality and semantic alignment with text, leading to a poor correlation with human judgments. To address this critical issue, we propose cFreD, a general metric based on a Conditional Fr\'echet Distance that unifies the assessment of visual fidelity and text-prompt consistency into a single score. Existing metrics such as Fr\'echet Inception Distance (FID) capture image quality but ignore text conditioning while alignment scores such as CLIPScore are insensitive to visual quality. Furthermore, learned preference models require constant retraining and are unlikely to generalize to novel architectures or out-of-distribution prompts. Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, cFreD exhibits a higher correlation with human judgments compared to statistical metrics , including metrics trained with human preferences. Our findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text conditioned models, standardizing benchmarking in this rapidly evolving field. We release our evaluation toolkit and benchmark.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion</title>
<link>https://arxiv.org/abs/2504.19258</link>
<guid>https://arxiv.org/abs/2504.19258</guid>
<content:encoded><![CDATA[
arXiv:2504.19258v3 Announce Type: replace 
Abstract: LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel framework for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies observable regions from both modalities to guide feature alignment. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on KITTI and KITTI-360 datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at 1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and data are publicly available at: https://github.com/kang-1-2-3/OPAL.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heat Diffusion Models -- Interpixel Attention Mechanism</title>
<link>https://arxiv.org/abs/2504.19600</link>
<guid>https://arxiv.org/abs/2504.19600</guid>
<content:encoded><![CDATA[
arXiv:2504.19600v3 Announce Type: replace 
Abstract: Denoising Diffusion Probabilistic Models (DDPM) process images as a whole. Since adjacent pixels are highly likely to belong to the same object, we propose the Heat Diffusion Model (HDM) to further preserve image details and generate more realistic images. HDM essentially is a DDPM that incorporates an attention mechanism between pixels. In HDM, the discrete form of the two-dimensional heat equation is integrated into the diffusion and generation formulas of DDPM, enabling the model to compute relationships between neighboring pixels during image processing. Our experiments demonstrate that HDM can generate higher-quality samples compared to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing</title>
<link>https://arxiv.org/abs/2505.22141</link>
<guid>https://arxiv.org/abs/2505.22141</guid>
<content:encoded><![CDATA[
arXiv:2505.22141v2 Announce Type: replace 
Abstract: Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is indispensable for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, flexible adjustment of visual attributes, such as hairstyle, accessories, and subtle facial features, is essential for aligning with user preferences, reflecting diverse brand identities and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method achieves comparable or superior performance to representative baseline methods in lip-sync accuracy, video quality, and attribute controllability. Project page: https://peterfanfan.github.io/FaceEditTalker/
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</title>
<link>https://arxiv.org/abs/2506.10568</link>
<guid>https://arxiv.org/abs/2506.10568</guid>
<content:encoded><![CDATA[
arXiv:2506.10568v2 Announce Type: replace 
Abstract: In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://lizhenwangt.github.io/DreamActor-H1/.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition</title>
<link>https://arxiv.org/abs/2506.14243</link>
<guid>https://arxiv.org/abs/2506.14243</guid>
<content:encoded><![CDATA[
arXiv:2506.14243v3 Announce Type: replace 
Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel-Optimization-Free Patch Attack on Stereo Depth Estimation</title>
<link>https://arxiv.org/abs/2506.17632</link>
<guid>https://arxiv.org/abs/2506.17632</guid>
<content:encoded><![CDATA[
arXiv:2506.17632v2 Announce Type: replace 
Abstract: Stereo Depth Estimation (SDE) is essential for scene perception in vision-based systems such as autonomous driving. Prior work shows SDE is vulnerable to pixel-optimization attacks, but these methods are limited to digital, static, and view-specific settings, making them impractical. This raises a central question: how to design deployable, adaptive, and transferable attacks under realistic constraints? We present two contributions to answer it. First, we build a unified framework that extends pixel-optimization attacks to four stereo-matching stages: feature extraction, cost-volume construction, cost aggregation, and disparity regression. Through systematic evaluation across nine SDE models with realistic constraints like photometric consistency, we show existing attacks suffer from poor transferability. Second, we propose PatchHunter, the first pixel-optimization-free attack. PatchHunter casts patch generation as a search in a structured space of visual patterns that disrupt core SDE assumptions, and uses a reinforcement learning policy to discover effective and transferable patterns efficiently. We evaluate PatchHunter on three levels: autonomous driving dataset, high-fidelity simulator, and real-world deployment. On KITTI, PatchHunter outperforms pixel-level attacks in both effectiveness and black-box transferability. Tests in CARLA and on vehicles with industrial-grade stereo cameras confirm robustness to physical variations. Even under challenging conditions such as low lighting, PatchHunter achieves a D1-all error above 0.4, while pixel-level attacks remain near 0.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models</title>
<link>https://arxiv.org/abs/2507.14811</link>
<guid>https://arxiv.org/abs/2507.14811</guid>
<content:encoded><![CDATA[
arXiv:2507.14811v4 Announce Type: replace 
Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection</title>
<link>https://arxiv.org/abs/2507.16224</link>
<guid>https://arxiv.org/abs/2507.16224</guid>
<content:encoded><![CDATA[
arXiv:2507.16224v2 Announce Type: replace 
Abstract: Existing LiDAR-Camera fusion methods have achieved strong results in 3D object detection. To address the sparsity of point clouds, previous approaches typically construct spatial pseudo point clouds via depth completion as auxiliary input and adopts a proposal-refinement framework to generate detection results. However, introducing pseudo points inevitably brings noise, potentially resulting in inaccurate predictions. Considering the differing roles and reliability levels of each modality, we propose LDRFusion, a novel Lidar-dominant two-stage refinement framework for multi-sensor fusion. The first stage soley relies on LiDAR to produce accurately localized proposals, followed by a second stage where pseudo point clouds are incorporated to detect challenging instances. The instance-level results from both stages are subsequently merged. To further enhance the representation of local structures in pseudo point clouds, we present a hierarchical pseudo point residual encoding module, which encodes neighborhood sets using both feature and positional residuals. Experiments on the KITTI dataset demonstrate that our framework consistently achieves strong performance across multiple categories and difficulty levels.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion</title>
<link>https://arxiv.org/abs/2508.03252</link>
<guid>https://arxiv.org/abs/2508.03252</guid>
<content:encoded><![CDATA[
arXiv:2508.03252v2 Announce Type: replace 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a Robust single-stage fully Sparse 3D object Detection Network with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID</title>
<link>https://arxiv.org/abs/2508.05038</link>
<guid>https://arxiv.org/abs/2508.05038</guid>
<content:encoded><![CDATA[
arXiv:2508.05038v2 Announce Type: replace 
Abstract: Recently, research interest in person re-identification (ReID) has increasingly focused on video-based scenarios, which are essential for robust surveillance and security in varied and dynamic environments. However, existing video-based ReID methods often overlook the necessity of identifying and selecting the most discriminative features from both videos in a query-gallery pair for effective matching. To address this issue, we propose a novel Hierarchical and Adaptive Mixture of Biometric Experts (HAMoBE) framework, which leverages multi-layer features from a pre-trained large model (e.g., CLIP) and is designed to mimic human perceptual mechanisms by independently modeling key biometric features--appearance, static body shape, and dynamic gait--and adaptively integrating them. Specifically, HAMoBE includes two levels: the first level extracts low-level features from multi-layer representations provided by the frozen large model, while the second level consists of specialized experts focusing on long-term, short-term, and temporal features. To ensure robust matching, we introduce a new dual-input decision gating network that dynamically adjusts the contributions of each expert based on their relevance to the input scenarios. Extensive evaluations on benchmarks like MEVID demonstrate that our approach yields significant performance improvements (e.g., +13.0% Rank-1 accuracy).
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Biases in Surgical Operating Rooms with Geometry</title>
<link>https://arxiv.org/abs/2508.08028</link>
<guid>https://arxiv.org/abs/2508.08028</guid>
<content:encoded><![CDATA[
arXiv:2508.08028v2 Announce Type: replace 
Abstract: Deep neural networks are prone to learning spurious correlations, exploiting dataset-specific artifacts rather than meaningful features for prediction. In surgical operating rooms (OR), these manifest through the standardization of smocks and gowns that obscure robust identifying landmarks, introducing model bias for tasks related to modeling OR personnel. Through gradient-based saliency analysis on two public OR datasets, we reveal that CNN models succumb to such shortcuts, fixating on incidental visual cues such as footwear beneath surgical gowns, distinctive eyewear, or other role-specific identifiers. Avoiding such biases is essential for the next generation of intelligent assistance systems in the OR, which should accurately recognize personalized workflow traits, such as surgical skill level or coordination with other staff members. We address this problem by encoding personnel as 3D point cloud sequences, disentangling identity-relevant shape and motion patterns from appearance-based confounders. Our experiments demonstrate that while RGB and geometric methods achieve comparable performance on datasets with apparent simulation artifacts, RGB models suffer a 12% accuracy drop in realistic clinical settings with decreased visual diversity due to standardizations. This performance gap confirms that geometric representations capture more meaningful biometric features, providing an avenue to developing robust methods of modeling humans in the OR.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</title>
<link>https://arxiv.org/abs/2508.10576</link>
<guid>https://arxiv.org/abs/2508.10576</guid>
<content:encoded><![CDATA[
arXiv:2508.10576v2 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision</title>
<link>https://arxiv.org/abs/2403.01777</link>
<guid>https://arxiv.org/abs/2403.01777</guid>
<content:encoded><![CDATA[
arXiv:2403.01777v3 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multimodal understanding, yet their reasoning abilities remain underexplored. Existing benchmarks tend to focus on perception or text-based comprehension, offering limited insight into how well these models perform on structured, logic-driven tasks that require both visual and linguistic reasoning. To address this gap, we introduce NPHardEval4V, a multimodal benchmark suite grounded in four classical NP-hard problems: Knapsack, Set Cover, Traveling Salesperson, and Vertex Cover. Each task is presented through a combination of structured visual layouts and textual prompts, designed to assess the ability of LVLMs to perform combinatorial reasoning under visual-linguistic constraints. We evaluate a set of advanced open-source and closed-source vision-language models under a unified prompting and problem representation framework. This enables fair comparison across models and task types, while isolating key variables affecting performance. Our results show that while these models perform reasonably well on perception-based inputs, they struggle with global optimization, abstraction, and constraint satisfaction. No single model demonstrates consistent reasoning capability across all problem types, and common failure patterns reveal fundamental limitations in current architectures. By leveraging the structure and complexity of NP-hard problems, NPHardEval4V provides a scalable, interpretable, and challenging testbed for diagnosing reasoning behaviors in LVLMs. We hope this benchmark can support the community in building more robust, inference-capable multimodal systems. The benchmark dataset and code are available at https://github.com/lizhouf/NPHardEval4.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTS-Net: Dual-Enhanced Positional Multi-Head Self-Attention for 3D CT Diagnosis of May-Thurner Syndrome</title>
<link>https://arxiv.org/abs/2406.04680</link>
<guid>https://arxiv.org/abs/2406.04680</guid>
<content:encoded><![CDATA[
arXiv:2406.04680v2 Announce Type: replace-cross 
Abstract: May-Thurner Syndrome (MTS) is a vascular condition that affects over 20\% of the population and significantly increases the risk of iliofemoral deep venous thrombosis. Accurate and early diagnosis of MTS using computed tomography (CT) remains a clinical challenge due to the subtle anatomical compression and variability across patients. In this paper, we propose MTS-Net, an end-to-end 3D deep learning framework designed to capture spatial-temporal patterns from CT volumes for reliable MTS diagnosis. MTS-Net builds upon 3D ResNet-18 by embedding a novel dual-enhanced positional multi-head self-attention (DEP-MHSA) module into the Transformer encoder of the network's final stages. The proposed DEP-MHSA employs multi-scale convolution and integrates positional embeddings into both attention weights and residual paths, enhancing spatial context preservation, which is crucial for identifying venous compression. To validate our approach, we curate the first publicly available dataset for MTS, MTS-CT, containing over 747 gender-balanced subjects with standard and enhanced CT scans. Experimental results demonstrate that MTS-Net achieves average 0.79 accuracy, 0.84 AUC, and 0.78 F1-score, outperforming baseline models including 3D ResNet, DenseNet-BC, and BabyNet. Our work not only introduces a new diagnostic architecture for MTS but also provides a high-quality benchmark dataset to facilitate future research in automated vascular syndrome detection. We make our code and dataset publicly available at:https://github.com/Nutingnon/MTS_dep_mhsa.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Asymptomatic Ratoon Stunting Disease Detection With Freely Available Satellite Based Multispectral Imaging</title>
<link>https://arxiv.org/abs/2410.03141</link>
<guid>https://arxiv.org/abs/2410.03141</guid>
<content:encoded><![CDATA[
arXiv:2410.03141v3 Announce Type: replace-cross 
Abstract: Disease detection in sugarcane, particularly the identification of asymptomatic infectious diseases such as Ratoon Stunting Disease (RSD), is critical for effective crop management. This study employed various machine learning techniques to detect the presence of RSD in different sugarcane varieties, using vegetation indices derived from freely available satellite-based spectral data. Our results show that the Support Vector Machine with a Radial Basis Function Kernel (SVM-RBF) was the most effective algorithm, achieving classification accuracy between 85.64% and 96.55%, depending on the variety. Gradient Boosting and Random Forest also demonstrated high performance achieving accuracy between 83.33% to 96.55%, while Logistic Regression and Quadratic Discriminant Analysis showed variable results across different varieties. The inclusion of sugarcane variety and vegetation indices was important in the detection of RSD. This agreed with what was identified in the current literature. Our study highlights the potential of satellite-based remote sensing as a cost-effective and efficient method for large-scale sugarcane disease detection alternative to traditional manual laboratory testing methods.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play Algorithms</title>
<link>https://arxiv.org/abs/2411.13276</link>
<guid>https://arxiv.org/abs/2411.13276</guid>
<content:encoded><![CDATA[
arXiv:2411.13276v3 Announce Type: replace-cross 
Abstract: In this work we study the behavior of the forward-backward (FB) algorithm when the proximity operator is replaced by a sub-iterative procedure to approximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. In particular, we consider both analysis and synthesis Gaussian denoisers within a dictionary framework, obtained by unrolling dual-FB iterations or FB iterations, respectively. We analyze the associated minimization problems as well as the asymptotic behavior of the resulting FB-PnP iterations. In particular, we show that the synthesis Gaussian denoising problem can be viewed as a proximity operator. For each case, analysis and synthesis, we show that the FB-PnP algorithms solve the same problem whether we use only one or an infinite number of sub-iteration to solve the denoising problem at each iteration. To this aim, we show that each "one sub-iteration" strategy within the FB-PnP can be interpreted as a primal-dual algorithm when a warm-restart strategy is used. We further present similar results when using a Moreau-Yosida smoothing of the global problem, for an arbitrary number of sub-iterations. Finally, we provide numerical simulations to illustrate our theoretical results. In particular we first consider a toy compressive sensing example, as well as an image restoration problem in a deep dictionary framework.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Domain-Adaptive Post-Training for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2411.19930</link>
<guid>https://arxiv.org/abs/2411.19930</guid>
<content:encoded><![CDATA[
arXiv:2411.19930v4 Announce Type: replace-cross 
Abstract: Adapting general multimodal large language models (MLLMs) to specific domains, such as scientific and industrial fields, is highly significant in promoting their practical applications. This paper systematically investigates domain adaptation of MLLMs via post-training, focusing on data synthesis, training pipeline, and task evaluation. (1) Data Synthesis: Using only open-source models, we develop a generate-then-filter pipeline that curates diverse visual instruction tasks based on domain-specific image-caption pairs. The resulting data surpass the data synthesized by manual rules or strong closed-source models in enhancing domain-specific performance. (2) Training Pipeline: Unlike general MLLMs that typically adopt a two-stage training paradigm, we find that a single-stage approach is more effective for domain adaptation. (3) Task Evaluation: We conduct extensive experiments in high-impact domains such as biomedicine, food, and remote sensing, by post-training a variety of MLLMs and then evaluating MLLM performance on various domain-specific tasks. Finally, we fully open-source our models, code, and data to encourage future research in this area.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2503.04126</link>
<guid>https://arxiv.org/abs/2503.04126</guid>
<content:encoded><![CDATA[
arXiv:2503.04126v2 Announce Type: replace-cross 
Abstract: Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple agents to work together in mapping unknown environments while simultaneously estimating their own positions. This approach enhances robustness, scalability, and accuracy by sharing information between agents, reducing drift, and enabling collective exploration of larger areas. In this paper, we present Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source decentralized monocular C-SLAM system. By only utilizing low-cost and light-weight monocular vision sensors, our system is well suited for small robots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is validated on physical robots with a custom collision avoidance framework, showcasing its potential in real-time multi-agent autonomous navigation scenarios. We also demonstrate comparable accuracy to state-of-the-art centralized monocular C-SLAM systems. We open-source our code and provide supplementary material online.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGAD: Prototype-Guided Adaptive Distillation for Multi-Modal Learning in AD Diagnosis</title>
<link>https://arxiv.org/abs/2503.04836</link>
<guid>https://arxiv.org/abs/2503.04836</guid>
<content:encoded><![CDATA[
arXiv:2503.04836v2 Announce Type: replace-cross 
Abstract: Missing modalities pose a major issue in Alzheimer's Disease (AD) diagnosis, as many subjects lack full imaging data due to cost and clinical constraints. While multi-modal learning leverages complementary information, most existing methods train only on complete data, ignoring the large proportion of incomplete samples in real-world datasets like ADNI. This reduces the effective training set and limits the full use of valuable medical data. While some methods incorporate incomplete samples, they fail to effectively address inter-modal feature alignment and knowledge transfer challenges under high missing rates. To address this, we propose a Prototype-Guided Adaptive Distillation (PGAD) framework that directly incorporates incomplete multi-modal data into training. PGAD enhances missing modality representations through prototype matching and balances learning with a dynamic sampling strategy. We validate PGAD on the ADNI dataset with varying missing rates (20%, 50%, and 70%) and demonstrate that it significantly outperforms state-of-the-art approaches. Ablation studies confirm the effectiveness of prototype matching and adaptive sampling, highlighting the potential of our framework for robust and scalable AD diagnosis in real-world clinical settings.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning</title>
<link>https://arxiv.org/abs/2504.11195</link>
<guid>https://arxiv.org/abs/2504.11195</guid>
<content:encoded><![CDATA[
arXiv:2504.11195v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and lacks of flexibility for downstream tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in https://github.com/TomSheng21/R-TPT.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title>
<link>https://arxiv.org/abs/2505.11717</link>
<guid>https://arxiv.org/abs/2505.11717</guid>
<content:encoded><![CDATA[
arXiv:2505.11717v2 Announce Type: replace-cross 
Abstract: Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--denoted as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAGS: 3D Tumor-Adaptive Guidance for SAM</title>
<link>https://arxiv.org/abs/2505.17096</link>
<guid>https://arxiv.org/abs/2505.17096</guid>
<content:encoded><![CDATA[
arXiv:2505.17096v2 Announce Type: replace-cross 
Abstract: Foundation models (FMs) such as CLIP and SAM have recently shown great promise in image segmentation tasks, yet their adaptation to 3D medical imaging-particularly for pathology detection and segmentation-remains underexplored. A critical challenge arises from the domain gap between natural images and medical volumes: existing FMs, pre-trained on 2D data, struggle to capture 3D anatomical context, limiting their utility in clinical applications like tumor segmentation. To address this, we propose an adaptation framework called TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D medical tasks through multi-prompt fusion. By preserving most of the pre-trained weights, our approach enhances SAM's spatial feature extraction using CLIP's semantic insights and anatomy-specific prompts. Extensive experiments on three open-source tumor segmentation datasets prove that our model surpasses the state-of-the-art medical image segmentation models (+46.88% over nnUNet), interactive segmentation frameworks, and other established medical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and SAM-B (at least +13% over them). This highlights the robustness and adaptability of our proposed framework across diverse medical segmentation tasks.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction</title>
<link>https://arxiv.org/abs/2506.03804</link>
<guid>https://arxiv.org/abs/2506.03804</guid>
<content:encoded><![CDATA[
arXiv:2506.03804v2 Announce Type: replace-cross 
Abstract: Recent work has shown improved lesion detectability and flexibility to reconstruction hyperparameters (e.g. scanner geometry or dose level) when PET images are reconstructed by leveraging pre-trained diffusion models. Such methods train a diffusion model (without sinogram data) on high-quality, but still noisy, PET images. In this work, we propose a simple method for generating subject-specific PET images from a dataset of multi-subject PET-MR scans, synthesizing "pseudo-PET" images by transforming between different patients' anatomy using image registration. The images we synthesize retain information from the subject's MR scan, leading to higher resolution and the retention of anatomical features compared to the original set of PET images. With simulated and real [$^{18}$F]FDG datasets, we show that pre-training a personalized diffusion model with subject-specific "pseudo-PET" images improves reconstruction accuracy with low-count data. In particular, the method shows promise in combining information from a guidance MR scan without overly imposing anatomical features, demonstrating an improved trade-off between reconstructing PET-unique image features versus features present in both PET and MR. We believe this approach for generating and utilizing synthetic data has further applications to medical imaging tasks, particularly because patient-specific PET images can be generated without resorting to generative deep learning or large training datasets.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-Simulation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.04218</link>
<guid>https://arxiv.org/abs/2506.04218</guid>
<content:encoded><![CDATA[
arXiv:2506.04218v2 Announce Type: replace-cross 
Abstract: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</title>
<link>https://arxiv.org/abs/2506.18088</link>
<guid>https://arxiv.org/abs/2506.18088</guid>
<content:encoded><![CDATA[
arXiv:2506.18088v2 Announce Type: replace-cross 
Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for advancing real-world robotic manipulation. Yet existing datasets remain insufficient for robust bimanual manipulation due to (1) the lack of scalable task generation methods and (2) oversimplified simulation environments. We present RoboTwin 2.0, a scalable framework for automated, large-scale generation of diverse and realistic data, together with unified evaluation protocols for dual-arm manipulation. At its core is RoboTwin-OD, an object library of 731 instances across 147 categories with semantic and manipulation-relevant annotations. Building on this, we design an expert data synthesis pipeline that leverages multimodal language models (MLLMs) and simulation-in-the-loop refinement to automatically generate task-level execution code. To improve sim-to-real transfer, RoboTwin 2.0 applies structured domain randomization along five axes: clutter, lighting, background, tabletop height, and language, enhancing data diversity and policy robustness. The framework is instantiated across 50 dual-arm tasks and five robot embodiments. Empirically, it yields a 10.9% gain in code generation success rate. For downstream policy learning, a VLA model trained with synthetic data plus only 10 real demonstrations achieves a 367% relative improvement over the 10-demo baseline, while zero-shot models trained solely on synthetic data obtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0 in strengthening sim-to-real transfer and robustness to environmental variations. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation. Project Page: https://robotwin-platform.github.io/, Code: https://github.com/robotwin-Platform/robotwin/.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks</title>
<link>https://arxiv.org/abs/2506.23016</link>
<guid>https://arxiv.org/abs/2506.23016</guid>
<content:encoded><![CDATA[
arXiv:2506.23016v2 Announce Type: replace-cross 
Abstract: The global prevalence of dementia is projected to double by 2050, highlighting the urgent need for scalable diagnostic tools. This study utilizes digital cognitive tasks with eye-tracking data correlated with memory processes to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment (MCI), a precursor to dementia. A deep learning model based on VTNet was trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who performed a visual memory task. The model utilizes both time series and spatial data derived from eye-tracking. It was modified to incorporate scan paths, heat maps, and image content. These modifications also enabled testing parameters such as image resolution and task performance, analyzing their impact on model performance. The best model, utilizing $700\times700px$ resolution heatmaps, achieved 68% sensitivity and 76% specificity. Despite operating under more challenging conditions (e.g., smaller dataset size, shorter task duration, or a less standardized task), the model's performance is comparable to an Alzheimer's study using similar methods (70% sensitivity and 73% specificity). These findings contribute to the development of automated diagnostic tools for MCI. Future work should focus on refining the model and using a standardized long-term visual memory task.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyVision: Agentic Vision with Dynamic Tooling</title>
<link>https://arxiv.org/abs/2507.07998</link>
<guid>https://arxiv.org/abs/2507.07998</guid>
<content:encoded><![CDATA[
arXiv:2507.07998v3 Announce Type: replace-cross 
Abstract: LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2508.01490</link>
<guid>https://arxiv.org/abs/2508.01490</guid>
<content:encoded><![CDATA[
arXiv:2508.01490v2 Announce Type: replace-cross 
Abstract: Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches</title>
<link>https://arxiv.org/abs/2508.18293</link>
<guid>https://arxiv.org/abs/2508.18293</guid>
<content:encoded><![CDATA[
<div> deep learning, underwater object detection, sonar simulation, template matching, autonomous underwater vehicles

Summary: 
The article discusses the challenges of underwater 3D object detection due to the scarcity of training data. Traditional approaches struggle in the harsh acoustic environment, leading to the development of new paradigms for training-free detection. The study introduces a dual approach that combines a physics-based sonar simulation pipeline generating synthetic training data for neural networks and a model-based template matching system utilizing geometric priors of target objects. While neural networks trained on synthetic data show high performance in simulated scenes, they suffer from domain shift on real sonar data. In contrast, the template matching approach achieves impressive results on real data without the need for training, showcasing robustness to acoustic noise and environmental variations. This research challenges the notion of data-heavy deep learning in underwater scenarios and establishes a benchmark for training-free underwater 3D detection, opening up possibilities for applications in autonomous underwater vehicle navigation, marine archaeology, and offshore infrastructure monitoring in data-scarce environments. 


<br /><br /> <div>
arXiv:2508.18293v1 Announce Type: new 
Abstract: Underwater 3D object detection remains one of the most challenging frontiers in computer vision, where traditional approaches struggle with the harsh acoustic environment and scarcity of training data. While deep learning has revolutionized terrestrial 3D detection, its application underwater faces a critical bottleneck: obtaining sufficient annotated sonar data is prohibitively expensive and logistically complex, often requiring specialized vessels, expert surveyors, and favorable weather conditions. This work addresses a fundamental question: Can we achieve reliable underwater 3D object detection without real-world training data? We tackle this challenge by developing and comparing two paradigms for training-free detection of artificial structures in multibeam echo-sounder point clouds. Our dual approach combines a physics-based sonar simulation pipeline that generates synthetic training data for state-of-the-art neural networks, with a robust model-based template matching system that leverages geometric priors of target objects. Evaluation on real bathymetry surveys from the Baltic Sea reveals surprising insights: while neural networks trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated scenes, they drop to 40% mAP on real sonar data due to domain shift. Conversely, our template matching approach maintains 83% mAP on real data without requiring any training, demonstrating remarkable robustness to acoustic noise and environmental variations. Our findings challenge conventional wisdom about data-hungry deep learning in underwater domains and establish the first large-scale benchmark for training-free underwater 3D detection. This work opens new possibilities for autonomous underwater vehicle navigation, marine archaeology, and offshore infrastructure monitoring in data-scarce environments where traditional machine learning approaches fail.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection</title>
<link>https://arxiv.org/abs/2508.18294</link>
<guid>https://arxiv.org/abs/2508.18294</guid>
<content:encoded><![CDATA[
<div> Keywords: brain tumor detection, MRI, MobileDenseAttn, fusion model, interpretable

Summary:
MobileDenseAttn is introduced as a fusion model of MobileNetV2 and DenseNet201 for efficient and interpretable brain tumor detection using MRI scans. The model shows high accuracy with a testing accuracy of 98.35% and a stable F1 score of 0.9835. It outperforms baseline models like VGG19, with a +3.67% accuracy increase and a 39.3% decrease in training time. The model is trained on a dataset of 6,020 MRI scans representing different tumor types and normal samples. The GradCAM heatmaps generated by the model accurately show tumor-affected areas, enhancing interpretability. MobileDenseAttn demonstrates stability and potential as a clinically practical tool for identifying brain tumors in real-world scenarios. The model offers improved feature representation scale, computing efficiency, and visual explanations, addressing the limitations of current approaches. 

<br /><br />Summary: <div>
arXiv:2508.18294v1 Announce Type: new 
Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely diagnostics and treatment; however, manual analysis is commonly long and error-prone. Current approaches are not universal because they have limited generalization to heterogeneous tumors, are computationally inefficient, are not interpretable, and lack transparency, thus limiting trustworthiness. To overcome these issues, we introduce MobileDenseAttn, a fusion model of dual streams of MobileNetV2 and DenseNet201 that can help gradually improve the feature representation scale, computing efficiency, and visual explanations via GradCAM. Our model uses feature level fusion and is trained on an augmented dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors, and normal samples. Measured under strict 5-fold cross-validation protocols, MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of 98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The extensive validation shows the stability of the model, and the comparative analysis proves that it is a great advancement over the baseline models (VGG19, DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease in training time compared to VGG19. The GradCAM heatmaps clearly show tumor-affected areas, offering clinically significant localization and improving interpretability. These findings position MobileDenseAttn as an efficient, high performance, interpretable model with a high probability of becoming a clinically practical tool in identifying brain tumors in the real world.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can VLMs Recall Factual Associations From Visual References?</title>
<link>https://arxiv.org/abs/2508.18297</link>
<guid>https://arxiv.org/abs/2508.18297</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, multimodal grounding, factual knowledge, internal states, probes

Summary: 
Vision Language Models (VLMs) show a deficiency in their ability to link factual knowledge with visual representations of entities, leading to reduced accuracy in recalling information from visual references. By analyzing internal model states, probes are able to predict unreliable responses with over 92% accuracy. These probes can be used to identify cases where a VLM will fail to correctly answer questions based on multimodal input. Integrating these probes into a visual question answering task increases coverage by 7.87% and reduces the risk of error by 0.9%. Addressing this systematic deficiency in language grounding is crucial for improving the performance of VLMs in understanding and generating responses based on both textual and visual information. Future research should focus on enhancing the multimodal grounding capabilities of VLMs to bridge the gap between textual and visual representations for more accurate and reliable performance.<br /><br />Summary: <div>
arXiv:2508.18297v1 Announce Type: new 
Abstract: Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs). While VLMs can recall factual associations when provided a textual reference to an entity; their ability to do so is significantly diminished when the reference is visual instead. Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. We show that such linking failures are correlated with the expression of distinct patterns in model internal states, and that probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute). Addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SERES: Semantic-aware neural reconstruction from sparse views</title>
<link>https://arxiv.org/abs/2508.18314</link>
<guid>https://arxiv.org/abs/2508.18314</guid>
<content:encoded><![CDATA[
<div> semantic-aware neural reconstruction, 3D high-fidelity models, sparse images, radiance ambiguity, patch-based semantic logits

Summary:
The article introduces a novel semantic-aware neural reconstruction method to generate high-fidelity 3D models from sparse images. The method addresses the issue of radiance ambiguity caused by mismatched features in sparse input by enriching neural implicit representations with patch-based semantic logits. A new regularization technique based on geometric primitive masks is also proposed to reduce shape ambiguity. Experimental evaluations demonstrate the effectiveness of the approach, showing significant reductions in average chamfer distances on the DTU dataset for SparseNeuS and VolRecon. When used as a plugin for dense reconstruction baselines like NeuS and Neuralangelo, the method achieves substantial improvements in accuracy on the DTU dataset. <div>
arXiv:2508.18314v1 Announce Type: new 
Abstract: We propose a semantic-aware neural reconstruction method to generate 3D high-fidelity models from sparse images. To tackle the challenge of severe radiance ambiguity caused by mismatched features in sparse input, we enrich neural implicit representations by adding patch-based semantic logits that are optimized together with the signed distance field and the radiance field. A novel regularization based on the geometric primitive masks is introduced to mitigate shape ambiguity. The performance of our approach has been verified in experimental evaluation. The average chamfer distances of our reconstruction on the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When working as a plugin for those dense reconstruction baselines such as NeuS and Neuralangelo, the average error on the DTU dataset can be reduced by 69% and 68% respectively.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset</title>
<link>https://arxiv.org/abs/2508.18315</link>
<guid>https://arxiv.org/abs/2508.18315</guid>
<content:encoded><![CDATA[
<div> Keywords: illegal landfill, deep learning, AerialWaste dataset, lightweight models, ensemble model

Summary: 
Illegal landfills pose a significant hazard worldwide, with their identification being a challenge due to manual efforts. Deep learning offers a solution by efficiently identifying these landfills, saving time and resources. The AerialWaste dataset, containing images from the Lombardy region of Italy, provides high-quality data for research on illegal landfill detection. Lightweight models like Mobilenetv2, Googlenet, and Densenet were found to be effective in detecting landfills without overfitting. By combining the best performing models into an ensemble model, binary classification on the dataset achieved high accuracy of 92.33%, precision of 92.67%, sensitivity of 92.33%, F1 score of 92.41%, and specificity of 92.71%. This study demonstrates the potential of deep learning and lightweight models in efficiently detecting illegal landfills, improving overall performance and accuracy. 

<br /><br />Summary: <div>
arXiv:2508.18315v1 Announce Type: new 
Abstract: Illegal landfills are posing as a hazardous threat to people all over the world. Due to the arduous nature of manually identifying the location of landfill, many landfills go unnoticed by authorities and later cause dangerous harm to people and environment. Deep learning can play a significant role in identifying these landfills while saving valuable time, manpower and resources. Despite being a burning concern, good quality publicly released datasets for illegal landfill detection are hard to find due to security concerns. However, AerialWaste Dataset is a large collection of 10434 images of Lombardy region of Italy. The images are of varying qualities, collected from three different sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains professionally curated, diverse and high-quality images which makes it particularly suitable for scalable and impactful research. As we trained several models to compare results, we found complex and heavy models to be prone to overfitting and memorizing training data instead of learning patterns. Therefore, we chose lightweight simpler models which could leverage general features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet, MobileVit and other lightweight deep learning models were used to train and validate the dataset as they achieved significant success with less overfitting. As we saw substantial improvement in the performance using some of these models, we combined the best performing models and came up with an ensemble model. With the help of ensemble and fusion technique, binary classification could be performed on this dataset with 92.33% accuracy, 92.67% precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.18322</link>
<guid>https://arxiv.org/abs/2508.18322</guid>
<content:encoded><![CDATA[
<div> graph construction, multimodal fusion, semantic grounding, contrastive learning, sentiment analysis  
Summary:  
The article introduces a novel framework, the Structural-Semantic Unifier (SSU), for multimodal sentiment analysis that effectively integrates text, acoustic, and visual modalities. SSU dynamically constructs modality-specific graphs using linguistic syntax for text and a text-guided attention mechanism for acoustic and visual modalities to capture detailed intra-modal relationships and semantic interactions. It also introduces a semantic anchor to align heterogeneous semantic spaces across modalities. A multiview contrastive learning objective is developed to promote discriminability, semantic consistency, and structural coherence. SSU achieves state-of-the-art performance on benchmark datasets CMU-MOSI and CMU-MOSEI while reducing computational overhead. Qualitative analyses confirm SSU's interpretability and ability to capture nuanced emotional patterns through semantically grounded interactions.  
<br /><br />Summary: <div>
arXiv:2508.18322v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by effectively integrating textual, acoustic, and visual modalities. Despite notable progress, existing multimodal fusion methods often neglect modality-specific structural dependencies and semantic misalignment, limiting their quality, interpretability, and robustness. To address these challenges, we propose a novel framework called the Structural-Semantic Unifier (SSU), which systematically integrates modality-specific structural information and cross-modal semantic grounding for enhanced multimodal representations. Specifically, SSU dynamically constructs modality-specific graphs by leveraging linguistic syntax for text and a lightweight, text-guided attention mechanism for acoustic and visual modalities, thus capturing detailed intra-modal relationships and semantic interactions. We further introduce a semantic anchor, derived from global textual semantics, that serves as a cross-modal alignment hub, effectively harmonizing heterogeneous semantic spaces across modalities. Additionally, we develop a multiview contrastive learning objective that promotes discriminability, semantic consistency, and structural coherence across intra- and inter-modal views. Extensive evaluations on two widely used benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently achieves state-of-the-art performance while significantly reducing computational overhead compared to prior methods. Comprehensive qualitative analyses further validate SSU's interpretability and its ability to capture nuanced emotional patterns through semantically grounded interactions.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses</title>
<link>https://arxiv.org/abs/2508.18389</link>
<guid>https://arxiv.org/abs/2508.18389</guid>
<content:encoded><![CDATA[
<div> encoder-decoder neural network, 3D Gaussian Splatting, FastAvatar, pose-invariant, face reconstruction<br />
Summary:<br />
FastAvatar is a novel framework that efficiently generates pose-invariant 3D Gaussian Splatting (3DGS) models from a single face image in milliseconds. It utilizes an encoder-decoder neural network architecture to create a 3DGS face template model from a training dataset, encoding input face images into a pose-invariant embedding for accurate reconstruction. By predicting residuals within a feed-forward process, FastAvatar achieves superior reconstruction quality compared to existing methods like GAGAvatar, while running 1000 times faster than optimization-based techniques like FlashAvatar. Its unique latent space design enables real-time identity interpolation and attribute editing, broadening the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.<br /><br />Summary: <div>
arXiv:2508.18389v1 Announce Type: new 
Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Face and Fingerprint Templates in Humanitarian Biometric Systems</title>
<link>https://arxiv.org/abs/2508.18415</link>
<guid>https://arxiv.org/abs/2508.18415</guid>
<content:encoded><![CDATA[
<div> mobile biometric system, biometric template protection, BTP scheme, vulnerability, PolyProtect

Summary:
A new mobile biometric system implementing a biometric template protection scheme has been developed to address the risks associated with using biometrics in humanitarian and emergency scenarios. The system meets functional, operational, security, and privacy requirements specific to vulnerable contexts. PolyProtect, a method working with neural network face embeddings, is identified as the most suitable due to its effectiveness and lightweight computational burden. The system's evaluation on real-world face data from a project in Ethiopia shows promising results for verification, identification accuracy, irreversibility, and unlinkability. Additionally, PolyProtect is tested with fingerprints, showcasing its modality-independence. This evaluation marks the first time PolyProtect has been assessed for identification scenarios and fingerprint biometrics. Further research and code release are planned. <br /><br />Summary: <div>
arXiv:2508.18415v1 Announce Type: new 
Abstract: In humanitarian and emergency scenarios, the use of biometrics can dramatically improve the efficiency of operations, but it poses risks for the data subjects, which are exacerbated in contexts of vulnerability. To address this, we present a mobile biometric system implementing a biometric template protection (BTP) scheme suitable for these scenarios. After rigorously formulating the functional, operational, and security and privacy requirements of these contexts, we perform a broad comparative analysis of the BTP landscape. PolyProtect, a method designed to operate on neural network face embeddings, is identified as the most suitable method due to its effectiveness, modularity, and lightweight computational burden. We evaluate PolyProtect in terms of verification and identification accuracy, irreversibility, and unlinkability, when this BTP method is applied to face embeddings extracted using EdgeFace, a novel state-of-the-art efficient feature extractor, on a real-world face dataset from a humanitarian field project in Ethiopia. Moreover, as PolyProtect promises to be modality-independent, we extend its evaluation to fingerprints. To the best of our knowledge, this is the first time that PolyProtect has been evaluated for the identification scenario and for fingerprint biometrics. Our experimental results are promising, and we plan to release our code
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</title>
<link>https://arxiv.org/abs/2508.18421</link>
<guid>https://arxiv.org/abs/2508.18421</guid>
<content:encoded><![CDATA[
<div> relational competence, dynamic relational graphs, multimodal corpora, human activity recognition, spatio-temporal relations
Summary:<br /><br />Vision foundation models (FMs) in computer vision have limitations in tasks requiring entity reasoning, spatio-temporal relations. Next-gen FMs should incorporate dynamic relational graphs for improved semantic fidelity, robustness, interpretability, efficiency. Evidence from human manipulation action recognition, brain tumor segmentation supports this approach. Lightweight graph-reasoning modules enhance performance and out-of-distribution robustness. Hybrid models achieve memory and hardware efficiency, suitable for resource-constrained deployment. Research agenda focuses on dynamic graph construction, multi-level relational reasoning, cross-modal fusion, and evaluating relational competence in structured vision tasks. <div>
arXiv:2508.18421v1 Announce Type: new 
Abstract: Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPLC: A Dataset for License Plate Legibility Classification</title>
<link>https://arxiv.org/abs/2508.18425</link>
<guid>https://arxiv.org/abs/2508.18425</guid>
<content:encoded><![CDATA[
<div> Dataset, Automatic License Plate Recognition, Super-resolution, Legibility classification, Image pre-processing <br />
Summary:
A new dataset, the LPLC dataset, containing 10,210 images of vehicles with 12,687 annotated license plates (LPs) for legibility classification has been introduced for research in Automatic License Plate Recognition (ALPR). The dataset includes various vehicle types, lighting conditions, and image quality levels. It features fine-grained annotations for vehicle and LP occlusions, four legibility categories, and character labels. A benchmark classification task using three image recognition networks (ViT, ResNet, YOLO) was conducted to determine if an LP image is good, needs super-resolution, or is illegible. Results showed F1 scores below 80% for all models, indicating the difficulty of the task. The study emphasizes the importance of further research in improving ALPR for illegible LPs. The dataset is publicly available for further research on GitHub. <br /> <div>
arXiv:2508.18425v1 Announce Type: new 
Abstract: Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera/image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at https://github.com/lmlwojcik/lplc-dataset.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering</title>
<link>https://arxiv.org/abs/2508.18430</link>
<guid>https://arxiv.org/abs/2508.18430</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, dermatological visual question answering, Specialist-Generalist framework, diagnostic accuracy, computational efficiency.

Summary:
CLARIFY is a new Specialist-Generalist framework for dermatological visual question answering that combines a domain-trained image classifier (the Specialist) and a compressed conversational VLM (the Generalist). The Specialist provides fast and accurate diagnostic predictions, guiding the Generalist's reasoning and enhancing diagnostic accuracy. A knowledge graph-based retrieval module ensures accuracy and reliability by grounding responses in dermatological knowledge. CLARIFY outperforms baseline models in diagnostic accuracy, reducing VRAM requirements and latency. The hierarchical design improves computational efficiency, making it practical for real-world clinical deployment. This framework demonstrates the power of combining specialized and general models for building lightweight, trustworthy AI systems in medical tasks.<br /><br />Summary: <div>
arXiv:2508.18430v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have shown significant potential for medical tasks; however, their general-purpose nature can limit specialized diagnostic accuracy, and their large size poses substantial inference costs for real-world clinical deployment. To address these challenges, we introduce CLARIFY, a Specialist-Generalist framework for dermatological visual question answering (VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image classifier (the Specialist) that provides fast and highly accurate diagnostic predictions, and (ii) a powerful yet compressed conversational VLM (the Generalist) that generates natural language explanations to user queries. In our framework, the Specialist's predictions directly guide the Generalist's reasoning, focusing it on the correct diagnostic path. This synergy is further enhanced by a knowledge graph-based retrieval module, which grounds the Generalist's responses in factual dermatological knowledge, ensuring both accuracy and reliability. This hierarchical design not only reduces diagnostic errors but also significantly improves computational efficiency. Experiments on our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an 18\% improvement in diagnostic accuracy over the strongest baseline, a fine-tuned, uncompressed single-line VLM, while reducing the average VRAM requirement and latency by at least 20\% and 5\%, respectively. These results indicate that a Specialist-Generalist system provides a practical and powerful paradigm for building lightweight, trustworthy, and clinically viable AI systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results</title>
<link>https://arxiv.org/abs/2508.18445</link>
<guid>https://arxiv.org/abs/2508.18445</guid>
<content:encoded><![CDATA[
<div> Challenge, Face Image Quality Assessment, Lightweight models, Mean Opinion Scores, Correlation metrics
Summary:
The VQualA 2025 Challenge on Face Image Quality Assessment focused on creating efficient models for predicting Mean Opinion Scores on face images with degradations. Participants were limited to 0.5 GFLOPs and 5 million parameters for their models. The challenge attracted 127 participants who submitted a total of 1519 entries. Submissions were evaluated based on correlation metrics using a dataset of in-the-wild face images. The goal was to advance the development of practical FIQA approaches. <div>
arXiv:2508.18445v1 Announce Type: new 
Abstract: Face images play a crucial role in numerous applications; however, real-world conditions frequently introduce degradations such as noise, blur, and compression artifacts, affecting overall image quality and hindering subsequent tasks. To address this challenge, we organized the VQualA 2025 Challenge on Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops. Participants created lightweight and efficient models (limited to 0.5 GFLOPs and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on face images with arbitrary resolutions and realistic degradations. Submissions underwent comprehensive evaluations through correlation metrics on a dataset of in-the-wild face images. This challenge attracted 127 participants, with 1519 final submissions. This report summarizes the methodologies and findings for advancing the development of practical FIQA approaches.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</title>
<link>https://arxiv.org/abs/2508.18463</link>
<guid>https://arxiv.org/abs/2508.18463</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, surveillance footage, context-aware, zero-shot, spatiotemporal dynamics <br />
Summary: <br />
This article presents a novel framework for context-aware zero-shot anomaly detection in surveillance footage, addressing the challenge of detecting unpredictable anomalies without prior exposure during training. The framework combines TimeSformer, DPC, and CLIP to capture spatiotemporal dynamics and semantic context. TimeSformer extracts spatial-temporal features, while DPC forecasts future representations for identifying temporal deviations. A CLIP-based semantic stream enables concept-level anomaly detection using context-specific text prompts. The system is trained using InfoNCE and CPC losses, aligning visual inputs with temporal and semantic representations. A context-gating mechanism enhances decision-making by modulating predictions with scene-aware cues or global video features. By integrating predictive modeling with vision-language understanding, the framework can generalize to unseen behaviors in complex environments, bridging the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance. <div>
arXiv:2508.18463v1 Announce Type: new 
Abstract: Detecting anomalies in surveillance footage is inherently challenging due to their unpredictable and context-dependent nature. This work introduces a novel context-aware zero-shot anomaly detection framework that identifies abnormal events without exposure to anomaly examples during training. The proposed hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context. TimeSformer serves as the vision backbone to extract rich spatial-temporal features, while DPC forecasts future representations to identify temporal deviations. Furthermore, a CLIP-based semantic stream enables concept-level anomaly detection through context-specific text prompts. These components are jointly trained using InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic representations. A context-gating mechanism further enhances decision-making by modulating predictions with scene-aware cues or global video features. By integrating predictive modeling with vision-language understanding, the system can generalize to previously unseen behaviors in complex environments. This framework bridges the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance. The code for this research has been made available at https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance</title>
<link>https://arxiv.org/abs/2508.18506</link>
<guid>https://arxiv.org/abs/2508.18506</guid>
<content:encoded><![CDATA[
<div> LiDAR, scene flow estimation, self-supervised, cross-modal label transfer, DoGFlow<br />
Summary:<br />
DoGFlow introduces a self-supervised framework for accurate 3D scene flow estimation without manual annotations. By leveraging radar Doppler measurements, it computes motion pseudo-labels in real-time and transfers them to the LiDAR domain, achieving superior performance on the MAN TruckScenes dataset. The dynamic-aware association and ambiguity-resolved propagation techniques improve label efficiency, enabling LiDAR backbones to achieve over 90% of fully supervised performance using only 10% of ground truth data. This novel approach addresses the challenge of long-range and adverse weather scenarios, outperforming existing self-supervised methods in autonomous navigation systems. To learn more about DoGFlow, visit https://ajinkyakhoche.github.io/DogFlow/.<br /> <div>
arXiv:2508.18506v1 Announce Type: new 
Abstract: Accurate 3D scene flow estimation is critical for autonomous systems to navigate dynamic environments safely, but creating the necessary large-scale, manually annotated datasets remains a significant bottleneck for developing robust perception models. Current self-supervised methods struggle to match the performance of fully supervised approaches, especially in challenging long-range and adverse weather scenarios, while supervised methods are not scalable due to their reliance on expensive human labeling. We introduce DoGFlow, a novel self-supervised framework that recovers full 3D object motions for LiDAR scene flow estimation without requiring any manual ground truth annotations. This paper presents our cross-modal label transfer approach, where DoGFlow computes motion pseudo-labels in real-time directly from 4D radar Doppler measurements and transfers them to the LiDAR domain using dynamic-aware association and ambiguity-resolved propagation. On the challenging MAN TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised methods and improves label efficiency by enabling LiDAR backbones to achieve over 90% of fully supervised performance with only 10% of the ground truth data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors</title>
<link>https://arxiv.org/abs/2508.18531</link>
<guid>https://arxiv.org/abs/2508.18531</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D building generation, satellite imagery, geometric priors, Skylines-50K dataset, generalization ability <br />
Summary: <br />
The article introduces SatSkylines, a novel approach for generating 3D buildings using satellite images and coarse geometric priors. Existing methods struggle to accurately reconstruct building structures from top-down satellite views without proper geometric guidance. To overcome this limitation, SatSkylines models the transformation from coarse priors to detailed geometries, enabling flexible geometric control without additional computational cost. The researchers have created the Skylines-50K dataset, containing over 50,000 unique and stylized 3D building assets to support detailed building model generation. Extensive evaluations demonstrate the effectiveness of the proposed model and its strong generalization ability across various scenarios. <div>
arXiv:2508.18531v1 Announce Type: new 
Abstract: We present SatSkylines, a 3D building generation approach that takes satellite imagery and coarse geometric priors. Without proper geometric guidance, existing image-based 3D generation methods struggle to recover accurate building structures from the top-down views of satellite images alone. On the other hand, 3D detailization methods tend to rely heavily on highly detailed voxel inputs and fail to produce satisfying results from simple priors such as cuboids. To address these issues, our key idea is to model the transformation from interpolated noisy coarse priors to detailed geometries, enabling flexible geometric control without additional computational cost. We have further developed Skylines-50K, a large-scale dataset of over 50,000 unique and stylized 3D building assets in order to support the generations of detailed building models. Extensive evaluations indicate the effectiveness of our model and strong generalization ability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Visual Navigation Assistant in 3D RPGs</title>
<link>https://arxiv.org/abs/2508.18539</link>
<guid>https://arxiv.org/abs/2508.18539</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D game environments, visual affordances, Spatial Transition Points, deep-learning pipeline, MSTP selection

Summary:<br /><br />
The article introduces a new research focus on the detection of traversable Spatial Transition Points (STPs) in complex 3D game environments. The task involves identifying the Main STP (MSTP), which lies on the critical path towards the player's macro-objective. A two-stage deep-learning pipeline is proposed, utilizing Faster R-CNN for STP detection and a lightweight MSTP selector for ranking. The study highlights the trade-off between full-network fine-tuning and adapter-only transfer, where the latter proves more effective in low-data scenarios. The research aims to establish baseline performance metrics for future AI-driven navigation aids and level-design tools. The experiments conducted on a diverse dataset from Action RPG titles showcase the feasibility of the proposed approach and offer insights into efficient model adaptation. This work contributes to the advancement of AI-driven map cue presentation and client-side auto-mapping in complex game environments. 

<br /><br /> <div>
arXiv:2508.18539v1 Announce Type: new 
Abstract: In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wan-S2V: Audio-Driven Cinematic Video Generation</title>
<link>https://arxiv.org/abs/2508.18621</link>
<guid>https://arxiv.org/abs/2508.18621</guid>
<content:encoded><![CDATA[
<div> Character animation, audio-driven, Wan-S2V, film-level, expressiveness<br />
<br />
Summary: 
The article introduces a new audio-driven character animation model called Wan-S2V, designed to enhance expressiveness and fidelity in cinematic contexts. The model is built upon Wan and demonstrates improved performance compared to existing methods like Hunyuan-Avatar and Omnihuman. It aims to address the limitations of current state-of-the-art techniques in complex film and television productions by incorporating nuanced character interactions, realistic body movements, and dynamic camera work. Extensive experiments show the superiority of Wan-S2V in achieving film-level character animation, with applications in long-form video generation and precise video lip-sync editing. The model's versatility and ability to handle sophisticated elements make it a promising solution for enhancing audio-driven character animation in cinematic scenarios.<br /><br /> <div>
arXiv:2508.18621v1 Announce Type: new 
Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction</title>
<link>https://arxiv.org/abs/2508.18632</link>
<guid>https://arxiv.org/abs/2508.18632</guid>
<content:encoded><![CDATA[
<div> Decoupling-Reorganization-Fusion framework, cancer survival analysis, modalities integration, random feature reorganization, dynamic MoE fusion<br />
<br />
Summary:<br />
The paper introduces a novel framework called DeReF for cancer survival analysis by integrating information from diverse medical modalities. Existing methods face challenges of fixed fusion schemes and limited information interaction among decoupled features. DeReF addresses these challenges by introducing a random feature reorganization strategy to enhance feature combinations and diversity. It also incorporates a regional cross-attention network to improve the representation quality of decoupled features. Experimental results on Liver Cancer and TCGA datasets demonstrate the effectiveness of DeReF in improving feature fusion and prediction accuracy. The proposed approach shows promise for improving survival-time predictions in cancer research.<br /> <div>
arXiv:2508.18632v1 Announce Type: new 
Abstract: Cancer survival analysis commonly integrates information across diverse medical modalities to make survival-time predictions. Existing methods primarily focus on extracting different decoupled features of modalities and performing fusion operations such as concatenation, attention, and MoE-based (Mixture-of-Experts) fusion. However, these methods still face two key challenges: i) Fixed fusion schemes (concatenation and attention) can lead to model over-reliance on predefined feature combinations, limiting the dynamic fusion of decoupled features; ii) in MoE-based fusion methods, each expert network handles separate decoupled features, which limits information interaction among the decoupled features. To address these challenges, we propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which devises a random feature reorganization strategy between modalities decoupling and dynamic MoE fusion modules.Its advantages are: i) it increases the diversity of feature combinations and granularity, enhancing the generalization ability of the subsequent expert networks; ii) it overcomes the problem of information closure and helps expert networks better capture information among decoupled features. Additionally, we incorporate a regional cross-attention network within the modality decoupling module to improve the representation quality of decoupled features. Extensive experimental results on our in-house Liver Cancer (LC) and three widely used TCGA public datasets confirm the effectiveness of our proposed method. The code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSE: Remove Objects with Side Effects in Videos</title>
<link>https://arxiv.org/abs/2508.18633</link>
<guid>https://arxiv.org/abs/2508.18633</guid>
<content:encoded><![CDATA[
<div> Video object removal, side effects, generative models, 3D rendering engine, synthetic data generation

Summary:
ROSE (Remove Objects with Side Effects) addresses the challenge of eliminating side effects such as shadows and reflections when removing objects from videos. The framework categorizes the effects into five common cases and uses a 3D rendering engine for generating synthetic data due to the scarcity of paired video data for supervision. A fully automatic pipeline is constructed for data preparation, creating a large-scale paired dataset with diverse scenes and objects. ROSE, implemented as a video inpainting model based on diffusion transformer, localizes object-correlated areas in the video for reference-based erasing. Additional supervision is introduced to predict areas affected by side effects, improving performance. The ROSE-Bench benchmark evaluates the model's performance on various side effect removal scenarios, demonstrating superior performance compared to existing methods and generalizability to real-world video situations. <div>
arXiv:2508.18633v1 Announce Type: new 
Abstract: Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</title>
<link>https://arxiv.org/abs/2508.18634</link>
<guid>https://arxiv.org/abs/2508.18634</guid>
<content:encoded><![CDATA[
<div> Dataset construction, Motion-Detail Fusion, Fine-Grained Examination, Caption Set Equivalence Reward, Group Relative Policy Optimization

Summary:
The paper introduces solutions to address the motion-detail imbalance in video captioning. It constructs the HMD-270K dataset through Motion-Detail Fusion and Fine-Grained Examination and introduces the Caption Set Equivalence Reward based on Group Relative Policy Optimization to enhance completeness and accuracy in capturing motion and details. The OwlCap model, developed using supervised fine-tuning and post-training with CSER, achieves significant improvements on VDC and DREAM-1K benchmarks. The HMD-270K dataset and OwlCap model will be publicly released to advance video captioning research. <div>
arXiv:2508.18634v1 Announce Type: new 
Abstract: Video captioning aims to generate comprehensive and coherent descriptions of the video content, contributing to the advancement of both video understanding and generation. However, existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation. To address this issue, we propose solutions from two aspects: 1) Data aspect: We constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2) Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO). CSER enhances completeness and accuracy in capturing both motion and details through unit-to-set matching and bidirectional validation. Based on the HMD-270K supervised fine-tuning and GRPO post-training with CSER, we developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance. Experimental results demonstrate that OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap model will be publicly released to facilitate video captioning research community advancements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection</title>
<link>https://arxiv.org/abs/2508.18641</link>
<guid>https://arxiv.org/abs/2508.18641</guid>
<content:encoded><![CDATA[
<div> Keywords: Oracle Bone Inscriptions, feature space representation learning, clustering, detection networks, digital archaeology

Summary:<br /><br />
Oracle Bone Inscriptions (OBIs) are essential in understanding ancient Chinese civilization. Detecting OBIs from rubbing images is challenging due to noise and cracks. The proposed method utilizes clustering-based representation learning using the Oracle Bones Character (OBC) font library dataset to enhance feature extraction in detection networks. A specialized loss function derived from clustering results optimizes feature representation, improving performance in detection frameworks like Faster R-CNN, DETR, and Sparse R-CNN. Experimental results confirm significant performance enhancements, validating the effectiveness of the approach in digital archaeology research. <div>
arXiv:2508.18641v1 Announce Type: new 
Abstract: Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient Chinese civilization. The automated detection of OBIs from rubbing images represents a fundamental yet challenging task in digital archaeology, primarily due to various degradation factors including noise and cracks that limit the effectiveness of conventional detection networks. To address these challenges, we propose a novel clustering-based feature space representation learning method. Our approach uniquely leverages the Oracle Bones Character (OBC) font library dataset as prior knowledge to enhance feature extraction in the detection network through clustering-based representation learning. The method incorporates a specialized loss function derived from clustering results to optimize feature representation, which is then integrated into the total network loss. We validate the effectiveness of our method by conducting experiments on two OBIs detection dataset using three mainstream detection frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive experimentation, all frameworks demonstrate significant performance improvements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain</title>
<link>https://arxiv.org/abs/2508.18664</link>
<guid>https://arxiv.org/abs/2508.18664</guid>
<content:encoded><![CDATA[
<div> SNR prior, underwater image enhancement, deep neural networks, FAST, FAT<br />
<br />
Keywords: SNR prior, underwater image enhancement, spectral interactions, deep neural networks, SFormer<br />
Summary: 
- Recent advancements in learning-based underwater image enhancement have incorporated physical priors, such as the signal-to-noise ratio (SNR), to reduce attenuation. 
- However, spatial domain SNR priors face limitations in separating cross-channel interference and amplifying informative structures while suppressing noise.
- To address these limitations, this study proposes using the SNR prior in the frequency domain and decomposing features into amplitude and phase spectra. 
- The Fourier Attention SNR-prior Transformer (FAST) and Frequency Adaptive Transformer (FAT) modules are introduced to highlight key spectral components and enhance perceptual quality through spectral interactions with SNR cues.
- Integrated into a unified U-shaped architecture called SFormer, trained on 4,800 paired underwater images, the model shows significant improvement in image quality, surpassing recent methods with a 3.1 dB gain in PSNR and 0.08 in SSIM. <div>
arXiv:2508.18664v1 Announce Type: new 
Abstract: Recent learning-based underwater image enhancement (UIE) methods have advanced by incorporating physical priors into deep neural networks, particularly using the signal-to-noise ratio (SNR) prior to reduce wavelength-dependent attenuation. However, spatial domain SNR priors have two limitations: (i) they cannot effectively separate cross-channel interference, and (ii) they provide limited help in amplifying informative structures while suppressing noise. To overcome these, we propose using the SNR prior in the frequency domain, decomposing features into amplitude and phase spectra for better channel modulation. We introduce the Fourier Attention SNR-prior Transformer (FAST), combining spectral interactions with SNR cues to highlight key spectral components. Additionally, the Frequency Adaptive Transformer (FAT) bottleneck merges low- and high-frequency branches using a gated attention mechanism to enhance perceptual quality. Embedded in a unified U-shaped architecture, these modules integrate a conventional RGB stream with an SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB, EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and 0.08 in SSIM, successfully restoring colors, textures, and contrast in underwater scenes.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos</title>
<link>https://arxiv.org/abs/2508.18681</link>
<guid>https://arxiv.org/abs/2508.18681</guid>
<content:encoded><![CDATA[
<div> Hierarchical Spatio-temporal Segmentation Network, echocardiography videos, EF estimation, Mamba architecture, Spatio-temporal Cross Scan module <br />
<br />
Summary: 
The Hierarchical Spatio-temporal Segmentation Network is proposed for automated left ventricular endocardium segmentation in echocardiography videos to improve EF estimation accuracy. The network utilizes a hierarchical design incorporating convolutional networks for single-frame image processing and the Mamba architecture for capturing spatio-temporal relationships. The Spatio-temporal Cross Scan module is introduced to integrate long-range context across frames and positions, addressing limitations of local spatio-temporal processing and helping mitigate biases in EF calculation. This approach aims to enhance the accuracy of cardiac structure and function assessment through EF estimation, providing a more comprehensive and detailed analysis of cardiac health. <div>
arXiv:2508.18681v1 Announce Type: new 
Abstract: Automated segmentation of the left ventricular endocardium in echocardiography videos is a key research area in cardiology. It aims to provide accurate assessment of cardiac structure and function through Ejection Fraction (EF) estimation. Although existing studies have achieved good segmentation performance, their results do not perform well in EF estimation. In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network (\ourmodel) for echocardiography video, aiming to improve EF estimation accuracy by synergizing local detail modeling with global dynamic perception. The network employs a hierarchical design, with low-level stages using convolutional networks to process single-frame images and preserve details, while high-level stages utilize the Mamba architecture to capture spatio-temporal relationships. The hierarchical design balances single-frame and multi-frame processing, avoiding issues such as local error accumulation when relying solely on single frames or neglecting details when using only multi-frame data. To overcome local spatio-temporal limitations, we propose the Spatio-temporal Cross Scan (STCS) module, which integrates long-range context through skip scanning across frames and positions. This approach helps mitigate EF calculation biases caused by ultrasound image noise and other factors.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency</title>
<link>https://arxiv.org/abs/2508.18693</link>
<guid>https://arxiv.org/abs/2508.18693</guid>
<content:encoded><![CDATA[
<div> Keywords: Domain shift, Unsupervised domain adaptation, Feature-space Planes Searcher, Transfer learning, Geometric patterns

Summary: 
Domain shift challenges deep learning systems when transitioning from labeled source domains to unlabeled target domains. New unsupervised domain adaptation method, Feature-space Planes Searcher (FPS), optimizes decision boundaries using geometric patterns in feature space from pre-trained models. This approach retains interpretability, scalability, and efficiency while reducing memory and computational costs. FPS outperforms state-of-the-art methods in public benchmarks, scaling efficiently with large models and across various domains like protein structure prediction, remote sensing classification, and earthquake detection. By leveraging discriminative structures and preserving transferable geometric patterns, FPS provides a simple, effective, and generalizable paradigm for domain adaptation tasks. 

<br /><br />Summary: <div>
arXiv:2508.18693v1 Announce Type: new 
Abstract: Domain shift, characterized by degraded model performance during transition from labeled source domains to unlabeled target domains, poses a persistent challenge for deploying deep learning systems. Current unsupervised domain adaptation (UDA) methods predominantly rely on fine-tuning feature extractors - an approach limited by inefficiency, reduced interpretability, and poor scalability to modern architectures.
  Our analysis reveals that models pretrained on large-scale data exhibit domain-invariant geometric patterns in their feature space, characterized by intra-class clustering and inter-class separation, thereby preserving transferable discriminative structures. These findings indicate that domain shifts primarily manifest as boundary misalignment rather than feature degradation.
  Unlike fine-tuning entire pre-trained models - which risks introducing unpredictable feature distortions - we propose the Feature-space Planes Searcher (FPS): a novel domain adaptation framework that optimizes decision boundaries by leveraging these geometric patterns while keeping the feature encoder frozen. This streamlined approach enables interpretative analysis of adaptation while substantially reducing memory and computational costs through offline feature extraction, permitting full-dataset optimization in a single computation cycle.
  Evaluations on public benchmarks demonstrate that FPS achieves competitive or superior performance to state-of-the-art methods. FPS scales efficiently with multimodal large models and shows versatility across diverse domains including protein structure prediction, remote sensing classification, and earthquake detection. We anticipate FPS will provide a simple, effective, and generalizable paradigm for transfer learning, particularly in domain adaptation tasks. .
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition</title>
<link>https://arxiv.org/abs/2508.18695</link>
<guid>https://arxiv.org/abs/2508.18695</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Deep Learning, InceptionV3, LSTM, Ensemble-Based Feature Selection

Summary: 
The article presents a new optimized hybrid deep learning framework for Human Activity Recognition (HAR), addressing challenges such as high computational costs and limited scalability in real-time scenarios. The framework combines a customized InceptionV3 model for spatial feature extraction, an LSTM architecture for modeling temporal dependencies, and an ensemble-based feature selection strategy using Genetic Algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA). Experimental results on the UCF-YouTube dataset show a high recognition accuracy of 99.65% with feature reduction to as few as 7, leading to enhanced inference time. The approach enables lightweight machine learning classifiers to achieve accurate and robust HAR in diverse environments with challenges like occlusion and poor illumination. The lightweight and scalable nature of the proposed system allows for real-time deployment on edge devices like Raspberry Pi, making it suitable for applications in smart surveillance, healthcare, sports analytics, and more. 

<br /><br />Summary: <div>
arXiv:2508.18695v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) plays a pivotal role in various applications, including smart surveillance, healthcare, assistive technologies, sports analytics, etc. However, HAR systems still face critical challenges, including high computational costs, redundant features, and limited scalability in real-time scenarios. An optimized hybrid deep learning framework is introduced that integrates a customized InceptionV3, an LSTM architecture, and a novel ensemble-based feature selection strategy. The proposed framework first extracts spatial descriptors using the customized InceptionV3 model, which captures multilevel contextual patterns, region homogeneity, and fine-grained localization cues. The temporal dependencies across frames are then modeled using LSTMs to effectively encode motion dynamics. Finally, an ensemble-based genetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA) is employed to select a compact and optimized feature set by dynamically balancing objectives such as accuracy, redundancy, uniqueness, and complexity reduction. Consequently, the selected feature subsets, which are both diverse and discriminative, enable various lightweight machine learning classifiers to achieve accurate and robust HAR in heterogeneous environments. Experimental results on the robust UCF-YouTube dataset, which presents challenges such as occlusion, cluttered backgrounds, motion dynamics, and poor illumination, demonstrate good performance. The proposed approach achieves 99.65% recognition accuracy, reduces features to as few as 7, and enhances inference time. The lightweight and scalable nature of the HAR system supports real-time deployment on edge devices such as Raspberry Pi, enabling practical applications in intelligent, resource-aware environments, including public safety, assistive technology, and autonomous monitoring systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.18696</link>
<guid>https://arxiv.org/abs/2508.18696</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, surgical scene reconstruction, ColorGS, spatially adaptive, deformation modeling

Summary: 
ColorGS introduces Colored Gaussian Primitives for adaptive color encoding and an Enhanced Deformation Model (EDM) for precise tissue reconstruction in surgical scenes. The Colored Gaussian Primitives improve color expressiveness under complex lighting, enhancing texture capture. The Enhanced Deformation Model combines time-aware Gaussian basis functions with learnable deformations, enabling accurate modeling of localized tissue deformations and global motion consistency. Extensive experiments demonstrate ColorGS achieves state-of-the-art performance with high PSNR and SSIM metrics while maintaining real-time rendering efficiency. This advancement in surgical scene reconstruction balances computational practicality with high fidelity, crucial for intraoperative guidance and AR/VR applications.<br /><br />Summary: <div>
arXiv:2508.18696v1 Announce Type: new 
Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class-wise Flooding Regularization for Imbalanced Image Classification</title>
<link>https://arxiv.org/abs/2508.18723</link>
<guid>https://arxiv.org/abs/2508.18723</guid>
<content:encoded><![CDATA[
<div> regularization, neural networks, imbalanced datasets, class-wise flooding, minority classes  
Summary:  
- The article introduces class-wise flooding regularization, a method to address the issue of imbalanced datasets in training neural networks.  
- Class-wise flooding assigns a specific flooding level to each class based on class frequencies, suppressing overfitting in majority classes while allowing learning in minority classes.  
- The regularization technique helps prevent a model from favoring majority classes over minority classes, improving recognition performance overall.  
- Tested on imbalanced image classification, the method outperforms conventional flooding regularizations by enhancing the classification performance of minority classes.  
- By utilizing class-specific flooding levels, the proposed approach achieves better generalization performance on unseen inputs.  
<br /><br />Summary: <div>
arXiv:2508.18723v1 Announce Type: new 
Abstract: The purpose of training neural networks is to achieve high generalization performance on unseen inputs. However, when trained on imbalanced datasets, a model's prediction tends to favor majority classes over minority classes, leading to significant degradation in the recognition performance of minority classes. To address this issue, we propose class-wise flooding regularization, an extension of flooding regularization applied at the class level. Flooding is a regularization technique that mitigates overfitting by preventing the training loss from falling below a predefined threshold, known as the flooding level, thereby discouraging memorization. Our proposed method assigns a class-specific flooding level based on class frequencies. By doing so, it suppresses overfitting in majority classes while allowing sufficient learning for minority classes. We validate our approach on imbalanced image classification. Compared to conventional flooding regularizations, our method improves the classification performance of minority classes and achieves better overall generalization.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flatness-aware Curriculum Learning via Adversarial Difficulty</title>
<link>https://arxiv.org/abs/2508.18726</link>
<guid>https://arxiv.org/abs/2508.18726</guid>
<content:encoded><![CDATA[
<div> Curriculum Learning, Sharpness-Aware Minimization, Adversarial Difficulty Measure, Neural Networks, Generalization<br />
Summary:<br />
Neural networks trained using empirical risk minimization often suffer from overfitting, leading to poor generalization. Curriculum Learning (CL) tackles this issue by selecting training samples based on difficulty, while Sharpness-Aware Minimization (SAM) aims to find flat minima for improved robustness. However, combining CL and SAM poses challenges due to difficulty in evaluating sample difficulty in flat regions. To address this, the proposed Adversarial Difficulty Measure (ADM) leverages robustness properties of models trained towards flat minima to quantify adversarial vulnerability. By incorporating ADM into CL-based training with SAM, sample difficulty can be dynamically assessed. Evaluation on image classification tasks, fine-grained recognition, and domain generalization demonstrates that the method surpasses existing curriculum-based and flatness-aware training strategies, preserving the strengths of both CL and SAM. <br /><br />Summary: <div>
arXiv:2508.18726v1 Announce Type: new 
Abstract: Neural networks trained by empirical risk minimization often suffer from overfitting, especially to specific samples or domains, which leads to poor generalization. Curriculum Learning (CL) addresses this issue by selecting training samples based on the difficulty. From the optimization perspective, methods such as Sharpness-Aware Minimization (SAM) improve robustness and generalization by seeking flat minima. However, combining CL with SAM is not straightforward. In flat regions, both the loss values and the gradient norms tend to become uniformly small, which makes it difficult to evaluate sample difficulty and design an effective curriculum. To overcome this problem, we propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial vulnerability by leveraging the robustness properties of models trained toward flat minima. Unlike loss- or gradient-based measures, which become ineffective as training progresses into flatter regions, ADM remains informative by measuring the normalized loss gap between original and adversarial examples. We incorporate ADM into CL-based training with SAM to dynamically assess sample difficulty. We evaluated our approach on image classification tasks, fine-grained recognition, and domain generalization. The results demonstrate that our method preserves the strengths of both CL and SAM while outperforming existing curriculum-based and flatness-aware training strategies.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection</title>
<link>https://arxiv.org/abs/2508.18729</link>
<guid>https://arxiv.org/abs/2508.18729</guid>
<content:encoded><![CDATA[
<div> scallops, underwater object detection, imbalanced class distribution, localization analysis, classification experiments 

Summary:
Underwater object detection in marine ecosystems faces challenges such as degraded image quality and imbalanced class distribution. The study focuses on understanding the performance disparities in detecting marine species, particularly scallops. The research investigates factors beyond data quantity influencing class-specific performance disparities and proposes systematic improvements for under-performing marine species. Results from the DUO dataset manipulation reveal that foreground-background discrimination is a crucial challenge in object localization. Despite balanced data, precision gaps persist in classification, indicating intrinsic feature-based challenges. The study recommends imbalanced distributions for prioritizing precision and balanced distributions for recall. To enhance under-performing classes, the focus should be on algorithmic advancements, particularly within localization modules. The release of code and datasets promotes further research in underwater object detection. 

<br /><br />Summary: <div>
arXiv:2508.18729v1 Announce Type: new 
Abstract: Underwater object detection is critical for monitoring marine ecosystems but poses unique challenges, including degraded image quality, imbalanced class distribution, and distinct visual characteristics. Not every species is detected equally well, yet underlying causes remain unclear. We address two key research questions: 1) What factors beyond data quantity drive class-specific performance disparities? 2) How can we systematically improve detection of under-performing marine species? We manipulate the DUO dataset to separate the object detection task into localization and classification and investigate the under-performance of the scallop class. Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies. We recommend imbalanced distributions when prioritizing precision, and balanced distributions when prioritizing recall. Improving under-performing classes should focus on algorithmic advances, especially within localization modules. We publicly release our code and datasets.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings</title>
<link>https://arxiv.org/abs/2508.18733</link>
<guid>https://arxiv.org/abs/2508.18733</guid>
<content:encoded><![CDATA[
<div> Keywords: CAD, generative modeling, 2D vector drawings, sequence-to-sequence learning, transformer architecture

Summary:
Drawing2CAD introduces a new framework for automatically generating parametric CAD models from 2D vector drawings. This approach reframes CAD generation as a sequence-to-sequence learning problem, utilizing vector drawing primitives to inform the generation of CAD operations. The framework includes a network-friendly vector primitive representation, a dual-decoder transformer architecture, and a soft target distribution loss function. The CAD-VGDrawing dataset is created for training and evaluation purposes. Thorough experiments demonstrate the effectiveness of Drawing2CAD in preserving geometric precision and design intent throughout the transformation process. The code and dataset are available for further exploration and development. <div>
arXiv:2508.18733v1 Announce Type: new 
Abstract: Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion</title>
<link>https://arxiv.org/abs/2508.18734</link>
<guid>https://arxiv.org/abs/2508.18734</guid>
<content:encoded><![CDATA[
<div> Keywords: audio-visual speech recognition, noisy environments, feature fusion, cross-modal, robustness

Summary: 
The article presents a novel framework for robust audio-visual speech recognition (AVSR) in noisy environments. Existing systems often struggle to estimate audio reliability and adjust modality reliance effectively. The proposed approach, called router-gated cross-modal feature fusion, dynamically reweights audio and visual features based on token-level acoustic corruption scores. By incorporating an audio-visual feature fusion-based router, the method is able to down-weight unreliable audio tokens and enhance visual cues through gated cross-attention in each decoder layer. This adaptive reweighting mechanism allows the model to prioritize visual cues when audio quality is compromised. Experimental results on the LRS3 dataset demonstrate significant improvements in word error rate compared to existing methods. Ablation studies further validate the effectiveness of both the router and gating mechanism in enhancing robustness in real-world noisy conditions. 

<br /><br />Summary: <div>
arXiv:2508.18734v1 Announce Type: new 
Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains challenging, as existing systems struggle to estimate audio reliability and dynamically adjust modality reliance. We propose router-gated cross-modal feature fusion, a novel AVSR framework that adaptively reweights audio and visual features based on token-level acoustic corruption scores. Using an audio-visual feature fusion-based router, our method down-weights unreliable audio tokens and reinforces visual cues through gated cross-attention in each decoder layer. This enables the model to pivot toward the visual modality when audio quality deteriorates. Experiments on LRS3 demonstrate that our approach achieves an 16.51-42.67% relative reduction in word error rate compared to AV-HuBERT. Ablation studies confirm that both the router and gating mechanism contribute to improved robustness under real-world acoustic noise.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods</title>
<link>https://arxiv.org/abs/2508.18753</link>
<guid>https://arxiv.org/abs/2508.18753</guid>
<content:encoded><![CDATA[
<div> Keywords: human-object interaction detection, vision-language models, benchmark, multiple-answer multiple-choice task, evaluation protocol

Summary: 
The article discusses the integration of early vision-language models (VLMs) in human-object interaction (HOI) detection methods and proposes a new benchmark to evaluate the effectiveness of standalone VLMs in solving HOI detection tasks. Existing HOI benchmarks are not suitable for evaluating VLMs due to their exact matching evaluation protocols. The new benchmark formulates HOI detection as a multiple-answer multiple-choice task, reducing ambiguity in annotations and allowing for valid predictions to be recognized. By reshaping the evaluation process, the benchmark enables direct comparison between general-purpose VLMs and specialized HOI methods, providing new insights into the current state of progress in HOI understanding.<br /><br />Summary: <div>
arXiv:2508.18753v1 Announce Type: new 
Abstract: Prior human-object interaction (HOI) detection methods have integrated early vision-language models (VLMs) such as CLIP, but only as supporting components within their frameworks. In contrast, recent advances in large, generative VLMs suggest that these models may already possess strong ability to understand images involving HOI. This naturally raises an important question: can general-purpose standalone VLMs effectively solve HOI detection, and how do they compare with specialized HOI methods? Answering this requires a benchmark that can accommodate both paradigms. However, existing HOI benchmarks such as HICO-DET were developed before the emergence of modern VLMs, and their evaluation protocols require exact matches to annotated HOI classes. This is poorly aligned with the generative nature of VLMs, which often yield multiple valid interpretations in ambiguous cases. For example, a static image may capture a person mid-motion with a frisbee, which can plausibly be interpreted as either "throwing" or "catching". When only "catching" is annotated, the other, though equally plausible for the image, is marked incorrect when exact matching is used. As a result, correct predictions might be penalized, affecting both VLMs and HOI-specific methods. To avoid penalizing valid predictions, we introduce a new benchmark that reformulates HOI detection as a multiple-answer multiple-choice task, where each question includes only ground-truth positive options and a curated set of negatives that are constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing" is not selected as a negative to avoid penalizing valid predictions). The proposed evaluation protocol is the first of its kind for both VLMs and HOI methods, enabling direct comparison and offering new insight into the current state of progress in HOI understanding.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Textual: Generating Coherent Visual Options for MCQs</title>
<link>https://arxiv.org/abs/2508.18772</link>
<guid>https://arxiv.org/abs/2508.18772</guid>
<content:encoded><![CDATA[
<div> Keywords: multiple-choice questions, visual options, education, question generation, content discrimination

Summary:
The study introduces a new framework, Cross-modal Options Synthesis (CmOS), for generating educational multiple-choice questions (MCQs) with visual options. The framework combines Multimodal Chain-of-Thought (MCoT) reasoning and Retrieval-Augmented Generation (RAG) to create semantically plausible answer choices and visually similar distractors. It includes a discrimination module to identify suitable content for visual options. Experimental results show that CmOS outperforms existing methods in content discrimination, question generation, and visual option generation across different subjects and educational levels. The research addresses the limitations of previous studies that primarily focused on textual MCQs and highlights the importance of integrating visual options to enhance deep thinking and knowledge integration in education.<br /><br />Summary: <div>
arXiv:2508.18772v1 Announce Type: new 
Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep thinking and knowledge integration in education. However, previous research has primarily focused on generating MCQs with textual options, but it largely overlooks the visual options. Moreover, generating high-quality distractors remains a major challenge due to the high cost and limited scalability of manual authoring. To tackle these problems, we propose a Cross-modal Options Synthesis (CmOS), a novel framework for generating educational MCQs with visual options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning process and Retrieval-Augmented Generation (RAG) to produce semantically plausible and visually similar answer and distractors. It also includes a discrimination module to identify content suitable for visual options. Experimental results on test tasks demonstrate the superiority of CmOS in content discrimination, question generation and visual option generation over existing methods across various subjects and educational levels.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring</title>
<link>https://arxiv.org/abs/2508.18787</link>
<guid>https://arxiv.org/abs/2508.18787</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time, remote photoplethysmography, low-power devices, physiological monitoring, signal extraction

Summary:
The paper introduces a real-time remote photoplethysmography (rPPG) system that is optimized for low-power devices. This system is designed to extract physiological signals such as heart rate, respiratory rate, and oxygen saturation from facial video streams. The system, called Face2PPG, utilizes a multithreaded architecture to handle simultaneous tasks such as video processing, network communication, and GUI updates, ensuring continuous operation at 30 fps. It leverages a hybrid programming model combining Functional Reactive Programming and the Actor Model for efficient task parallelization and event-driven processing. The system also includes an HTTP server for continuous video streaming and a RESTful API for on-demand vital sign retrieval. The evaluation shows that the system performs robustly under real-time constraints while minimizing computational overhead. This work addresses challenges in real-time biosignal monitoring and provides practical solutions for optimizing performance in healthcare and human-computer interaction applications.<br /><br />Summary: <div>
arXiv:2508.18787v1 Announce Type: new 
Abstract: The growing integration of smart environments and low-power computing devices, coupled with mass-market sensor technologies, is driving advancements in remote and non-contact physiological monitoring. However, deploying these systems in real-time on resource-constrained platforms introduces significant challenges related to scalability, interoperability, and performance. This paper presents a real-time remote photoplethysmography (rPPG) system optimized for low-power devices, designed to extract physiological signals, such as heart rate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial video streams. The system is built on the Face2PPG pipeline, which processes video frames sequentially for rPPG signal extraction and analysis, while leveraging a multithreaded architecture to manage video capture, real-time processing, network communication, and graphical user interface (GUI) updates concurrently. This design ensures continuous, reliable operation at 30 frames per second (fps), with adaptive feedback through a collaborative user interface to guide optimal signal capture conditions. The network interface includes both an HTTP server for continuous video streaming and a RESTful API for on-demand vital sign retrieval. To ensure accurate performance despite the limitations of low-power devices, we use a hybrid programming model combining Functional Reactive Programming (FRP) and the Actor Model, allowing event-driven processing and efficient task parallelization. The system is evaluated under real-time constraints, demonstrating robustness while minimizing computational overhead. Our work addresses key challenges in real-time biosignal monitoring, offering practical solutions for optimizing performance in modern healthcare and human-computer interaction applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PseudoMapTrainer: Learning Online Mapping without HD Maps</title>
<link>https://arxiv.org/abs/2508.18788</link>
<guid>https://arxiv.org/abs/2508.18788</guid>
<content:encoded><![CDATA[
<div> Keywords: Online mapping, PseudoMapTrainer, Gaussian splatting, Semi-supervised learning, Unlabeled sensor data<br />
Summary:
PseudoMapTrainer is a new approach for online mapping, eliminating the need for expensive ground-truth high-definition maps during training. It generates pseudo-labels from unlabeled sensor data by reconstructing the road surface from multi-camera imagery using Gaussian splatting and a pre-trained 2D segmentation network. A mask-aware assignment algorithm and loss function are introduced to handle partially masked pseudo-labels, enabling training without ground-truth maps. The pseudo-labels can also be used for pre-training online models in a semi-supervised manner, utilizing large-scale unlabeled crowdsourced data. This innovative method opens up possibilities for more geographically diverse and reliable online mapping models without the constraints of relying on costly ground-truth data. The code for PseudoMapTrainer is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2508.18788v1 Announce Type: new 
Abstract: Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Label-Efficient Deep Waste Detection</title>
<link>https://arxiv.org/abs/2508.18799</link>
<guid>https://arxiv.org/abs/2508.18799</guid>
<content:encoded><![CDATA[
<div> datasets, waste sorting, AI-driven, ensemble-based, semi-supervised learning<br />
Summary:<br />
Effective waste sorting is crucial for sustainable recycling, but AI research in this area lags due to limited datasets and reliance on old object detectors. The study introduces a semi-supervised learning framework for waste detection using ensemble models. Benchmarking OVOD models on the ZeroWaste dataset shows LLM-optimized prompts enhance zero-shot accuracy. Modern transformer-based detectors achieve a new baseline of 51.6 mAP after fine-tuning to address domain-specific limitations. A soft pseudo-labeling strategy, combining ensemble predictions with weighting, boosts performance on the ZeroWaste-s subset beyond fully supervised training. The study contributes by establishing baselines, introducing a robust pseudo-labeling pipeline, creating annotations for the ZeroWaste-s subset, and evaluating OVOD models under real-world waste sorting conditions. The code is available on GitHub at https://github.com/h-abid97/robust-waste-detection.<br /> <div>
arXiv:2508.18799v1 Announce Type: new 
Abstract: Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions. Our code is available at: https://github.com/h-abid97/robust-waste-detection.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Font Impression Word Tags Based on Co-occurrence</title>
<link>https://arxiv.org/abs/2508.18825</link>
<guid>https://arxiv.org/abs/2508.18825</guid>
<content:encoded><![CDATA[
<div> Keywords: font styles, impression tags, embedding method, spectral embedding, font generation

Summary:
Different font styles convey distinct impressions, showing a strong relationship between font shapes and the impression tags used to describe them. A novel embedding method is proposed in this paper that capitalizes on these shape-impression relationships. By assigning similar vectors to frequently co-occurring impression tags, the method effectively represents font impressions. Comparing this method to standard word embedding models like BERT and CLIP, it proves superior in impression-guided font generation. The approach involves constructing a graph representing impression tags and utilizing spectral embedding to generate impression vectors for each tag. The method demonstrates better performance in both qualitative and quantitative evaluations, showcasing its effectiveness in impression-based font generation and retrieval. Essentially, the novel embedding method captures the nuances of font impressions, allowing for more accurate representation and manipulation in font-related tasks. <div>
arXiv:2508.18825v1 Announce Type: new 
Abstract: Different font styles (i.e., font shapes) convey distinct impressions, indicating a close relationship between font shapes and word tags describing those impressions. This paper proposes a novel embedding method for impression tags that leverages these shape-impression relationships. For instance, our method assigns similar vectors to impression tags that frequently co-occur in order to represent impressions of fonts, whereas standard word embedding methods (e.g., BERT and CLIP) yield very different vectors. This property is particularly useful for impression-based font generation and font retrieval. Technically, we construct a graph whose nodes represent impression tags and whose edges encode co-occurrence relationships. Then, we apply spectral embedding to obtain the impression vectors for each tag. We compare our method with BERT and CLIP in qualitative and quantitative evaluations, demonstrating that our approach performs better in impression-guided font generation.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory</title>
<link>https://arxiv.org/abs/2508.18829</link>
<guid>https://arxiv.org/abs/2508.18829</guid>
<content:encoded><![CDATA[
<div> Keywords: National Forest Inventory, remote sensing, machine learning, deep learning, tree species classification

Summary:
Using remote sensing and machine learning, this study explores how deep features from pre-trained models can enhance tree species classification accuracy in the Netherlands with limited labeled data. By leveraging data from Sentinel-1, Sentinel-2, ERA5 satellites, and SRTM using Google Earth Engine, the researchers fine-tuned a remote sensing model and outperformed traditional methods by up to 10%. The results suggest that utilizing deep AI features from pre-trained models improves NFI classification accuracy significantly, highlighting the inadequacy of traditional hand-designed features for this task. The study demonstrates the potential of integrating deep learning techniques with remote sensing data to update NFIs more efficiently and at larger scales. This approach offers a promising strategy to complement existing forest inventory processes and improve the accuracy of tree species distribution data for better forest management. 

<br /><br />Summary: <div>
arXiv:2508.18829v1 Announce Type: new 
Abstract: National Forest Inventory (NFI)s serve as the primary source of forest information, providing crucial tree species distribution data. However, maintaining these inventories requires labor-intensive on-site campaigns. Remote sensing approaches, particularly when combined with machine learning, offer opportunities to update NFIs more frequently and at larger scales. While the use of Satellite Image Time Series has proven effective for distinguishing tree species through seasonal canopy reflectance patterns, current approaches rely primarily on Random Forest classifiers with hand-designed features and phenology-based metrics. Using deep features from an available pre-trained remote sensing foundation models offers a complementary strategy. These pre-trained models leverage unannotated global data and are meant to used for general-purpose applications and can then be efficiently fine-tuned with smaller labeled datasets for specific classification tasks. This work systematically investigates how deep features improve tree species classification accuracy in the Netherlands with few annotated data. Data-wise, we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites data and SRTM data using Google Earth Engine. Our results demonstrate that fine-tuning a publicly available remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by a large margin of up to 10% across all datasets. This demonstrates that classic hand-defined harmonic features are too simple for this task and highlights the potential of using deep AI features for data-limited application like NFI classification. By leveraging openly available satellite data and pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2</title>
<link>https://arxiv.org/abs/2508.18831</link>
<guid>https://arxiv.org/abs/2508.18831</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDOG 2025 Challenge, binary classification, histopathological images, ConvNeXt V2, ensemble strategy

Summary:<br />
This paper presents a solution for the MIDOG 2025 Challenge Track 2, focusing on binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs) in histopathological images. The approach utilizes a ConvNeXt V2 base model with center cropping preprocessing and 5-fold cross-validation ensemble strategy to address challenges like class imbalance, morphological variability, and domain heterogeneity. By employing 60% center cropping and mixed precision training, the model achieves robust performance on the diverse MIDOG 2025 dataset, showcasing the effectiveness of modern convolutional architectures for mitotic figure subtyping. Computational efficiency is maintained through careful architectural choices and training optimizations, demonstrating the efficacy of the approach for accurate classification of NMFs and AMFs in histopathological images.<br /> <div>
arXiv:2508.18831v1 Announce Type: new 
Abstract: This paper presents our solution for the MIDOG 2025 Challenge Track 2, which focuses on binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs) in histopathological images. Our approach leverages a ConvNeXt V2 base model with center cropping preprocessing and 5-fold cross-validation ensemble strategy. The method addresses key challenges including severe class imbalance, high morphological variability, and domain heterogeneity across different tumor types, species, and scanners. Through strategic preprocessing with 60% center cropping and mixed precision training, our model achieved robust performance on the diverse MIDOG 2025 dataset. The solution demonstrates the effectiveness of modern convolutional architectures for mitotic figure subtyping while maintaining computational efficiency through careful architectural choices and training optimizations.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression</title>
<link>https://arxiv.org/abs/2508.18834</link>
<guid>https://arxiv.org/abs/2508.18834</guid>
<content:encoded><![CDATA[
<div> Keywords: Micro-expressions, video-level regression, interval selection, prior guidance, performance improvement

Summary: 
In this paper, a novel method for analyzing micro-expressions (MEs) is introduced. Traditional ME analysis methods often struggle to capture the complex temporal dynamics of MEs due to fixed window sizes and hard decisions. The proposed method utilizes a prior-guided video-level regression approach that considers the temporal evolution, duration, and class distribution characteristics of MEs. A scalable interval selection strategy is implemented to accurately spot the onset, apex, and offset phases of MEs. Additionally, a synergistic optimization framework is employed, allowing the model to efficiently utilize limited data and enhance its capabilities by sharing parameters between spotting and recognition tasks. Extensive experiments on benchmark datasets showcase the state-of-the-art performance of the method, achieving an STRS of 0.0562 on CAS(ME)3 and 0.2000 on SAMMLV.<br /><br />Summary: <div>
arXiv:2508.18834v1 Announce Type: new 
Abstract: Micro-expressions (MEs) are involuntary, low-intensity, and short-duration facial expressions that often reveal an individual's genuine thoughts and emotions. Most existing ME analysis methods rely on window-level classification with fixed window sizes and hard decisions, which limits their ability to capture the complex temporal dynamics of MEs. Although recent approaches have adopted video-level regression frameworks to address some of these challenges, interval decoding still depends on manually predefined, window-based methods, leaving the issue only partially mitigated. In this paper, we propose a prior-guided video-level regression method for ME analysis. We introduce a scalable interval selection strategy that comprehensively considers the temporal evolution, duration, and class distribution characteristics of MEs, enabling precise spotting of the onset, apex, and offset phases. In addition, we introduce a synergistic optimization framework, in which the spotting and recognition tasks share parameters except for the classification heads. This fully exploits complementary information, makes more efficient use of limited data, and enhances the model's capability. Extensive experiments on multiple benchmark datasets demonstrate the state-of-the-art performance of our method, with an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is available at https://github.com/zizheng-guo/BoostingVRME.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Outcome-Oriented Assessment of Microsurgical Anastomosis</title>
<link>https://arxiv.org/abs/2508.18836</link>
<guid>https://arxiv.org/abs/2508.18836</guid>
<content:encoded><![CDATA[
<div> Keywords: microsurgical anastomosis, dexterity, visuospatial skills, image-processing techniques, proficiency assessment <br />
Summary: 
This study focuses on the objective assessment of microsurgical anastomoses using image-processing techniques. Traditional methods of evaluation rely on subjective judgment, leading to biases that can affect accuracy. By leveraging three hospital datasets, the researchers developed a quantitative framework that includes geometric modeling of errors and a scoring mechanism to enhance proficiency assessment. The results demonstrate that the geometric metrics effectively replicate expert raters' scoring for identified errors. This approach not only improves the efficiency and reliability of assessing microsurgical proficiency but also advances training protocols in this specialized field. <div>
arXiv:2508.18836v1 Announce Type: new 
Abstract: Microsurgical anastomosis demands exceptional dexterity and visuospatial skills, underscoring the importance of comprehensive training and precise outcome assessment. Currently, methods such as the outcome-oriented anastomosis lapse index are used to evaluate this procedure. However, they often rely on subjective judgment, which can introduce biases that affect the reliability and efficiency of the assessment of competence. Leveraging three datasets from hospitals with participants at various levels, we introduce a quantitative framework that uses image-processing techniques for objective assessment of microsurgical anastomoses. The approach uses geometric modeling of errors along with a detection and scoring mechanism, enhancing the efficiency and reliability of microsurgical proficiency assessment and advancing training protocols. The results show that the geometric metrics effectively replicate expert raters' scoring for the errors considered in this work.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization</title>
<link>https://arxiv.org/abs/2508.18859</link>
<guid>https://arxiv.org/abs/2508.18859</guid>
<content:encoded><![CDATA[
<div> adaptation, video stabilization, pixel-level synthesis, rapid adaptation, jerk localization

Summary:<br />
The article introduces a novel method for improving pixel-level synthesis video stabilization by rapidly adapting models to each input video at test time. This approach leverages low-level visual cues during inference to enhance both stability and visual quality of the output, achieving significant performance gains even with just a single adaptation pass. The method includes a jerk localization module and targeted adaptation strategy, focusing on high-jerk segments for maximizing stability with fewer adaptation steps. By enabling modern stabilizers to outperform previous state-of-the-art methods while maintaining full-frame synthesis, users are provided with control mechanisms similar to classical approaches. Extensive experiments on real-world datasets demonstrate the versatility and efficacy of this approach, consistently enhancing the performance of various full-frame synthesis models in both qualitative and quantitative terms, including downstream applications. <br />Summary: <div>
arXiv:2508.18859v1 Announce Type: new 
Abstract: Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.18886</link>
<guid>https://arxiv.org/abs/2508.18886</guid>
<content:encoded><![CDATA[
<div> framework, fairness, medical imaging, multimodal prompt-learning, debiasing
<br />
Summary: 
The article introduces DualFairVL, a framework designed to ensure fairness in medical imaging diagnosis across different demographic groups. It addresses the challenge of cross-modal misalignment and fairness gaps by jointly debiasing and aligning visual and text modalities. DualFairVL uses a dual-branch architecture to disentangle sensitive and target attributes and generate aligned representations. It incorporates orthogonal text anchors and a hypernetwork to encode dual-modal cues for fairness and robustness. Prototype-based regularization is applied in the visual branch to enforce separation of sensitive features. Experimental results on various medical imaging datasets demonstrate that DualFairVL outperforms existing baselines in terms of fairness and accuracy, even with a smaller number of trainable parameters. The code for DualFairVL will be released upon publication. 
<br /> <div>
arXiv:2508.18886v1 Announce Type: new 
Abstract: Ensuring fairness across demographic groups in medical diagnosis is essential for equitable healthcare, particularly under distribution shifts caused by variations in imaging equipment and clinical practice. Vision-language models (VLMs) exhibit strong generalization, and text prompts encode identity attributes, enabling explicit identification and removal of sensitive directions. However, existing debiasing approaches typically address vision and text modalities independently, leaving residual cross-modal misalignment and fairness gaps. To address this challenge, we propose DualFairVL, a multimodal prompt-learning framework that jointly debiases and aligns cross-modal representations. DualFairVL employs a parallel dual-branch architecture that separates sensitive and target attributes, enabling disentangled yet aligned representations across modalities. Approximately orthogonal text anchors are constructed via linear projections, guiding cross-attention mechanisms to produce fused features. A hypernetwork further disentangles attribute-related information and generates instance-aware visual prompts, which encode dual-modal cues for fairness and robustness. Prototype-based regularization is applied in the visual branch to enforce separation of sensitive features and strengthen alignment with textual anchors. Extensive experiments on eight medical imaging datasets across four modalities show that DualFairVL achieves state-of-the-art fairness and accuracy under both in- and out-of-distribution settings, outperforming full fine-tuning and parameter-efficient baselines with only 3.6M trainable parameters. Code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DQEN: Dual Query Enhancement Network for DETR-based HOI Detection</title>
<link>https://arxiv.org/abs/2508.18896</link>
<guid>https://arxiv.org/abs/2508.18896</guid>
<content:encoded><![CDATA[
<div> Keywords: Human-Object Interaction Detection, DETR-based Framework, Object Queries, Interaction Queries, Semantic Fusion

Summary: 
The paper introduces the Dual Query Enhancement Network (DQEN) for Human-Object Interaction (HOI) detection, focusing on enhancing object and interaction queries for improved detection accuracy. The proposed method enhances object queries with object-aware encoder features to better capture interactions between humans and objects. It also leverages an Interaction Semantic Fusion module to enhance interaction queries using features from HOI candidates identified by the CLIP model. Additionally, an Auxiliary Prediction Unit is introduced to enhance interaction features further. The DQEN model achieves competitive performance on the HICO-Det and V-COCO datasets, showcasing its effectiveness in improving HOI detection accuracy. The source code for DQEN is publicly available on GitHub for reproducibility and further research advancements.<br /><br />Summary: <div>
arXiv:2508.18896v1 Announce Type: new 
Abstract: Human-Object Interaction (HOI) detection focuses on localizing human-object pairs and recognizing their interactions. Recently, the DETR-based framework has been widely adopted in HOI detection. In DETR-based HOI models, queries with clear meaning are crucial for accurately detecting HOIs. However, prior works have typically relied on randomly initialized queries, leading to vague representations that limit the model's effectiveness. Meanwhile, humans in the HOI categories are fixed, while objects and their interactions are variable. Therefore, we propose a Dual Query Enhancement Network (DQEN) to enhance object and interaction queries. Specifically, object queries are enhanced with object-aware encoder features, enabling the model to focus more effectively on humans interacting with objects in an object-aware way. On the other hand, we design a novel Interaction Semantic Fusion module to exploit the HOI candidates that are promoted by the CLIP model. Semantic features are extracted to enhance the initialization of interaction queries, thereby improving the model's ability to understand interactions. Furthermore, we introduce an Auxiliary Prediction Unit aimed at improving the representation of interaction features. Our proposed method achieves competitive performance on both the HICO-Det and the V-COCO datasets. The source code is available at https://github.com/lzzhhh1019/DQEN.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Decision-Making for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.18898</link>
<guid>https://arxiv.org/abs/2508.18898</guid>
<content:encoded><![CDATA[
<div> AI-driven decisions, autonomous vehicles, interpretability, loss functions, feature maps <br />
<br />
Trustworthy AI is crucial for autonomous vehicles' widespread deployment. End-to-end approaches face challenges in interpreting decisions, especially in complex urban settings due to deep neural networks' non-linear boundaries. This paper introduces a method to improve interpretability and optimize control commands in autonomous driving. The proposed loss functions enhance model interpretability by generating sparse and localized feature maps, helping explain the image regions influencing control commands. Ablation studies on feature extraction validate the method on CARLA benchmarks, showing improved interpretability leading to reduced infractions and safer driving. The monocular, non-ensemble model outperforms top-performing approaches on the CARLA Leaderboard, achieving lower infractions and higher route completion rates while ensuring interpretability. <br /><br />Summary: <div>
arXiv:2508.18898v1 Announce Type: new 
Abstract: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles. Although end-to-end approaches derive control commands directly from raw data, interpreting these decisions remains challenging, especially in complex urban scenarios. This is mainly attributed to very deep neural networks with non-linear decision boundaries, making it challenging to grasp the logic behind AI-driven decisions. This paper presents a method to enhance interpretability while optimizing control commands in autonomous driving. To address this, we propose loss functions that promote the interpretability of our model by generating sparse and localized feature maps. The feature activations allow us to explain which image regions contribute to the predicted control command. We conduct comprehensive ablation studies on the feature extraction step and validate our method on the CARLA benchmarks. We also demonstrate that our approach improves interpretability, which correlates with reducing infractions, yielding a safer, high-performance driving model. Notably, our monocular, non-ensemble model surpasses the top-performing approaches from the CARLA Leaderboard by achieving lower infraction scores and the highest route completion rate, all while ensuring interpretability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025</title>
<link>https://arxiv.org/abs/2508.18904</link>
<guid>https://arxiv.org/abs/2508.18904</guid>
<content:encoded><![CDATA[
<div> Event-Enriched Image Analysis, EVENTA Grand Challenge, Event-Level Multimodal Understanding, Contextual Information, Temporal Information<br />
Summary:<br />
The EVENTA Grand Challenge, held at ACM Multimedia 2025, introduces a benchmark for event-level multimodal understanding, focusing on the contextual and semantic dimensions of real-world events. It integrates contextual, temporal, and semantic information to capture the who, when, where, what, and why behind an image. The challenge includes two tracks: Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval. 45 teams from six countries participated, with evaluation conducted through Public and Private Test phases for fairness. The top three teams presented their solutions at the conference. EVENTA aims to advance context-aware, narrative-driven multimedia AI, with applications in journalism, media analysis, cultural archiving, and accessibility.<br /> <div>
arXiv:2508.18904v1 Announce Type: new 
Abstract: The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM Multimedia 2025, introduces the first large-scale benchmark for event-level multimodal understanding. Traditional captioning and retrieval tasks largely focus on surface-level recognition of people, objects, and scenes, often overlooking the contextual and semantic dimensions that define real-world events. EVENTA addresses this gap by integrating contextual, temporal, and semantic information to capture the who, when, where, what, and why behind an image. Built upon the OpenEvents V1 dataset, the challenge features two tracks: Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval. A total of 45 teams from six countries participated, with evaluation conducted through Public and Private Test phases to ensure fairness and reproducibility. The top three teams were invited to present their solutions at ACM Multimedia 2025. EVENTA establishes a foundation for context-aware, narrative-driven multimedia AI, with applications in journalism, media analysis, cultural archiving, and accessibility. Further details about the challenge are available at the official homepage: https://ltnghia.github.io/eventa/eventa-2025.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories</title>
<link>https://arxiv.org/abs/2508.18939</link>
<guid>https://arxiv.org/abs/2508.18939</guid>
<content:encoded><![CDATA[
<div> Keywords: pedestrian, trajectory data, group classification, spatial utilization, behavioral analysis

Summary:
This study introduces a framework for distinguishing between group and single pedestrians using real-world trajectory data. A Transformer-based pair classification model is employed to identify cohesive groups and isolate single pedestrians. Spatial utilization metrics such as convex hull area and smallest enclosing circle radius are used to characterize how different pedestrian types occupy space. Behavioral metrics like velocity change and trajectory straightness capture local adaptations during interactions. A typology of encounter types-single-to-single, single-to-group, and group-to-group-is introduced to categorize interaction scenarios. The study establishes a comprehensive metric framework for deeper analysis and paves the way for scalable analysis across different sequence lengths. Future versions will focus on complete quantitative analysis of the metrics and their implications for pedestrian simulation and space design validation in crowd dynamics research.<br /><br />Summary: <div>
arXiv:2508.18939v1 Announce Type: new 
Abstract: This study presents an initial framework for distinguishing group and single pedestrians based on real-world trajectory data, with the aim of analyzing their differences in space utilization and emergent behavioral patterns. By segmenting pedestrian trajectories into fixed time bins and applying a Transformer-based pair classification model, we identify cohesive groups and isolate single pedestrians over a structured sequence-based filtering process. To prepare for deeper analysis, we establish a comprehensive metric framework incorporating both spatial and behavioral dimensions. Spatial utilization metrics include convex hull area, smallest enclosing circle radius, and heatmap-based spatial densities to characterize how different pedestrian types occupy and interact with space. Behavioral metrics such as velocity change, motion angle deviation, clearance radius, and trajectory straightness are designed to capture local adaptations and responses during interactions. Furthermore, we introduce a typology of encounter types-single-to-single, single-to-group, and group-to-group to categorize and later quantify different interaction scenarios. Although this version focuses primarily on the classification pipeline and dataset structuring, it establishes the groundwork for scalable analysis across different sequence lengths 60, 100, and 200 frames. Future versions will incorporate complete quantitative analysis of the proposed metrics and their implications for pedestrian simulation and space design validation in crowd dynamics research.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The point is the mask: scaling coral reef segmentation with weak supervision</title>
<link>https://arxiv.org/abs/2508.18958</link>
<guid>https://arxiv.org/abs/2508.18958</guid>
<content:encoded><![CDATA[
<div> drone imagery, coral reef monitoring, weakly supervised learning, semantic segmentation, remote sensing

Summary:
This study addresses the challenge of monitoring coral reefs at large spatial scales using drone-based aerial imagery. The limited resolution of drone imagery makes it difficult to distinguish fine-scale classes, such as coral morphotypes, without costly and labor-intensive pixel-level annotations. The proposed multi-scale weakly supervised semantic segmentation framework transfers fine-scale ecological information from underwater imagery to aerial data, enabling large-scale coral reef mapping with minimal manual annotation. By combining classification-based supervision, spatial interpolation, and self-distillation techniques, the method allows for large-area segmentation of coral morphotypes and flexibility for integrating new classes. This approach offers a cost-effective and scalable methodology for high-resolution reef monitoring by combining low-cost data collection, weakly supervised deep learning, and multi-scale remote sensing. 

<br /><br /> <div>
arXiv:2508.18958v1 Announce Type: new 
Abstract: Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers</title>
<link>https://arxiv.org/abs/2508.18959</link>
<guid>https://arxiv.org/abs/2508.18959</guid>
<content:encoded><![CDATA[
<div> vector data, map generation, generative AI, web application, user study
<br />
Summary: 
This article introduces a new approach to map-making using generative AI models, specifically image diffusion models. By integrating vector data to guide map generation, the model can accurately create maps in controlled styles based on textual prompts. The researchers have developed a web application to improve usability and accessibility. A user study with professional cartographers has shown the potential of the application in helping both novices and experts create maps more efficiently. The study also highlights the implications of generative AI in map-making and suggests a new role for cartographers in advancing AI-assisted map-making. Further technical improvements have been outlined to enhance the model's capabilities. <div>
arXiv:2508.18959v1 Announce Type: new 
Abstract: Traditional map-making relies heavily on Geographic Information Systems (GIS), requiring domain expertise and being time-consuming, especially for repetitive tasks. Recent advances in generative AI (GenAI), particularly image diffusion models, offer new opportunities for automating and democratizing the map-making process. However, these models struggle with accurate map creation due to limited control over spatial composition and semantic layout. To address this, we integrate vector data to guide map generation in different styles, specified by the textual prompts. Our model is the first to generate accurate maps in controlled styles, and we have integrated it into a web application to improve its usability and accessibility. We conducted a user study with professional cartographers to assess the fidelity of generated maps, the usability of the web application, and the implications of ever-emerging GenAI in map-making. The findings have suggested the potential of our developed application and, more generally, the GenAI models in helping both non-expert users and professionals in creating maps more efficiently. We have also outlined further technical improvements and emphasized the new role of cartographers to advance the paradigm of AI-assisted map-making.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing compact convolutional transformers with super attention</title>
<link>https://arxiv.org/abs/2508.18960</link>
<guid>https://arxiv.org/abs/2508.18960</guid>
<content:encoded><![CDATA[
<div> Keywords: vision model, token mixing, sequence-pooling, convolutional tokenizers, CIFAR100 benchmark 

Summary: 
The paper presents a vision model that utilizes token mixing, sequence-pooling, and convolutional tokenizers to achieve high performance and efficiency in fixed context-length tasks. The proposed model outperforms the baseline on the CIFAR100 benchmark, achieving a significant improvement in validation accuracy. It is shown to be more efficient than Scaled Dot Product Attention (SDPA) transformers when the context length is less than the embedding dimension and at only 60% of the size. The architecture exhibits high training stability and does not require data augmentation techniques like mixup, positional embeddings, or learning rate scheduling. The code for the model is available on Github for further exploration and implementation. <div>
arXiv:2508.18960v1 Announce Type: new 
Abstract: In this paper, we propose a vision model that adopts token mixing, sequence-pooling, and convolutional tokenizers to achieve state-of-the-art performance and efficient inference in fixed context-length tasks. In the CIFAR100 benchmark, our model significantly improves the baseline of the top 1% and top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%, while being more efficient than the Scaled Dot Product Attention (SDPA) transformers when the context length is less than the embedding dimension and only 60% the size. In addition, the architecture demonstrates high training stability and does not rely on techniques such as data augmentation like mixup, positional embeddings, or learning rate scheduling. We make our code available on Github.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning</title>
<link>https://arxiv.org/abs/2508.18966</link>
<guid>https://arxiv.org/abs/2508.18966</guid>
<content:encoded><![CDATA[
arXiv:2508.18966v1 Announce Type: new 
Abstract: Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of content and style, a long-standing theme in style-driven research. To this end, we present USO, a Unified Style-Subject Optimized customization model. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content-style disentanglement training. Third, we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: https://github.com/bytedance/USO
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we make NeRF-based visual localization privacy-preserving?</title>
<link>https://arxiv.org/abs/2508.18971</link>
<guid>https://arxiv.org/abs/2508.18971</guid>
<content:encoded><![CDATA[
arXiv:2508.18971v1 Announce Type: new 
Abstract: Visual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Document VQA Models via Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.18984</link>
<guid>https://arxiv.org/abs/2508.18984</guid>
<content:encoded><![CDATA[
arXiv:2508.18984v1 Announce Type: new 
Abstract: Document Visual Question Answering (Document VQA) must cope with documents that span dozens of pages, yet leading systems still concatenate every page or rely on very large vision-language models, both of which are memory-hungry. Retrieval-Augmented Generation (RAG) offers an attractive alternative, first retrieving a concise set of relevant segments before generating answers from this selected evidence. In this paper, we systematically evaluate the impact of incorporating RAG into Document VQA through different retrieval variants - text-based retrieval using OCR tokens and purely visual retrieval without OCR - across multiple models and benchmarks. Evaluated on the multi-page datasets MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the "concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variant achieves +5.0 ANLS improvement without requiring any text extraction. An ablation confirms that retrieval and reranking components drive most of the gain, whereas the layout-guided chunking strategy - proposed in several recent works to leverage page structure - fails to help on these datasets. Our experiments demonstrate that careful evidence selection consistently boosts accuracy across multiple model sizes and multi-page benchmarks, underscoring its practical value for real-world Document VQA.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone</title>
<link>https://arxiv.org/abs/2508.18989</link>
<guid>https://arxiv.org/abs/2508.18989</guid>
<content:encoded><![CDATA[
arXiv:2508.18989v1 Announce Type: new 
Abstract: As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</title>
<link>https://arxiv.org/abs/2508.19003</link>
<guid>https://arxiv.org/abs/2508.19003</guid>
<content:encoded><![CDATA[
arXiv:2508.19003v1 Announce Type: new 
Abstract: Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a transformer encoder-decoder-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroDetect-Net (MDN): Leveraging Deep Learning to Detect Microplastics in Clam Blood, a Step Towards Human Blood Analysis</title>
<link>https://arxiv.org/abs/2508.19021</link>
<guid>https://arxiv.org/abs/2508.19021</guid>
<content:encoded><![CDATA[
arXiv:2508.19021v1 Announce Type: new 
Abstract: With the prevalence of plastics exceeding 368 million tons yearly, microplastic pollution has grown to an extent where air, water, soil, and living organisms have all tested positive for microplastic presence. These particles, which are smaller than 5 millimeters in size, are no less harmful to humans than to the environment. Toxicity research on microplastics has shown that exposure may cause liver infection, intestinal injuries, and gut flora imbalance, leading to numerous potential health hazards. This paper presents a new model, MicroDetect-Net (MDN), which applies fluorescence microscopy with Nile Red dye staining and deep learning to scan blood samples for microplastics. Although clam blood has certain limitations in replicating real human blood, this study opens avenues for applying the approach to human samples, which are more consistent for preliminary data collection. The MDN model integrates dataset preparation, fluorescence imaging, and segmentation using a convolutional neural network to localize and count microplastic fragments. The combination of convolutional networks and Nile Red dye for segmentation produced strong image detection and accuracy. MDN was evaluated on a dataset of 276 Nile Red-stained fluorescent blood images and achieved an accuracy of ninety two percent. Robust performance was observed with an Intersection over Union of 87.4 percent, F1 score of 92.1 percent, Precision of 90.6 percent, and Recall of 93.7 percent. These metrics demonstrate the effectiveness of MDN in the detection of microplastics.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval</title>
<link>https://arxiv.org/abs/2508.19024</link>
<guid>https://arxiv.org/abs/2508.19024</guid>
<content:encoded><![CDATA[
arXiv:2508.19024v1 Announce Type: new 
Abstract: Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task that involves retrieving videos based on queries relevant to only specific segments. While existing works follow the paradigm of developing models to process unimodal features, powerful pretrained vision-language models like CLIP remain underexplored in this field. To bridge this gap, we propose ProPy, a model with systematic architectural adaption of CLIP specifically designed for PRVR. Drawing insights from the semantic relevance of multi-granularity events, ProPy introduces two key innovations: (1) A Prompt Pyramid structure that organizes event prompts to capture semantics at multiple granularity levels, and (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that enables dynamic semantic interaction among events. With these designs, ProPy achieves SOTA performance on three public datasets, outperforming previous models by significant margins. Code is available at https://github.com/BUAAPY/ProPy.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GReAT: leveraging geometric artery data to improve wall shear stress assessment</title>
<link>https://arxiv.org/abs/2508.19030</link>
<guid>https://arxiv.org/abs/2508.19030</guid>
<content:encoded><![CDATA[
arXiv:2508.19030v1 Announce Type: new 
Abstract: Leveraging big data for patient care is promising in many medical fields such as cardiovascular health. For example, hemodynamic biomarkers like wall shear stress could be assessed from patient-specific medical images via machine learning algorithms, bypassing the need for time-intensive computational fluid simulation. However, it is extremely challenging to amass large-enough datasets to effectively train such models. We could address this data scarcity by means of self-supervised pre-training and foundations models given large datasets of geometric artery models. In the context of coronary arteries, leveraging learned representations to improve hemodynamic biomarker assessment has not yet been well studied. In this work, we address this gap by investigating whether a large dataset (8449 shapes) consisting of geometric models of 3D blood vessels can benefit wall shear stress assessment in coronary artery models from a small-scale clinical trial (49 patients). We create a self-supervised target for the 3D blood vessels by computing the heat kernel signature, a quantity obtained via Laplacian eigenvectors, which captures the very essence of the shapes. We show how geometric representations learned from this datasets can boost segmentation of coronary arteries into regions of low, mid and high (time-averaged) wall shear stress even when trained on limited data.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</title>
<link>https://arxiv.org/abs/2508.19060</link>
<guid>https://arxiv.org/abs/2508.19060</guid>
<content:encoded><![CDATA[
arXiv:2508.19060v1 Announce Type: new 
Abstract: Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Binary Sampling Patterns for Single-Pixel Imaging using Bilevel Optimisation</title>
<link>https://arxiv.org/abs/2508.19068</link>
<guid>https://arxiv.org/abs/2508.19068</guid>
<content:encoded><![CDATA[
arXiv:2508.19068v1 Announce Type: new 
Abstract: Single-Pixel Imaging enables reconstructing objects using a single detector through sequential illuminations with structured light patterns. We propose a bilevel optimisation method for learning task-specific, binary illumination patterns, optimised for applications like single-pixel fluorescence microscopy. We address the non-differentiable nature of binary pattern optimisation using the Straight-Through Estimator and leveraging a Total Deep Variation regulariser in the bilevel formulation. We demonstrate our method on the CytoImageNet microscopy dataset and show that learned patterns achieve superior reconstruction performance compared to baseline methods, especially in highly undersampled regimes.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VibES: Induced Vibration for Persistent Event-Based Sensing</title>
<link>https://arxiv.org/abs/2508.19094</link>
<guid>https://arxiv.org/abs/2508.19094</guid>
<content:encoded><![CDATA[
arXiv:2508.19094v1 Announce Type: new 
Abstract: Event cameras are a bio-inspired class of sensors that asynchronously measure per-pixel intensity changes. Under fixed illumination conditions in static or low-motion scenes, rigidly mounted event cameras are unable to generate any events, becoming unsuitable for most computer vision tasks. To address this limitation, recent work has investigated motion-induced event stimulation that often requires complex hardware or additional optical components. In contrast, we introduce a lightweight approach to sustain persistent event generation by employing a simple rotating unbalanced mass to induce periodic vibrational motion. This is combined with a motion-compensation pipeline that removes the injected motion and yields clean, motion-corrected events for downstream perception tasks. We demonstrate our approach with a hardware prototype and evaluate it on real-world captured datasets. Our method reliably recovers motion parameters and improves both image reconstruction and edge detection over event-based sensing without motion induction.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents</title>
<link>https://arxiv.org/abs/2508.19162</link>
<guid>https://arxiv.org/abs/2508.19162</guid>
<content:encoded><![CDATA[
arXiv:2508.19162v1 Announce Type: new 
Abstract: A foundational task for the digital analysis of documents is text line segmentation. However, automating this process with deep learning models is challenging because it requires large, annotated datasets that are often unavailable for historical documents. Additionally, the annotation process is a labor- and cost-intensive task that requires expert knowledge, which makes few-shot learning a promising direction for reducing data requirements. In this work, we demonstrate that small and simple architectures, coupled with a topology-aware loss function, are more accurate and data-efficient than more complex alternatives. We pair a lightweight UNet++ with a connectivity-aware loss, initially developed for neuron morphology, which explicitly penalizes structural errors like line fragmentation and unintended line merges. To increase our limited data, we train on small patches extracted from a mere three annotated pages per manuscript. Our methodology significantly improves upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200% increase in Recognition Accuracy and a 75% increase in Line Intersection over Union. Our method also achieves an F-Measure score on par with or even exceeding that of the competition winner of the DIVA-HisDB baseline detection task, all while requiring only three annotated pages, exemplifying the efficacy of our approach. Our implementation is publicly available at: https://github.com/RafaelSterzinger/acpr_few_shot_hist.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2508.19165</link>
<guid>https://arxiv.org/abs/2508.19165</guid>
<content:encoded><![CDATA[
arXiv:2508.19165v1 Announce Type: new 
Abstract: Monocular 3D visual grounding is a novel task that aims to locate 3D objects in RGB images using text descriptions with explicit geometry information. Despite the inclusion of geometry details in the text, we observe that the text embeddings are sensitive to the magnitude of numerical values but largely ignore the associated measurement units. For example, simply equidistant mapping the length with unit "meter" to "decimeters" or "centimeters" leads to severe performance degradation, even though the physical length remains equivalent. This observation signifies the weak 3D comprehension of pre-trained language model, which generates misguiding text features to hinder 3D perception. Therefore, we propose to enhance the 3D perception of model on text embeddings and geometry features with two simple and effective methods. Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE), which enhances the comprehension of mapping relationships between different units by augmenting the diversity of distance descriptors in text queries. Next, we propose a Text-Guided Geometry Enhancement (TGE) module to further enhance the 3D-text information by projecting the basic text features into geometrically consistent space. These 3D-enhanced text features are then leveraged to precisely guide the attention of geometry features. We evaluate the proposed method through extensive comparisons and ablation studies on the Mono3DRefer dataset. Experimental results demonstrate substantial improvements over previous methods, achieving new state-of-the-art results with a notable accuracy gain of 11.94\% in the "Far" scenario. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</title>
<link>https://arxiv.org/abs/2508.19167</link>
<guid>https://arxiv.org/abs/2508.19167</guid>
<content:encoded><![CDATA[
arXiv:2508.19167v1 Announce Type: new 
Abstract: Vision Transformers have demonstrated remarkable success in computer vision tasks, yet their reliance on learnable one-dimensional positional embeddings fundamentally disrupts the inherent two-dimensional spatial structure of images through patch flattening procedures. Traditional positional encoding approaches lack geometric constraints and fail to establish monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Comprehensive experiments demonstrate that WEF-PE achieves superior performance across diverse scenarios, including 63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture, 93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay property through rigorous mathematical proof, while attention visualization reveals enhanced geometric inductive bias and more coherent semantic focus compared to conventional approaches.The source code implementing the methods described in this paper is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoccerNet 2025 Challenges Results</title>
<link>https://arxiv.org/abs/2508.19182</link>
<guid>https://arxiv.org/abs/2508.19182</guid>
<content:encoded><![CDATA[
arXiv:2508.19182v1 Announce Type: new 
Abstract: The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet open benchmarking effort, dedicated to advancing computer vision research in football video understanding. This year's challenges span four vision-based tasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions in football broadcasts and assigning actions to teams; (2) Monocular Depth Estimation, targeting the recovery of scene geometry from single-camera broadcast clips through relative depth estimation for each pixel; (3) Multi-View Foul Recognition, requiring the analysis of multiple synchronized camera views to classify fouls and their severity; and (4) Game State Reconstruction, aimed at localizing and identifying all players from a broadcast video to reconstruct the game state on a 2D top-view of the field. Across all tasks, participants were provided with large-scale annotated datasets, unified evaluation protocols, and strong baselines as starting points. This report presents the results of each challenge, highlights the top-performing solutions, and provides insights into the progress made by the community. The SoccerNet Challenges continue to serve as a driving force for reproducible, open research at the intersection of computer vision, artificial intelligence, and sports. Detailed information about the tasks, challenges, and leaderboards can be found at https://www.soccer-net.org, with baselines and development kits available at https://github.com/SoccerNet.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastMesh:Efficient Artistic Mesh Generation via Component Decoupling</title>
<link>https://arxiv.org/abs/2508.19188</link>
<guid>https://arxiv.org/abs/2508.19188</guid>
<content:encoded><![CDATA[
arXiv:2508.19188v1 Announce Type: new 
Abstract: Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8$\times$ faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All-in-One Slider for Attribute Manipulation in Diffusion Models</title>
<link>https://arxiv.org/abs/2508.19195</link>
<guid>https://arxiv.org/abs/2508.19195</guid>
<content:encoded><![CDATA[
arXiv:2508.19195v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models have made significant strides in generating high-quality images. However, progressively manipulating certain attributes of generated images to meet the desired user expectations remains challenging, particularly for content with rich details, such as human faces. Some studies have attempted to address this by training slider modules. However, they follow a One-for-One manner, where an independent slider is trained for each attribute, requiring additional training whenever a new attribute is introduced. This not only results in parameter redundancy accumulated by sliders but also restricts the flexibility of practical applications and the scalability of attribute manipulation. To address this issue, we introduce the All-in-One Slider, a lightweight module that decomposes the text embedding space into sparse, semantically meaningful attribute directions. Once trained, it functions as a general-purpose slider, enabling interpretable and fine-grained continuous control over various attributes. Moreover, by recombining the learned directions, the All-in-One Slider supports zero-shot manipulation of unseen attributes (e.g., races and celebrities) and the composition of multiple attributes. Extensive experiments demonstrate that our method enables accurate and scalable attribute manipulation, achieving notable improvements compared to previous methods. Furthermore, our method can be extended to integrate with the inversion framework to perform attribute manipulation on real images, broadening its applicability to various real-world scenarios. The code and trained model will be released at: https://github.com/ywxsuperstar/KSAE-FaceSteer.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding</title>
<link>https://arxiv.org/abs/2508.19204</link>
<guid>https://arxiv.org/abs/2508.19204</guid>
<content:encoded><![CDATA[
arXiv:2508.19204v1 Announce Type: new 
Abstract: Large-scale scene data is essential for training and testing in robot learning. Neural reconstruction methods have promised the capability of reconstructing large physically-grounded outdoor scenes from captured sensor data. However, these methods have baked-in static environments and only allow for limited scene control -- they are functionally constrained in scene and trajectory diversity by the captures from which they are reconstructed. In contrast, generating driving data with recent image or video diffusion models offers control, however, at the cost of geometry grounding and causality. In this work, we aim to bridge this gap and present a method that directly generates large-scale 3D driving scenes with accurate geometry, allowing for causal novel view synthesis with object permanence and explicit 3D geometry estimation. The proposed method combines the generation of a proxy geometry and environment representation with score distillation from learned 2D image priors. We find that this approach allows for high controllability, enabling the prompt-guided geometry and high-fidelity texture and structure that can be conditioned on map layouts -- producing realistic and geometrically consistent 3D generations of complex driving scenes.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation</title>
<link>https://arxiv.org/abs/2508.19209</link>
<guid>https://arxiv.org/abs/2508.19209</guid>
<content:encoded><![CDATA[
arXiv:2508.19209v1 Announce Type: new 
Abstract: Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, \textbf{we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: \href{https://omnihuman-lab.github.io/v1_5/}
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth</title>
<link>https://arxiv.org/abs/2508.19232</link>
<guid>https://arxiv.org/abs/2508.19232</guid>
<content:encoded><![CDATA[
arXiv:2508.19232v1 Announce Type: new 
Abstract: Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yet the characterization of their dynamic growth is limited by the experimental challenges in nanoscale motion measurement using scanning electron microscopy (SEM) imaging. Existing ex situ methods offer only static analysis, while in situ techniques often require manual initialization and lack continuous per-particle trajectory decomposition. We present Visual Feature Tracking (VFTrack) an in-situ real-time particle tracking framework that automatically detects and tracks individual CNT particles in SEM image sequences. VFTrack integrates handcrafted or deep feature detectors and matchers within a particle tracking framework to enable kinematic analysis of CNT micropillar growth. A systematic using 13,540 manually annotated trajectories identifies the ALIKED detector with LightGlue matcher as an optimal combination (F1-score of 0.78, $\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth, lateral drift, and oscillations, facilitate the calculation of heterogeneous regional growth rates and the reconstruction of evolving CNT pillar morphologies. This work enables advancement in automated nano-material characterization, bridging the gap between physics-based models and experimental observation to enable real-time optimization of CNT synthesis.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoregressive Universal Video Segmentation Model</title>
<link>https://arxiv.org/abs/2508.19242</link>
<guid>https://arxiv.org/abs/2508.19242</guid>
<content:encoded><![CDATA[
arXiv:2508.19242v1 Announce Type: new 
Abstract: Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style4D-Bench: A Benchmark Suite for 4D Stylization</title>
<link>https://arxiv.org/abs/2508.19243</link>
<guid>https://arxiv.org/abs/2508.19243</guid>
<content:encoded><![CDATA[
arXiv:2508.19243v1 Announce Type: new 
Abstract: We introduce Style4D-Bench, the first benchmark suite specifically designed for 4D stylization, with the goal of standardizing evaluation and facilitating progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive evaluation protocol measuring spatial fidelity, temporal coherence, and multi-view consistency through both perceptual and quantitative metrics, 2) a strong baseline that make an initial attempt for 4D stylization, and 3) a curated collection of high-resolution dynamic 4D scenes with diverse motions and complex backgrounds. To establish a strong baseline, we present Style4D, a novel framework built upon 4D Gaussian Splatting. It consists of three key components: a basic 4DGS scene representation to capture reliable geometry, a Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for temporally and spatially aware appearance control, and a Holistic Geometry-Preserved Style Transfer module designed to enhance spatio-temporal consistency via contrastive coherence learning and structural content preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D achieves state-of-the-art performance in 4D stylization, producing fine-grained stylistic details with stable temporal dynamics and consistent multi-view rendering. We expect Style4D-Bench to become a valuable resource for benchmarking and advancing research in stylized rendering of dynamic 3D scenes. Project page: https://becky-catherine.github.io/Style4D . Code: https://github.com/Becky-catherine/Style4D-Bench .
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulate3D: Zero-Shot Text-Driven 3D Object Posing</title>
<link>https://arxiv.org/abs/2508.19244</link>
<guid>https://arxiv.org/abs/2508.19244</guid>
<content:encoded><![CDATA[
arXiv:2508.19244v1 Announce Type: new 
Abstract: We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\% of the time, confirm its superiority over existing approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space</title>
<link>https://arxiv.org/abs/2508.19247</link>
<guid>https://arxiv.org/abs/2508.19247</guid>
<content:encoded><![CDATA[
arXiv:2508.19247v1 Announce Type: new 
Abstract: 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges</title>
<link>https://arxiv.org/abs/2508.18296</link>
<guid>https://arxiv.org/abs/2508.18296</guid>
<content:encoded><![CDATA[
arXiv:2508.18296v1 Announce Type: cross 
Abstract: Stroke is the second leading cause of death and the third leading cause of disability worldwide. Clinical guidelines establish diffusion resonance imaging (DWI, ADC) as the standard for localizing, characterizing, and measuring infarct volume, enabling treatment support and prognosis. Nonetheless, such lesion analysis is highly variable due to different patient demographics, scanner vendors, and expert annotations. Computational support approaches have been key to helping with the localization and segmentation of lesions. However, these strategies are dedicated solutions that learn patterns from only one institution, lacking the variability to generalize geometrical lesions shape models. Even worse, many clinical centers lack sufficient labeled samples to adjust these dedicated solutions. This work developed a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. From 14 emulated healthcare centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm 0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm 0.26$ over all centers, outperforming both the centralized and other federated rules. Interestingly, the model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD of $4.44 \pm 8.74$ without any additional training).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR</title>
<link>https://arxiv.org/abs/2508.18481</link>
<guid>https://arxiv.org/abs/2508.18481</guid>
<content:encoded><![CDATA[
arXiv:2508.18481v1 Announce Type: cross 
Abstract: Optical see-through augmented reality (OST-AR) systems like Microsoft HoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth perception of the hologram and occlusion of real instruments remain challenging. We present an evaluation of how visualizing the target object with different transparencies and visualizing a tracked tool (virtual proxy vs. real tool vs. no tool tracking) affects depth perception and system usability. Ten participants performed two experiments on HoloLens 2. In Experiment 1, we compared high-transparency vs. low-transparency target rendering in a depth matching task at arm's length. In Experiment 2, participants performed a simulated surgical pinpoint task on a frontal bone target under six visualization conditions ($2 \times 3$: two target transparencies and three tool visualization modes: virtual tool hologram, real tool, or no tool tracking). We collected data on depth matching error, target localization error, system usability, task workload, and qualitative feedback. Results show that a more opaque target yields significantly lower depth estimation error than a highly transparent target at arm's distance. Moreover, showing the real tool (occluding the virtual target) led to the highest accuracy and usability with the lowest workload, while not tracking the tool yielded the worst performance and user ratings. However, making the target highly transparent, while allowing the real tool to remain visible, slightly impaired depth cues and did not improve usability. Our findings underscore that correct occlusion cues, rendering virtual content opaque and occluding it with real tools in real time, are critical for depth perception and precision in OST-AR. Designers of arm-distance AR systems should prioritize robust tool tracking and occlusion handling; if unavailable, cautiously use transparency to balance depth perception and tool visibility.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas</title>
<link>https://arxiv.org/abs/2508.18509</link>
<guid>https://arxiv.org/abs/2508.18509</guid>
<content:encoded><![CDATA[
arXiv:2508.18509v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove private or sensitive data from a pre-trained model while preserving the model's robustness. Despite recent advances, this technique has not been explored in medical image classification. This work evaluates the SalUn unlearning model by conducting experiments on the PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of data augmentation on the quality of unlearning. Results show that SalUn achieves performance close to full retraining, indicating an efficient solution for use in medical applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Single-shot Animation Blending with Temporal Conditioning</title>
<link>https://arxiv.org/abs/2508.18525</link>
<guid>https://arxiv.org/abs/2508.18525</guid>
<content:encoded><![CDATA[
arXiv:2508.18525v1 Announce Type: cross 
Abstract: Training a generative model on a single human skeletal motion sequence without being bound to a specific kinematic tree has drawn significant attention from the animation community. Unlike text-to-motion generation, single-shot models allow animators to controllably generate variations of existing motion patterns without requiring additional data or extensive retraining. However, existing single-shot methods do not explicitly offer a controllable framework for blending two or more motions within a single generative pass. In this paper, we present the first single-shot motion blending framework that enables seamless blending by temporally conditioning the generation process. Our method introduces a skeleton-aware normalization mechanism to guide the transition between motions, allowing smooth, data-driven control over when and how motions blend. We perform extensive quantitative and qualitative evaluations across various animation styles and different kinematic skeletons, demonstrating that our approach produces plausible, smooth, and controllable motion blends in a unified and efficient manner.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Application for Psoriasis Detection</title>
<link>https://arxiv.org/abs/2508.18528</link>
<guid>https://arxiv.org/abs/2508.18528</guid>
<content:encoded><![CDATA[
arXiv:2508.18528v1 Announce Type: cross 
Abstract: In this paper a comparative study of the performance of three Convolutional Neural Network models, ResNet50, Inception v3 and VGG19 for classification of skin images with lesions affected by psoriasis is presented. The images used for training and validation of the models were obtained from specialized platforms. Some techniques were used to adjust the evaluation metrics of the neural networks. The results found suggest the model Inception v3 as a valuable tool for supporting the diagnosis of psoriasis. This is due to its satisfactory performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation</title>
<link>https://arxiv.org/abs/2508.18569</link>
<guid>https://arxiv.org/abs/2508.18569</guid>
<content:encoded><![CDATA[
arXiv:2508.18569v1 Announce Type: cross 
Abstract: Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Our self-evaluation approach combines existing metrics with our newly proposed metaphor decomposition score and a meaning alignment (MA) metric. Within this setup, we explore two novel approaches: a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining. On the held-out test set, the training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. We evaluate our framework output using a user-facing study, and observed that participants preferred GPT-4o overall, while our training-free pipeline led open-source methods and edged Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases; we also observe sensitivity to sampler settings. Overall, structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis</title>
<link>https://arxiv.org/abs/2508.18597</link>
<guid>https://arxiv.org/abs/2508.18597</guid>
<content:encoded><![CDATA[
arXiv:2508.18597v1 Announce Type: cross 
Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor scenes across multiple room types. The model introduces a scene layout representation combining a top-down semantic map and attributes for each object. Unlike prior approaches, which cannot condition on architectural constraints, SemLayoutDiff employs a categorical diffusion model capable of conditioning scene synthesis explicitly on room masks. It first generates a coherent semantic map, followed by a cross-attention-based network to predict furniture placements that respect the synthesized layout. Our method also accounts for architectural elements such as doors and windows, ensuring that generated furniture arrangements remain practical and unobstructed. Experiments on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent, realistic, and varied scenes, outperforming previous methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data</title>
<link>https://arxiv.org/abs/2508.18630</link>
<guid>https://arxiv.org/abs/2508.18630</guid>
<content:encoded><![CDATA[
arXiv:2508.18630v1 Announce Type: cross 
Abstract: Unsupervised domain adaptation methods seek to generalize effectively on unlabeled test data, especially when encountering the common challenge in time series data that distribution shifts occur between training and testing datasets. In this paper, we propose incorporating multi-scale feature extraction and uncertainty estimation to improve the model's generalization and robustness across domains. Our approach begins with a multi-scale mixed input architecture that captures features at different scales, increasing training diversity and reducing feature discrepancies between the training and testing domains. Based on the mixed input architecture, we further introduce an uncertainty awareness mechanism based on evidential learning by imposing a Dirichlet prior on the labels to facilitate both target prediction and uncertainty estimation. The uncertainty awareness mechanism enhances domain adaptation by aligning features with the same labels across different domains, which leads to significant performance improvements in the target domain. Additionally, our uncertainty-aware model demonstrates a much lower Expected Calibration Error (ECE), indicating better-calibrated prediction confidence. Our experimental results show that this combined approach of mixed input architecture with the uncertainty awareness mechanism achieves state-of-the-art performance across multiple benchmark datasets, underscoring its effectiveness in unsupervised domain adaptation for time series data.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Video-Based Robot Failure Detection Using Task Knowledge</title>
<link>https://arxiv.org/abs/2508.18705</link>
<guid>https://arxiv.org/abs/2508.18705</guid>
<content:encoded><![CDATA[
arXiv:2508.18705v1 Announce Type: cross 
Abstract: Robust robotic task execution hinges on the reliable detection of execution failures in order to trigger safe operation modes, recovery strategies, or task replanning. However, many failure detection methods struggle to provide meaningful performance when applied to a variety of real-world scenarios. In this paper, we propose a video-based failure detection approach that uses spatio-temporal knowledge in the form of the actions the robot performs and task-relevant objects within the field of view. Both pieces of information are available in most robotic scenarios and can thus be readily obtained. We demonstrate the effectiveness of our approach on three datasets that we amend, in part, with additional annotations of the aforementioned task-relevant knowledge. In light of the results, we also propose a data augmentation method that improves performance by applying variable frame rates to different parts of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the ARMBench dataset without additional computational expense and an additional increase to 81.4 with test-time augmentation. The results emphasize the importance of spatio-temporal information during failure detection and suggest further investigation of suitable heuristics in future implementations. Code and annotations are available.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature</title>
<link>https://arxiv.org/abs/2508.18717</link>
<guid>https://arxiv.org/abs/2508.18717</guid>
<content:encoded><![CDATA[
arXiv:2508.18717v1 Announce Type: cross 
Abstract: We present a unified framework combining statistical physics, coding theory, and algebraic topology for efficient multi-class image classification. High-dimensional feature vectors from a frozen MobileNetV2 backbone are interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC (MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local trapping sets in the code's graph and topological invariants (Betti numbers, bordism classes) of the feature manifold. A practical algorithm estimates $\beta_N$ efficiently with a quadratic interpolant and Newton correction, achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph ensembles, using permanent bounds to suppress harmful trapping sets. This compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and -100 subsets. Despite massive compression (40x fewer parameters), we achieve 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that topology-guided graph design yields highly efficient, physics-inspired embeddings with state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction</title>
<link>https://arxiv.org/abs/2508.18751</link>
<guid>https://arxiv.org/abs/2508.18751</guid>
<content:encoded><![CDATA[
arXiv:2508.18751v1 Announce Type: cross 
Abstract: Deep neural networks demonstrate strong performance under aligned training-test distributions. However, real-world test data often exhibit domain shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the model to test data during inference. While most TTA studies assume that the training and test data share the same class set (closed-set TTA), real-world scenarios often involve open-set data (open-set TTA), which can degrade closed-set accuracy. A recent study showed that identifying open-set data during adaptation and maximizing its entropy is an effective solution. However, the previous method relies on the source model for filtering, resulting in suboptimal filtering accuracy on domain-shifted test data. In contrast, we found that the adapting model, which learns domain knowledge from noisy test streams, tends to be unstable and leads to error accumulation when used for filtering. To address this problem, we propose Primary-Auxiliary Filtering (PAF), which employs an auxiliary filter to validate data filtered by the primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model to integrate their complementary knowledge for OSTTA. We validate our approach across diverse closed-set and open-set datasets. Our method enhances both closed-set accuracy and open-set discrimination over existing methods. The code is available at https://github.com/powerpowe/PAF-KIP-OSTTA .
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding</title>
<link>https://arxiv.org/abs/2508.18785</link>
<guid>https://arxiv.org/abs/2508.18785</guid>
<content:encoded><![CDATA[
arXiv:2508.18785v1 Announce Type: cross 
Abstract: Deep understanding of electromagnetic signals is fundamental to dynamic spectrum management, intelligent transportation, autonomous driving and unmanned vehicle perception. The field faces challenges because electromagnetic signals differ greatly from text and images, showing high heterogeneity, strong background noise and complex joint time frequency structure, which prevents existing general models from direct use. Electromagnetic communication and sensing tasks are diverse, current methods lack cross task generalization and transfer efficiency, and the scarcity of large high quality datasets blocks the creation of a truly general multitask learning framework. To overcome these issue, we introduce EMind, an electromagnetic signals foundation model that bridges large scale pretraining and the unique nature of this modality. We build the first unified and largest standardized electromagnetic signal dataset covering multiple signal types and tasks. By exploiting the physical properties of electromagnetic signals, we devise a length adaptive multi-signal packing method and a hardware-aware training strategy that enable efficient use and representation learning from heterogeneous multi-source signals. Experiments show that EMind achieves strong performance and broad generalization across many downstream tasks, moving decisively from task specific models to a unified framework for electromagnetic intelligence. The code is available at: https://github.com/GabrielleTse/EMind.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework</title>
<link>https://arxiv.org/abs/2508.18790</link>
<guid>https://arxiv.org/abs/2508.18790</guid>
<content:encoded><![CDATA[
arXiv:2508.18790v1 Announce Type: cross 
Abstract: The development of artificial intelligence models for macular edema (ME) analy-sis always relies on expert-annotated pixel-level image datasets which are expen-sive to collect prospectively. While anomaly-detection-based weakly-supervised methods have shown promise in edema area (EA) segmentation task, their per-formance still lags behind fully-supervised approaches. In this paper, we leverage the strong correlation between EA and retinal layers in spectral-domain optical coherence tomography (SD-OCT) images, along with the update characteristics of weakly-supervised learning, to enhance an off-the-shelf adversarial framework for EA segmentation with a novel layer-structure-guided post-processing step and a test-time-adaptation (TTA) strategy. By incorporating additional retinal lay-er information, our framework reframes the dense EA prediction task as one of confirming intersection points between the EA contour and retinal layers, result-ing in predictions that better align with the shape prior of EA. Besides, the TTA framework further helps address discrepancies in the manifestations and presen-tations of EA between training and test sets. Extensive experiments on two pub-licly available datasets demonstrate that these two proposed ingredients can im-prove the accuracy and robustness of EA segmentation, bridging the gap between weakly-supervised and fully-supervised models.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.18805</link>
<guid>https://arxiv.org/abs/2508.18805</guid>
<content:encoded><![CDATA[
arXiv:2508.18805v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-world applications, but their high inference cost makes them vulnerable to resource consumption attacks. Prior attacks attempt to extend VLM output sequences by optimizing adversarial images, thereby increasing inference costs. However, these extended outputs often introduce irrelevant abnormal content, compromising attack stealthiness. This trade-off between effectiveness and stealthiness poses a major limitation for existing attacks. To address this challenge, we propose \textit{Hidden Tail}, a stealthy resource consumption attack that crafts prompt-agnostic adversarial images, inducing VLMs to generate maximum-length outputs by appending special tokens invisible to users. Our method employs a composite loss function that balances semantic preservation, repetitive special token induction, and suppression of the end-of-sequence (EOS) token, optimized via a dynamic weighting strategy. Extensive experiments show that \textit{Hidden Tail} outperforms existing attacks, increasing output length by up to 19.2$\times$ and reaching the maximum token limit, while preserving attack stealthiness. These results highlight the urgent need to improve the robustness of VLMs against efficiency-oriented adversarial threats. Our code is available at https://github.com/zhangrui4041/Hidden_Tail.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation</title>
<link>https://arxiv.org/abs/2508.18826</link>
<guid>https://arxiv.org/abs/2508.18826</guid>
<content:encoded><![CDATA[
arXiv:2508.18826v1 Announce Type: cross 
Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias in real-world scenarios, posing significant challenges in ethically sensitive domains such as healthcare. Such bias can negatively affect model fairness, model generalization abilities and further risks amplifying social discrimination. There is a need to remove biases from trained models. Existing debiasing approaches often necessitate access to original training data and need extensive model retraining; they also typically exhibit trade-offs between model fairness and discriminative performance. To address these challenges, we propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that efficiently improves fairness while preserving discriminative performance with much less debiasing costs. Notably, SWiFT requires only a small external dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to first find the relative, and yet distinct, contributions of model parameters to both bias and predictive performance. Then, a two-step fine-tuning process updates each parameter with different gradient flows defined by its contribution. Extensive experiments with three bias sensitive attributes (gender, skin tone, and age) across four dermatological and two chest X-ray datasets demonstrate that SWiFT can consistently reduce model bias while achieving competitive or even superior diagnostic accuracy under common fairness and accuracy metrics, compared to the state-of-the-art. Specifically, we demonstrate improved model generalization ability as evidenced by superior performance on several out-of-distribution (OOD) datasets.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Circuit-Based Visual Fractal Image Generation in Qiskit and Analytics</title>
<link>https://arxiv.org/abs/2508.18835</link>
<guid>https://arxiv.org/abs/2508.18835</guid>
<content:encoded><![CDATA[
arXiv:2508.18835v1 Announce Type: cross 
Abstract: As nature is ascribed as quantum, the fractals also pose some intriguing appearance which is found in many micro and macro observable entities or phenomena. Fractals show self-similarity across sizes; structures that resemble the entire are revealed when zoomed in. In Quantum systems, the probability density or wavefunction may exhibit recurring interference patterns at various energy or length scales. Fractals are produced by basic iterative rules (such as Mandelbrot or Julia sets), and they provide limitless complexity. Despite its simplicity, the Schr\"odinger equation in quantum mechanics produces incredibly intricate patterns of interference and entanglement, particularly in chaotic quantum systems. Quantum computing, the root where lies to the using the principles of quantum-mechanical phenomenon, when applied in fractal image generation, what outcomes are expected? The paper outlines the generation of a Julia set dataset using an approach coupled with building quantum circuit, highlighting the concepts of superposition, randomness, and entanglement as foundational elements to manipulate the generated dataset patterns. As Quantum computing is finding many application areas, the possibility of using quantum circuits for fractal Julia image generation posits a unique direction of future research where it can be applied to quantum generative arts across various ecosystems with a customised approach, such as producing an exciting landscape based on a quantum art theme.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2508.18860</link>
<guid>https://arxiv.org/abs/2508.18860</guid>
<content:encoded><![CDATA[
arXiv:2508.18860v1 Announce Type: cross 
Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge is crucial in continual learning (CL). Recently, sharpness-aware minimization has proven effective in transfer learning and has also been adopted in continual learning (CL) to improve memory retention and learning efficiency. However, relying on zeroth-order sharpness alone may favor sharper minima over flatter ones in certain settings, leading to less robust and potentially suboptimal solutions. In this paper, we propose \textbf{C}ontinual \textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling easy integration with minimal modifications to the code pipeline. Besides, we present a general framework that integrates C-Flat into all major CL paradigms and conduct comprehensive comparisons with loss-minima optimizers and flat-minima-based CL methods. Our results show that C-Flat consistently improves performance across a wide range of settings. In addition, we introduce C-Flat++, an efficient yet effective framework that leverages selective flatness-driven promotion, significantly reducing the update cost required by C-Flat. Extensive experiments across multiple CL methods, datasets, and scenarios demonstrate the effectiveness and efficiency of our proposed approaches. Code is available at https://github.com/WanNaa/C-Flat.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads</title>
<link>https://arxiv.org/abs/2508.18944</link>
<guid>https://arxiv.org/abs/2508.18944</guid>
<content:encoded><![CDATA[
arXiv:2508.18944v1 Announce Type: cross 
Abstract: Achieving realistic hair strand synthesis is essential for creating lifelike digital humans, but producing high-fidelity hair strand geometry remains a significant challenge. Existing methods require a complex setup for data acquisition, involving multi-view images captured in constrained studio environments. Additionally, these methods have longer hair volume estimation and strand synthesis times, which hinder efficiency. We introduce PanoHair, a model that estimates head geometry as signed distance fields using knowledge distillation from a pre-trained generative teacher model for head synthesis. Our approach enables the prediction of semantic segmentation masks and 3D orientations specifically for the hair region of the estimated geometry. Our method is generative and can generate diverse hairstyles with latent space manipulations. For real images, our approach involves an inversion process to infer latent codes and produces visually appealing hair strands, offering a streamlined alternative to complex multi-view data acquisition setups. Given the latent code, PanoHair generates a clean manifold mesh for the hair region in under 5 seconds, along with semantic and orientation maps, marking a significant improvement over existing methods, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm</title>
<link>https://arxiv.org/abs/2508.18967</link>
<guid>https://arxiv.org/abs/2508.18967</guid>
<content:encoded><![CDATA[
arXiv:2508.18967v1 Announce Type: cross 
Abstract: Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical for various applications, including combat support, package delivery and Search and Rescue Operations. This paper introduces the Tangent Intersection Guidance (TIG) algorithm, an advanced approach for UAV path planning in both static and dynamic environments. The algorithm uses the elliptic tangent intersection method to generate feasible paths. It generates two sub-paths for each threat, selects the optimal route based on a heuristic rule, and iteratively refines the path until the target is reached. Considering the UAV kinematic and dynamic constraints, a modified smoothing technique based on quadratic B\'ezier curves is adopted to generate a smooth and efficient route. Experimental results show that the TIG algorithm can generate the shortest path in less time, starting from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent Graph, and Static APPATT algorithms in static environments. Furthermore, in completely unknown and partially known environments, TIG demonstrates efficient real-time path planning capabilities for collision avoidance, outperforming APF and Dynamic APPATT algorithms.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Benefits and Pitfalls of Current Methods for the Segmentation of Undersampled MRI Data</title>
<link>https://arxiv.org/abs/2508.18975</link>
<guid>https://arxiv.org/abs/2508.18975</guid>
<content:encoded><![CDATA[
arXiv:2508.18975v1 Announce Type: cross 
Abstract: MR imaging is a valuable diagnostic tool allowing to non-invasively visualize patient anatomy and pathology with high soft-tissue contrast. However, MRI acquisition is typically time-consuming, leading to patient discomfort and increased costs to the healthcare system. Recent years have seen substantial research effort into the development of methods that allow for accelerated MRI acquisition while still obtaining a reconstruction that appears similar to the fully-sampled MR image. However, for many applications a perfectly reconstructed MR image may not be necessary, particularly, when the primary goal is a downstream task such as segmentation. This has led to growing interest in methods that aim to perform segmentation directly on accelerated MRI data. Despite recent advances, existing methods have largely been developed in isolation, without direct comparison to one another, often using separate or private datasets, and lacking unified evaluation standards. To date, no high-quality, comprehensive comparison of these methods exists, and the optimal strategy for segmenting accelerated MR data remains unknown. This paper provides the first unified benchmark for the segmentation of undersampled MRI data comparing 7 approaches. A particular focus is placed on comparing \textit{one-stage approaches}, that combine reconstruction and segmentation into a unified model, with \textit{two-stage approaches}, that utilize established MRI reconstruction methods followed by a segmentation network. We test these methods on two MRI datasets that include multi-coil k-space data as well as a human-annotated segmentation ground-truth. We find that simple two-stage methods that consider data-consistency lead to the best segmentation scores, surpassing complex specialized methods that are developed specifically for this task.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Analysis of Spiking Neural Systems via Transfer Entropy and Directed Persistent Homology</title>
<link>https://arxiv.org/abs/2508.19048</link>
<guid>https://arxiv.org/abs/2508.19048</guid>
<content:encoded><![CDATA[
arXiv:2508.19048v1 Announce Type: cross 
Abstract: We present a topological framework for analysing neural time series that integrates Transfer Entropy (TE) with directed Persistent Homology (PH) to characterize information flow in spiking neural systems. TE quantifies directional influence between neurons, producing weighted, directed graphs that reflect dynamic interactions. These graphs are then analyzed using PH, enabling assessment of topological complexity across multiple structural scales and dimensions.
  We apply this TE+PH pipeline to synthetic spiking networks trained on logic gate tasks, image-classification networks exposed to structured and perturbed inputs, and mouse cortical recordings annotated with behavioral events. Across all settings, the resulting topological signatures reveal distinctions in task complexity, stimulus structure, and behavioral regime. Higher-dimensional features become more prominent in complex or noisy conditions, reflecting interaction patterns that extend beyond pairwise connectivity. Our findings offer a principled approach to mapping directed information flow onto global organizational patterns in both artificial and biological neural systems. The framework is generalizable and interpretable, making it well suited for neural systems with time-resolved and binary spiking data.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v1 Announce Type: cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution decoder, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments</title>
<link>https://arxiv.org/abs/2508.19131</link>
<guid>https://arxiv.org/abs/2508.19131</guid>
<content:encoded><![CDATA[
arXiv:2508.19131v1 Announce Type: cross 
Abstract: The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bag of Tricks for Efficient Implicit Neural Point Clouds</title>
<link>https://arxiv.org/abs/2508.19140</link>
<guid>https://arxiv.org/abs/2508.19140</guid>
<content:encoded><![CDATA[
arXiv:2508.19140v1 Announce Type: cross 
Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that combines the expressiveness of neural fields with the efficiency of point-based rendering, achieving state-of-the-art image quality in novel view synthesis. However, as with other high-quality approaches that query neural networks during rendering, the practical usability of INPC is limited by comparatively slow rendering. In this work, we present a collection of optimizations that significantly improve both the training and inference performance of INPC without sacrificing visual fidelity. The most significant modifications are an improved rasterizer implementation, more effective sampling techniques, and the incorporation of pre-training for the convolutional neural network used for hole-filling. Furthermore, we demonstrate that points can be modeled as small Gaussians during inference to further improve quality in extrapolated, e.g., close-up views of the scene. We design our implementations to be broadly applicable beyond INPC and systematically evaluate each modification in a series of experiments. Our optimized INPC pipeline achieves up to 25% faster training, 2x faster rendering, and 20% reduced VRAM usage paired with slight image quality improvements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</title>
<link>https://arxiv.org/abs/2508.19154</link>
<guid>https://arxiv.org/abs/2508.19154</guid>
<content:encoded><![CDATA[
arXiv:2508.19154v1 Announce Type: cross 
Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and realistic generation. As these models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM bypasses this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP) + IR pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts the out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE (RVAE) learning optimal latent representations, (2) a differentiable Post Tone Processing (PTP) module enabling joint RAW and sRGB space optimization. To compensate for the deficiency in the dataset, we develop a scalable degradation pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Furthermore, we devise a configurable multi-bayer (CMB) LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2508.19236</link>
<guid>https://arxiv.org/abs/2508.19236</guid>
<content:encoded><![CDATA[
arXiv:2508.19236v1 Announce Type: cross 
Abstract: Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation</title>
<link>https://arxiv.org/abs/2207.13340</link>
<guid>https://arxiv.org/abs/2207.13340</guid>
<content:encoded><![CDATA[
arXiv:2207.13340v2 Announce Type: replace 
Abstract: Online stereo adaptation tackles the domain shift problem, caused by different environments between synthetic (training) and real (test) datasets, to promptly adapt stereo models in dynamic real-world applications such as autonomous driving. However, previous methods often fail to counteract particular regions related to dynamic objects with more severe environmental changes. To mitigate this issue, we propose to incorporate an auxiliary point-selective network into a meta-learning framework, called PointFix, to provide a robust initialization of stereo models for online stereo adaptation. In a nutshell, our auxiliary network learns to fix local variants intensively by effectively back-propagating local information through the meta-gradient for the robust initialization of the baseline model. This network is model-agnostic, so can be used in any kind of architectures in a plug-and-play manner. We conduct extensive experiments to verify the effectiveness of our method under three adaptation settings such as short-, mid-, and long-term sequences. Experimental results show that the proper initialization of the base stereo model by the auxiliary network enables our learning paradigm to achieve state-of-the-art performance at inference.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffBlender: Composable and Versatile Multimodal Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2305.15194</link>
<guid>https://arxiv.org/abs/2305.15194</guid>
<content:encoded><![CDATA[
arXiv:2305.15194v3 Announce Type: replace 
Abstract: In this study, we aim to enhance the capabilities of diffusion-based text-to-image (T2I) generation models by integrating diverse modalities beyond textual descriptions within a unified framework. To this end, we categorize widely used conditional inputs into three modality types: structure, layout, and attribute. We propose a multimodal T2I diffusion model, which is capable of processing all three modalities within a single architecture without modifying the parameters of the pre-trained diffusion model, as only a small subset of components is updated. Our approach sets new benchmarks in multimodal generation through extensive quantitative and qualitative comparisons with existing conditional generation methods. We demonstrate that DiffBlender effectively integrates multiple sources of information and supports diverse applications in detailed image synthesis. The code and demo are available at https://github.com/sungnyun/diffblender.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory augment is All You Need for image restoration</title>
<link>https://arxiv.org/abs/2309.01377</link>
<guid>https://arxiv.org/abs/2309.01377</guid>
<content:encoded><![CDATA[
arXiv:2309.01377v2 Announce Type: replace 
Abstract: Image restoration is a low-level vision task, most CNN methods are designed as a black box, lacking transparency and internal aesthetics. Although some methods combining traditional optimization algorithms with DNNs have been proposed, they all have some limitations. In this paper, we propose a three-granularity memory layer and contrast learning named MemoryNet, specifically, dividing the samples into positive, negative, and actual three samples for contrastive learning, where the memory layer is able to preserve the deep features of the image and the contrastive learning converges the learned features to balance. Experiments on Derain/Deshadow/Deblur task demonstrate that these methods are effective in improving restoration performance. In addition, this paper's model obtains significant PSNR, SSIM gain on three datasets with different degradation types, which is a strong proof that the recovered images are perceptually realistic. The source code of MemoryNet can be obtained from https://github.com/zhangbaijin/MemoryNet
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal</title>
<link>https://arxiv.org/abs/2309.11715</link>
<guid>https://arxiv.org/abs/2309.11715</guid>
<content:encoded><![CDATA[
arXiv:2309.11715v4 Announce Type: replace 
Abstract: Segment Anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning county from pixels: corn yield prediction with attention-weighted multiple instance learning</title>
<link>https://arxiv.org/abs/2312.01001</link>
<guid>https://arxiv.org/abs/2312.01001</guid>
<content:encoded><![CDATA[
arXiv:2312.01001v4 Announce Type: replace 
Abstract: Remote sensing technology has become a promising tool in yield prediction. Most prior work employs satellite imagery for county-level corn yield prediction by spatially aggregating all pixels within a county into a single value, potentially overlooking the detailed information and valuable insights offered by more granular data. To this end, this research examines each county at the pixel level and applies multiple instance learning to leverage detailed information within a county. In addition, our method addresses the "mixed pixel" issue caused by the inconsistent resolution between feature datasets and crop mask, which may introduce noise into the model and therefore hinder accurate yield prediction. Specifically, the attention mechanism is employed to automatically assign weights to different pixels, which can mitigate the influence of mixed pixels. The experimental results show that the developed model outperforms four other machine learning models over the past five years in the U.S. corn belt and demonstrates its best performance in 2022, achieving a coefficient of determination (R2) value of 0.84 and a root mean square error (RMSE) of 0.83. This paper demonstrates the advantages of our approach from both spatial and temporal perspectives. Furthermore, through an in-depth study of the relationship between mixed pixels and attention, it is verified that our approach can capture critical feature information while filtering out noise from mixed pixels.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment</title>
<link>https://arxiv.org/abs/2312.09625</link>
<guid>https://arxiv.org/abs/2312.09625</guid>
<content:encoded><![CDATA[
arXiv:2312.09625v5 Announce Type: replace 
Abstract: Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Linguistic Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learned Modality-Weighted Knowledge Distillation for Robust Multi-Modal Learning with Missing Data</title>
<link>https://arxiv.org/abs/2405.07155</link>
<guid>https://arxiv.org/abs/2405.07155</guid>
<content:encoded><![CDATA[
arXiv:2405.07155v4 Announce Type: replace 
Abstract: In multi-modal learning, some modalities are more influential than others, and their absence can have a significant impact on classification/segmentation accuracy. Addressing this challenge, we propose a novel approach called Meta-learned Modality-weighted Knowledge Distillation (MetaKD), which enables multi-modal models to maintain high accuracy even when key modalities are missing. MetaKD adaptively estimates the importance weight of each modality through a meta-learning process. These learned importance weights guide a pairwise modality-weighted knowledge distillation process, allowing high-importance modalities to transfer knowledge to lower-importance ones, resulting in robust performance despite missing inputs. Unlike previous methods in the field, which are often task-specific and require significant modifications, our approach is designed to work in multiple tasks (e.g., segmentation and classification) with minimal adaptation. Experimental results on five prevalent datasets, including three Brain Tumor Segmentation datasets (BraTS2018, BraTS2019 and BraTS2020), the Alzheimer's Disease Neuroimaging Initiative (ADNI) classification dataset and the Audiovision-MNIST classification dataset, demonstrate the proposed model is able to outperform the compared models by a large margin. The code is available at https://github.com/billhhh/MetaKD.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images</title>
<link>https://arxiv.org/abs/2407.21604</link>
<guid>https://arxiv.org/abs/2407.21604</guid>
<content:encoded><![CDATA[
arXiv:2407.21604v4 Announce Type: replace 
Abstract: Cancer diagnosis has greatly benefited from the integration of whole-slide images (WSIs) with multiple instance learning (MIL), enabling high-resolution analysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches have emerged as powerful solutions for capturing contextual information in WSIs, thereby improving diagnostic accuracy. However, WSIs require significant computational and infrastructural resources, limiting accessibility in resource-constrained settings. Conventional light microscopes offer a cost-effective alternative, but applying GNN-MIL to such data is challenging due to extensive redundant images and missing spatial coordinates, which hinder contextual learning. To address these issues, we introduce MicroMIL, the first weakly-supervised MIL framework specifically designed for images acquired from conventional light microscopes. MicroMIL leverages a representative image extractor (RIE) that employs deep cluster embedding (DCE) and hard Gumbel-Softmax to dynamically reduce redundancy and select representative images. These images serve as graph nodes, with edges computed via cosine similarity, eliminating the need for spatial coordinates while preserving contextual information. Extensive experiments on a real-world colon cancer dataset and the BreakHis dataset demonstrate that MicroMIL achieves state-of-the-art performance, improving both diagnostic accuracy and robustness to redundancy. The code is available at https://github.com/kimjongwoo-cell/MicroMIL
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUSELOC: Fusing Global and Local Descriptors to Disambiguate 2D-3D Matching in Visual Localization</title>
<link>https://arxiv.org/abs/2408.12037</link>
<guid>https://arxiv.org/abs/2408.12037</guid>
<content:encoded><![CDATA[
arXiv:2408.12037v2 Announce Type: replace 
Abstract: Hierarchical visual localization methods achieve state-of-the-art accuracy but require substantial memory as they need to store all database images. Direct 2D-3D matching requires significantly less memory but suffers from lower accuracy due to the larger and more ambiguous search space. We address this ambiguity by fusing local and global descriptors using a weighted average operator. This operator rearranges the local descriptor space so that geographically nearby local descriptors are closer in the feature space according to the global descriptors. This decreases the number of irrelevant competing descriptors, especially if they are geographically distant, thus increasing the correct matching likelihood. We consistently improve the accuracy over local-only systems, and we achieve performance close to hierarchical methods while using 43\% less memory and running 1.6 times faster. Extensive experiments on four challenging datasets -- Cambridge Landmarks, Aachen Day/Night, RobotCar Seasons, and Extended CMU Seasons -- demonstrate that, for the first time, direct matching algorithms can benefit from global descriptors without compromising computational efficiency. Our code is available at \href{https://github.com/sontung/descriptor-disambiguation}{https://github.com/sontung/descriptor-disambiguation}.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian Radiance Fields</title>
<link>https://arxiv.org/abs/2410.11394</link>
<guid>https://arxiv.org/abs/2410.11394</guid>
<content:encoded><![CDATA[
arXiv:2410.11394v2 Announce Type: replace 
Abstract: Radiance fields represented by 3D Gaussians excel at synthesizing novel views, offering both high training efficiency and fast rendering. However, with sparse input views, the lack of multi-view consistency constraints results in poorly initialized Gaussians and unreliable heuristics for optimization, leading to suboptimal performance. Existing methods often incorporate depth priors from dense estimation networks but overlook the inherent multi-view consistency in input images. Additionally, they rely on dense initialization, which limits the efficiency of scene representation. To overcome these challenges, we propose a view synthesis framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic scene reconstruction from sparse views. The key innovations of MCGS in enhancing multi-view consistency are as follows: i) We leverage matching priors from a sparse matcher to initialize Gaussians primarily on textured regions, while low-texture areas are populated with randomly distributed Gaussians. This yields a compact yet sufficient set of initial Gaussians. ii) We propose a multi-view consistency-guided progressive pruning strategy to dynamically eliminate inconsistent Gaussians. This approach confines their optimization to a consistency-constrained space, which ensures robust and coherent scene reconstruction. These strategies enhance robustness to sparse views, accelerate rendering, and reduce memory consumption, making MCGS a practical framework for 3D Gaussian Splatting.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking XAI Explanations with Human-Aligned Evaluations</title>
<link>https://arxiv.org/abs/2411.02470</link>
<guid>https://arxiv.org/abs/2411.02470</guid>
<content:encoded><![CDATA[
arXiv:2411.02470v2 Announce Type: replace 
Abstract: We introduce PASTA (Perceptual Assessment System for explanaTion of Artificial Intelligence), a novel human-centric framework for evaluating eXplainable AI (XAI) techniques in computer vision. Our first contribution is the creation of the PASTA-dataset, the first large-scale benchmark that spans a diverse set of models and both saliency-based and concept-based explanation methods. This dataset enables robust, comparative analysis of XAI techniques based on human judgment. Our second contribution is an automated, data-driven benchmark that predicts human preferences using the PASTA-dataset. This scoring called PASTA-score method offers scalable, reliable, and consistent evaluation aligned with human perception. Additionally, our benchmark allows for comparisons between explanations across different modalities, an aspect previously unaddressed. We then propose to apply our scoring method to probe the interpretability of existing models and to build more human interpretable XAI methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAGUE: Visual Contexts Clarify Ambiguous Expressions</title>
<link>https://arxiv.org/abs/2411.14137</link>
<guid>https://arxiv.org/abs/2411.14137</guid>
<content:encoded><![CDATA[
arXiv:2411.14137v3 Announce Type: replace 
Abstract: Human communication often relies on visual cues to resolve ambiguity. While humans can intuitively integrate these cues, AI systems often find it challenging to engage in sophisticated multimodal reasoning. We introduce VAGUE, a benchmark evaluating multimodal AI systems' ability to integrate visual context for intent disambiguation. VAGUE consists of 1.6K ambiguous textual expressions, each paired with an image and multiple-choice interpretations, where the correct answer is only apparent with visual context. The dataset spans both staged, complex (Visual Commonsense Reasoning) and natural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal that existing multimodal AI models struggle to infer the speaker's true intent. While performance consistently improves from the introduction of more visual cues, the overall accuracy remains far below human performance, highlighting a critical gap in multimodal reasoning. Analysis of failure cases demonstrates that current models fail to distinguish true intent from superficial correlations in the visual scene, indicating that they perceive images but do not effectively reason with them. We release our code and data at https://hazel-heejeong-nam.github.io/vague/.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration</title>
<link>https://arxiv.org/abs/2411.16044</link>
<guid>https://arxiv.org/abs/2411.16044</guid>
<content:encoded><![CDATA[
arXiv:2411.16044v2 Announce Type: replace 
Abstract: An image, especially with high-resolution, typically consists of numerous visual elements, ranging from dominant large objects to fine-grained detailed objects. When perceiving such images, multimodal large language models~(MLLMs) face limitations due to the restricted input resolution of the pretrained vision encoder and the cluttered, dense context of the image, resulting in a focus on primary objects while easily overlooking detailed ones. In this paper, we propose Zoom Eye, a tree search algorithm designed to navigate the hierarchical and visual nature of images to capture relevant information. Zoom Eye conceptualizes an image as a tree, with each children node representing a zoomed sub-patch of the parent node and the root represents the overall image. Moreover, Zoom Eye is model-agnostic and training-free, so it enables any MLLMs to simulate human zooming actions by searching along the image tree from root to leaf nodes, seeking out pertinent information, and accurately responding to related queries. We experiment on a series of elaborate high-resolution benchmarks and the results demonstrate that Zoom Eye not only consistently improves the performance of a series base MLLMs with large margin~(e.g., LLaVA-v1.5-7B increases by 34.57\% on $V^*$ Bench and 17.88\% on HR-Bench), but also enables small 7B MLLMs to outperform strong large models such as GPT-4o. Our code is available at \href{https://github.com/om-ai-lab/ZoomEye}{https://github.com/om-ai-lab/ZoomEye}.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Vision Constrained Super-Resolution</title>
<link>https://arxiv.org/abs/2411.17513</link>
<guid>https://arxiv.org/abs/2411.17513</guid>
<content:encoded><![CDATA[
arXiv:2411.17513v2 Announce Type: replace 
Abstract: Modern deep-learning super-resolution (SR) techniques process images and videos independently of the underlying content and viewing conditions. However, the sensitivity of the human visual system (HVS) to image details changes depending on the underlying image characteristics, such as spatial frequency, luminance, color, contrast, or motion; as well viewing condition aspects such as ambient lighting and distance to the display. This observation suggests that computational resources spent on up-sampling images/videos may be wasted whenever a viewer cannot resolve the synthesized details i.e the resolution of details exceeds the resolving capability of human vision. Motivated by this observation, we propose a human vision inspired and architecture-agnostic approach for controlling SR techniques to deliver visually optimal results while limiting computational complexity. Its core is an explicit Human Visual Processing Framework (HVPF) that dynamically and locally guides SR methods according to human sensitivity to specific image details and viewing conditions. We demonstrate the application of our framework in combination with network branching to improve the computational efficiency of SR methods. Quantitative and qualitative evaluations, including user studies, demonstrate the effectiveness of our approach in reducing FLOPS by factors of 2$\times$ and greater, without sacrificing perceived quality.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Multi-Scene Modeling via Continual Neural Graphics Primitives</title>
<link>https://arxiv.org/abs/2411.19903</link>
<guid>https://arxiv.org/abs/2411.19903</guid>
<content:encoded><![CDATA[
arXiv:2411.19903v4 Announce Type: replace 
Abstract: Neural radiance fields (NeRF) have revolutionized photorealistic rendering of novel views for 3D scenes. Despite their growing popularity and efficiency as 3D resources, NeRFs face scalability challenges due to the need for separate models per scene and the cumulative increase in training time for multiple scenes. The potential for incrementally encoding multiple 3D scenes into a single NeRF model remains largely unexplored. To address this, we introduce Continual-Neural Graphics Primitives (C-NGP), a novel continual learning framework that integrates multiple scenes incrementally into a single neural radiance field. Using a generative replay approach, C-NGP adapts to new scenes without requiring access to old data. We demonstrate that C-NGP can accommodate multiple scenes without increasing the parameter count, producing high-quality novel-view renderings on synthetic and real datasets. Notably, C-NGP models all $8$ scenes from the Real-LLFF dataset together, with only a $2.2\%$ drop in PSNR compared to vanilla NeRF, which models each scene independently. Further, C-NGP allows multiple style edits in the same network.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes</title>
<link>https://arxiv.org/abs/2412.01240</link>
<guid>https://arxiv.org/abs/2412.01240</guid>
<content:encoded><![CDATA[
arXiv:2412.01240v3 Announce Type: replace 
Abstract: As large-scale foundation models trained on billions of image--mask pairs covering a vast diversity of scenes, objects, and contexts, SAM and its upgraded version, SAM~2, have significantly influenced multiple fields within computer vision. Leveraging such unprecedented data diversity, they exhibit strong open-world segmentation capabilities, with SAM~2 further enhancing these capabilities to support high-quality video segmentation. While SAMs (SAM and SAM~2) have demonstrated excellent performance in segmenting context-independent concepts like people, cars, and roads, they overlook more challenging context-dependent (CD) concepts, such as visual saliency, camouflage, industrial defects, and medical lesions. CD concepts rely heavily on global and local contextual information, making them susceptible to shifts in different contexts, which requires strong discriminative capabilities from the model. The lack of comprehensive evaluation of SAMs limits understanding of their performance boundaries, which may hinder the design of future models. In this paper, we conduct a thorough evaluation of SAMs on 11 CD concepts across 2D and 3D images and videos in various visual modalities within natural, medical, and industrial scenes. We develop a unified evaluation framework for SAM and SAM~2 that supports manual, automatic, and intermediate self-prompting, aided by our specific prompt generation and interaction strategies. We further explore the potential of SAM~2 for in-context learning and introduce prompt robustness testing to simulate real-world imperfect prompts. Finally, we analyze the benefits and limitations of SAMs in understanding CD concepts and discuss their future development in segmentation tasks.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models</title>
<link>https://arxiv.org/abs/2412.13188</link>
<guid>https://arxiv.org/abs/2412.13188</guid>
<content:encoded><![CDATA[
arXiv:2412.13188v3 Announce Type: replace 
Abstract: This paper aims to tackle the problem of photorealistic view synthesis from vehicle sensor data. Recent advancements in neural scene representation have achieved notable success in rendering high-quality autonomous driving scenes, but the performance significantly degrades as the viewpoint deviates from the training trajectory. To mitigate this problem, we introduce StreetCrafter, a novel controllable video diffusion model that utilizes LiDAR point cloud renderings as pixel-level conditions, which fully exploits the generative prior for novel view synthesis, while preserving precise camera control. Moreover, the utilization of pixel-level LiDAR conditions allows us to make accurate pixel-level edits to target scenes. In addition, the generative prior of StreetCrafter can be effectively incorporated into dynamic scene representations to achieve real-time rendering. Experiments on Waymo Open Dataset and PandaSet demonstrate that our model enables flexible control over viewpoint changes, enlarging the view synthesis regions for satisfying rendering, which outperforms existing methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers</title>
<link>https://arxiv.org/abs/2412.13810</link>
<guid>https://arxiv.org/abs/2412.13810</guid>
<content:encoded><![CDATA[
arXiv:2412.13810v3 Announce Type: replace 
Abstract: We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design. Our approach is based on a powerful Vision and Large Language Model (VLLM) as a planner and a tool-augmentation paradigm using CAD-specific tools. CAD-Assistant addresses multimodal user queries by generating actions that are iteratively executed on a Python interpreter equipped with the FreeCAD software, accessed via its Python API. Our framework is able to assess the impact of generated CAD commands on geometry and adapts subsequent actions based on the evolving state of the CAD design. We consider a wide range of CAD-specific tools including a sketch image parameterizer, rendering modules, a 2D cross-section generator, and other specialized routines. CAD-Assistant is evaluated on multiple CAD benchmarks, where it outperforms VLLM baselines and supervised task-specific methods. Beyond existing benchmarks, we qualitatively demonstrate the potential of tool-augmented VLLMs as general-purpose CAD solvers across diverse workflows.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Monocular Metric Depth Estimation</title>
<link>https://arxiv.org/abs/2501.11841</link>
<guid>https://arxiv.org/abs/2501.11841</guid>
<content:encoded><![CDATA[
arXiv:2501.11841v4 Announce Type: replace 
Abstract: Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance</title>
<link>https://arxiv.org/abs/2502.03835</link>
<guid>https://arxiv.org/abs/2502.03835</guid>
<content:encoded><![CDATA[
arXiv:2502.03835v2 Announce Type: replace 
Abstract: Single-domain generalization for object detection (S-DGOD) seeks to transfer learned representations from a single source domain to unseen target domains. While recent approaches have primarily focused on achieving feature invariance, they ignore that domain diversity also presents significant challenges for the task. First, such invariance-driven strategies often lead to the loss of domain-specific information, resulting in incomplete feature representations. Second, cross-domain feature alignment forces the model to overlook domain-specific discrepancies, thereby increasing the complexity of the training process. To address these limitations, this paper proposes the Diversity Invariant Detection Model (DIDM), which achieves a harmonious integration of domain-specific diversity and domain invariance. Our key idea is to learn the invariant representations by keeping the inherent domain-specific features. Specifically, we introduce the Diversity Learning Module (DLM). This module limits the invariant semantics while explicitly enhancing domain-specific feature representation through a proposed feature diversity loss. Furthermore, to ensure cross-domain invariance without sacrificing diversity, we incorporate the Weighted Aligning Module (WAM) to enable feature alignment while maintaining the discriminative domain-specific information. Extensive experiments on multiple diverse datasets demonstrate the effectiveness of the proposed model, achieving superior performance compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGAR: Flexible Promptive Group Activity Recognition</title>
<link>https://arxiv.org/abs/2503.08933</link>
<guid>https://arxiv.org/abs/2503.08933</guid>
<content:encoded><![CDATA[
arXiv:2503.08933v2 Announce Type: replace 
Abstract: We present PromptGAR, a novel framework for Group Activity Recognition (GAR) that offering both input flexibility and high recognition accuracy. The existing approaches suffer from limited real-world applicability due to their reliance on full prompt annotations, fixed number of frames and instances, and the lack of actor consistency. To bridge the gap, we proposed PromptGAR, which is the first GAR model to provide input flexibility across prompts, frames, and instances without the need for retraining. We leverage diverse visual prompts, like bounding boxes, skeletal keypoints, and instance identities, by unifying them as point prompts. A recognition decoder then cross-updates class and prompt tokens for enhanced performance. To ensure actor consistency for extended activity durations, we also introduce a relative instance attention mechanism that directly encodes instance identities. Comprehensive evaluations demonstrate that PromptGAR achieves competitive performances both on full prompts and partial prompt inputs, establishing its effectiveness on input flexibility and generalization ability for real-world applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness</title>
<link>https://arxiv.org/abs/2503.09487</link>
<guid>https://arxiv.org/abs/2503.09487</guid>
<content:encoded><![CDATA[
arXiv:2503.09487v3 Announce Type: replace 
Abstract: While image-text foundation models have succeeded across diverse downstream tasks, they still face challenges in the presence of spurious correlations between the input and label. To address this issue, we propose a simple three-step approach,Project-Probe-Aggregate (PPA), that enables parameter-efficient fine-tuning for foundation models without relying on group annotations. Building upon the failure-based debiasing scheme, our method, PPA, improves its two key components: minority samples identification and the robust training algorithm. Specifically, we first train biased classifiers by projecting image features onto the nullspace of class proxies from text encoders. Next, we infer group labels using the biased classifier and probe group targets with prior correction. Finally, we aggregate group weights of each class to produce the debiased classifier. Our theoretical analysis shows that our PPA enhances minority group identification and is Bayes optimal for minimizing the balanced group error, mitigating spurious correlations. Extensive experimental results confirm the effectiveness of our PPA: it outperforms the state-of-the-art by an average worst-group accuracy while requiring less than 0.01% tunable parameters without training group labels.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Parameter-Efficient Tuning with Token Redundancy Reduction</title>
<link>https://arxiv.org/abs/2503.20282</link>
<guid>https://arxiv.org/abs/2503.20282</guid>
<content:encoded><![CDATA[
arXiv:2503.20282v2 Announce Type: replace 
Abstract: Parameter-efficient tuning (PET) aims to transfer pre-trained foundation models to downstream tasks by learning a small number of parameters. Compared to traditional fine-tuning, which updates the entire model, PET significantly reduces storage and transfer costs for each task regardless of exponentially increasing pre-trained model capacity. However, most PET methods inherit the inference latency of their large backbone models and often introduce additional computational overhead due to additional modules (e.g. adapters), limiting their practicality for compute-intensive applications. In this paper, we propose Faster Parameter-Efficient Tuning (FPET), a novel approach that enhances inference speed and training efficiency while maintaining high storage efficiency. Specifically, we introduce a plug-and-play token redundancy reduction module delicately designed for PET. This module refines tokens from the self-attention layer using an adapter to learn the accurate similarity between tokens and cuts off the tokens through a fully-differentiable token merging strategy, which uses a straight-through estimator for optimal token reduction. Experimental results prove that our FPET achieves faster inference and higher memory efficiency than the pre-trained backbone while keeping competitive performance on par with state-of-the-art PET methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification</title>
<link>https://arxiv.org/abs/2503.23722</link>
<guid>https://arxiv.org/abs/2503.23722</guid>
<content:encoded><![CDATA[
arXiv:2503.23722v2 Announce Type: replace 
Abstract: As an important task in intelligent transportation systems, Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific persons across heterogeneous cameras in different viewpoints. Previous methods typically adopt deep learning-based models, focusing on extracting view-invariant features. However, they usually overlook the semantic information in person attributes. In addition, existing training strategies often rely on full fine-tuning large-scale models, which significantly increases training costs. To address these issues, we propose a novel framework named LATex for AG-ReID, which adopts prompt-tuning strategies to leverage attribute-based text knowledge. More specifically, we first introduce the Contrastive Language-Image Pre-training (CLIP) model as the backbone, and propose an Attribute-aware Image Encoder (AIE) to extract both global semantic features and attribute-aware features from input images. Then, with these features, we propose a Prompted Attribute Classifier Group (PACG) to predict person attributes and obtain attribute representations. Finally, we design a Coupled Prompt Template (CPT) to transform attribute representations and view information into structured sentences. These sentences are processed by the text encoder of CLIP to generate more discriminative features. As a result, our framework can fully leverage attribute-based text knowledge to improve AG-ReID performance. Extensive experiments on three AG-ReID benchmarks demonstrate the effectiveness of our proposed methods. The source code will be available.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</title>
<link>https://arxiv.org/abs/2504.04633</link>
<guid>https://arxiv.org/abs/2504.04633</guid>
<content:encoded><![CDATA[
arXiv:2504.04633v3 Announce Type: replace 
Abstract: Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\% with substantial improvements in overall efficiency.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Disease Detection from Retinal Fundus Images</title>
<link>https://arxiv.org/abs/2504.08481</link>
<guid>https://arxiv.org/abs/2504.08481</guid>
<content:encoded><![CDATA[
arXiv:2504.08481v2 Announce Type: replace 
Abstract: In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for retinal disease detection. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the mode's decision process. We evaluated our method on two medical tasks focused on disease detection using color fundus images. Our model achieves state-of-the-art predictive performance compared to black-box and interpretable models and provides class-specific sparse evidence maps in a single forward pass. The code is available at: https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForgetMe: Evaluating Selective Forgetting in Generative Models</title>
<link>https://arxiv.org/abs/2504.12574</link>
<guid>https://arxiv.org/abs/2504.12574</guid>
<content:encoded><![CDATA[
arXiv:2504.12574v3 Announce Type: replace 
Abstract: The widespread adoption of diffusion models in image generation has increased the demand for privacy-compliant unlearning. However, due to the high-dimensional nature and complex feature representations of diffusion models, achieving selective unlearning remains challenging, as existing methods struggle to remove sensitive information while preserving the consistency of non-sensitive regions. To address this, we propose an Automatic Dataset Creation Framework based on prompt-based layered editing and training-free local feature removal, constructing the ForgetMe dataset and introducing the Entangled evaluation metric. The Entangled metric quantifies unlearning effectiveness by assessing the similarity and consistency between the target and background regions and supports both paired (Entangled-D) and unpaired (Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe dataset encompasses a diverse set of real and synthetic scenarios, including CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on this dataset and validate the effectiveness of both the ForgetMe dataset and the Entangled metric, establishing them as benchmarks for selective unlearning. Our work provides a scalable and adaptable solution for advancing privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WMKA-Net: A Weighted Multi-Kernel Attention Network for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2504.14888</link>
<guid>https://arxiv.org/abs/2504.14888</guid>
<content:encoded><![CDATA[
arXiv:2504.14888v2 Announce Type: replace 
Abstract: Retinal vessel segmentation is crucial for intelligent ophthalmic diagnosis, yet it faces three major challenges: insufficient multi-scale feature fusion, disruption of contextual continuity, and noise interference. This study proposes a dual-stage solution to address these issues. The first stage employs a Reversible Multi-Scale Fusion Module (RMS) that uses hierarchical adaptive convolution to dynamically merge cross-scale features from capillaries to main vessels, self-adaptively calibrating feature biases. The second stage introduces a Vascular-Oriented Attention Mechanism, which models long-distance vascular continuity through an axial pathway and enhances the capture of topological key nodes, such as bifurcation points, via a dedicated bifurcation attention pathway. The synergistic operation of these two pathways effectively restores the continuity of vascular structures and improves the segmentation accuracy of complex vascular networks. Systematic experiments on the DRIVE, STARE, and CHASE-DB1 datasets demonstrate that WMKA-Net achieves an accuracy of 0.9909, sensitivity of 0.9198, and specificity of 0.9953, significantly outperforming existing methods. This model provides an efficient, precise, and robust intelligent solution for the early screening of diabetic retinopathy.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Global-Local Alignment for Improving Compositional Understanding</title>
<link>https://arxiv.org/abs/2504.16801</link>
<guid>https://arxiv.org/abs/2504.16801</guid>
<content:encoded><![CDATA[
arXiv:2504.16801v3 Announce Type: replace 
Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition</title>
<link>https://arxiv.org/abs/2504.17163</link>
<guid>https://arxiv.org/abs/2504.17163</guid>
<content:encoded><![CDATA[
arXiv:2504.17163v2 Announce Type: replace 
Abstract: Electroencephalography (EEG) signals provide a promising and involuntary reflection of brain activity related to emotional states, offering significant advantages over behavioral cues like facial expressions. However, EEG signals are often noisy, affected by artifacts, and vary across individuals, complicating emotion recognition. While multimodal approaches have used Peripheral Physiological Signals (PPS) like GSR to complement EEG, they often overlook the dynamic synchronization and consistent semantics between the modalities. Additionally, the temporal dynamics of emotional fluctuations across different time resolutions in PPS remain underexplored. To address these challenges, we propose PhysioSync, a novel pre-training framework leveraging temporal and cross-modal contrastive learning, inspired by physiological synchronization phenomena. PhysioSync incorporates Cross-Modal Consistency Alignment (CM-CA) to model dynamic relationships between EEG and complementary PPS, enabling emotion-related synchronizations across modalities. Besides, it introduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to capture emotional synchronization at different temporal resolutions within modalities. After pre-training, cross-resolution and cross-modal features are hierarchically fused and fine-tuned to enhance emotion recognition. Experiments on DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance under uni-modal and cross-modal conditions, highlighting its effectiveness for EEG-centered emotion recognition.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video CLIP Model for Multi-View Echocardiography Interpretation</title>
<link>https://arxiv.org/abs/2504.18800</link>
<guid>https://arxiv.org/abs/2504.18800</guid>
<content:encoded><![CDATA[
arXiv:2504.18800v2 Announce Type: replace 
Abstract: Echocardiography records ultrasound videos of the heart, enabling clinicians to assess cardiac function. Recent advances in large-scale vision-language models (VLMs) have spurred interest in automating echocardiographic interpretation. However, most existing medical VLMs rely on single-frame (image) inputs, which can reduce diagnostic accuracy for conditions identifiable only through cardiac motion. In addition, echocardiographic videos are captured from multiple views, each varying in suitability for detecting specific conditions. Leveraging multiple views may therefore improve diagnostic performance. We developed a video-language model that processes full video sequences from five standard views, trained on 60,747 echocardiographic video-report pairs. We evaluated the gains in retrieval performance from video input and multi-view support, including the contributions of various pretrained models.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
<link>https://arxiv.org/abs/2505.04529</link>
<guid>https://arxiv.org/abs/2505.04529</guid>
<content:encoded><![CDATA[
arXiv:2505.04529v3 Announce Type: replace 
Abstract: Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real "SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.04594</link>
<guid>https://arxiv.org/abs/2505.04594</guid>
<content:encoded><![CDATA[
arXiv:2505.04594v5 Announce Type: replace 
Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion</title>
<link>https://arxiv.org/abs/2505.17367</link>
<guid>https://arxiv.org/abs/2505.17367</guid>
<content:encoded><![CDATA[
arXiv:2505.17367v4 Announce Type: replace 
Abstract: Medical image classification is critical for clinical decision-making, yet demands for accuracy, interpretability, and generalizability remain challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for multi-organ medical image classification. EVM-Fusion leverages a multipath design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim) modules, operate in parallel with a traditional feature pathway. These diverse features are dynamically integrated via a two-stage fusion process: cross-modal attention followed by the iterative NAF block, which learns an adaptive fusion algorithm. Intrinsic explainability is embedded through path-specific spatial attention, Vim {\Delta}-value maps, traditional feature SE-attention, and cross-modal attention weights. Experiments on a diverse 9-class multi-organ medical image dataset demonstrate EVM-Fusion's strong classification performance, achieving 99.75% test accuracy and provide multi-faceted insights into its decision-making process, highlighting its potential for trustworthy AI in medical diagnostics.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Data Augmentation for Object Point Cloud Segmentation</title>
<link>https://arxiv.org/abs/2505.17783</link>
<guid>https://arxiv.org/abs/2505.17783</guid>
<content:encoded><![CDATA[
arXiv:2505.17783v2 Announce Type: replace 
Abstract: Data augmentation is widely used to train deep learning models to address data scarcity. However, traditional data augmentation (TDA) typically relies on simple geometric transformation, such as random rotation and rescaling, resulting in minimal data diversity enrichment and limited model performance improvement. State-of-the-art generative models for 3D shape generation rely on the denoising diffusion probabilistic models and manage to generate realistic novel point clouds for 3D content creation and manipulation. Nevertheless, the generated 3D shapes lack associated point-wise semantic labels, restricting their usage in enlarging the training data for point cloud segmentation tasks. To bridge the gap between data augmentation techniques and the advanced diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a part-aware generative model that can generate high-quality point clouds conditioned on given segmentation masks. Leveraging the novel generative model, we introduce a 3-step generative data augmentation (GDA) pipeline for point cloud segmentation training. Our GDA approach requires only a small amount of labeled samples but enriches the training data with generated variants and pseudo-labeled samples, which are validated by a novel diffusion-based pseudo-label filtering method. Extensive experiments on two large-scale synthetic datasets and a real-world medical dataset demonstrate that our GDA method outperforms TDA approach and related semi-supervised and self-supervised methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric Human-Object Interaction Detection: A New Benchmark and Method</title>
<link>https://arxiv.org/abs/2506.14189</link>
<guid>https://arxiv.org/abs/2506.14189</guid>
<content:encoded><![CDATA[
arXiv:2506.14189v2 Announce Type: replace 
Abstract: Egocentric human-object interaction (Ego-HOI) detection is crucial for intelligent agents to understand and assist human activities from a first-person perspective. However, progress has been hindered by the lack of benchmarks and methods tailored to egocentric challenges such as severe hand-object occlusion. In this paper, we introduce the real-world Ego-HOI detection task and the accompanying Ego-HOIBench, a new dataset with over 27K egocentric images and explicit, fine-grained hand-verb-object triplet annotations across 123 categories. Ego-HOIBench covers diverse daily scenarios, object types, and both single- and two-hand interactions, offering a comprehensive testbed for Ego-HOI research. Benchmarking existing third-person HOI detectors on Ego-HOIBench reveals significant performance gaps, highlighting the need for egocentric-specific solutions. To this end, we propose Hand Geometry and Interactivity Refinement (HGIR), a lightweight, plug-and-play scheme that leverages hand pose and geometric cues to enhance interaction representations. Specifically, HGIR explicitly extracts global hand geometric features from the estimated hand pose proposals, and further refines interaction features through pose-interaction attention, enabling the model to focus on subtle hand-object relationship differences even under severe occlusion. HGIR significantly improves Ego-HOI detection performance across multiple baselines, achieving new state-of-the-art results on Ego-HOIBench. Our dataset and method establish a solid foundation for future research in egocentric vision and human-object interaction understanding. Project page: https://dengkunyuan.github.io/EgoHOIBench/
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical Images</title>
<link>https://arxiv.org/abs/2506.16369</link>
<guid>https://arxiv.org/abs/2506.16369</guid>
<content:encoded><![CDATA[
arXiv:2506.16369v2 Announce Type: replace 
Abstract: The high computational demands of Vision Transformers (ViTs) in processing a large number of tokens often constrain their practical application in analyzing medical images. This research proposes a Prompt-driven Adaptive Token ({\it PrATo}) pruning method to selectively reduce the processing of irrelevant tokens in the segmentation pipeline. The prompt-based spatial prior helps to rank the tokens according to their relevance. Tokens with low-relevance scores are down-weighted, ensuring that only the relevant ones are propagated for processing across subsequent stages. This data-driven pruning strategy improves segmentation accuracy and inference speed by allocating computational resources to essential regions. The proposed framework is integrated with several state-of-the-art models to facilitate the elimination of irrelevant tokens, thereby enhancing computational efficiency while preserving segmentation accuracy. The experimental results show a reduction of $\sim$ 35-55% tokens; thus reducing the computational costs relative to baselines. Cost-effective medical image processing, using our framework, facilitates real-time diagnosis by expanding its applicability in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding</title>
<link>https://arxiv.org/abs/2506.18372</link>
<guid>https://arxiv.org/abs/2506.18372</guid>
<content:encoded><![CDATA[
arXiv:2506.18372v2 Announce Type: replace 
Abstract: We introduce OpenEvents V1a large-scale benchmark dataset designed to advance event-centric vision-language understanding. Unlike conventional image captioning and retrieval datasets that focus on surface-level descriptions, OpenEvents V1 dataset emphasizes contextual and temporal grounding through three primary tasks: (1) generating rich, event-aware image captions, (2) retrieving event-relevant news articles from image queries, and (3) retrieving event-relevant images from narrative-style textual queries. The dataset comprises over 200,000 news articles and 400,000 associated images sourced from CNN and The Guardian, spanning diverse domains and time periods. We provide extensive baseline results and standardized evaluation protocols for all tasks. OpenEvents V1 establishes a robust foundation for developing multimodal AI systems capable of deep reasoning over complex real-world events. The dataset is publicly available at https://ltnghia.github.io/eventa/openevents-v1.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach</title>
<link>https://arxiv.org/abs/2507.02826</link>
<guid>https://arxiv.org/abs/2507.02826</guid>
<content:encoded><![CDATA[
arXiv:2507.02826v3 Announce Type: replace 
Abstract: Sensor-based Human Activity Recognition (HAR) is a core technology that enables intelligent systems to perceive and interact with their environment. However, multimodal HAR systems still encounter key challenges, such as difficulties in cross-modal feature alignment and imbalanced modality contributions. To address these issues, we propose a novel framework called the Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three key components. First, a dual-path feature extraction architecture is employed, where ResNet and DenseNet branches collaboratively process multimodal sensor data. Second, a multi-stage contrastive learning mechanism is introduced to achieve progressive alignment from local perception to semantic abstraction. Third, we present a confidence-driven gradient modulation strategy that dynamically monitors and adjusts the learning intensity of each modality branch during backpropagation, effectively alleviating modality competition. In addition, a momentum-based gradient accumulation strategy is adopted to enhance training stability. We conduct ablation studies to validate the effectiveness of each component and perform extensive comparative experiments on four public benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge</title>
<link>https://arxiv.org/abs/2507.04447</link>
<guid>https://arxiv.org/abs/2507.04447</guid>
<content:encoded><![CDATA[
arXiv:2507.04447v3 Announce Type: replace 
Abstract: Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Altitude Guided Scene Illumination</title>
<link>https://arxiv.org/abs/2507.05812</link>
<guid>https://arxiv.org/abs/2507.05812</guid>
<content:encoded><![CDATA[
arXiv:2507.05812v2 Announce Type: replace 
Abstract: The development of safe and robust autonomous driving functions is heavily dependent on large-scale, high-quality sensor data. However, real-world data acquisition requires extensive human labor and is strongly limited by factors such as labeling cost, driver safety protocols and scenario coverage. Thus, multiple lines of work focus on the conditional generation of synthetic camera sensor data. We identify a significant gap in research regarding daytime variation, presumably caused by the scarcity of available labels. Consequently, we present solar altitude as global conditioning variable. It is readily computable from latitude-longitude coordinates and local time, eliminating the need for manual labeling. Our work is complemented by a tailored normalization approach, targeting the sensitivity of daylight towards small numeric changes in altitude. We demonstrate its ability to accurately capture lighting characteristics and illumination-dependent image noise in the context of diffusion models.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention</title>
<link>https://arxiv.org/abs/2507.09512</link>
<guid>https://arxiv.org/abs/2507.09512</guid>
<content:encoded><![CDATA[
arXiv:2507.09512v2 Announce Type: replace 
Abstract: In this paper, we introduce the latest solution developed by our team, HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA Challenge. The Micro-gesture Online Recognition task is a highly challenging problem that aims to locate the temporal positions and recognize the categories of multiple micro-gesture instances in untrimmed videos. Compared to traditional temporal action detection, this task places greater emphasis on distinguishing between micro-gesture categories and precisely identifying the start and end times of each instance. Moreover, micro-gestures are typically spontaneous human actions, with greater differences than those found in other human actions. To address these challenges, we propose hand-crafted data augmentation and spatial-temporal attention to enhance the model's ability to classify and localize micro-gestures more accurately. Our solution achieved an F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a result, our method ranked first in the Micro-gesture Online Recognition track.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic-aware fine-grained classification of pediatric wrist fractures</title>
<link>https://arxiv.org/abs/2507.12964</link>
<guid>https://arxiv.org/abs/2507.12964</guid>
<content:encoded><![CDATA[
arXiv:2507.12964v3 Announce Type: replace 
Abstract: Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. In this study, we employ a multifaceted approach to address the challenge of recognizing wrist pathologies using an extremely limited dataset. Initially, we approach the problem as a fine-grained recognition task. Secondly, we enhance network performance by fusing patient metadata with X-rays. Thirdly, we improve the performance further by utilizing weights trained on a separate fine-grained dataset. While metadata integration has been used in other medical domains, this is a novel application for wrist pathologies.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13152</link>
<guid>https://arxiv.org/abs/2507.13152</guid>
<content:encoded><![CDATA[
arXiv:2507.13152v3 Announce Type: replace 
Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes</title>
<link>https://arxiv.org/abs/2507.19912</link>
<guid>https://arxiv.org/abs/2507.19912</guid>
<content:encoded><![CDATA[
arXiv:2507.19912v4 Announce Type: replace 
Abstract: We introduce DriveIndia, a large-scale object detection dataset purpose-built to capture the complexity and unpredictability of Indian traffic environments. The dataset contains 66,986 high-resolution images annotated in YOLO format across 24 traffic-relevant object categories, encompassing diverse conditions such as varied weather (fog, rain), illumination changes, heterogeneous road infrastructure, and dense, mixed traffic patterns and collected over 120+ hours and covering 3,400+ kilometers across urban, rural, and highway routes. DriveIndia offers a comprehensive benchmark for real-world autonomous driving challenges. We provide baseline results using state-of-the-art YOLO family models, with the top-performing variant achieving a mAP50 of 78.7%. Designed to support research in robust, generalizable object detection under uncertain road conditions, DriveIndia will be publicly available via the TiHAN-IIT Hyderabad dataset repository https://tihan.iith.ac.in/TiAND.html (Terrestrial Datasets -> Camera Dataset).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model</title>
<link>https://arxiv.org/abs/2507.22675</link>
<guid>https://arxiv.org/abs/2507.22675</guid>
<content:encoded><![CDATA[
arXiv:2507.22675v2 Announce Type: replace 
Abstract: Recently, large foundation models trained on vast datasets have demonstrated exceptional capabilities in feature extraction and general feature representation. The ongoing advancements in deep learning-driven large models have shown great promise in accelerating unsupervised change detection methods, thereby enhancing the practical applicability of change detection technologies. Building on this progress, this paper introduces MergeSAM, an innovative unsupervised change detection method for high-resolution remote sensing imagery, based on the Segment Anything Model (SAM). Two novel strategies, MaskMatching and MaskSplitting, are designed to address real-world complexities such as object splitting, merging, and other intricate changes. The proposed method fully leverages SAM's object segmentation capabilities to construct multitemporal masks that capture complex changes, embedding the spatial structure of land cover into the change detection process.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization</title>
<link>https://arxiv.org/abs/2507.23569</link>
<guid>https://arxiv.org/abs/2507.23569</guid>
<content:encoded><![CDATA[
arXiv:2507.23569v2 Announce Type: replace 
Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Uncertainty Quantification a Viable Alternative to Learned Deferral?</title>
<link>https://arxiv.org/abs/2508.02319</link>
<guid>https://arxiv.org/abs/2508.02319</guid>
<content:encoded><![CDATA[
arXiv:2508.02319v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) holds the potential to dramatically improve patient care. However, it is not infallible, necessitating human-AI-collaboration to ensure safe implementation. One aspect of AI safety is the models' ability to defer decisions to a human expert when they are likely to misclassify autonomously. Recent research has focused on methods that learn to defer by optimising a surrogate loss function that finds the optimal trade-off between predicting a class label or deferring. However, during clinical translation, models often face challenges such as data shift. Uncertainty quantification methods aim to estimate a model's confidence in its predictions. However, they may also be used as a deferral strategy which does not rely on learning from specific training distribution. We hypothesise that models developed to quantify uncertainty are more robust to out-of-distribution (OOD) input than learned deferral models that have been trained in a supervised fashion. To investigate this hypothesis, we constructed an extensive evaluation study on a large ophthalmology dataset, examining both learned deferral models and established uncertainty quantification methods, assessing their performance in- and out-of-distribution. Specifically, we evaluate their ability to accurately classify glaucoma from fundus images while deferring cases with a high likelihood of error. We find that uncertainty quantification methods may be a promising choice for AI deferral.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation</title>
<link>https://arxiv.org/abs/2508.03055</link>
<guid>https://arxiv.org/abs/2508.03055</guid>
<content:encoded><![CDATA[
arXiv:2508.03055v2 Announce Type: replace 
Abstract: Face filters have become a key element of short-form video content, enabling a wide array of visual effects such as stylization and face swapping. However, their performance often degrades in the presence of occlusions, where objects like hands, hair, or accessories obscure the face. To address this limitation, we introduce the novel task of face matting, which estimates fine-grained alpha mattes to separate occluding elements from facial regions. We further present FaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality alpha mattes under complex occlusions. Our approach leverages a two-stage training pipeline: a teacher model is trained to jointly estimate alpha mattes and per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this uncertainty is then used to guide the student model through spatially adaptive knowledge distillation. This formulation enables the student to focus on ambiguous or occluded regions, improving generalization and preserving semantic consistency. Unlike previous approaches that rely on trimaps or segmentation masks, our framework requires no auxiliary inputs making it well-suited for real-time applications. In addition, we reformulate the matting objective by explicitly treating skin as foreground and occlusions as background, enabling clearer compositing strategies. To support this task, we newly constructed CelebAMat, a large-scale synthetic dataset specifically designed for occlusion-aware face matting. Extensive experiments show that FaceMat outperforms state-of-the-art methods across multiple benchmarks, enhancing the visual quality and robustness of face filters in real-world, unconstrained video scenarios. The source code and CelebAMat dataset are available at https://github.com/hyebin-c/FaceMat.git
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title>
<link>https://arxiv.org/abs/2508.03337</link>
<guid>https://arxiv.org/abs/2508.03337</guid>
<content:encoded><![CDATA[
arXiv:2508.03337v4 Announce Type: replace 
Abstract: The practical application of Multimodal Large Language Models (MLLMs) to Video Question Answering (Video-QA) is severely hindered by the high token cost of processing numerous video frames. While increasing the number of sampled frames is a common strategy, we observe a "less is more" phenomenon where excessive frames can paradoxically degrade performance due to context dilution. Concurrently, state-of-the-art keyframe selection methods, while effective, still yield significant temporal redundancy, which we term 'visual echoes'. To address these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel post-processing method that intelligently prunes the selected keyframes. AFP employs an adaptive hierarchical clustering algorithm on a fused ResNet-50 and CLIP feature space to identify and merge these echoes into single representatives. To compensate for information loss, we then introduce a lightweight, text-based semantic graph that provides critical context with minimal token overhead. Conducting extensive experiments on the LongVideoBench and VideoMME benchmarks across multiple leading MLLMs, our full approach demonstrates a drastic reduction in required frames by up to 86.9% and total input tokens by up to 83.2%. Crucially, by providing a concise, high-quality set of frames, our method not only enhances efficiency but often improves accuracy over baselines that use more frames. The code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision</title>
<link>https://arxiv.org/abs/2508.05990</link>
<guid>https://arxiv.org/abs/2508.05990</guid>
<content:encoded><![CDATA[
arXiv:2508.05990v2 Announce Type: replace 
Abstract: The efficiency of video computer vision system remains a challenging task due to the high temporal redundancy inside a video. Existing works have been proposed for efficient vision computer vision. However, they do not fully reduce the temporal redundancy and neglect the front end computation overhead. In this paper, we propose an efficient video computer vision system. First, image signal processor is removed and Bayer-format data is directly fed into video computer vision models, thus saving the front end computation. Second, instead of optical flow models and video codecs, a fast block matching-based motion estimation algorithm is proposed specifically for efficient video computer vision, with a MV refinement module. To correct the error, context-aware block refinement network is introduced to refine regions with large error. To further balance the accuracy and efficiency, a frame selection strategy is employed. Experiments on multiple video computer vision tasks demonstrate that our method achieves significant acceleration with slight performance loss.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection</title>
<link>https://arxiv.org/abs/2508.11167</link>
<guid>https://arxiv.org/abs/2508.11167</guid>
<content:encoded><![CDATA[
arXiv:2508.11167v2 Announce Type: replace 
Abstract: Unsupervised domain adaptation methods have been widely explored to bridge domain gaps. However, in real-world remote-sensing scenarios, privacy and transmission constraints often preclude access to source domain data, which limits their practical applicability. Recently, Source-Free Object Detection (SFOD) has emerged as a promising alternative, aiming at cross-domain adaptation without relying on source data, primarily through a self-training paradigm. Despite its potential, SFOD frequently suffers from training collapse caused by noisy pseudo-labels, especially in remote sensing imagery with dense objects and complex backgrounds. Considering that limited target domain annotations are often feasible in practice, we propose a Vision foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised framework for SFOD in remote sensing images. VG-DETR integrates a Vision Foundation Model (VFM) into the training pipeline in a "free lunch" manner, leveraging a small amount of labeled target data to mitigate pseudo-label noise while improving the detector's feature-extraction capability. Specifically, we introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's semantic priors to further assess the reliability of the generated pseudo-labels. By recovering potentially correct predictions from low-confidence outputs, our strategy improves pseudo-label quality and quantity. In addition, a dual-level VFM-guided alignment method is proposed, which aligns detector features with VFM embeddings at both the instance and image levels. Through contrastive learning among fine-grained prototypes and similarity matching between feature maps, this dual-level alignment further enhances the robustness of feature representations against domain gaps. Extensive experiments demonstrate that VG-DETR achieves superior performance in source-free remote sensing detection tasks.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</title>
<link>https://arxiv.org/abs/2508.11433</link>
<guid>https://arxiv.org/abs/2508.11433</guid>
<content:encoded><![CDATA[
arXiv:2508.11433v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.12720</link>
<guid>https://arxiv.org/abs/2508.12720</guid>
<content:encoded><![CDATA[
arXiv:2508.12720v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</title>
<link>https://arxiv.org/abs/2508.12777</link>
<guid>https://arxiv.org/abs/2508.12777</guid>
<content:encoded><![CDATA[
arXiv:2508.12777v2 Announce Type: replace 
Abstract: As a key research direction in the field of multi-object tracking (MOT), UAV-based multi-object tracking has significant application value in the analysis and understanding of urban intelligent transportation systems. However, in complex UAV perspectives, challenges such as small target scale variations, occlusions, nonlinear crossing motions, and motion blur severely hinder the stability of multi-object tracking. To address these challenges, this paper proposes a novel multi-object tracking framework, SocialTrack, aimed at enhancing the tracking accuracy and robustness of small targets in complex urban traffic environments. The specialized small-target detector enhances the detection performance by employing a multi-scale feature enhancement mechanism. The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of trajectory prediction by incorporating a velocity dynamic modeling mechanism. The Group Motion Compensation Strategy (GMCS) models social group motion priors to provide stable state update references for low-quality tracks, significantly improving the target association accuracy in complex dynamic environments. Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical trajectory information to predict the future state of low-quality tracks, effectively mitigating identity switching issues. Extensive experiments on the UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing state-of-the-art (SOTA) methods across several key metrics. Significant improvements in MOTA and IDF1, among other core performance indicators, highlight its superior robustness and adaptability. Additionally, SocialTrack is highly modular and compatible, allowing for seamless integration with existing trackers to further enhance performance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation</title>
<link>https://arxiv.org/abs/2401.11856</link>
<guid>https://arxiv.org/abs/2401.11856</guid>
<content:encoded><![CDATA[
arXiv:2401.11856v3 Announce Type: replace-cross 
Abstract: Medical image segmentation takes an important position in various clinical applications. 2.5D-based segmentation models bridge the computational efficiency of 2D-based models with the spatial perception capabilities of 3D-based models. However, existing 2.5D-based models primarily adopt a single encoder to extract features of target and neighborhood slices, failing to effectively fuse inter-slice information, resulting in suboptimal segmentation performance. In this study, a novel momentum encoder-based inter-slice fusion transformer (MOSformer) is proposed to overcome this issue by leveraging inter-slice information from multi-scale feature maps extracted by different encoders. Specifically, dual encoders are employed to enhance feature distinguishability among different slices. One of the encoders is moving-averaged to maintain consistent slice representations. Moreover, an inter-slice fusion transformer (IF-Trans) module is developed to fuse inter-slice multi-scale features. MOSformer is evaluated on three benchmark datasets (Synapse, ACDC, and AMOS), achieving a new state-of-the-art with 85.63%, 92.19%, and 85.43% DSC, respectively. These results demonstrate MOSformer's competitiveness in medical image segmentation.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeFlow: Temporal Conditioning for Longitudinal Brain MRI Registration and Aging Analysis</title>
<link>https://arxiv.org/abs/2501.08667</link>
<guid>https://arxiv.org/abs/2501.08667</guid>
<content:encoded><![CDATA[
arXiv:2501.08667v3 Announce Type: replace-cross 
Abstract: Longitudinal brain analysis is essential for understanding healthy aging and identifying pathological deviations. Longitudinal registration of sequential brain MRI underpins such analyses. However, existing methods are limited by reliance on densely sampled time series, a trade-off between accuracy and temporal smoothness, and an inability to prospectively forecast future brain states. To overcome these challenges, we introduce \emph{TimeFlow}, a learning-based framework for longitudinal brain MRI registration. TimeFlow uses a U-Net backbone with temporal conditioning to model neuroanatomy as a continuous function of age. Given only two scans from an individual, TimeFlow estimates accurate and temporally coherent deformation fields, enabling non-linear extrapolation to predict future brain states. This is achieved by our proposed inter-/extra-polation consistency constraints applied to both the deformation fields and deformed images. Remarkably, these constraints preserve temporal consistency and continuity without requiring explicit smoothness regularizers or densely sampled sequential data. Extensive experiments demonstrate that TimeFlow outperforms state-of-the-art methods in terms of both future timepoint forecasting and registration accuracy. Moreover, TimeFlow supports novel biological brain aging analyses by differentiating neurodegenerative trajectories from normal aging without requiring segmentation, thereby eliminating the need for labor-intensive annotations and mitigating segmentation inconsistency. TimeFlow offers an accurate, data-efficient, and annotation-free framework for longitudinal analysis of brain aging and chronic diseases, capable of forecasting brain changes beyond the observed study period.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze Information and Motion Bottlenecks</title>
<link>https://arxiv.org/abs/2502.18121</link>
<guid>https://arxiv.org/abs/2502.18121</guid>
<content:encoded><![CDATA[
arXiv:2502.18121v3 Announce Type: replace-cross 
Abstract: Autonomous agents capable of diverse object manipulations should be able to acquire a wide range of manipulation skills with high reusability. Although advances in deep learning have made it increasingly feasible to replicate the dexterity of human teleoperation in robots, generalizing these acquired skills to previously unseen scenarios remains a significant challenge. In this study, we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation (GazeBot), which enables high reusability of learned motions without sacrificing dexterity or reactivity. By leveraging gaze information and motion bottlenecks, both crucial features for object manipulation, GazeBot achieves high success rates compared with state-of-the-art imitation learning methods, particularly when the object positions and end-effector poses differ from those in the provided demonstrations. Furthermore, the training process of GazeBot is entirely data-driven once a demonstration dataset with gaze data is provided. Videos and code are available at https://crumbyrobotics.github.io/gazebot.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization</title>
<link>https://arxiv.org/abs/2504.02216</link>
<guid>https://arxiv.org/abs/2504.02216</guid>
<content:encoded><![CDATA[
arXiv:2504.02216v2 Announce Type: replace-cross 
Abstract: Many images and videos are primarily processed by computer vision algorithms, involving only occasional human inspection. When this content requires compression before processing, e.g., in distributed applications, coding methods must optimize for both visual quality and downstream task performance. We first show theoretically that an approach to reduce the effect of compression for a given task loss is to perform rate-distortion optimization (RDO) using the distance between features, obtained from the original and the decoded images, as a distortion metric. However, optimizing directly such a rate-distortion objective is computationally impractical because it requires iteratively encoding and decoding the entire image-plus feature evaluation-for each possible coding configuration. We address this problem by simplifying the RDO formulation to make the distortion term computable using block-based encoders. We first apply Taylor's expansion to the feature extractor, recasting the feature distance as a quadratic metric involving the Jacobian matrix of the neural network. Then, we replace the linearized metric with a block-wise approximation, which we call input-dependent squared error (IDSE). To make the metric computable, we approximate IDSE using sketches of the Jacobian. The resulting loss can be evaluated block-wise in the transform domain and combined with the sum of squared errors (SSE) to address both visual quality and computer vision performance. Simulations with AVC and HEVC across multiple feature extractors and downstream networks show up to 17 % bit-rate savings for the same task accuracy compared to RDO based on SSE, with no decoder complexity overhead and a small (7.86 %) encoder complexity increase.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-AIMS: AI-Powered Microscopy Image Analysis</title>
<link>https://arxiv.org/abs/2505.06918</link>
<guid>https://arxiv.org/abs/2505.06918</guid>
<content:encoded><![CDATA[
arXiv:2505.06918v2 Announce Type: replace-cross 
Abstract: This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The model effectively identifies and separates thousands of closely situated targets, even in cluttered visual environments. Furthermore, our solution supports the precise automatic recognition of image scale bars, an essential feature in quantitative microscopic analysis. Building upon these components, we have constructed a comprehensive intelligent analysis platform and validated its effectiveness and practicality in real-world applications. This study not only advances automatic recognition in microscopy imaging but also ensures scalability and generalizability across multiple application domains, offering a powerful tool for automated microscopic analysis in interdisciplinary research. A online application is made available for researchers to access and evaluate the proposed automated analysis service.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Guided Interpretability via Neural Chunking</title>
<link>https://arxiv.org/abs/2505.11576</link>
<guid>https://arxiv.org/abs/2505.11576</guid>
<content:encoded><![CDATA[
arXiv:2505.11576v2 Announce Type: replace-cross 
Abstract: Neural networks are often described as black boxes, reflecting the significant challenge of understanding their internal workings and interactions. We propose a different perspective that challenges the prevailing view: rather than being inscrutable, neural networks exhibit patterns in their raw population activity that mirror regularities in the training data. We refer to this as the Reflection Hypothesis and provide evidence for this phenomenon in both simple recurrent neural networks (RNNs) and complex large language models (LLMs). Building on this insight, we propose to leverage our cognitive tendency of chunking to segment high-dimensional neural population dynamics into interpretable units that reflect underlying concepts. We propose three methods to extract recurring chunks on a neural population level, complementing each other based on label availability and neural data dimensionality. Discrete sequence chunking (DSC) learns a dictionary of entities in a lower-dimensional neural space; population averaging (PA) extracts recurring entities that correspond to known labels; and unsupervised chunk discovery (UCD) can be used when labels are absent. We demonstrate the effectiveness of these methods in extracting concept-encoding entities agnostic to model architectures. These concepts can be both concrete (words), abstract (POS tags), or structural (narrative schema). Additionally, we show that extracted chunks play a causal role in network behavior, as grafting them leads to controlled and predictable changes in the model's behavior. Our work points to a new direction for interpretability, one that harnesses both cognitive principles and the structure of naturalistic data to reveal the hidden computations of complex learning systems, gradually transforming them from black boxes into systems we can begin to understand.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.24073</link>
<guid>https://arxiv.org/abs/2505.24073</guid>
<content:encoded><![CDATA[
arXiv:2505.24073v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</title>
<link>https://arxiv.org/abs/2506.09284</link>
<guid>https://arxiv.org/abs/2506.09284</guid>
<content:encoded><![CDATA[
arXiv:2506.09284v2 Announce Type: replace-cross 
Abstract: Understanding fine-grained object affordances is imperative for robots to manipulate objects in unstructured environments given open-ended task instructions. However, existing methods of visual affordance predictions often rely on manually annotated data or conditions only on a predefined set of tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for distilling affordance knowledge from foundation models into a task-conditioned affordance model without any manual annotations. By leveraging the complementary strengths of large vision models and vision-language models, UAD automatically annotates a large-scale dataset with detailed $<$instruction, visual affordance$>$ pairs. Training only a lightweight task-conditioned decoder atop frozen features, UAD exhibits notable generalization to in-the-wild robotic scenes and to various human activities, despite only being trained on rendered objects in simulation. Using affordance provided by UAD as the observation space, we show an imitation learning policy that demonstrates promising generalization to unseen object instances, object categories, and even variations in task instructions after training on as few as 10 demonstrations. Project website: https://unsup-affordance.github.io/
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[
arXiv:2506.20430v2 Announce Type: replace-cross 
Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Task-based Evaluation of Quantitative Medical Imaging Methods: Emerging Frameworks and Future Directions</title>
<link>https://arxiv.org/abs/2507.04591</link>
<guid>https://arxiv.org/abs/2507.04591</guid>
<content:encoded><![CDATA[
arXiv:2507.04591v3 Announce Type: replace-cross 
Abstract: Quantitative imaging (QI) is demonstrating strong promise across multiple clinical applications. For clinical translation of QI methods, objective evaluation on clinically relevant tasks is essential. To address this need, multiple evaluation strategies are being developed. In this paper, based on previous literature, we outline four emerging frameworks to perform evaluation studies of QI methods. We first discuss the use of virtual imaging trials (VITs) to evaluate QI methods. Next, we outline a no-gold-standard evaluation framework to clinically evaluate QI methods without ground truth. Third, a framework to evaluate QI methods for joint detection and quantification tasks is outlined. Finally, we outline a framework to evaluate QI methods that output multi-dimensional parameters, such as radiomic features. We review these frameworks, discussing their utilities and limitations. Further, we examine future research areas in evaluation of QI methods. Given the recent advancements in PET, including long axial field-of-view scanners and the development of artificial-intelligence algorithms, we present these frameworks in the context of PET.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update</title>
<link>https://arxiv.org/abs/2507.11069</link>
<guid>https://arxiv.org/abs/2507.11069</guid>
<content:encoded><![CDATA[
arXiv:2507.11069v3 Announce Type: replace-cross 
Abstract: Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handcrafted vs. Deep Radiomics vs. Fusion vs. Deep Learning: A Comprehensive Review of Machine Learning -Based Cancer Outcome Prediction in PET and SPECT Imaging</title>
<link>https://arxiv.org/abs/2507.16065</link>
<guid>https://arxiv.org/abs/2507.16065</guid>
<content:encoded><![CDATA[
arXiv:2507.16065v3 Announce Type: replace-cross 
Abstract: Machine learning (ML), including deep learning (DL) and radiomics-based methods, is increasingly used for cancer outcome prediction with PET and SPECT imaging. However, the comparative performance of handcrafted radiomics features (HRF), deep radiomics features (DRF), DL models, and hybrid fusion approaches remains inconsistent across clinical applications. This systematic review analyzed 226 studies published from 2020 to 2025 that applied ML to PET or SPECT imaging for outcome prediction. Each study was evaluated using a 59-item framework covering dataset construction, feature extraction, validation methods, interpretability, and risk of bias. We extracted key details including model type, cancer site, imaging modality, and performance metrics such as accuracy and area under the curve (AUC). PET-based studies (95%) generally outperformed those using SPECT, likely due to higher spatial resolution and sensitivity. DRF models achieved the highest mean accuracy (0.862), while fusion models yielded the highest AUC (0.861). ANOVA confirmed significant differences in performance (accuracy: p=0.0006, AUC: p=0.0027). Common limitations included inadequate handling of class imbalance (59%), missing data (29%), and low population diversity (19%). Only 48% of studies adhered to IBSI standards. These findings highlight the need for standardized pipelines, improved data quality, and explainable AI to support clinical integration.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping</title>
<link>https://arxiv.org/abs/2508.13065</link>
<guid>https://arxiv.org/abs/2508.13065</guid>
<content:encoded><![CDATA[
<div> shape editing, body transformation, dataset, diffusion-based method, semantic attributes 

Summary:
- A new dataset of 18,573 images across 1523 subjects is introduced for controlled human shape editing, featuring variations in body shape like fat, muscular, and thin under consistent conditions.
- The Odo method is proposed for realistic body reshaping using frozen UNet and ControlNet to preserve appearance details and guide shape transformation.
- Extensive experiments show that Odo outperforms previous methods with lower reconstruction errors and realistic results matching desired shapes. 
- The lack of large-scale datasets has hindered progress in human shape editing, but this dataset aims to address this limitation.
- Shape editing techniques have lagged behind pose editing methods, but Odo shows promising results in enabling intuitive and realistic body transformations. 

<br /><br />Summary: <div>
arXiv:2508.13065v2 Announce Type: replace 
Abstract: Human shape editing enables controllable transformation of a person's body shape, such as thin, muscular, or overweight, while preserving pose, identity, clothing, and background. Unlike human pose editing, which has advanced rapidly, shape editing remains relatively underexplored. Current approaches typically rely on 3D morphable models or image warping, often introducing unrealistic body proportions, texture distortions, and background inconsistencies due to alignment errors and deformations. A key limitation is the lack of large-scale, publicly available datasets for training and evaluating body shape manipulation methods. In this work, we introduce the first large-scale dataset of 18,573 images across 1523 subjects, specifically designed for controlled human shape editing. It features diverse variations in body shape, including fat, muscular and thin, captured under consistent identity, clothing, and background conditions. Using this dataset, we propose Odo, an end-to-end diffusion-based method that enables realistic and intuitive body reshaping guided by simple semantic attributes. Our approach combines a frozen UNet that preserves fine-grained appearance and background details from the input image with a ControlNet that guides shape transformation using target SMPL depth maps. Extensive experiments demonstrate that our method outperforms prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm, significantly lower than the 13.6mm observed in baseline methods, while producing realistic results that accurately match the desired target shapes.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-Cache: Training-Free Retrieval for Real-Time Manipulation</title>
<link>https://arxiv.org/abs/2505.09040</link>
<guid>https://arxiv.org/abs/2505.09040</guid>
<content:encoded><![CDATA[
<div> Keywords: real robots, training-free retrieval, image action trajectories, hierarchical search, real-time control

Summary:
RT-Cache proposes a novel approach for training-free retrieval-as-control in real robots, enabling them to repeat behaviors in new environments with minimal new data. The system caches diverse image action trajectories in a unified vector memory and retrieves and replays multi-step snippets at test time, eliminating the need for per-step model calls. A hierarchical search ensures quick lookups at a large scale, shifting the computational cost to storage and enabling real-time control on standard GPUs. Across various real-robot tasks and datasets, RT-Cache outperforms strong retrieval baselines with approximately double the success rate and 30% faster completion time. An anchoring study demonstrates immediate adaptation to more complex tasks without requiring fine-tuning. RT-Cache facilitates few-shot deployment and serves as a foundation for incorporating multimodal keys and integrating with high-level policies. This approach transforms experience into an append-only memory, offering scalability and simplicity in real-world robotic applications.<br /><br />Summary: <div>
arXiv:2505.09040v3 Announce Type: replace-cross 
Abstract: Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration</title>
<link>https://arxiv.org/abs/2508.12445</link>
<guid>https://arxiv.org/abs/2508.12445</guid>
<content:encoded><![CDATA[
<div> Fractional Fourier Transform, Cross-Image Feature Matching, Deformable Image Registration, Transformer-Based Architecture, Medical Image Alignment <br />
Summary: <br />
The article introduces FractMorph, a novel 3D dual-parallel transformer-based architecture for deformable image registration in medical imaging. By utilizing Fractional Cross-Attention blocks with multi-domain fractional Fourier transform branches, FractMorph can capture both local and global deformations simultaneously. This approach significantly improves feature matching between fixed and moving image streams, leading to state-of-the-art performance on the ACDC cardiac MRI dataset. FractMorph-Light, a lighter variant of the model, maintains high accuracy while reducing model complexity. The method shows promising results on a cerebral atlas-to-patient dataset, demonstrating its generalizability. The study highlights the efficiency and robustness of utilizing multi-domain spectral-spatial attention in transformers for modeling complex non-rigid deformations in medical images, eliminating the need for specialized tuning or hierarchical multi-scale networks. <div>
arXiv:2508.12445v2 Announce Type: replace-cross 
Abstract: Deformable image registration (DIR) is a crucial and challenging technique for aligning anatomical structures in medical images and is widely applied in diverse clinical applications. However, existing approaches often struggle to capture fine-grained local deformations and large-scale global deformations simultaneously within a unified framework. We present FractMorph, a novel 3D dual-parallel transformer-based architecture that enhances cross-image feature matching through multi-domain fractional Fourier transform (FrFT) branches. Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at fractional angles of $0^\circ$, $45^\circ$, $90^\circ$, along with a log-magnitude branch, to effectively extract local, semi-global, and global features at the same time. These features are fused via cross-attention between the fixed and moving image streams. A lightweight U-Net style network then predicts a dense deformation field from the transformer-enriched features. On the intra-patient ACDC cardiac MRI dataset, FractMorph achieves state-of-the-art performance with an overall Dice Similarity Coefficient (DSC) of $86.45\%$, an average per-structure DSC of $75.15\%$, and a 95th-percentile Hausdorff distance (HD95) of $1.54~\mathrm{mm}$ on our data split. FractMorph-Light, a lightweight variant of our model with only 29.6M parameters, preserves high accuracy while halving model complexity. Furthermore, we demonstrate the generality of our approach with solid performance on a cerebral atlas-to-patient dataset. Our results demonstrate that multi-domain spectral-spatial attention in transformers can robustly and efficiently model complex non-rigid deformations in medical images using a single end-to-end network, without the need for scenario-specific tuning or hierarchical multi-scale networks. The source code is available at https://github.com/shayankebriti/FractMorph.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</title>
<link>https://arxiv.org/abs/2508.12687</link>
<guid>https://arxiv.org/abs/2508.12687</guid>
<content:encoded><![CDATA[
<div> benchmark, MLLMs, hallucinations, egocentric videos, EgoIllusion  
Summary:  
EgoIllusion is a new benchmark created to evaluate hallucinations in multimodal large language models (MLLMs) specifically in egocentric videos. The benchmark consists of 1,400 videos with 8,000 human-annotated questions aimed at triggering hallucinations in both visual and auditory cues. Ten MLLMs were evaluated on EgoIllusion, with even powerful models like GPT-4o and Gemini achieving only 59% accuracy. This highlights the significant challenges MLLMs face in accurately interpreting egocentric video content without hallucinations. The benchmark aims to spur the development of more effective egocentric MLLMs with reduced hallucination rates. EgoIllusion will be open-sourced for reproducibility and serves as a foundation for future research in assessing the performance of MLLMs in complex multimodal tasks.  
<br /><br />Summary: <div>
arXiv:2508.12687v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in complex multimodal tasks. While MLLMs excel at visual perception and reasoning in third-person and egocentric videos, they are prone to hallucinations, generating coherent yet inaccurate responses. We present EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated open and closed-ended questions designed to trigger hallucinations in both visual and auditory cues in egocentric videos. Evaluations across ten MLLMs reveal significant challenges, including powerful models like GPT-4o and Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs the development of better egocentric MLLMs with reduced hallucination rates. Our benchmark will be open-sourced for reproducibility.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration</title>
<link>https://arxiv.org/abs/2508.16579</link>
<guid>https://arxiv.org/abs/2508.16579</guid>
<content:encoded><![CDATA[
<div> fusion framework, iToF-RGB, depth sensing, spatial resolution, depth super-resolution 

Summary:
This paper introduces a novel iToF-RGB fusion framework aimed at addressing the limitations of indirect Time-of-Flight (iToF) depth sensing. The method involves reprojecting the iToF depth map onto the RGB coordinate system to achieve pixel-level correspondence between modalities. A dual-encoder fusion network is utilized to extract features from the depth and RGB images, guided by monocular depth priors for depth super-resolution. By incorporating cross-modal structural cues and depth consistency constraints, the proposed approach enhances depth accuracy, sharpens edges, and expands the field-of-view seamlessly. Experimental results on synthetic and real-world datasets demonstrate the superior performance of the framework in terms of accuracy, structural consistency, and visual quality.<br /><br />Summary: <div>
arXiv:2508.16579v1 Announce Type: new 
Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as low spatial resolution, limited field-of-view (FoV), and structural distortion in complex scenes. The proposed method first reprojects the narrow-FoV iToF depth map onto the wide-FoV RGB coordinate system through a precise geometric calibration and alignment module, ensuring pixel-level correspondence between modalities. A dual-encoder fusion network is then employed to jointly extract complementary features from the reprojected iToF depth and RGB image, guided by monocular depth priors to recover fine-grained structural details and perform depth super-resolution. By integrating cross-modal structural cues and depth consistency constraints, our approach achieves enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods in terms of accuracy, structural consistency, and visual quality.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance</title>
<link>https://arxiv.org/abs/2508.16644</link>
<guid>https://arxiv.org/abs/2508.16644</guid>
<content:encoded><![CDATA[
<div> Instance control, Diffusion models, Image synthesis, CountLoop, Object counting<br />
<br />
Summary: CountLoop is a novel framework that enhances diffusion models in generating precise numbers of object instances in complex scenes. Through iterative structured feedback, a language-guided planner and critic evaluate object counts, spatial arrangements, and attribute consistency to guide image generation. Instance-driven attention masking and compositional generation techniques improve object separation, especially in occluded scenes. CountLoop achieves counting accuracy of up to 98% on various benchmarks while maintaining spatial fidelity and visual quality, outperforming existing baselines. The framework's training-free approach and multimodal feedback loop make it a promising solution for reliable instance control in photorealistic image synthesis. <div>
arXiv:2508.16644v1 Announce Type: new 
Abstract: Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2508.16652</link>
<guid>https://arxiv.org/abs/2508.16652</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, compositional generalization, object binding, mechanistic interpretability, CLIP

Summary:
Vision-Language Models (VLMs) have excelled in integrating visual and textual information for tasks like image captioning and visual question answering. However, they face challenges in compositional generalization and object binding. This study investigates the underlying reasons for these difficulties using mechanistic interpretability techniques. The researchers found that neurons in the MLP layers of CLIP's vision encoder exhibit "superposition," representing multiple features, which impairs compositional feature representation. This limitation adversely affects compositional reasoning and object binding capabilities in VLMs. By shedding light on these mechanisms, this research aims to lay the groundwork for understanding the root causes of compositional failures in VLMs.

<br /><br />Summary: Vision-Language Models excel at integrating visual and textual information but struggle with compositional generalization and object binding. This study examines the mechanisms behind these challenges in VLMs, revealing that neurons in the vision encoder of models like CLIP exhibit superposition, representing multiple features. This superposition impedes the models' ability to effectively represent compositional features, ultimately impacting their performance in compositional reasoning and object binding tasks. By identifying these underlying mechanisms, this research sets the stage for gaining deeper insights into the fundamental causes of compositional failures in VLMs. <div>
arXiv:2508.16652v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown remarkable performance in integrating visual and textual information for tasks such as image captioning and visual question answering. However, these models struggle with compositional generalization and object binding, which limit their ability to handle novel combinations of objects and their attributes. Our work explores the root causes of these failures using mechanistic interpretability techniques. We show evidence that individual neurons in the MLP layers of CLIP's vision encoder represent multiple features, and this "superposition" directly hinders its compositional feature representation which consequently affects compositional reasoning and object binding capabilities. We hope this study will serve as an initial step toward uncovering the mechanistic roots of compositional failures in VLMs. The code and supporting results can be found https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</title>
<link>https://arxiv.org/abs/2508.16654</link>
<guid>https://arxiv.org/abs/2508.16654</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-and-Language Navigation, Memory Spatial Navigation, Spatial reasoning, Object relationship inference, Long-horizon tasks <br />
Summary: <br />
Memory Spatial Navigation (MSNav) addresses vulnerabilities in Vision-and-Language Navigation by integrating a Memory Module for long-range exploration, a Spatial Module for spatial reasoning and object relationship inference, and a Decision Module for path planning. The framework overcomes memory overload, weak spatial reasoning, and poor cross-modal grounding. A new dataset and the Qwen-Spatial model enhance object list extraction. MSNav achieves state-of-the-art performance on Room-to-Room and REVERIE datasets, with improved Success Rate and Success weighted by Path Length metrics. The integration of these modules transforms traditional black-box approaches into a more robust, integrated intelligence system, showcasing significant advancements in the VLN domain. <br /> <div>
arXiv:2508.16654v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a "black-box" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm</title>
<link>https://arxiv.org/abs/2508.16660</link>
<guid>https://arxiv.org/abs/2508.16660</guid>
<content:encoded><![CDATA[
<div> Keywords: soil image classification, Convolutional Neural Networks, machine learning algorithms, swarm algorithms, hyperparameters selection 

Summary: 
This study focuses on the classification of soil images using artificial intelligence techniques to enhance land management and agricultural output. A Convolutional Neural Networks model was developed to classify soil types, and machine learning algorithms were utilized to improve classification performance. Swarm algorithms, specifically the Whale Optimization Algorithm and Particle Swarm Optimization Algorithm, were implemented to optimize the CNN network's hyperparameters and improve classification accuracy. The system's performance was evaluated using Accuracy and F1 measures. The results showed that the proposed approach led to efficient classification of soil types, demonstrating the potential of using AI in soil image analysis for various applications in agriculture, civil engineering, and natural resource management. <br /><br />Summary: <div>
arXiv:2508.16660v1 Announce Type: new 
Abstract: Classifying soil images contributes to better land management, increased agricultural output, and practical solutions for environmental issues. The development of various disciplines, particularly agriculture, civil engineering, and natural resource management, is aided by understanding of soil quality since it helps with risk reduction, performance improvement, and sound decision-making . Artificial intelligence has recently been used in a number of different fields. In this study, an intelligent model was constructed using Convolutional Neural Networks to classify soil kinds, and machine learning algorithms were used to enhance the performance of soil classification . To achieve better implementation and performance of the Convolutional Neural Networks algorithm and obtain valuable results for the process of classifying soil type images, swarm algorithms were employed to obtain the best performance by choosing Hyper parameters for the Convolutional Neural Networks network using the Whale optimization algorithm and the Particle swarm optimization algorithm, and comparing the results of using the two algorithms in the process of multiple classification of soil types. The Accuracy and F1 measures were adopted to test the system, and the results of the proposed work were efficient result
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models</title>
<link>https://arxiv.org/abs/2508.16661</link>
<guid>https://arxiv.org/abs/2508.16661</guid>
<content:encoded><![CDATA[
<div> Keywords: image-based quality assessment, additive manufacturing, machine learning, deep learning, vision-language models 

Summary: 
This article introduces a novel QA-VLM framework for image-based quality assessment in additive manufacturing (AM). Current methods rely on human expertise, but this new framework utilizes vision-language models enriched with application-specific knowledge to provide human-interpretable quality assessments. By leveraging attention mechanisms and reasoning capabilities, the framework outperforms off-the-shelf VLMs in generating valid and consistent explanations. The framework was tested on 24 single-bead samples created through laser wire direct energy deposition (DED-LW), demonstrating its potential for trustworthy and interpretable quality assessment in AM applications. <div>
arXiv:2508.16661v1 Announce Type: new 
Abstract: Image-based quality assessment (QA) in additive manufacturing (AM) often relies heavily on the expertise and constant attention of skilled human operators. While machine learning and deep learning methods have been introduced to assist in this task, they typically provide black-box outputs without interpretable justifications, limiting their trust and adoption in real-world settings. In this work, we introduce a novel QA-VLM framework that leverages the attention mechanisms and reasoning capabilities of vision-language models (VLMs), enriched with application-specific knowledge distilled from peer-reviewed journal articles, to generate human-interpretable quality assessments. Evaluated on 24 single-bead samples produced by laser wire direct energy deposition (DED-LW), our framework demonstrates higher validity and consistency in explanation quality than off-the-shelf VLMs. These results highlight the potential of our approach to enable trustworthy, interpretable quality assessment in AM applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers</title>
<link>https://arxiv.org/abs/2508.16663</link>
<guid>https://arxiv.org/abs/2508.16663</guid>
<content:encoded><![CDATA[
<div> Keywords: Fine-Grained Visual Classification, Vision Transformers, Interpretability, Attention Module, Swin Transformer <br />
Summary: 
Fine-Grained Visual Classification (FGVC) is a challenging area in computer vision, crucial for applications like biodiversity monitoring and medical diagnostics. While large-scale Vision Transformers have shown impressive performance, their decision-making processes lack interpretability. This paper introduces The Loupe, a lightweight attention module designed to enhance pre-trained models like the Swin Transformer. The Loupe is trained with a composite loss function to focus on discriminative object parts without explicit annotations. Results on the CUB-200-2011 dataset demonstrate that The Loupe boosts accuracy significantly. Furthermore, qualitative analysis shows that The Loupe effectively localizes meaningful features, providing valuable insight into the model's decision-making process. The Loupe acts as a powerful regularizer, improving performance while offering clear visual explanations. <br /><br />Summary: <div>
arXiv:2508.16663v1 Announce Type: new 
Abstract: Fine-Grained Visual Classification (FGVC) is a critical and challenging area within computer vision, demanding the identification of highly subtle, localized visual cues. The importance of FGVC extends to critical applications such as biodiversity monitoring and medical diagnostics, where precision is paramount. While large-scale Vision Transformers have achieved state-of-the-art performance, their decision-making processes often lack the interpretability required for trust and verification in such domains. In this paper, we introduce The Loupe, a novel, lightweight, and plug-and-play attention module designed to be inserted into pre-trained backbones like the Swin Transformer. The Loupe is trained end-to-end with a composite loss function that implicitly guides the model to focus on the most discriminative object parts without requiring explicit part-level annotations. Our unique contribution lies in demonstrating that a simple, intrinsic attention mechanism can act as a powerful regularizer, significantly boosting performance while simultaneously providing clear visual explanations. Our experimental evaluation on the challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%. Crucially, our qualitative analysis of the learned attention maps reveals that The Loupe effectively localizes semantically meaningful features, providing a valuable tool for understanding and trusting the model's decision-making process.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture</title>
<link>https://arxiv.org/abs/2508.16670</link>
<guid>https://arxiv.org/abs/2508.16670</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID19, SARSCoV2 virus, respiratory system, Convolutional Neural Network, severity of infection

Summary:<br /><br />
The article discusses the impact of COVID19, caused by the SARSCoV2 virus, which led to a global pandemic in 2020. The disease primarily affects the respiratory system, with patients experiencing symptoms similar to a cold, flu, or pneumonia. Due to the respiratory failure, many lives were lost, highlighting the urgent need for effective diagnostic tools. To address this, the project aims to utilize Convolutional Neural Network models to analyze the severity of COVID19 infection based on CT scans of the lungs. By using machine learning technology, doctors can better assess the severity of the disease and determine appropriate treatment plans. This approach can help in early identification of critical cases, potentially reducing the mortality rate and improving patient outcomes. <div>
arXiv:2508.16670v1 Announce Type: new 
Abstract: COVID19 took the world by storm since December 2019. A highly infectious communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020, the World Health Organization (WHO) declared COVID19 as a global pandemic. A pandemic in the 21st century after almost 100 years was something the world was not prepared for, which resulted in the deaths of around 1.6 million people worldwide. The most common symptoms of COVID19 were associated with the respiratory system and resembled a cold, flu, or pneumonia. After extensive research, doctors and scientists concluded that the main reason for lives being lost due to COVID19 was failure of the respiratory system. Patients were dying gasping for breath. Top healthcare systems of the world were failing badly as there was an acute shortage of hospital beds, oxygen cylinders, and ventilators. Many were dying without receiving any treatment at all. The aim of this project is to help doctors decide the severity of COVID19 by reading the patient's Computed Tomography (CT) scans of the lungs. Computer models are less prone to human error, and Machine Learning or Neural Network models tend to give better accuracy as training improves over time. We have decided to use a Convolutional Neural Network model. Given that a patient tests positive, our model will analyze the severity of COVID19 infection within one month of the positive test result. The severity of the infection may be promising or unfavorable (if it leads to intubation or death), based entirely on the CT scans in the dataset.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation</title>
<link>https://arxiv.org/abs/2508.16674</link>
<guid>https://arxiv.org/abs/2508.16674</guid>
<content:encoded><![CDATA[
<div> Keywords: medical report interpretation, vision-language models, benchmark, structured understanding, OCR+LLM pipeline 

Summary: 
Medical report interpretation is vital for healthcare, but standardized benchmarks for evaluating structured interpretation quality are lacking. The MedRepBench benchmark, comprising 1,900 real-world Chinese medical reports, evaluates end-to-end vision-language models and text-only models using OCR outputs combined with large language models. Two evaluation protocols measure field-level recall and automated subjective evaluation. A reward function and Group Relative Policy Optimization improve VLM performance by up to 6% recall gain. The OCR+LLM pipeline shows strong performance but suffers from layout-blindness and latency issues, highlighting the need for further advancements in vision-based report understanding.<br /><br />Summary: <div>
arXiv:2508.16674v1 Announce Type: new 
Abstract: Medical report interpretation plays a crucial role in healthcare, enabling both patient-facing explanations and effective information flow across clinical systems. While recent vision-language models (VLMs) and large language models (LLMs) have demonstrated general document understanding capabilities, there remains a lack of standardized benchmarks to assess structured interpretation quality in medical reports. We introduce MedRepBench, a comprehensive benchmark built from 1,900 de-identified real-world Chinese medical reports spanning diverse departments, patient demographics, and acquisition formats. The benchmark is designed primarily to evaluate end-to-end VLMs for structured medical report understanding. To enable controlled comparisons, we also include a text-only evaluation setting using high-quality OCR outputs combined with LLMs, allowing us to estimate the upper-bound performance when character recognition errors are minimized. Our evaluation framework supports two complementary protocols: (1) an objective evaluation measuring field-level recall of structured clinical items, and (2) an automated subjective evaluation using a powerful LLM as a scoring agent to assess factuality, interpretability, and reasoning quality. Based on the objective metric, we further design a reward function and apply Group Relative Policy Optimization (GRPO) to improve a mid-scale VLM, achieving up to 6% recall gain. We also observe that the OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and latency issues, motivating further progress toward robust, fully vision-based report understanding.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection</title>
<link>https://arxiv.org/abs/2508.16739</link>
<guid>https://arxiv.org/abs/2508.16739</guid>
<content:encoded><![CDATA[
<div> Keywords: Unmanned Aerial Vehicles, real-time wildfire monitoring, fire source detection, lightweight framework, video analysis

Summary:
- The study introduces a two-stage framework for real-time wildfire monitoring and fire source detection on UAV platforms.
- Stage 1 utilizes a policy network to identify and discard redundant video clips, reducing computational costs through frame compression techniques.
- A station point mechanism is introduced to leverage future frame information within the sequential policy network, improving prediction accuracy.
- Stage 2 employs an improved YOLOv8 model for fire source localization once a frame is classified as "fire".
- Evaluation on FLAME, HMDB51, and Fire & Smoke datasets shows reduced computational costs and maintained classification accuracy in Stage 1, and higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.

<br /><br />Summary: 
The study presents a lightweight and efficient framework for real-time wildfire monitoring and fire source detection on UAV platforms. Stage 1 utilizes a policy network and frame compression techniques to reduce computational costs, with a station point mechanism enhancing prediction accuracy. In Stage 2, an improved YOLOv8 model is used for fire source localization. Evaluation results demonstrate reduced computational costs, maintained classification accuracy, and improved detection accuracy compared to baseline methods. <div>
arXiv:2508.16739v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) have become increasingly important in disaster emergency response by enabling real-time aerial video analysis. Due to the limited computational resources available on UAVs, large models cannot be run independently for real-time analysis. To overcome this challenge, we propose a lightweight and efficient two-stage framework for real-time wildfire monitoring and fire source detection on UAV platforms. Specifically, in Stage 1, we utilize a policy network to identify and discard redundant video clips using frame compression techniques, thereby reducing computational costs. In addition, we introduce a station point mechanism that leverages future frame information within the sequential policy network to improve prediction accuracy. In Stage 2, once the frame is classified as "fire", we employ the improved YOLOv8 model to localize the fire source. We evaluate the Stage 1 method using the FLAME and HMDB51 datasets, and the Stage 2 method using the Fire & Smoke dataset. Experimental results show that our method significantly reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction</title>
<link>https://arxiv.org/abs/2508.16742</link>
<guid>https://arxiv.org/abs/2508.16742</guid>
<content:encoded><![CDATA[
<div> Keywords: lung adenocarcinoma, deep learning, whole slide images, recurrence risk, tumor microenvironment

Summary: CellEcoNet is a novel deep learning framework that uses a "language of pathology" to analyze whole slide images of invasive lung adenocarcinoma. By treating cells as words, cellular neighborhoods as phrases, and tissue architecture as sentences, CellEcoNet captures subtle variations and spatial interactions that contribute to recurrence risk. In a study of 456 H&amp;E-stained WSIs, CellEcoNet outperformed traditional grading systems and other computational methods in predicting recurrence (AUC: 77.8%, HR: 9.54). The model was found to be fair and consistently effective across different demographic and clinical subgroups. This innovative approach not only improves prognosis for ILA patients but also decodes the cellular "language" of the tumor microenvironment, providing insights into how cell variations contribute to recurrence risk.<br /><br />Summary: CellEcoNet introduces a spatially aware deep learning framework for analyzing whole slide images of invasive lung adenocarcinoma, achieving superior predictive performance in identifying patients at risk of recurrence. The model's "language of pathology" approach decodes cellular interactions and subtle variations in tissue architecture to reveal how they encode recurrence risk. CellEcoNet's innovative methodology marks a paradigm shift in understanding the tumor microenvironment and has the potential to improve prognostic accuracy for ILA patients. <div>
arXiv:2508.16742v1 Announce Type: new 
Abstract: Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA) patients recur within five years, and current tools fail to identify those needing adjuvant therapy. To address this unmet clinical need, we introduce CellEcoNet, a novel spatially aware deep learning framework that models whole slide images (WSIs) through natural language analogy, defining a "language of pathology," where cells act as words, cellular neighborhoods become phrases, and tissue architecture forms sentences. CellEcoNet learns these context-dependent meanings automatically, capturing how subtle variations and spatial interactions derive recurrence risk. On a dataset of 456 H&amp;E-stained WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54), outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0% HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%). CellEcoNet demonstrated fairness and consistent performance across diverse demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a paradigm shift by decoding the tumor microenvironment's cellular "language" to reveal how subtle cell variations encode recurrence risk.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers</title>
<link>https://arxiv.org/abs/2508.16752</link>
<guid>https://arxiv.org/abs/2508.16752</guid>
<content:encoded><![CDATA[
<div> fairness, utility, text-to-image models, debiasing methods, hyperparameterization
Summary:
- The article proposes a method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparametrization of debiasing methods.
- This method enables comparison between different models, outlining configurations that optimize fairness for a given utility.
- Normalized Shannon Entropy and ClipScore are used for fairness and utility evaluation, respectively.
- Evaluation is conducted on Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX text-to-image models.
- Default hyperparameterizations of text-to-image models are shown to be dominated solutions in the fairness-utility space, indicating the potential for improvement with better hyperparameters. 

<br /><br />Summary: <div>
arXiv:2508.16752v1 Announce Type: new 
Abstract: Achieving fairness in text-to-image generation demands mitigating social biases without compromising visual fidelity, a challenge critical to responsible AI. Current fairness evaluation procedures for text-to-image models rely on qualitative judgment or narrow comparisons, which limit the capacity to assess both fairness and utility in these models and prevent reproducible assessment of debiasing methods. Existing approaches typically employ ad-hoc, human-centered visual inspections that are both error-prone and difficult to replicate. We propose a method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparametrization of debiasing methods. Our method allows for comparison between distinct text-to-image models, outlining all configurations that optimize fairness for a given utility and vice-versa. To illustrate our evaluation method, we use Normalized Shannon Entropy and ClipScore for fairness and utility evaluation, respectively. We assess fairness and utility in Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that most default hyperparameterizations of the text-to-image model are dominated solutions in the fairness-utility space, and it is straightforward to find better hyperparameters.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation</title>
<link>https://arxiv.org/abs/2508.16763</link>
<guid>https://arxiv.org/abs/2508.16763</guid>
<content:encoded><![CDATA[
<div> benchmark, WebMMU, web tasks, multimodal large language models, web development

Summary:
WebMMU is a multilingual benchmark that evaluates three core web tasks: website visual question answering, code editing involving HTML/CSS/JavaScript, and mockup-to-code generation. Unlike previous benchmarks, WebMMU combines these tasks using expert-annotated web data to assess models' abilities in multi-step reasoning, element grounding, and UI comprehension and coding. The evaluation shows that multimodal large language models excel in basic information extraction but struggle with reasoning, code editing for functionality preservation, and design-to-code generation while maintaining hierarchy and supporting multilingual content. These findings reveal limitations in current multimodal large language models and highlight the need for enhanced multimodal and cross-lingual reasoning to develop future web agents capable of automating diverse web development tasks. 

<br /><br />Summary: <div>
arXiv:2508.16763v1 Announce Type: new 
Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web tasks: (1) website visual question answering, (2) code editing involving HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks that treat these tasks separately, WebMMU unifies them using expert-annotated, real-world web data to assess models' abilities in complex multi-step reasoning, precise element grounding, and functional UI comprehension and coding. Our evaluation shows that while multimodal large language models (MLLMs) perform well on basic information extraction, they struggle with reasoning and grounding, editing code to preserve functionality, and generating design-to-code that maintains hierarchy and supports multilingual content. These findings reveal key limitations in current MLLMs and underscore the need for improved multimodal and cross-lingual reasoning to build future web agents capable of automating diverse web development tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data</title>
<link>https://arxiv.org/abs/2508.16783</link>
<guid>https://arxiv.org/abs/2508.16783</guid>
<content:encoded><![CDATA[
<div> Patient diversity, deep learning, synthetic data, diagnostic imaging, fairness <br />
Summary: <br />
The study introduces RoentGen-v2, a text-to-image diffusion model for chest radiographs that allows control over radiographic findings and patient demographic attributes. It generates a large, demographically balanced synthetic dataset of over 565,000 images. A new training strategy leveraging synthetic data for supervised pretraining and fine-tuning on real data improves model performance and fairness across demographic subgroups. Synthetic pretraining led to a 6.5% accuracy increase compared to a 2.7% increase when combining real and synthetic data. This approach also reduced the underdiagnosis fairness gap by 19.3%. The findings demonstrate the potential of synthetic imaging to enhance equitable and generalizable medical deep learning under data constraints. The code, trained models, and synthetic dataset are open-sourced on GitHub at https://github.com/StanfordMIMI/RoentGen-v2. <br /> <div>
arXiv:2508.16783v1 Announce Type: new 
Abstract: Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes</title>
<link>https://arxiv.org/abs/2508.16812</link>
<guid>https://arxiv.org/abs/2508.16812</guid>
<content:encoded><![CDATA[
<div> OVODA, open-vocabulary, 3D object detection, attribute detection, foundation models, OVAD<br />
<br />
Summary:<br />
OVODA is a new framework proposed for open-vocabulary 3D object detection and attribute recognition in real-world scenarios. It addresses limitations of existing methods by not relying on closed-set assumptions and being able to recognize novel objects and attributes without prior anchor size information. The framework utilizes foundation models to bridge the semantic gap between 3D features and texts, while also detecting attributes such as spatial relationships and motion states. The authors introduce the OVAD dataset, which provides comprehensive attribute annotations to supplement existing 3D object detection benchmarks. Key innovations in OVODA include foundation model feature concatenation, prompt tuning strategies, and specialized techniques for attribute detection. Experimental results on nuScenes and Argoverse 2 datasets demonstrate that OVODA outperforms state-of-the-art methods in open-vocabulary 3D object detection and successful attribute recognition. <div>
arXiv:2508.16812v1 Announce Type: new 
Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing methods are limited by closed-set assumptions and struggle to recognize novel objects and their attributes in real-world scenarios. We propose OVODA, a novel framework enabling both open-vocabulary 3D object and attribute detection with no need to know the novel class anchor size. OVODA uses foundation models to bridge the semantic gap between 3D features and texts while jointly detecting attributes, e.g., spatial relationships, motion states, etc. To facilitate such research direction, we propose OVAD, a new dataset that supplements existing 3D object detection benchmarks with comprehensive attribute annotations. OVODA incorporates several key innovations, including foundation model feature concatenation, prompt tuning strategies, and specialized techniques for attribute detection, including perspective-specified prompts and horizontal flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets show that under the condition of no given anchor sizes of novel classes, OVODA outperforms the state-of-the-art methods in open-vocabulary 3D object detection while successfully recognizing object attributes. Our OVAD dataset is released here: https://doi.org/10.5281/zenodo.16904069 .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results</title>
<link>https://arxiv.org/abs/2508.16830</link>
<guid>https://arxiv.org/abs/2508.16830</guid>
<content:encoded><![CDATA[
<div> Challenge, Low-Light, RAW, Denoising, Benchmark

Summary:
The AIM 2025 Low-Light RAW Video Denoising Challenge focuses on denoising low-light RAW video while adhering to exposure-time limits imposed by frame rate and adapting to sensor-specific noise. The challenge dataset comprises 756 ten-frame sequences captured with 14 smartphone camera sensors under varying illumination and exposure conditions. Participants are tasked with processing linear RAW sequences to output a denoised 10th frame while maintaining the Bayer pattern. Evaluation is based on full-reference PSNR and SSIM on a private test set, with final ranking determined by the mean of per-metric ranks. The report details the dataset, challenge protocol, and approaches submitted by participants. <div>
arXiv:2508.16830v1 Announce Type: new 
Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light RAW Video Denoising Challenge. The task is to develop methods that denoise low-light RAW video by exploiting temporal redundancy while operating under exposure-time limits imposed by frame rate and adapting to sensor-specific, signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences captured with 14 smartphone camera sensors across nine conditions (illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR references obtained via burst averaging. Participants process linear RAW sequences and output the denoised 10th frame while preserving the Bayer pattern. Submissions are evaluated on a private test set using full-reference PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This report describes the dataset, challenge protocol, and submitted approaches.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Neural Network for Transient Detection without Image Subtraction</title>
<link>https://arxiv.org/abs/2508.16844</link>
<guid>https://arxiv.org/abs/2508.16844</guid>
<content:encoded><![CDATA[
<div> Transformer-based neural network, classification, astronomical images, convolutional neural network, supernova detection<br />
<br />
Summary:<br />
- Introduction of a transformer-based neural network for accurate classification of real and bogus transient detections in astronomical images<br />
- Adoption of an architecture suited for detailed pixel-by-pixel comparison, advancing beyond conventional CNN methods<br />
- Efficient analysis of search and template images without requiring computationally-expensive difference imaging<br />
- Achieved a classification accuracy of 97.4% using the autoScan dataset from the Dark Energy Survey<br />
- Network maintains high performance even when input images are not centered on the supernova candidate<br /> <div>
arXiv:2508.16844v1 Announce Type: new 
Abstract: We introduce a transformer-based neural network for the accurate classification of real and bogus transient detections in astronomical images. This network advances beyond the conventional convolutional neural network (CNN) methods, widely used in image processing tasks, by adopting an architecture better suited for detailed pixel-by-pixel comparison. The architecture enables efficient analysis of search and template images only, thus removing the necessity for computationally-expensive difference imaging, while maintaining high performance. Our primary evaluation was conducted using the autoScan dataset from the Dark Energy Survey (DES), where the network achieved a classification accuracy of 97.4% and diminishing performance utility for difference image as the size of the training set grew. Further experiments with DES data confirmed that the network can operate at a similar level even when the input images are not centered on the supernova candidate. These findings highlight the network's effectiveness in enhancing both accuracy and efficiency of supernova detection in large-scale astronomical surveys.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows</title>
<link>https://arxiv.org/abs/2508.16845</link>
<guid>https://arxiv.org/abs/2508.16845</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, NinA, Normalizing Flows, diffusion models, VLA control <br />
Summary: 
NinA is introduced as an alternative to diffusion-based decoders in Vision-Language-Action (VLA) models. It utilizes Normalizing Flows (NF) to enable one-shot sampling through an invertible transformation, resulting in significantly faster inference compared to diffusion models. The integration of NinA into the FLOWER VLA architecture and fine-tuning on the LIBERO benchmark demonstrates that NinA can achieve performance matching its diffusion-based counterpart while providing faster inference. This suggests that NinA offers a promising solution for efficient, high-frequency VLA control without sacrificing performance. <div>
arXiv:2508.16845v1 Announce Type: new 
Abstract: Recent advances in Vision-Language-Action (VLA) models have established a two-component architecture, where a pre-trained Vision-Language Model (VLM) encodes visual observations and task descriptions, and an action decoder maps these representations to continuous actions. Diffusion models have been widely adopted as action decoders due to their ability to model complex, multimodal action distributions. However, they require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial. In this work, we present NinA (Normalizing Flows in Action), a fast and expressive alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation, significantly reducing inference time. We integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO benchmark. Our experiments show that NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference. These results suggest that NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.16849</link>
<guid>https://arxiv.org/abs/2508.16849</guid>
<content:encoded><![CDATA[
<div> Keywords: 6G, spatial channel state information, radiance field, radio propagation, RF-PGS

Summary: 
RF-PGS introduces a novel framework for efficient and accurate modeling of spatial channel state information in the 6G era. By utilizing Planar Gaussians as geometry primitives and optimizing radio radiance, RF-PGS reconstructs high-fidelity radio propagation paths from sparse path loss spectra. The framework combines a geometry training stage for dense scene reconstruction and an RF training stage for modeling radio propagation behavior. This approach significantly improves reconstruction accuracy, reduces training costs, and enables scalable representation of wireless channels. RF-PGS offers a practical solution for implementing emerging 6G technologies that require large-scale antenna arrays and accurate spatial channel state information. <br /><br />Summary: <div>
arXiv:2508.16849v1 Announce Type: new 
Abstract: In the 6G era, the demand for higher system throughput and the implementation of emerging 6G technologies require large-scale antenna arrays and accurate spatial channel state information (Spatial-CSI). Traditional channel modeling approaches, such as empirical models, ray tracing, and measurement-based methods, face challenges in spatial resolution, efficiency, and scalability. Radiance field-based methods have emerged as promising alternatives but still suffer from geometric inaccuracy and costly supervision. This paper proposes RF-PGS, a novel framework that reconstructs high-fidelity radio propagation paths from only sparse path loss spectra. By introducing Planar Gaussians as geometry primitives with certain RF-specific optimizations, RF-PGS achieves dense, surface-aligned scene reconstruction in the first geometry training stage. In the subsequent Radio Frequency (RF) training stage, the proposed fully-structured radio radiance, combined with a tailored multi-view loss, accurately models radio propagation behavior. Compared to prior radiance field methods, RF-PGS significantly improves reconstruction accuracy, reduces training costs, and enables efficient representation of wireless channels, offering a practical solution for scalable 6G Spatial-CSI modeling.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Primitive Optimized Deformable Retinal Image Registration</title>
<link>https://arxiv.org/abs/2508.16852</link>
<guid>https://arxiv.org/abs/2508.16852</guid>
<content:encoded><![CDATA[
<div> iterative framework, retinal image registration, Gaussian Primitive Optimization, structured message passing, keypoints

Summary:
Gaussian Primitive Optimization (GPO) is introduced as an iterative framework for deformable retinal image registration, addressing challenges like homogeneous regions and sparse vascular features. GPO utilizes structured message passing by extracting keypoints at anatomical structures and modelling them as Gaussian primitives. These primitives adapt their spatial influence to local deformation scales and propagate displacement signals globally using a KNN Gaussian interpolation. By anchoring nodes in high-gradient regions, GPO ensures robust gradient flow for accurate registration. The framework is optimized via a multi-term loss enforcing keypoint consistency and intensity alignment. Experimental results on the FIRE dataset demonstrate a significant reduction in target registration error and improved AUC, outperforming existing methods. The source code for GPO can be accessed on GitHub. <br /><br />Summary: <div>
arXiv:2508.16852v1 Announce Type: new 
Abstract: Deformable retinal image registration is notoriously difficult due to large homogeneous regions and sparse but critical vascular features, which cause limited gradient signals in standard learning-based frameworks. In this paper, we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework that performs structured message passing to overcome these challenges. After an initial coarse alignment, we extract keypoints at salient anatomical structures (e.g., major vessels) to serve as a minimal set of descriptor-based control nodes (DCN). Each node is modelled as a Gaussian primitive with trainable position, displacement, and radius, thus adapting its spatial influence to local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation then blends and propagates displacement signals from these information-rich nodes to construct a globally coherent displacement field; focusing interpolation on the top (K) neighbors reduces computational overhead while preserving local detail. By strategically anchoring nodes in high-gradient regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal in textureless areas. The framework is optimized end-to-end via a multi-term loss that enforces both keypoint consistency and intensity alignment. Experiments on the FIRE dataset show that GPO reduces the target registration error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to 0.938, substantially outperforming existing methods. The source code can be accessed via https://github.com/xintian-99/GPOreg.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2508.16859</link>
<guid>https://arxiv.org/abs/2508.16859</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, emotion reasoning, human-machine interactions, benchmark, multi-agent framework

Summary:
Multimodal large language models (MLLMs) have shown promise in understanding human emotions and behaviors. However, current research primarily focuses on enhancing emotion recognition, neglecting emotion reasoning crucial for natural human-machine interactions. A new benchmark, MTMEUR, comprising real-life video data and progressive questions, aims to advance emotion understanding and reasoning. A multi-agent framework is proposed, with specialized agents for different aspects, to enhance reasoning capabilities. Experiments with existing MLLMs and the agent-based method on the benchmark highlight significant challenges in this task. This work underscores the importance of advancing emotion reasoning in MLLMs for more effective and natural interactions between humans and machines. <br /><br />Summary: <div>
arXiv:2508.16859v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have been widely applied across various fields due to their powerful perceptual and reasoning capabilities. In the realm of psychology, these models hold promise for a deeper understanding of human emotions and behaviors. However, recent research primarily focuses on enhancing their emotion recognition abilities, leaving the substantial potential in emotion reasoning, which is crucial for improving the naturalness and effectiveness of human-machine interactions. Therefore, in this paper, we introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR) benchmark, which encompasses 1,451 video data from real-life scenarios, along with 5,101 progressive questions. These questions cover various aspects, including emotion recognition, potential causes of emotions, future action prediction, etc. Besides, we propose a multi-agent framework, where each agent specializes in a specific aspect, such as background context, character dynamics, and event details, to improve the system's reasoning capabilities. Furthermore, we conduct experiments with existing MLLMs and our agent-based method on the proposed benchmark, revealing that most models face significant challenges with this task.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta-SVD: Efficient Compression for Personalized Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.16863</link>
<guid>https://arxiv.org/abs/2508.16863</guid>
<content:encoded><![CDATA[
<div> Compression, Text-to-image models, Personalization, Delta-SVD, Singular Value Decomposition<br />
Summary:
The article introduces Delta-SVD, a compression method for personalized text-to-image models like DreamBooth that reduces storage overhead. Delta-SVD targets the weight updates in fine-tuning and leverages their low-rank structure. By applying Singular Value Decomposition and an energy-based rank truncation strategy, it efficiently compresses the models while maintaining high reconstruction fidelity. The compressed models are plug-and-play and retain the original architecture, making them easy to deploy during inference. Experiments show that Delta-SVD achieves significant compression with minimal impact on generation quality, as measured by CLIP score, SSIM, and FID. This method facilitates the scalable and efficient deployment of personalized diffusion models, addressing the challenge of storing and deploying subject-specific models in real-world applications. <br /><br />Summary: <div>
arXiv:2508.16863v1 Announce Type: new 
Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning large-scale diffusion backbones, resulting in significant storage overhead when maintaining many subject-specific models. We present Delta-SVD, a post-hoc, training-free compression method that targets the parameter weights update induced by DreamBooth fine-tuning. Our key observation is that these delta weights exhibit strong low-rank structure due to the sparse and localized nature of personalization. Delta-SVD first applies Singular Value Decomposition (SVD) to factorize the weight deltas, followed by an energy-based rank truncation strategy to balance compression efficiency and reconstruction fidelity. The resulting compressed models are fully plug-and-play and can be re-constructed on-the-fly during inference. Notably, the proposed approach is simple, efficient, and preserves the original model architecture. Experiments on a multiple subject dataset demonstrate that Delta-SVD achieves substantial compression with negligible loss in generation quality measured by CLIP score, SSIM and FID. Our method enables scalable and efficient deployment of personalized diffusion models, making it a practical solution for real-world applications that require storing and deploying large-scale subject customizations.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Multimodal LLMs See Sentiment?</title>
<link>https://arxiv.org/abs/2508.16873</link>
<guid>https://arxiv.org/abs/2508.16873</guid>
<content:encoded><![CDATA[
<div> Keywords: visual content, sentiment analysis, multimodal large language models, fine-tuning, affective computing<br />
Summary:<br />
- The paper introduces the MLLMsent framework for studying how Multimodal Large Language Models (MLLMs) reason sentiment from visual content.
- The framework explores three perspectives: direct sentiment classification from images, sentiment analysis on generated image descriptions, and fine-tuning on sentiment-labeled image descriptions.
- Experiments on a benchmark dataset show that the fine-tuned approach of MLLMsent achieves state-of-the-art performance, outperforming baselines by significant margins.
- The results demonstrate the potential of the proposed visual reasoning scheme for advancing affective computing.
- In cross-dataset testing, the model outperforms the best runner-up without any training on the new data, showcasing its robustness and effectiveness in sentiment analysis tasks.<br /><br />Summary: <div>
arXiv:2508.16873v1 Announce Type: new 
Abstract: Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-of-the-art results outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception</title>
<link>https://arxiv.org/abs/2508.16881</link>
<guid>https://arxiv.org/abs/2508.16881</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modality image fusion, adverse weather, text perception, semantic perception, AWM-Fuse <br />
Summary:<br />
The proposed method, AWM-Fuse, addresses the challenge of multi-modality image fusion in adverse weather conditions by incorporating textual information for improved semantic perception. It utilizes a global feature perception module to extract scene features and identify degradation types using BLIP-produced captions, promoting generalization across different adverse weather scenarios. The local module leverages detailed scene descriptions generated by ChatGPT to focus on specific degradation effects through textual cues, capturing finer details. By using textual descriptions to guide the fusion image generation process, the network learns to align with real semantic labels more effectively. Extensive experiments demonstrate that AWM-Fuse outperforms existing methods in complex weather conditions and downstream tasks. The code for AWM-Fuse is available on GitHub for further research and implementation. <br /> <div>
arXiv:2508.16881v1 Announce Type: new 
Abstract: Multi-modality image fusion (MMIF) in adverse weather aims to address the loss of visual information caused by weather-related degradations, providing clearer scene representations. Although less studies have attempted to incorporate textual information to improve semantic perception, they often lack effective categorization and thorough analysis of textual content. In response, we propose AWM-Fuse, a novel fusion method for adverse weather conditions, designed to handle multiple degradations through global and local text perception within a unified, shared weight architecture. In particular, a global feature perception module leverages BLIP-produced captions to extract overall scene features and identify primary degradation types, thus promoting generalization across various adverse weather conditions. Complementing this, the local module employs detailed scene descriptions produced by ChatGPT to concentrate on specific degradation effects through concrete textual cues, thereby capturing finer details. Furthermore, textual descriptions are used to constrain the generation of fusion images, effectively steering the network learning process toward better alignment with real semantic labels, thereby promoting the learning of more meaningful visual features. Extensive experiments demonstrate that AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks. Our code is available at https://github.com/Feecuin/AWM-Fuse.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism</title>
<link>https://arxiv.org/abs/2508.16884</link>
<guid>https://arxiv.org/abs/2508.16884</guid>
<content:encoded><![CDATA[
<div> Transformer, Computer Vision, SAEViT, Sparsely Aggregated Attention, Convolution Blocks <br />
Summary: <br />
SAEViT is a lightweight Vision Transformer model designed for efficient downstream vision tasks, addressing the challenges of high computational cost and weak local feature modeling in traditional ViT models. It introduces a Sparsely Aggregated Attention module that utilizes adaptive sparse sampling and deconvolution operations to reduce computational complexity. The model also incorporates a Channel-Interactive Feed-Forward Network layer to enhance inter-channel information exchange and a hierarchical pyramid structure with depth-wise separable convolutional blocks to strengthen convolutional features. Experimental results on ImageNet-1K classification task demonstrate that SAEViT achieves high accuracies with significantly lower GFLOPs, making it a practical solution for various fundamental vision tasks. <br /> <div>
arXiv:2508.16884v1 Announce Type: new 
Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its strong long-range dependency modelling ability. However, its large model size with high computational cost and weak local feature modeling ability hinder its application in real scenarios. To balance computation efficiency and performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight ViT based model with convolution blocks, in this paper to achieve efficient downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated Attention (SAA) module that performs adaptive sparse sampling based on image redundancy and recovers the feature map via deconvolution operation, which significantly reduces the computational complexity of attention operations. In addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed to enhance inter-channel information exchange through feature decomposition and redistribution, mitigating redundancy in traditional feed-forward networks (FNN). Finally, a hierarchical pyramid structure with embedded depth-wise separable convolutional blocks (DWSConv) is devised to further strengthen convolutional features. Extensive experiments on mainstream datasets show that SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively, demonstrating a lightweight solution for various fundamental vision tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration</title>
<link>https://arxiv.org/abs/2508.16887</link>
<guid>https://arxiv.org/abs/2508.16887</guid>
<content:encoded><![CDATA[
<div> Keywords: image quality assessment, deep neural network, multi-dimensional, perceptual dimensions, image restoration

Summary: 
The article introduces a new approach to image quality assessment (IQA) called multi-dimensional image quality assessment (MDIQA). Unlike existing methods that focus on overall image quality scores, the MDIQA framework considers different perceptual dimensions that humans use to evaluate image quality. By modeling image quality across various technical and aesthetic dimensions, the MDIQA captures the multifaceted nature of human visual perception. The framework consists of separate branches trained for each dimension, whose features are then combined to generate a final IQA score. Additionally, the MDIQA can be used to train image restoration (IR) models, allowing for adjustments in perceptual dimension weights to align with user preferences. Experimental results show that the MDIQA outperforms existing methods and is flexible in its application to image restoration tasks. The code for the MDIQA framework is publicly available for use. 

<br /><br />Summary: <div>
arXiv:2508.16887v1 Announce Type: new 
Abstract: Recent advancements in image quality assessment (IQA), driven by sophisticated deep neural network designs, have significantly improved the ability to approach human perceptions. However, most existing methods are obsessed with fitting the overall score, neglecting the fact that humans typically evaluate image quality from different dimensions before arriving at an overall quality assessment. To overcome this problem, we propose a multi-dimensional image quality assessment (MDIQA) framework. Specifically, we model image quality across various perceptual dimensions, including five technical and four aesthetic dimensions, to capture the multifaceted nature of human visual perception within distinct branches. Each branch of our MDIQA is initially trained under the guidance of a separate dimension, and the respective features are then amalgamated to generate the final IQA score. Additionally, when the MDIQA model is ready, we can deploy it for a flexible training of image restoration (IR) models, enabling the restoration results to better align with varying user preferences through the adjustment of perceptual dimension weights. Extensive experiments demonstrate that our MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Energy-Guided Sampling for View-Consistent Text-to-3D</title>
<link>https://arxiv.org/abs/2508.16917</link>
<guid>https://arxiv.org/abs/2508.16917</guid>
<content:encoded><![CDATA[
<div> viewpoint bias, 3D generation, Structural Energy-Guided Sampling, multi-view consistency, Janus artifacts

Summary:
Structural Energy-Guided Sampling (SEGS) addresses the issue of viewpoint bias in text-to-3D generation by enforcing multi-view consistency at sampling time. The approach utilizes a structural energy in a PCA subspace of U-Net features to guide the denoising trajectory and improve geometric alignment while maintaining appearance fidelity. SEGS is a training-free framework that can be seamlessly integrated into existing pipelines without the need for retraining or weight modification. By injecting gradients into the denoising process, SEGS significantly reduces Janus artifacts and enhances viewpoint consistency in 3D objects. This approach exemplifies a novel solution to the challenge of generating accurate 3D representations from textual inputs, offering a practical and effective method for improving the quality of generated geometries. 

<br /><br />Summary: <div>
arXiv:2508.16917v1 Announce Type: new 
Abstract: Text-to-3D generation often suffers from the Janus problem, where objects look correct from the front but collapse into duplicated or distorted geometry from other angles. We attribute this failure to viewpoint bias in 2D diffusion priors, which propagates into 3D optimization. To address this, we propose Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play framework that enforces multi-view consistency entirely at sampling time. SEGS defines a structural energy in a PCA subspace of intermediate U-Net features and injects its gradients into the denoising trajectory, steering geometry toward the intended viewpoint while preserving appearance fidelity. Integrated seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts, achieving improved geometric alignment and viewpoint consistency without retraining or weight modification.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition</title>
<link>https://arxiv.org/abs/2508.16922</link>
<guid>https://arxiv.org/abs/2508.16922</guid>
<content:encoded><![CDATA[
<div> Capsule Network, Multi-Scale Features, Feature Fusion, Routing, Classification Accuracy
Summary:
The article introduces the Multi-Scale Patchify Capsule Network (MSPCaps), which enhances visual recognition by integrating multi-scale feature learning and efficient capsule routing. The architecture includes a Multi-Scale ResNet Backbone (MSRB) for extracting diverse feature representations, a Patchify Capsule Layer (PatchifyCaps) for partitioning features into capsules, and Cross-Agreement Routing (CAR) blocks for adaptive capsule routing. By leveraging multi-scale features and enhancing routing mechanisms, MSPCaps achieves superior classification accuracy compared to baseline methods. The model's scalability and robustness are demonstrated across different configurations, from the highly efficient Tiny model to the powerful Large model, showcasing its potential for advancing feature representation learning. <div>
arXiv:2508.16922v1 Announce Type: new 
Abstract: Capsule Network (CapsNet) has demonstrated significant potential in visual recognition by capturing spatial relationships and part-whole hierarchies for learning equivariant feature representations. However, existing CapsNet and variants often rely on a single high-level feature map, overlooking the rich complementary information from multi-scale features. Furthermore, conventional feature fusion strategies (e.g., addition and concatenation) struggle to reconcile multi-scale feature discrepancies, leading to suboptimal classification performance. To address these limitations, we propose the Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that integrates multi-scale feature learning and efficient capsule routing. Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature representations from input images, preserving both fine-grained details and global contextual information. Second, the PatchifyCaps partitions these multi-scale features into primary capsules using a uniform patch size, equipping the model with the ability to learn from diverse receptive fields. Finally, the CAR block adaptively routes the multi-scale capsules by identifying cross-scale prediction pairs with maximum agreement. Unlike the simple concatenation of multiple self-routing blocks, CAR ensures that only the most coherent capsules contribute to the final voting. Our proposed MSPCaps achieves remarkable scalability and superior robustness, consistently surpassing multiple baseline methods in terms of classification accuracy, with configurations ranging from a highly efficient Tiny model (344.3K parameters) to a powerful Large model (10.9M parameters), highlighting its potential in advancing feature representation learning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR</title>
<link>https://arxiv.org/abs/2508.16927</link>
<guid>https://arxiv.org/abs/2508.16927</guid>
<content:encoded><![CDATA[
<div> framework, gadolinium-free, cardiomyopathy screening, cine CMR, feature interaction module <br />
Summary: <br />
The article introduces CC-CMR, a framework for gadolinium-free cardiomyopathy screening using cine CMR sequences. The model aligns the latent spaces of cine CMR and Late Gadolinium Enhancement (LGE) sequences to encode fibrosis-specific pathology into cine CMR embeddings. A Feature Interaction Module optimizes diagnostic precision and feature congruence. An uncertainty-guided adaptive training mechanism calibrates objectives for model generalizability. Evaluated on 231 subjects, CC-CMR achieves 94.3% accuracy, surpassing cine-CMR-only models by 4.3% without needing gadolinium contrast. The framework demonstrates clinical viability for various populations and healthcare settings.  <div>
arXiv:2508.16927v1 Announce Type: new 
Abstract: Cardiomyopathy, a principal contributor to heart failure and sudden cardiac mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR), recognized as the diagnostic 'gold standard' through multiparametric protocols, holds the potential to serve as an accurate screening tool. However, its reliance on gadolinium contrast and labor-intensive interpretation hinders population-scale deployment. We propose CC-CMR, a Contrastive Learning and Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening using cine CMR sequences. By aligning the latent spaces of cine CMR and Late Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific pathology into cine CMR embeddings. A Feature Interaction Module concurrently optimizes diagnostic precision and cross-modal feature congruence, augmented by an uncertainty-guided adaptive training mechanism that dynamically calibrates task-specific objectives to ensure model generalizability. Evaluated on multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI: 0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while eliminating gadolinium dependency, demonstrating its clinical viability for wide range of populations and healthcare environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align 3D Representation and Text Embedding for 3D Content Personalization</title>
<link>https://arxiv.org/abs/2508.16932</link>
<guid>https://arxiv.org/abs/2508.16932</guid>
<content:encoded><![CDATA[
<div> Keywords: NeRF, 3DGS, Invert3D, 3D content personalization, vision-language models

Summary:
Invert3D introduces a novel framework for 3D content personalization that overcomes the limitations of current approaches. By aligning 3D representations with text embedding spaces, the framework allows for efficient manipulation of 3D content through natural language prompts. Unlike knowledge distillation-based methods, Invert3D eliminates the need for computationally expensive retraining procedures, making personalization more convenient. The proposed camera-conditioned 3D-to-text inverse mechanism enables effective personalization of 3D content without structural differences hindering the process. Extensive experiments demonstrate the effectiveness of Invert3D in personalizing 3D content. The framework bridges the gap between 3D content and 2D images, providing a seamless way to personalize 3D content through aligned vision-text embedding spaces. The code for Invert3D is available at the provided GitHub repository.<br /><br />Summary: <div>
arXiv:2508.16932v1 Announce Type: new 
Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency and quality of 3D content synthesis. However, efficient personalization of generated 3D content remains a critical challenge. Current 3D personalization approaches predominantly rely on knowledge distillation-based methods, which require computationally expensive retraining procedures. To address this challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D content personalization. Nowadays, vision-language models such as CLIP enable direct image personalization through aligned vision-text embedding spaces. However, the inherent structural differences between 3D content and 2D images preclude direct application of these techniques to 3D personalization. Our approach bridges this gap by establishing alignment between 3D representations and text embedding spaces. Specifically, we develop a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings. This alignment enables efficient manipulation and personalization of 3D content through natural language prompts, eliminating the need for computationally retraining procedures. Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content. Our work is available at: https://github.com/qsong2001/Invert3D.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2508.16934</link>
<guid>https://arxiv.org/abs/2508.16934</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, cerebral vasculature segmentation, hyperspectral brain images, unsupervised domain adaptation, label scarcity <br />
Summary: <br />
- The work introduces a novel deep learning framework for segmenting cerebral vasculature in hyperspectral brain images.
- The framework addresses the challenge of severe label scarcity in conventional supervised training by utilizing unsupervised domain adaptation.
- This approach combines a small, expert-annotated ground truth with unlabeled data to improve segmentation accuracy.
- Quantitative and qualitative evaluations show that the proposed method surpasses existing state-of-the-art techniques in biomedical imaging tasks.
- The study emphasizes the efficacy of domain adaptation for label-scarce scenarios, highlighting its potential for enhancing segmentation accuracy in medical imaging. <br /> 
 <div>
arXiv:2508.16934v1 Announce Type: new 
Abstract: This work presents a novel deep learning framework for segmenting cerebral vasculature in hyperspectral brain images. We address the critical challenge of severe label scarcity, which impedes conventional supervised training. Our approach utilizes a novel unsupervised domain adaptation methodology, using a small, expert-annotated ground truth alongside unlabeled data. Quantitative and qualitative evaluations confirm that our method significantly outperforms existing state-of-the-art approaches, demonstrating the efficacy of domain adaptation for label-scarce biomedical imaging tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability</title>
<link>https://arxiv.org/abs/2508.16937</link>
<guid>https://arxiv.org/abs/2508.16937</guid>
<content:encoded><![CDATA[
<div> adversarial perturbations, transferability, Neuron Attack for Transferability, ImageNet models, fooling rates <br />
Summary: <br />
The article introduces Neuron Attack for Transferability (NAT), a method aimed at generating transferable adversarial perturbations by targeting specific neurons within a neural network's embedding. Unlike previous approaches that focused on layer-level optimizations, NAT hones in on individual neurons to disrupt the core units of the network, resulting in enhanced transferability across different models. Extensive experiments on ImageNet and fine-grained models demonstrate NAT's superior fooling rates in both cross-model and cross-domain scenarios. Leveraging the attacking capabilities of trained generators, NAT achieves impressive fooling rates with minimal queries, outperforming existing baselines. The code for NAT is publicly available for further exploration and application. <br /> <div>
arXiv:2508.16937v1 Announce Type: new 
Abstract: The generation of transferable adversarial perturbations typically involves training a generator to maximize embedding separation between clean and adversarial images at a single mid-layer of a source model. In this work, we build on this approach and introduce Neuron Attack for Transferability (NAT), a method designed to target specific neuron within the embedding. Our approach is motivated by the observation that previous layer-level optimizations often disproportionately focus on a few neurons representing similar concepts, leaving other neurons within the attacked layer minimally affected. NAT shifts the focus from embedding-level separation to a more fundamental, neuron-specific approach. We find that targeting individual neurons effectively disrupts the core units of the neural network, providing a common basis for transferability across different models. Through extensive experiments on 41 diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates that surpass existing baselines by over 14\% in cross-model and 4\% in cross-domain settings. Furthermore, by leveraging the complementary attacking capabilities of the trained generators, we achieve impressive fooling rates within just 10 queries. Our code is available at: https://krishnakanthnakka.github.io/NAT/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis</title>
<link>https://arxiv.org/abs/2508.16942</link>
<guid>https://arxiv.org/abs/2508.16942</guid>
<content:encoded><![CDATA[
<div> vision-language model, action assessment, stepwise action reasoning, hierarchical policy learning, reinforcement learning

Summary: 
HieroAction is a vision-language model designed for evaluating human actions with detailed feedback. It introduces Stepwise Action Reasoning, a tailored chain of thought process that guides the model to assess actions step by step for enhanced interpretability. The model also incorporates Hierarchical Policy Learning, a reinforcement learning strategy that learns sub action dynamics to improve scoring precision. By structuring the evaluation process and refining each stage through reward-based optimization, HieroAction achieves accurate and interpretable assessments. The integration of these two key ideas results in superior performance across multiple benchmark datasets. The code for HieroAction will be released upon acceptance. <br /><br />Summary: <div>
arXiv:2508.16942v1 Announce Type: new 
Abstract: Evaluating human actions with clear and detailed feedback is important in areas such as sports, healthcare, and robotics, where decisions rely not only on final outcomes but also on interpretable reasoning. However, most existing methods provide only a final score without explanation or detailed analysis, limiting their practical applicability. To address this, we introduce HieroAction, a vision-language model that delivers accurate and structured assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise Action Reasoning, a tailored chain of thought process designed specifically for action assessment, which guides the model to evaluate actions step by step, from overall recognition through sub action analysis to final scoring, thus enhancing interpretability and structured understanding; and (2) Hierarchical Policy Learning, a reinforcement learning strategy that enables the model to learn fine grained sub action dynamics and align them with high level action quality, thereby improving scoring precision. The reasoning pathway structures the evaluation process, while policy learning refines each stage through reward based optimization. Their integration ensures accurate and interpretable assessments, as demonstrated by superior performance across multiple benchmark datasets. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze</title>
<link>https://arxiv.org/abs/2508.16956</link>
<guid>https://arxiv.org/abs/2508.16956</guid>
<content:encoded><![CDATA[
<div> Keywords: Single-image dehazing, Physics-guided Intermediate State Targeting, Haze-Aware Denoising, Diffusion model, Transmission map cross-attention 

Summary: 
- The research proposes a novel Region-adaptive Physics-guided Dehazing Diffusion Model (RPD-Diff) for enhancing visibility in dense and non-uniform haze conditions.
- RPD-Diff utilizes a Physics-guided Intermediate State Targeting (PIST) strategy to improve conditioning in dense haze scenarios by reformulating the diffusion Markov chain with generation target transitions.
- The Haze-Aware Denoising Timestep Predictor (HADTP) dynamically adjusts denoising timesteps using a transmission map cross-attention mechanism to address spatially varying haze distributions.
- Extensive experiments on real-world datasets demonstrate that RPD-Diff outperforms existing methods in restoring high-quality, haze-free images with enhanced detail clarity and color fidelity.
- The proposed model achieves state-of-the-art performance, showcasing its robustness and adaptability in tackling challenging haze scenarios. 

<br /><br />Summary: <div>
arXiv:2508.16956v1 Announce Type: new 
Abstract: Single-image dehazing under dense and non-uniform haze conditions remains challenging due to severe information degradation and spatial heterogeneity. Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack of adaptability to spatially varying haze distributions, which leads to suboptimal restoration. To address these limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing Diffusion Model for robust visibility enhancement in complex haze scenarios. RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST) strategy, which leverages physical priors to reformulate the diffusion Markov chain by generation target transitions, mitigating the issue of insufficient conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising timesteps employing a transmission map cross-attention mechanism, adeptly managing non-uniform haze distributions. Extensive experiments across four real-world datasets demonstrate that RPD-Diff achieves state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality, haze-free images with superior detail clarity and color fidelity.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Information Matters: A Rethink of Crowd Counting</title>
<link>https://arxiv.org/abs/2508.16970</link>
<guid>https://arxiv.org/abs/2508.16970</guid>
<content:encoded><![CDATA[
<div> Keywords: crowd counting, local modeling, window partitioning, contrastive learning, global attention<br />
<br />
Summary: <br />
This paper introduces a new crowd counting model called Local Information Matters Model (LIMM), which focuses on enhancing the local modeling capability of the model. The model utilizes a window partitioning design with grid windows for the input and employs window-wise contrastive learning to distinguish between local density levels. Additionally, a global attention module is incorporated to handle large-sized individuals. Experimental results on various datasets demonstrate that LIMM significantly improves local modeling performance while also maintaining the ability to count large-sized individuals, achieving state-of-the-art results. The proposed model outperforms existing methods, especially on high-density subsets like JHU-Crowd++. The code for LIMM is available on GitHub for further exploration and development. <br /> 
<br /> <div>
arXiv:2508.16970v1 Announce Type: new 
Abstract: The motivation of this paper originates from rethinking an essential characteristic of crowd counting: individuals (heads of humans) in the crowd counting task typically occupy a very small portion of the image. This characteristic has never been the focus of existing works: they typically use the same backbone as other visual tasks and pursue a large receptive field. This drives us to propose a new model design principle of crowd counting: emphasizing local modeling capability of the model. We follow the principle and design a crowd counting model named Local Information Matters Model (LIMM). The main innovation lies in two strategies: a window partitioning design that applies grid windows to the model input, and a window-wise contrastive learning design to enhance the model's ability to distinguish between local density levels. Moreover, a global attention module is applied to the end of the model to handle the occasionally occurring large-sized individuals. Extensive experiments on multiple public datasets illustrate that the proposed model shows a significant improvement in local modeling capability (8.7\% in MAE on the JHU-Crowd++ high-density subset for example), without compromising its ability to count large-sized ones, which achieves state-of-the-art performance. Code is available at: https://github.com/tianhangpan/LIMM.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams</title>
<link>https://arxiv.org/abs/2508.16972</link>
<guid>https://arxiv.org/abs/2508.16972</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multimodal Variants, Robust Diagram Reasoning, Perturbation Robustness Score, Scientific Diagrams <br />
Summary: <br />
- Large Language Models (LLMs) and their multimodal variants (LVLMs) show promise for scientific applications, but lack robustness to visual perturbations.
- The Robust Diagram Reasoning (RDR) framework enhances LVLM performance on visually degraded scientific diagrams.
- RDR includes an Adaptive Multi-View & Consistency Verification (AMCV) mechanism for generating perturbed versions of diagrams and self-correction.
- New metrics, Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC), quantify LVLM robustness under visual perturbations.
- The SciDiagram-Robust dataset is created for large-scale scientific diagram question-answering with diverse visual perturbations.
- Even advanced LVLMs like GPT-4V experience performance degradation when presented with perturbed inputs, highlighting a need for improved robust reasoning capabilities. <br /> <div>
arXiv:2508.16972v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and their multimodal variants (LVLMs) hold immense promise for scientific and engineering applications, particularly in processing visual information like scientific diagrams. However, their practical deployment is hindered by a critical lack of robustness to common visual perturbations such as noise, blur, and occlusions, which are prevalent in real-world scientific documents. Existing evaluation benchmarks largely overlook this challenge, leaving the robust reasoning capabilities of LVLMs on visually degraded scientific diagrams underexplored. To address this, we introduce the Robust Diagram Reasoning (RDR) framework, a novel approach designed to enhance and rigorously evaluate LVLMs' performance under such conditions. At its core, RDR employs an Adaptive Multi-View & Consistency Verification (AMCV) mechanism, which involves generating multiple perturbed versions of a diagram, performing parallel inference, and then applying a consistency-based self-correction loop. We also propose two new metrics, Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC), to quantify robustness. Furthermore, we construct SciDiagram-Robust, the first large-scale scientific diagram question-answering dataset specifically augmented with diverse, programmatically generated visual perturbations. Our extensive experiments demonstrate that even state-of-the-art closed-source LVLMs like GPT-4V exhibit significant performance degradation when faced with perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balanced Sharpness-Aware Minimization for Imbalanced Regression</title>
<link>https://arxiv.org/abs/2508.16973</link>
<guid>https://arxiv.org/abs/2508.16973</guid>
<content:encoded><![CDATA[
<div> Keywords: regression, imbalanced data, generalization ability, sharpness-aware minimization, vision tasks

Summary:
Regression is a key aspect of computer vision tasks, but it often struggles with imbalanced data distributions. This paper reframes the imbalanced regression problem as an imbalanced generalization challenge. The proposed approach, Balanced Sharpness-Aware Minimization (BSAM), focuses on improving the generalization ability of regression models throughout the observation space. By incorporating a novel targeted reweighting strategy, BSAM ensures uniform performance across all target observations, leading to better overall model performance. Experimental results on various vision regression tasks such as age and depth estimation showcase the superiority of the BSAM method compared to existing approaches. The code for the BSAM method is also available for further exploration and implementation. <div>
arXiv:2508.16973v1 Announce Type: new 
Abstract: Regression is fundamental in computer vision and is widely used in various tasks including age estimation, depth estimation, target localization, \etc However, real-world data often exhibits imbalanced distribution, making regression models perform poorly especially for target values with rare observations~(known as the imbalanced regression problem). In this paper, we reframe imbalanced regression as an imbalanced generalization problem. To tackle that, we look into the loss sharpness property for measuring the generalization ability of regression models in the observation space. Namely, given a certain perturbation on the model parameters, we check how model performance changes according to the loss values of different target observations. We propose a simple yet effective approach called Balanced Sharpness-Aware Minimization~(BSAM) to enforce the uniform generalization ability of regression models for the entire observation space. In particular, we start from the traditional sharpness-aware minimization and then introduce a novel targeted reweighting strategy to homogenize the generalization ability across the observation space, which guarantees a theoretical generalization bound. Extensive experiments on multiple vision regression tasks, including age and depth estimation, demonstrate that our BSAM method consistently outperforms existing approaches. The code is available \href{https://github.com/manmanjun/BSAM_for_Imbalanced_Regression}{here}.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding</title>
<link>https://arxiv.org/abs/2508.16974</link>
<guid>https://arxiv.org/abs/2508.16974</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vision-Language Large Models, fine-grained visual reasoning, robustness, hierarchical design <br />
Summary:<br />
The Hierarchical Contextual Grounding LVLM (HCG-LVLM) is proposed to improve the robustness and precision of Vision-Language Large Models (LVLMs). The model mimics human cognitive processing with a Global Contextual Perception layer for broad understanding and a Fine-grained Local Grounding layer for detailed visual reasoning. It incorporates a Local Detail Enhancement Module and Semantic Consistency Validator to ensure accurate visual-language alignment. By integrating information from both layers, the HCG-LVLM outperforms state-of-the-art models on challenging datasets such as GQA and RefCOCO. The model reduces hallucination and enhances fine-grained visual-language understanding, showcasing the effectiveness of its hierarchical design. <br /> <div>
arXiv:2508.16974v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have achieved remarkable progress in natural language processing and multimodal understanding. Despite their impressive generalization capabilities, current LVLMs often exhibit insufficient robustness, proneness to hallucination, and reasoning errors in complex real-world scenarios, particularly when precise image region localization and fine-grained visual reasoning are required. To address these limitations, we propose the Hierarchical Contextual Grounding LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine cognitive processing. HCG-LVLM employs a two-layered approach: a Global Contextual Perception layer for initial broad understanding and a Fine-grained Local Grounding layer. The latter incorporates a Local Detail Enhancement Module to extract high-resolution features and a Semantic Consistency Validator to ensure accurate, hallucination-free visual-language alignment. Through an adaptive fusion mechanism, information from both layers is integrated for robust and precise outputs. Extensive experiments on challenging datasets, including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model achieves superior accuracy and significantly reduces hallucination, validating the effectiveness of its hierarchical design in enhancing fine-grained visual-language understanding and precise grounding capabilities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Digitally Altered Images: Deepfake Detection</title>
<link>https://arxiv.org/abs/2508.16975</link>
<guid>https://arxiv.org/abs/2508.16975</guid>
<content:encoded><![CDATA[
<div> Vision Transformer, Deepfake detection, OpenForensics Dataset, augmentation techniques, class imbalance handling
<br />
Summary: 
A robust Deepfake detection model based on a modified Vision Transformer(ViT) has been developed. Trained on a subset of the OpenForensics Dataset using various augmentation techniques, the model effectively distinguishes between real and Deepfake images. Class imbalance issues are addressed through oversampling and a stratified train-validation split of the dataset. The model achieves state-of-the-art results in detecting Deepfake images, as demonstrated through accuracy metrics on training and testing datasets. Additionally, the model's performance is evaluated using a prediction score on random images of people, regardless of their authenticity. The study highlights the significance of combating Deepfake technology and presents a reliable solution that shows promise in accurately detecting manipulated images. 
<br /> <div>
arXiv:2508.16975v1 Announce Type: new 
Abstract: The rise of Deepfake technology to generate hyper-realistic manipulated images and videos poses a significant challenge to the public and relevant authorities. This study presents a robust Deepfake detection based on a modified Vision Transformer(ViT) model, trained to distinguish between real and Deepfake images. The model has been trained on a subset of the OpenForensics Dataset with multiple augmentation techniques to increase robustness for diverse image manipulations. The class imbalance issues are handled by oversampling and a train-validation split of the dataset in a stratified manner. Performance is evaluated using the accuracy metric on the training and testing datasets, followed by a prediction score on a random image of people, irrespective of their realness. The model demonstrates state-of-the-art results on the test dataset to meticulously detect Deepfake images.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection</title>
<link>https://arxiv.org/abs/2508.16976</link>
<guid>https://arxiv.org/abs/2508.16976</guid>
<content:encoded><![CDATA[
<div> Domain generalization, parameter-efficient adaptation, Joint Parameter Selection, sparsity, pre-trained models<br />
<br />
Summary:<br />
- Domain generalization aims to develop models that can generalize to unseen domains.<br />
- Full fine-tuning of pre-trained models can compromise their generalization capabilities.<br />
- Joint Parameter Selection (JPS) method selectively fine-tunes a small subset of parameters to balance task adaptation and generalization.<br />
- Theoretical analysis establishes a generalization error bound considering the sparsity of parameter updates.<br />
- JPS outperforms existing domain generalization methods in benchmark experiments, demonstrating its efficiency and efficacy. <br /> <div>
arXiv:2508.16976v1 Announce Type: new 
Abstract: Domain generalization seeks to develop models trained on a limited set of source domains that are capable of generalizing effectively to unseen target domains. While the predominant approach leverages large-scale pre-trained vision models as initialization, recent studies have highlighted that full fine-tuning can compromise the intrinsic generalization capabilities of these models. To address this limitation, parameter-efficient adaptation strategies have emerged, wherein only a subset of model parameters is selectively fine-tuned, thereby balancing task adaptation with the preservation of generalization. Motivated by this paradigm, we introduce Joint Parameter Selection (JPS), a novel method that restricts updates to a small, sparse subset of parameters, thereby retaining and harnessing the generalization strength of pre-trained models. Theoretically, we establish a generalization error bound that explicitly accounts for the sparsity of parameter updates, thereby providing a principled justification for selective fine-tuning. Practically, we design a selection mechanism employing dual operators to identify and update parameters exhibiting consistent and significant gradients across all source domains. Extensive benchmark experiments demonstrate that JPS achieves superior performance compared to state-of-the-art domain generalization methods, substantiating both the efficiency and efficacy of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching</title>
<link>https://arxiv.org/abs/2508.16984</link>
<guid>https://arxiv.org/abs/2508.16984</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, feature caching, HiCache, Hermite polynomials, speedup

Summary:<br /><br />Diffusion models have shown success in content generation but face high computational costs. Feature caching methods have been used to accelerate inference but often lead to server quality loss. This paper introduces HiCache, a training-free framework that enhances feature prediction by utilizing Hermite polynomials to model feature evolution accurately. The approach leverages the Gaussian characteristics of feature derivative approximations in Diffusion Transformers, leading to improved predictive accuracy. Additionally, a dual-scaling mechanism ensures numerical stability while maintaining high-quality predictions. Experimental results demonstrate HiCache's superiority by achieving a 6.24x speedup on FLUX.1-dev while surpassing baseline quality. The framework performs well across various tasks such as text-to-image, video generation, and super-resolution. The core implementation is provided in the appendix, with full code availability upon acceptance. <div>
arXiv:2508.16984v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation</title>
<link>https://arxiv.org/abs/2508.17007</link>
<guid>https://arxiv.org/abs/2508.17007</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image analysis, deep learning, segmentation, computational efficiency, model robustness

Summary:
The paper introduces an efficient dual-line decoder segmentation network (EDLDNet) for accurate and computationally efficient organ segmentation in medical imaging. The method incorporates a noisy decoder during training to improve model robustness, while using a noise-free decoder during inference to reduce computational cost. Multi-Scale Convolutional Attention Modules, Attention Gates, and Up-Convolution Blocks are employed to enhance feature representation and segmentation performance. Additionally, a mutation-based loss function is utilized to improve model generalization. EDLDNet outperforms state-of-the-art segmentation architectures on multiple medical imaging datasets, achieving a Dice score of 84.00% on the Synapse dataset and reducing Multiply-Accumulate Operations by 89.7%. The model demonstrates superior performance, computational efficiency, and robustness across diverse datasets, surpassing baseline models like UNet and recent approaches like EMCAD. The source code, pre-processed data, and pre-trained weights are available on GitHub for further research and application. 

<br /><br />Summary: <div>
arXiv:2508.17007v1 Announce Type: new 
Abstract: Proper segmentation of organs-at-risk is important for radiation therapy, surgical planning, and diagnostic decision-making in medical image analysis. While deep learning-based segmentation architectures have made significant progress, they often fail to balance segmentation accuracy with computational efficiency. Most of the current state-of-the-art methods either prioritize performance at the cost of high computational complexity or compromise accuracy for efficiency. This paper addresses this gap by introducing an efficient dual-line decoder segmentation network (EDLDNet). The proposed method features a noisy decoder, which learns to incorporate structured perturbation at training time for better model robustness, yet at inference time only the noise-free decoder is executed, leading to lower computational cost. Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs), and Up-Convolution Blocks (UCBs) are further utilized to optimize feature representation and boost segmentation performance. By leveraging multi-scale segmentation masks from both decoders, we also utilize a mutation-based loss function to enhance the model's generalization. Our approach outperforms SOTA segmentation architectures on four publicly available medical imaging datasets. EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse dataset, surpassing baseline model like UNet by 13.89% in Dice score while significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice score but also maintains comparable computational efficiency. The outstanding performance across diverse datasets establishes EDLDNet's strong generalization, computational efficiency, and robustness. The source code, pre-processed data, and pre-trained weights will be available at https://github.com/riadhassan/EDLDNet .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2508.17009</link>
<guid>https://arxiv.org/abs/2508.17009</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly Supervised Semantic Segmentation, Image-level labels, Contrastive Prompt Clustering, Large Language Models, Fine-grained discrimination

Summary: 
Contrastive Prompt Clustering (CPC) is introduced as a novel Weakly Supervised Semantic Segmentation (WSSS) framework. It leverages Large Language Models (LLMs) to derive category clusters that encapsulate inter-class relationships. The framework incorporates a class-aware patch-level contrastive loss to ensure both intra-class consistency and inter-class separation. By utilizing clusters as coarse-grained semantic priors while preserving fine-grained boundaries, CPC reduces confusion among visually similar categories. Experimental results on benchmark datasets such as PASCAL VOC 2012 and MS COCO 2014 showcase the superiority of CPC over existing state-of-the-art methods in weakly supervised semantic segmentation. <div>
arXiv:2508.17009v1 Announce Type: new 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fiducial Marker Splatting for High-Fidelity Robotics Simulations</title>
<link>https://arxiv.org/abs/2508.17012</link>
<guid>https://arxiv.org/abs/2508.17012</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D simulation, mobile robots, neural rendering, fiducial markers, greenhouse environment<br />
Summary:<br />
The article introduces a novel hybrid framework for high-fidelity 3D simulation to train mobile robots in complex environments such as densely packed greenhouses. Traditional mesh-based representations struggle in such settings, prompting the use of neural rendering methods like Gaussian Splatting (GS). However, existing methods lack the flexibility to incorporate fiducial markers crucial for robotic localization and control. The proposed framework combines the photorealism of GS with structured marker representations, offering a solution to this limitation. A novel algorithm efficiently generates GS-based fiducial markers within cluttered scenes, outperforming traditional image-fitting techniques in efficiency and pose-estimation accuracy. Experiments demonstrate the framework's potential in greenhouse simulations, a challenging testbed due to dense foliage, similar-looking elements, and occlusions, showcasing the framework's value for real-world applications.<br /><br />Summary: <div>
arXiv:2508.17012v1 Announce Type: new 
Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation</title>
<link>https://arxiv.org/abs/2508.17017</link>
<guid>https://arxiv.org/abs/2508.17017</guid>
<content:encoded><![CDATA[
<div> Diffusion-based Handwritten Text Generation, Dual Orthogonal Guidance, artifacts, style variability, generation clarity
<br />
Summary: 
The article introduces a novel sampling guidance strategy called Dual Orthogonal Guidance (DOG) to improve Diffusion-based Handwritten Text Generation (HTG) models. These models often struggle with memorizing training samples and producing artifacts or distortions in the generated text, especially for challenging writing styles. DOG leverages orthogonal projection to steer generation away from artifacts while maintaining content, leading to more diverse yet plausible outputs. Unlike standard guidance approaches, DOG provides stable, disentangled directions in the latent space, enhancing both content clarity and style variability. The technique applies a triangular schedule to control guidance strength throughout the denoising process, resulting in improved performance on state-of-the-art models like DiffusionPen and One-DM, even for out-of-vocabulary words and challenging writing styles.
<br /><br /> <div>
arXiv:2508.17017v1 Announce Type: new 
Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve impressive results on frequent, in-vocabulary words observed at training time and on regular styles. However, they are prone to memorizing training samples and often struggle with style variability and generation clarity. In particular, standard diffusion models tend to produce artifacts or distortions that negatively affect the readability of the generated text, especially when the style is hard to produce. To tackle these issues, we propose a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an orthogonal projection of a negatively perturbed prompt onto the original positive prompt. This approach helps steer the generation away from artifacts while maintaining the intended content, and encourages more diverse, yet plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which relies on unconditional predictions and produces noise at high guidance scales, DOG introduces a more stable, disentangled direction in the latent space. To control the strength of the guidance across the denoising process, we apply a triangular schedule: weak at the start and end of denoising, when the process is most sensitive, and strongest in the middle steps. Experimental results on the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Temporal Masked Attention for Cross-view Online Action Detection</title>
<link>https://arxiv.org/abs/2508.17025</link>
<guid>https://arxiv.org/abs/2508.17025</guid>
<content:encoded><![CDATA[
<div> Probabilistic Temporal Masked Attention, video sequence classification, Online Action Detection, cross-view setting, latent compressed representations<br />
Summary:<br />
The article introduces a novel Probabilistic Temporal Masked Attention (PTMA) model for Online Action Detection (OAD) in video sequences. The model incorporates a GRU-based temporal masked attention (TMA) cell to query input video sequences, enhancing information interaction for autoregressive frame-level analysis. It leverages probabilistic modeling to derive latent compressed representations of video frames in a cross-view setting, facilitating the extraction of view-invariant features. Experimental results on DAHLIA, IKEA ASM, and Breakfast datasets show that PTMA achieves state-of-the-art performance under cross-subject (cs), cross-view (cv), and cross-subject-view (csv) evaluation protocols. <div>
arXiv:2508.17025v1 Announce Type: new 
Abstract: As a critical task in video sequence classification within computer vision, Online Action Detection (OAD) has garnered significant attention. The sensitivity of mainstream OAD models to varying video viewpoints often hampers their generalization when confronted with unseen sources. To address this limitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA) model, which leverages probabilistic modeling to derive latent compressed representations of video frames in a cross-view setting. The PTMA model incorporates a GRU-based temporal masked attention (TMA) cell, which leverages these representations to effectively query the input video sequence, thereby enhancing information interaction and facilitating autoregressive frame-level video analysis. Additionally, multi-view information can be integrated into the probabilistic modeling to facilitate the extraction of view-invariant features. Experiments conducted under three evaluation protocols: cross-subject (cs), cross-view (cv), and cross-subject-view (csv) show that PTMA achieves state-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Local Focusing Mechanism for Deepfake Detection Generalization</title>
<link>https://arxiv.org/abs/2508.17029</link>
<guid>https://arxiv.org/abs/2508.17029</guid>
<content:encoded><![CDATA[
<div> face, car, deep learning, deepfake, detection

Summary: 
The article introduces a novel Local Focus Mechanism (LFM) for cross-domain deepfake detection. It addresses the limitations of existing methods by focusing on discriminative local features using a Salience Network (SNet) and Top-K Pooling (TKP) module. To enhance model robustness, Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS) regularization techniques are introduced. LFM outperforms the state-of-the-art Neighboring Pixel Relationships (NPR) method in accuracy and average precision, while maintaining high efficiency at 1789 FPS on a single NVIDIA A6000 GPU. The proposed approach sets a new benchmark for deepfake detection across different object categories and generation domains. The source code is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.17029v1 Announce Type: new 
Abstract: The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in https://github.com/lmlpy/LFM.git
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search</title>
<link>https://arxiv.org/abs/2508.17037</link>
<guid>https://arxiv.org/abs/2508.17037</guid>
<content:encoded><![CDATA[
<div> image-text matching, retrieval, food, feature fusion, VLM-guided framework

Summary: 
The article introduces a novel framework called F4-ITS for food image-to-text matching, essential for dietary monitoring and smart kitchen applications. F4-ITS utilizes a uni-directional and bi-directional multi-modal fusion strategy to combine image embeddings with VLM-generated textual descriptions, enhancing query expressiveness. Additionally, a feature-based re-ranking mechanism leveraging predicted food ingredients refines results for improved precision. The framework outperforms standard baselines with ~10% and ~7.7% improvements in top-1 retrieval under dense and sparse captions and a substantial ~28.6% gain in top-k ingredient-level retrieval. Interestingly, smaller models like ViT-B/32 can match or surpass larger counterparts through textual fusion, showcasing efficacy in resource-constrained settings. The code and test datasets will be openly available, aiding further research and development in the field. <br /><br />Summary: <div>
arXiv:2508.17037v1 Announce Type: new 
Abstract: The proliferation of digital food content has intensified the need for robust and accurate systems capable of fine-grained visual understanding and retrieval. In this work, we address the challenging task of food image-to-text matching, a critical component in applications such as dietary monitoring, smart kitchens, and restaurant automation. We propose F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search, a training-free, vision-language model (VLM)-guided framework that significantly improves retrieval performance through enhanced multi-modal feature representations. Our approach introduces two key contributions: (1) a uni-directional(and bi-directional) multi-modal fusion strategy that combines image embeddings with VLM-generated textual descriptions to improve query expressiveness, and (2) a novel feature-based re-ranking mechanism for top-k retrieval, leveraging predicted food ingredients to refine results and boost precision. Leveraging open-source image-text encoders, we demonstrate substantial gains over standard baselines - achieving ~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption scenarios, and a ~28.6% gain in top-k ingredient-level retrieval. Additionally, we show that smaller models (e.g., ViT-B/32) can match or outperform larger counterparts (e.g., ViT-H, ViT-G, ViT-bigG) when augmented with textual fusion, highlighting the effectiveness of our method in resource-constrained settings. Code and test datasets will be made publicly available at: https://github.com/mailcorahul/f4-its
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments</title>
<link>https://arxiv.org/abs/2508.17044</link>
<guid>https://arxiv.org/abs/2508.17044</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D mapping, dynamic environments, multimodal data, M3DMap, object-aware construction<br />Summary: In the realm of robotics and autonomous transportation, the challenge of mapping dynamic 3D environments with diverse data sources is addressed. A taxonomy of methods for creating multimodal 3D maps is proposed, categorizing approaches based on scene types, learning methods, and practical applications. An original modular method, M3DMap, is introduced for constructing object-aware multimodal 3D maps in both static and dynamic scenes. M3DMap comprises modules for object segmentation and tracking, odometry estimation, 3D map construction, and multimodal data retrieval. The article demonstrates the efficacy of these modules in tasks such as 3D object grounding and mobile manipulation. The incorporation of multimodal data and modern models is shown to enhance the performance of 3D mapping techniques. Detailed information on the taxonomy and method implementation can be found at https://yuddim.github.io/M3DMap.<br /><br />Summary: <div>
arXiv:2508.17044v1 Announce Type: new 
Abstract: 3D mapping in dynamic environments poses a challenge for modern researchers in robotics and autonomous transportation. There are no universal representations for dynamic 3D scenes that incorporate multimodal data such as images, point clouds, and text. This article takes a step toward solving this problem. It proposes a taxonomy of methods for constructing multimodal 3D maps, classifying contemporary approaches based on scene types and representations, learning methods, and practical applications. Using this taxonomy, a brief structured analysis of recent methods is provided. The article also describes an original modular method called M3DMap, designed for object-aware construction of multimodal 3D maps for both static and dynamic scenes. It consists of several interconnected components: a neural multimodal object segmentation and tracking module; an odometry estimation module, including trainable algorithms; a module for 3D map construction and updating with various implementations depending on the desired scene representation; and a multimodal data retrieval module. The article highlights original implementations of these modules and their advantages in solving various practical tasks, from 3D object grounding to mobile manipulation. Additionally, it presents theoretical propositions demonstrating the positive effect of using multimodal data and modern foundational models in 3D mapping methods. Details of the taxonomy and method implementation are available at https://yuddim.github.io/M3DMap.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Styleclone: Face Stylization with Diffusion Based Data Augmentation</title>
<link>https://arxiv.org/abs/2508.17045</link>
<guid>https://arxiv.org/abs/2508.17045</guid>
<content:encoded><![CDATA[
<div> StyleClone, image-to-image translation, stylize faces, style images, diffusion-based guided image generation <br />
Summary: <br />
StyleClone is a method for training image-to-image translation networks to stylize faces in specific styles, even with limited style images. It uses textual inversion and diffusion-based guided image generation to augment small style datasets, enhancing dataset diversity. By generating diverse style samples guided by original style images and real face images, it improves stylization quality, preserves source image content better, and accelerates inference. The method outperforms diffusion-based methods in speed and quality, demonstrating significant improvements in stylization performance. A systematic evaluation of augmentation techniques shows their positive impact on stylization quality. <div>
arXiv:2508.17045v1 Announce Type: new 
Abstract: We present StyleClone, a method for training image-to-image translation networks to stylize faces in a specific style, even with limited style images. Our approach leverages textual inversion and diffusion-based guided image generation to augment small style datasets. By systematically generating diverse style samples guided by both the original style images and real face images, we significantly enhance the diversity of the style dataset. Using this augmented dataset, we train fast image-to-image translation networks that outperform diffusion-based methods in speed and quality. Experiments on multiple styles demonstrate that our method improves stylization quality, better preserves source image content, and significantly accelerates inference. Additionally, we provide a systematic evaluation of the augmentation techniques and their impact on stylization performance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models</title>
<link>https://arxiv.org/abs/2508.17050</link>
<guid>https://arxiv.org/abs/2508.17050</guid>
<content:encoded><![CDATA[
arXiv:2508.17050v1 Announce Type: new 
Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on high-quality point clouds. However, LiDAR-scanned data often suffer from extreme sparsity, severely hindering downstream 3D perception tasks. Existing point cloud upsampling methods primarily focus on individual objects, thus demonstrating limited generalization capability for complex outdoor scenes. To address this issue, we propose PVNet, a diffusion model-based point-voxel interaction framework to perform LiDAR point cloud upsampling without dense supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to guide the generation, in which we employ a sparse point cloud as the guiding condition and the synthesized point clouds derived from its nearby frames as the input. Moreover, we design a voxel completion module to refine and complete the coarse voxel features for enriching the feature representation. In addition, we propose a point-voxel interaction module to integrate features from both points and voxels, which efficiently improves the environmental perception capability of each upsampled point. To the best of our knowledge, our approach is the first scene-level point cloud upsampling method supporting arbitrary upsampling rates. Extensive experiments on various benchmarks demonstrate that our method achieves state-of-the-art performance. The source code will be available at https://github.com/chengxianjing/PVNet.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method</title>
<link>https://arxiv.org/abs/2508.17054</link>
<guid>https://arxiv.org/abs/2508.17054</guid>
<content:encoded><![CDATA[
arXiv:2508.17054v1 Announce Type: new 
Abstract: Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($\Delta$Flow), a lightweight 3D framework that captures motion cues via a $\Delta$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2 and Waymo datasets show that $\Delta$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework</title>
<link>https://arxiv.org/abs/2508.17061</link>
<guid>https://arxiv.org/abs/2508.17061</guid>
<content:encoded><![CDATA[
arXiv:2508.17061v1 Announce Type: new 
Abstract: Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired image-to-image translation task. This enables training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2508.17062</link>
<guid>https://arxiv.org/abs/2508.17062</guid>
<content:encoded><![CDATA[
arXiv:2508.17062v1 Announce Type: new 
Abstract: Controllable video generation aims to synthesize video content that aligns precisely with user-provided conditions, such as text descriptions and initial images. However, a significant challenge persists in this domain: existing models often struggle to maintain strong semantic consistency, frequently generating videos that deviate from the nuanced details specified in the prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided Diffusion Transformer), a novel and efficient framework for high-fidelity controllable video generation. Our approach introduces a decoupled two-stage process. The first stage, Spatial Signal Prompting, generates a spatially aware visual prompt by leveraging the rich internal representations of a pre-trained multi-modal model. This prompt, combined with the original text, forms a joint condition that is then injected into a frozen video DiT backbone via our lightweight and parameter-efficient SSG-Adapter. This unique design, featuring a dual-branch attention mechanism, allows the model to simultaneously harness its powerful generative priors while being precisely steered by external spatial signals. Extensive experiments demonstrate that SSG-DiT achieves state-of-the-art performance, outperforming existing models on multiple key metrics in the VBench benchmark, particularly in spatial relationship control and overall consistency.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry</title>
<link>https://arxiv.org/abs/2508.17081</link>
<guid>https://arxiv.org/abs/2508.17081</guid>
<content:encoded><![CDATA[
arXiv:2508.17081v1 Announce Type: new 
Abstract: The Vision Transformer (ViT) architecture has become widely recognized in computer vision, leveraging its self-attention mechanism to achieve remarkable success across various tasks. Despite its strengths, ViT's optimization remains confined to modeling local relationships within individual images, limiting its ability to capture the global geometric relationships between data points. To address this limitation, this paper proposes a novel framework that integrates ViT with the proximal tools, enabling a unified geometric optimization approach to enhance feature representation and classification performance. In this framework, ViT constructs the tangent bundle of the manifold through its self-attention mechanism, where each attention head corresponds to a tangent space, offering geometric representations from diverse local perspectives. Proximal iterations are then introduced to define sections within the tangent bundle and project data from tangent spaces onto the base space, achieving global feature alignment and optimization. Experimental results confirm that the proposed method outperforms traditional ViT in terms of classification accuracy and data distribution.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PD-Loss: Proxy-Decidability for Efficient Metric Learning</title>
<link>https://arxiv.org/abs/2508.17082</link>
<guid>https://arxiv.org/abs/2508.17082</guid>
<content:encoded><![CDATA[
arXiv:2508.17082v1 Announce Type: new 
Abstract: Deep Metric Learning (DML) aims to learn embedding functions that map semantically similar inputs to proximate points in a metric space while separating dissimilar ones. Existing methods, such as pairwise losses, are hindered by complex sampling requirements and slow convergence. In contrast, proxy-based losses, despite their improved scalability, often fail to optimize global distribution properties. The Decidability-based Loss (D-Loss) addresses this by targeting the decidability index (d') to enhance distribution separability, but its reliance on large mini-batches imposes significant computational constraints. We introduce Proxy-Decidability Loss (PD-Loss), a novel objective that integrates learnable proxies with the statistical framework of d' to optimize embedding spaces efficiently. By estimating genuine and impostor distributions through proxies, PD-Loss combines the computational efficiency of proxy-based methods with the principled separability of D-Loss, offering a scalable approach to distribution-aware DML. Experiments across various tasks, including fine-grained classification and face verification, demonstrate that PD-Loss achieves performance comparable to that of state-of-the-art methods while introducing a new perspective on embedding optimization, with potential for broader applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRASP: Geospatial pixel Reasoning viA Structured Policy learning</title>
<link>https://arxiv.org/abs/2508.17102</link>
<guid>https://arxiv.org/abs/2508.17102</guid>
<content:encoded><![CDATA[
arXiv:2508.17102v1 Announce Type: new 
Abstract: Geospatial pixel reasoning is a nascent remote-sensing task that aims to generate segmentation masks directly from natural-language instructions. Prevailing MLLM-based systems co-train a language model and a mask decoder with dense pixel supervision, which is expensive and often weak on out-of-domain (OOD) data. We introduce GRASP, a structured policy-learning framework. In our design, a multimodal large language model first emits task-relevant bounding boxes and positive points from a vision-language instruction. These outputs are then passed to a pre-trained segmentation model, which consumes them as prompts to generate the final mask. Instead of supervised fine-tuning, we optimize the system purely with reinforcement learning: the model is trained solely with GRPO, guided by format rewards and accuracy rewards computed on boxes and points (no mask supervision). This leverages strong priors in foundation models, minimizes trainable parameters, and enables learning from inexpensive annotations. We additionally curate GRASP-1k, which contains reasoning-intensive queries, detailed reasoning traces, and fine-grained segmentation annotations. Evaluations on both in-domain and out-of-domain test sets show state-of-the-art results: about 4% improvement in-domain and up to 54% on OOD benchmarks. The experiment results evidence our model's robust generalization and demonstrate that complex geospatial segmentation behaviors can be learned via RL from weak spatial cues. Code and the dataset will be released open-source.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SugarcaneShuffleNet: A Very Fast, Lightweight Convolutional Neural Network for Diagnosis of 15 Sugarcane Leaf Diseases</title>
<link>https://arxiv.org/abs/2508.17107</link>
<guid>https://arxiv.org/abs/2508.17107</guid>
<content:encoded><![CDATA[
arXiv:2508.17107v1 Announce Type: new 
Abstract: Despite progress in AI-based plant diagnostics, sugarcane farmers in low-resource regions remain vulnerable to leaf diseases due to the lack of scalable, efficient, and interpretable tools. Many deep learning models fail to generalize under real-world conditions and require substantial computational resources, limiting their use in resource-constrained regions. In this paper, we present SugarcaneLD-BD, a curated dataset for sugarcane leaf-disease classification; SugarcaneShuffleNet, an optimized lightweight model for rapid on-device diagnosis; and SugarcaneAI, a Progressive Web Application for field deployment. SugarcaneLD-BD contains 638 curated images across five classes, including four major sugarcane diseases, collected in Bangladesh under diverse field conditions and verified by expert pathologists. To enhance diversity, we combined SugarcaneLD-BD with two additional datasets, yielding a larger and more representative corpus. Our optimized model, SugarcaneShuffleNet, offers the best trade-off between speed and accuracy for real-time, on-device diagnosis. This 9.26 MB model achieved 98.02% accuracy, an F1-score of 0.98, and an average inference time of 4.14 ms per image. For comparison, we fine-tuned five other lightweight convolutional neural networks: MnasNet, EdgeNeXt, EfficientNet-Lite, MobileNet, and SqueezeNet via transfer learning and Bayesian optimization. MnasNet and EdgeNeXt achieved comparable accuracy to SugarcaneShuffleNet, but required significantly more parameters, memory, and computation, limiting their suitability for low-resource deployment. We integrate SugarcaneShuffleNet into SugarcaneAI, delivering Grad-CAM-based explanations in the field. Together, these contributions offer a diverse benchmark, efficient models for low-resource environments, and a practical tool for sugarcane disease classification. It spans varied lighting, backgrounds and devices used on-farm
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science</title>
<link>https://arxiv.org/abs/2508.17117</link>
<guid>https://arxiv.org/abs/2508.17117</guid>
<content:encoded><![CDATA[
arXiv:2508.17117v1 Announce Type: new 
Abstract: PlantVillageVQA is a large-scale visual question answering (VQA) dataset derived from the widely used PlantVillage image corpus. It was designed to advance the development and evaluation of vision-language models for agricultural decision-making and analysis. The PlantVillageVQA dataset comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448 images spanning 14 crop species and 38 disease conditions. Questions are organised into 3 levels of cognitive complexity and 9 distinct categories. Each question category was phrased manually following expert guidance and generated via an automated two-stage pipeline: (1) template-based QA synthesis from image metadata and (2) multi-stage linguistic re-engineering. The dataset was iteratively reviewed by domain experts for scientific accuracy and relevancy. The final dataset was evaluated using three state-of-the-art models for quality assessment. Our objective remains to provide a publicly available, standardised and expert-verified database to enhance diagnostic accuracy for plant disease identifications and advance scientific research in the agricultural domain. Our dataset will be open-sourced at https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</title>
<link>https://arxiv.org/abs/2508.17128</link>
<guid>https://arxiv.org/abs/2508.17128</guid>
<content:encoded><![CDATA[
arXiv:2508.17128v1 Announce Type: new 
Abstract: Brain tumors remain among the most lethal human diseases, where early detection and accurate classification are critical for effective diagnosis and treatment planning. Although deep learning-based computer-aided diagnostic (CADx) systems have shown remarkable progress. However, conventional convolutional neural networks (CNNs) and Transformers face persistent challenges, including high computational cost, sensitivity to minor contrast variations, structural heterogeneity, and texture inconsistencies in MRI data. Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integrating residual and spatial learning-based CNNs with transformer-driven modules. The proposed framework exploits local fine-grained and global contextual cues through four core innovations: (i) a smoothing and boundary-based CNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learning CNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatial attention mechanism. The developed SBCIT employs stem convolution and contextual interaction transformer blocks with systematic smoothing and boundary operations, enabling efficient global feature modeling. Moreover, Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps, enrich the representation space, while the CE module amplifies discriminative channels and mitigates redundancy. Furthermore, the spatial attention mechanism selectively emphasizes subtle contrast and textural variations across tumor classes. Extensive evaluation on challenging MRI datasets from Kaggle and Figshare, encompassing glioma, meningioma, pituitary tumors, and healthy controls, demonstrates superior performance, achieving 98.30% accuracy, 98.08% sensitivity, 98.25% F1-score, and 98.43% precision.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Damage Detection Using AI Super Resolution and Visual Language Model</title>
<link>https://arxiv.org/abs/2508.17130</link>
<guid>https://arxiv.org/abs/2508.17130</guid>
<content:encoded><![CDATA[
arXiv:2508.17130v1 Announce Type: new 
Abstract: Natural disasters pose significant challenges to timely and accurate damage assessment due to their sudden onset and the extensive areas they affect. Traditional assessment methods are often labor-intensive, costly, and hazardous to personnel, making them impractical for rapid response, especially in resource-limited settings. This study proposes a novel, cost-effective framework that leverages aerial drone footage, an advanced AI-based video super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a 27 billion parameter Visual Language Model (VLM). This integrated system is designed to improve low-resolution disaster footage, identify structural damage, and classify buildings into four damage categories, ranging from no/slight damage to total destruction, along with associated risk levels. The methodology was validated using pre- and post-event drone imagery from the 2023 Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013 Moore Tornado (xBD dataset). The framework achieved a classification accuracy of 84.5%, demonstrating its ability to provide highly accurate results. Furthermore, the system's accessibility allows non-technical users to perform preliminary analyses, thereby improving the responsiveness and efficiency of disaster management efforts.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Play and Pause: Turning GPT-4o Spatial Weakness into a Strength for In-Depth Interactive Video Learning</title>
<link>https://arxiv.org/abs/2508.17160</link>
<guid>https://arxiv.org/abs/2508.17160</guid>
<content:encoded><![CDATA[
arXiv:2508.17160v1 Announce Type: new 
Abstract: Traditional video-based learning remains passive, offering limited opportunities for users to engage dynamically with content. While current AI-powered tools offer transcription and summarization, they lack real-time, region-specific interaction capabilities. This paper introduces Untwist, an AI-driven system that enables interactive video learning by allowing users to ask questions about the entire video or specific regions using a bounding box, receiving context-aware, multimodal responses. By integrating GPT APIs with Computer Vision techniques, Untwist extracts, processes, and structures video content to enhance comprehension. Our approach addresses GPT-4o spatial weakness by leveraging annotated frames instead of raw coordinate data, significantly improving accuracy in localizing and interpreting video content. This paper describes the system architecture, including video pre-processing and real-time interaction, and outlines how Untwist can transform passive video consumption into an interactive, AI-driven learning experience with the potential to enhance engagement and comprehension.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of an isotropic segmentation model for medial temporal lobe subregions on anisotropic MRI atlas using implicit neural representation</title>
<link>https://arxiv.org/abs/2508.17171</link>
<guid>https://arxiv.org/abs/2508.17171</guid>
<content:encoded><![CDATA[
arXiv:2508.17171v1 Announce Type: new 
Abstract: Imaging biomarkers in magnetic resonance imaging (MRI) are important tools for diagnosing and tracking Alzheimer's disease (AD). As medial temporal lobe (MTL) is the earliest region to show AD-related hallmarks, brain atrophy caused by AD can first be observed in the MTL. Accurate segmentation of MTL subregions and extraction of imaging biomarkers from them are important. However, due to imaging limitations, the resolution of T2-weighted (T2w) MRI is anisotropic, which makes it difficult to accurately extract the thickness of cortical subregions in the MTL. In this study, we used an implicit neural representation method to combine the resolution advantages of T1-weighted and T2w MRI to accurately upsample an MTL subregion atlas set from anisotropic space to isotropic space, establishing a multi-modality, high-resolution atlas set. Based on this atlas, we developed an isotropic MTL subregion segmentation model. In an independent test set, the cortical subregion thickness extracted using this isotropic model showed higher significance than an anisotropic method in distinguishing between participants with mild cognitive impairment and cognitively unimpaired (CU) participants. In longitudinal analysis, the biomarkers extracted using isotropic method showed greater stability in CU participants. This study improved the accuracy of AD imaging biomarkers without increasing the amount of atlas annotation work, which may help to more accurately quantify the relationship between AD and brain atrophy and provide more accurate measures for disease tracking.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VROOM - Visual Reconstruction over Onboard Multiview</title>
<link>https://arxiv.org/abs/2508.17172</link>
<guid>https://arxiv.org/abs/2508.17172</guid>
<content:encoded><![CDATA[
arXiv:2508.17172v1 Announce Type: new 
Abstract: We introduce VROOM, a system for reconstructing 3D models of Formula 1 circuits using only onboard camera footage from racecars. Leveraging video data from the 2023 Monaco Grand Prix, we address video challenges such as high-speed motion and sharp cuts in camera frames. Our pipeline analyzes different methods such as DROID-SLAM, AnyCam, and Monst3r and combines preprocessing techniques such as different methods of masking, temporal chunking, and resolution scaling to account for dynamic motion and computational constraints. We show that Vroom is able to partially recover track and vehicle trajectories in complex environments. These findings indicate the feasibility of using onboard video for scalable 4D reconstruction in real-world settings. The project page can be found at https://varun-bharadwaj.github.io/vroom, and our code is available at https://github.com/yajatyadav/vroom.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</title>
<link>https://arxiv.org/abs/2508.17186</link>
<guid>https://arxiv.org/abs/2508.17186</guid>
<content:encoded><![CDATA[
arXiv:2508.17186v1 Announce Type: new 
Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling</title>
<link>https://arxiv.org/abs/2508.17199</link>
<guid>https://arxiv.org/abs/2508.17199</guid>
<content:encoded><![CDATA[
arXiv:2508.17199v1 Announce Type: new 
Abstract: In this study, we introduce a novel cover image generation task that produces both a concise summary and a visually corresponding image from a given text-only document. Because no existing datasets are available for this task, we propose a multimodal pseudo-labeling method to construct high-quality datasets at low cost. We first collect documents that contain multiple images with their captions, and their summaries by excluding factually inconsistent instances. Our approach selects one image from the multiple images accompanying the documents. Using the gold summary, we independently rank both the images and their captions. Then, we annotate a pseudo-label for an image when both the image and its corresponding caption are ranked first in their respective rankings. Finally, we remove documents that contain direct image references within texts. Experimental results demonstrate that the proposed multimodal pseudo-labeling method constructs more precise datasets and generates higher quality images than text- and image-only pseudo-labeling methods, which consider captions and images separately. We release our code at: https://github.com/HyeyeeonKim/MMCIG
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</title>
<link>https://arxiv.org/abs/2508.17205</link>
<guid>https://arxiv.org/abs/2508.17205</guid>
<content:encoded><![CDATA[
arXiv:2508.17205v1 Announce Type: new 
Abstract: This paper introduces a multi-agent framework for comprehensive highway scene understanding, designed around a mixture-of-experts strategy. In this framework, a large generic vision-language model (VLM), such as GPT-4o, is contextualized with domain knowledge to generates task-specific chain-of-thought (CoT) prompts. These fine-grained prompts are then used to guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short videos, along with complementary modalities as applicable. The framework simultaneously addresses multiple critical perception tasks, including weather classification, pavement wetness assessment, and traffic congestion detection, achieving robust multi-task reasoning while balancing accuracy and computational efficiency. To support empirical validation, we curated three specialized datasets aligned with these tasks. Notably, the pavement wetness dataset is multimodal, combining video streams with road weather sensor data, highlighting the benefits of multimodal reasoning. Experimental results demonstrate consistently strong performance across diverse traffic and environmental conditions. From a deployment perspective, the framework can be readily integrated with existing traffic camera systems and strategically applied to high-risk rural locations, such as sharp curves, flood-prone lowlands, or icy bridges. By continuously monitoring the targeted sites, the system enhances situational awareness and delivers timely alerts, even in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Knowledge Decomposition based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology</title>
<link>https://arxiv.org/abs/2508.17213</link>
<guid>https://arxiv.org/abs/2508.17213</guid>
<content:encoded><![CDATA[
arXiv:2508.17213v1 Announce Type: new 
Abstract: Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data fusion analysis. However, the simultaneous acquisition of multi-modal data, such as genomic and pathological information, is often challenging due to cost or technical limitations. To address this challenge, we propose an online distillation approach based on Multi-modal Knowledge Decomposition (MKD) to enhance IHC biomarker prediction in haematoxylin and eosin (H\&amp;E) stained histopathology images. This method leverages paired genomic-pathology data during training while enabling inference using either pathology slides alone or both modalities. Two teacher and one student models are developed to extract modality-specific and modality-general features by minimizing the MKD loss. To maintain the internal structural relationships between samples, Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally, Collaborative Learning for Online Distillation (CLOD) facilitates mutual learning between teacher and student models, encouraging diverse and complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU datasets demonstrate that our approach achieves superior performance in IHC biomarker prediction using uni-modal data. Our code is available at https://github.com/qiyuanzz/MICCAI2025_MKD.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning with Self-Attention and Enhanced Preprocessing for Precise Diagnosis of Acute Lymphoblastic Leukemia from Bone Marrow Smears in Hemato-Oncology</title>
<link>https://arxiv.org/abs/2508.17216</link>
<guid>https://arxiv.org/abs/2508.17216</guid>
<content:encoded><![CDATA[
arXiv:2508.17216v1 Announce Type: new 
Abstract: Acute lymphoblastic leukemia (ALL) is a prevalent hematological malignancy in both pediatric and adult populations. Early and accurate detection with precise subtyping is essential for guiding therapy. Conventional workflows are complex, time-consuming, and prone to human error. We present a deep learning framework for automated ALL diagnosis from bone marrow smear images. The method combines a robust preprocessing pipeline with convolutional neural networks (CNNs) to standardize image quality and improve inference efficiency. As a key design, we insert a multi-head self-attention (MHSA) block into a VGG19 backbone to model long-range dependencies and contextual relationships among cellular features. To mitigate class imbalance, we train with Focal Loss. Across evaluated architectures, the enhanced VGG19+MHSA trained with Focal Loss achieves 99.25% accuracy, surpassing a strong ResNet101 baseline (98.62%). These results indicate that attention-augmented CNNs, coupled with targeted loss optimization and preprocessing, yield more discriminative representations of leukemic cell morphology. Our approach offers a highly accurate and computationally efficient tool for automated ALL recognition and subtyping, with potential to accelerate diagnostic workflows and support reliable decision-making in clinical settings.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D Visual Pre-training for Robot Learning</title>
<link>https://arxiv.org/abs/2508.17230</link>
<guid>https://arxiv.org/abs/2508.17230</guid>
<content:encoded><![CDATA[
arXiv:2508.17230v1 Announce Type: new 
Abstract: General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d- visual-pretraining.github.io/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation</title>
<link>https://arxiv.org/abs/2508.17239</link>
<guid>https://arxiv.org/abs/2508.17239</guid>
<content:encoded><![CDATA[
arXiv:2508.17239v1 Announce Type: new 
Abstract: Monocular 3D human pose estimation (HPE) methods estimate the 3D positions of joints from individual images. Existing 3D HPE approaches often use the cropped image alone as input for their models. However, the relative depths of joints cannot be accurately estimated from cropped images without the corresponding camera intrinsics, which determine the perspective relationship between 3D objects and the cropped images. In this work, we introduce Perspective Encoding (PE) to encode the camera intrinsics of the cropped images. Moreover, since the human subject can appear anywhere within the original image, the perspective relationship between the 3D scene and the cropped image differs significantly, which complicates model fitting. Additionally, the further the human subject deviates from the image center, the greater the perspective distortions in the cropped image. To address these issues, we propose Perspective Rotation (PR), a transformation applied to the original image that centers the human subject, thereby reducing perspective distortions and alleviating the difficulty of model fitting. By incorporating PE and PR, we propose a novel 3D HPE framework, PersPose. Experimental results demonstrate that PersPose achieves state-of-the-art (SOTA) performance on the 3DPW, MPIINF-3DHP, and Human3.6M datasets. For example, on the in-the-wild dataset 3DPW, PersPose achieves an MPJPE of 60.1 mm, 7.54% lower than the previous SOTA approach. Code is available at: https://github.com/ KenAdamsJoseph/PersPose.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17243</link>
<guid>https://arxiv.org/abs/2508.17243</guid>
<content:encoded><![CDATA[
arXiv:2508.17243v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics</title>
<link>https://arxiv.org/abs/2508.17247</link>
<guid>https://arxiv.org/abs/2508.17247</guid>
<content:encoded><![CDATA[
arXiv:2508.17247v1 Announce Type: new 
Abstract: With the rapid evolution of deepfake technologies and the wide dissemination of digital media, personal privacy is facing increasingly serious security threats. Deepfake proactive forensics, which involves embedding imperceptible watermarks to enable reliable source tracking, serves as a crucial defense against these threats. Although existing methods show strong forensic ability, they rely on an idealized assumption of single watermark embedding, which proves impractical in real-world scenarios. In this paper, we formally define and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first time. When a previously protected image undergoes additional rounds of watermark embedding, the original forensic watermark can be destroyed or removed, rendering the entire proactive forensic mechanism ineffective. To address this vulnerability, we propose a general training paradigm named Adversarial Interference Simulation (AIS). Rather than modifying the network architecture, AIS explicitly simulates MEA scenarios during fine-tuning and introduces a resilience-driven loss function to enforce the learning of sparse and stable watermark representations. Our method enables the model to maintain the ability to extract the original watermark correctly even after a second embedding. Extensive experiments demonstrate that our plug-and-play AIS training paradigm significantly enhances the robustness of various existing methods against MEA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A biological vision inspired framework for machine perception of abutting grating illusory contours</title>
<link>https://arxiv.org/abs/2508.17254</link>
<guid>https://arxiv.org/abs/2508.17254</guid>
<content:encoded><![CDATA[
arXiv:2508.17254v1 Announce Type: new 
Abstract: Higher levels of machine intelligence demand alignment with human perception and cognition. Deep neural networks (DNN) dominated machine intelligence have demonstrated exceptional performance across various real-world tasks. Nevertheless, recent evidence suggests that DNNs fail to perceive illusory contours like the abutting grating, a discrepancy that misaligns with human perception patterns. Departing from previous works, we propose a novel deep network called illusory contour perception network (ICPNet) inspired by the circuits of the visual cortex. In ICPNet, a multi-scale feature projection (MFP) module is designed to extract multi-scale representations. To boost the interaction between feedforward and feedback features, a feature interaction attention module (FIAM) is introduced. Moreover, drawing inspiration from the shape bias observed in human perception, an edge detection task conducted via the edge fusion module (EFM) injects shape constraints that guide the network to concentrate on the foreground. We assess our method on the existing AG-MNIST test set and the AG-Fashion-MNIST test sets constructed by this work. Comprehensive experimental results reveal that ICPNet is significantly more sensitive to abutting grating illusory contours than state-of-the-art models, with notable improvements in top-1 accuracy across various subsets. This work is expected to make a step towards human-level intelligence for DNN-based models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</title>
<link>https://arxiv.org/abs/2508.17255</link>
<guid>https://arxiv.org/abs/2508.17255</guid>
<content:encoded><![CDATA[
arXiv:2508.17255v1 Announce Type: new 
Abstract: We present SEER-VAR, a novel framework for egocentric vehicle-based augmented reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches (CASB), and LLM-driven recommendation. Unlike existing systems that assume static or single-view settings, SEER-VAR dynamically separates cabin and road scenes via depth-guided vision-language grounding. Two SLAM branches track egocentric motion in each context, while a GPT-based module generates context-aware overlays such as dashboard cues and hazard alerts. To support evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring synchronized egocentric views, 6DoF ground-truth poses, and AR annotations across diverse driving scenarios. Experiments demonstrate that SEER-VAR achieves robust spatial alignment and perceptually coherent AR rendering across varied environments. As one of the first to explore LLM-based AR recommendation in egocentric driving, we address the lack of comparable systems through structured prompting and detailed user studies. Results show that SEER-VAR enhances perceived scene understanding, overlay relevance, and driver ease, providing an effective foundation for future research in this direction. Code and dataset will be made open source.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResLink: A Novel Deep Learning Architecture for Brain Tumor Classification with Area Attention and Residual Connections</title>
<link>https://arxiv.org/abs/2508.17259</link>
<guid>https://arxiv.org/abs/2508.17259</guid>
<content:encoded><![CDATA[
arXiv:2508.17259v1 Announce Type: new 
Abstract: Brain tumors show significant health challenges due to their potential to cause critical neurological functions. Early and accurate diagnosis is crucial for effective treatment. In this research, we propose ResLink, a novel deep learning architecture for brain tumor classification using CT scan images. ResLink integrates novel area attention mechanisms with residual connections to enhance feature learning and spatial understanding for spatially rich image classification tasks. The model employs a multi-stage convolutional pipeline, incorporating dropout, regularization, and downsampling, followed by a final attention-based refinement for classification. Trained on a balanced dataset, ResLink achieves a high accuracy of 95% and demonstrates strong generalizability. This research demonstrates the potential of ResLink in improving brain tumor classification, offering a robust and efficient technique for medical imaging applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIFF: Continual Learning for Incremental Flake Features in 2D Material Identification</title>
<link>https://arxiv.org/abs/2508.17261</link>
<guid>https://arxiv.org/abs/2508.17261</guid>
<content:encoded><![CDATA[
arXiv:2508.17261v1 Announce Type: new 
Abstract: Identifying quantum flakes is crucial for scalable quantum hardware; however, automated layer classification from optical microscopy remains challenging due to substantial appearance shifts across different materials. In this paper, we propose a new Continual-Learning Framework for Flake Layer Classification (CLIFF). To our knowledge, this is the first systematic study of continual learning in the domain of two-dimensional (2D) materials. Our method enables the model to differentiate between materials and their physical and optical properties by freezing a backbone and base head trained on a reference material. For each new material, it learns a material-specific prompt, embedding, and a delta head. A prompt pool and a cosine-similarity gate modulate features and compute material-specific corrections. Additionally, we incorporate memory replay with knowledge distillation. CLIFF achieves competitive accuracy with significantly lower forgetting than naive fine-tuning and a prompt-based baseline.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.17265</link>
<guid>https://arxiv.org/abs/2508.17265</guid>
<content:encoded><![CDATA[
arXiv:2508.17265v1 Announce Type: new 
Abstract: Adversarial distillation (AD) is a knowledge distillation technique that facilitates the transfer of robustness from teacher deep neural network (DNN) models to lightweight target (student) DNN models, enabling the target models to perform better than only training the student model independently. Some previous works focus on using a small, learnable teacher (guide) model to improve the robustness of a student model. Since a learnable guide model starts learning from scratch, maintaining its optimal state for effective knowledge transfer during co-training is challenging. Therefore, we propose a novel Adaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT, dynamically adjusts the training state of the guide model to install robustness to the target model. Specifically, we develop two separate loss functions as part of the AdaGAT method, allowing the guide model to participate more actively in backpropagation to achieve its optimal state. We evaluated our approach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and TinyImageNet, using the WideResNet-34-10 model as the target model. Our observations reveal that appropriately adjusting the guide model within a certain accuracy range enhances the target model's robustness across various adversarial attacks compared to a variety of baseline models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Human-Object Interaction Detection</title>
<link>https://arxiv.org/abs/2508.17270</link>
<guid>https://arxiv.org/abs/2508.17270</guid>
<content:encoded><![CDATA[
arXiv:2508.17270v1 Announce Type: new 
Abstract: In this paper, we propose a new instance-level human-object interaction detection task on videos called ST-HOID, which aims to distinguish fine-grained human-object interactions (HOIs) and the trajectories of subjects and objects. It is motivated by the fact that HOI is crucial for human-centric video content understanding. To solve ST-HOID, we propose a novel method consisting of an object trajectory detection module and an interaction reasoning module. Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct extensive experiments to evaluate the effectiveness of our method. The experimental results demonstrate that our method outperforms the baselines generated by the state-of-the-art methods of image human-object interaction detection, video visual relation detection and video human-object interaction recognition.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Assisted Detection of Sarcopenia in Cross-Sectional Computed Tomography Imaging</title>
<link>https://arxiv.org/abs/2508.17275</link>
<guid>https://arxiv.org/abs/2508.17275</guid>
<content:encoded><![CDATA[
arXiv:2508.17275v1 Announce Type: new 
Abstract: Sarcopenia is a progressive loss of muscle mass and function linked to poor surgical outcomes such as prolonged hospital stays, impaired mobility, and increased mortality. Although it can be assessed through cross-sectional imaging by measuring skeletal muscle area (SMA), the process is time-consuming and adds to clinical workloads, limiting timely detection and management; however, this process could become more efficient and scalable with the assistance of artificial intelligence applications. This paper presents high-quality three-dimensional cross-sectional computed tomography (CT) images of patients with sarcopenia collected at the Freeman Hospital, Newcastle upon Tyne Hospitals NHS Foundation Trust. Expert clinicians manually annotated the SMA at the third lumbar vertebra, generating precise segmentation masks. We develop deep-learning models to measure SMA in CT images and automate this task. Our methodology employed transfer learning and self-supervised learning approaches using labelled and unlabeled CT scan datasets. While we developed qualitative assessment models for detecting sarcopenia, we observed that the quantitative assessment of SMA is more precise and informative. This approach also mitigates the issue of class imbalance and limited data availability. Our model predicted the SMA, on average, with an error of +-3 percentage points against the manually measured SMA. The average dice similarity coefficient of the predicted masks was 93%. Our results, therefore, show a pathway to full automation of sarcopenia assessment and detection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTNet: Learning modality-aware representation with transformer for RGBT tracking</title>
<link>https://arxiv.org/abs/2508.17280</link>
<guid>https://arxiv.org/abs/2508.17280</guid>
<content:encoded><![CDATA[
arXiv:2508.17280v1 Announce Type: new 
Abstract: The ability to learn robust multi-modality representation has played a critical role in the development of RGBT tracking. However, the regular fusion paradigm and the invariable tracking template remain restrictive to the feature interaction. In this paper, we propose a modality-aware tracker based on transformer, termed MTNet. Specifically, a modality-aware network is presented to explore modality-specific cues, which contains both channel aggregation and distribution module(CADM) and spatial similarity perception module (SSPM). A transformer fusion network is then applied to capture global dependencies to reinforce instance representations. To estimate the precise location and tackle the challenges, such as scale variation and deformation, we design a trident prediction head and a dynamic update strategy which jointly maintain a reliable template for facilitating inter-frame communication. Extensive experiments validate that the proposed method achieves satisfactory results compared with the state-of-the-art competitors on three RGBT benchmarks while reaching real-time speed.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quickly Tuning Foundation Models for Image Segmentation</title>
<link>https://arxiv.org/abs/2508.17283</link>
<guid>https://arxiv.org/abs/2508.17283</guid>
<content:encoded><![CDATA[
arXiv:2508.17283v1 Announce Type: new 
Abstract: Foundation models like SAM (Segment Anything Model) exhibit strong zero-shot image segmentation performance, but often fall short on domain-specific tasks. Fine-tuning these models typically requires significant manual effort and domain expertise. In this work, we introduce QTT-SEG, a meta-learning-driven approach for automating and accelerating the fine-tuning of SAM for image segmentation. Built on the Quick-Tune hyperparameter optimization framework, QTT-SEG predicts high-performing configurations using meta-learned cost and performance models, efficiently navigating a search space of over 200 million possibilities. We evaluate QTT-SEG on eight binary and five multiclass segmentation datasets under tight time constraints. Our results show that QTT-SEG consistently improves upon SAM's zero-shot performance and surpasses AutoGluon Multimodal, a strong AutoML baseline, on most binary tasks within three minutes. On multiclass datasets, QTT-SEG delivers consistent gains as well. These findings highlight the promise of meta-learning in automating model adaptation for specialized segmentation tasks. Code available at: https://github.com/ds-brx/QTT-SEG/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain Before You Answer: A Survey on Compositional Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.17298</link>
<guid>https://arxiv.org/abs/2508.17298</guid>
<content:encoded><![CDATA[
arXiv:2508.17298v1 Announce Type: new 
Abstract: Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising</title>
<link>https://arxiv.org/abs/2508.17299</link>
<guid>https://arxiv.org/abs/2508.17299</guid>
<content:encoded><![CDATA[
arXiv:2508.17299v1 Announce Type: new 
Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing</title>
<link>https://arxiv.org/abs/2508.17302</link>
<guid>https://arxiv.org/abs/2508.17302</guid>
<content:encoded><![CDATA[
arXiv:2508.17302v1 Announce Type: new 
Abstract: Localized subject-driven image editing aims to seamlessly integrate user-specified objects into target scenes. As generative models continue to scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks.To this end, we propose PosBridge an efficient and flexible framework for inserting custom objects. A key component of our method is positional embedding transplant, which guides the diffusion model to faithfully replicate the structural characteristics of reference objects.Meanwhile, we introduce the Corner Centered Layout, which concatenates reference images and the background image as input to the FLUX.1-Fill model. During progressive denoising, positional embedding transplant is applied to guide the noise distribution in the target region toward that of the reference object. In this way, Corner Centered Layout effectively directs the FLUX.1-Fill model to synthesize identity-consistent content at the desired location. Extensive experiments demonstrate that PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency, showcasing its practical value and potential for broad adoption.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in the Detail and Minority</title>
<link>https://arxiv.org/abs/2508.17305</link>
<guid>https://arxiv.org/abs/2508.17305</guid>
<content:encoded><![CDATA[
arXiv:2508.17305v1 Announce Type: new 
Abstract: In this report, we present our solution during the participation of the MLCAS 2025 GWFSS Challenge. This challenge hosts a semantic segmentation competition specific to wheat plants, which requires to segment three wheat organs including the head, leaf, and stem, and another background class. In 2025, participating a segmentation competition is significantly different from that in previous years where many tricks can play important roles. Nowadays most segmentation tricks have been well integrated into existing codebases such that our naive ViT-Adapter baseline has already achieved sufficiently good performance. Hence, we believe the key to stand out among other competitors is to focus on the problem nature of wheat per se. By probing visualizations, we identify the key -- the stem matters. In contrast to heads and leaves, stems exhibit fine structure and occupy only few pixels, which suffers from fragile predictions and class imbalance. Building on our baseline, we present three technical improvements tailored to stems: i) incorporating a dynamic upsampler SAPA used to enhance detail delineation; ii) leveraging semi-supervised guided distillation with stem-aware sample selection to mine the treasure beneath unlabeled data; and iii) applying a test-time scaling strategy to zoom in and segment twice the image. Despite being simple, the three improvements bring us to the first place of the competition, outperforming the second place by clear margins. Code and models will be released at https://github.com/tiny-smart/gwfss25.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Deepfake via Texture Feature Perturbation</title>
<link>https://arxiv.org/abs/2508.17315</link>
<guid>https://arxiv.org/abs/2508.17315</guid>
<content:encoded><![CDATA[
arXiv:2508.17315v1 Announce Type: new 
Abstract: The rapid development of Deepfake technology poses severe challenges to social trust and information security. While most existing detection methods primarily rely on passive analyses, due to unresolvable high-quality Deepfake contents, proactive defense has recently emerged by inserting invisible signals in advance of image editing. In this paper, we introduce a proactive Deepfake detection approach based on facial texture features. Since human eyes are more sensitive to perturbations in smooth regions, we invisibly insert perturbations within texture regions that have low perceptual saliency, applying localized perturbations to key texture regions while minimizing unwanted noise in non-textured areas. Our texture-guided perturbation framework first extracts preliminary texture features via Local Binary Patterns (LBP), and then introduces a dual-model attention strategy to generate and optimize texture perturbations. Experiments on CelebA-HQ and LFW datasets demonstrate the promising performance of our method in distorting Deepfake generation and producing obvious visual defects under multiple attack models, providing an efficient and scalable solution for proactive Deepfake detection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation</title>
<link>https://arxiv.org/abs/2508.17316</link>
<guid>https://arxiv.org/abs/2508.17316</guid>
<content:encoded><![CDATA[
arXiv:2508.17316v1 Announce Type: new 
Abstract: Synthesizing spectral images across different wavelengths is essential for photorealistic rendering. Unlike conventional spectral uplifting methods that convert RGB images into spectral ones, we introduce SpecGen, a novel method that generates spectral bidirectional reflectance distribution functions (BRDFs) from a single RGB image of a sphere. This enables spectral image rendering under arbitrary illuminations and shapes covered by the corresponding material. A key challenge in spectral BRDF generation is the scarcity of measured spectral BRDF data. To address this, we propose the Spectral-Spatial Tri-plane Aggregation (SSTA) network, which models reflectance responses across wavelengths and incident-outgoing directions, allowing the training strategy to leverage abundant RGB BRDF data to enhance spectral BRDF generation. Experiments show that our method accurately reconstructs spectral BRDFs from limited spectral data and surpasses state-of-the-art methods in hyperspectral image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs</title>
<link>https://arxiv.org/abs/2508.17334</link>
<guid>https://arxiv.org/abs/2508.17334</guid>
<content:encoded><![CDATA[
arXiv:2508.17334v1 Announce Type: new 
Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA) on cricket scorecards, designed to evaluate large vision-language models (LVLMs) on complex numerical and cross-lingual reasoning over semi-structured tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi scorecards, with all questions and answers kept in English to enable controlled cross-script evaluation. The task demands reasoning over structured numerical data, multi-image context, and implicit domain knowledge. Empirical results show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle on the English subset despite it being their primary training language and exhibit a further drop in performance on the Hindi subset. This reveals key limitations in structure-aware visual text understanding, numerical reasoning, and cross-lingual generalization. The dataset is publicly available via Hugging Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM research in this direction.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2508.17346</link>
<guid>https://arxiv.org/abs/2508.17346</guid>
<content:encoded><![CDATA[
arXiv:2508.17346v1 Announce Type: new 
Abstract: The rapid growth of high-resolution, meticulously crafted AI-generated images poses a significant challenge to existing detection methods, which are often trained and evaluated on low-resolution, automatically generated datasets that do not align with the complexities of high-resolution scenarios. A common practice is to resize or center-crop high-resolution images to fit standard network inputs. However, without full coverage of all pixels, such strategies risk either obscuring subtle, high-frequency artifacts or discarding information from uncovered regions, leading to input information loss. In this paper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net), a novel framework that ensures no pixel is left behind. We use the Feature Aggregation Module (FAM), which fuses features from multiple full-resolution local tiles with a down-sampled global view of the image. These local features are aggregated and fused with global representations for final prediction, ensuring that native-resolution details are preserved and utilized for detection. To enhance robustness against challenges such as localized AI manipulations and compression, we introduce Token-wise Forgery Localization (TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor Estimation (QFE) module to disentangle generative artifacts from compression noise explicitly. Furthermore, to facilitate future research, we introduce HiRes-50K, a new challenging benchmark consisting of 50,568 images with up to 64 megapixels. Extensive experiments show that HiDA-Net achieves state-of-the-art, increasing accuracy by over 13% on the challenging Chameleon dataset and 10% on our HiRes-50K.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCache: Let Diffusion Model Determine Its Own Cache</title>
<link>https://arxiv.org/abs/2508.17356</link>
<guid>https://arxiv.org/abs/2508.17356</guid>
<content:encoded><![CDATA[
arXiv:2508.17356v1 Announce Type: new 
Abstract: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[
arXiv:2508.17364v1 Announce Type: new 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.17394</link>
<guid>https://arxiv.org/abs/2508.17394</guid>
<content:encoded><![CDATA[
arXiv:2508.17394v1 Announce Type: new 
Abstract: Clinical decision-making often involves interpreting images (e.g., radiology) for making diagnoses. Retrieving relevant visual information from medical literature and hospital records could enhance diagnostic accuracy. In this paper, we develop a model in which a multimodal retriever is jointly optimized with an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal is not propagated down to the retriever. We show that using only general-purpose backbones, with only lightweight fine-tuning, our model is able to achieve competitive results with medically-pretrained models across clinical multi-label classification and visual question answering tasks. In a novel analysis, we additionally find that in many cases different top retrieved images each lead to different predictions for a given target, and that these cases are empirically challenging for all models, even for non-retrieval models. Our joint retrieval optimization significantly improves these challenging cases over standard RAG. However, oracle analysis reveals that while the correct diagnosis is frequently achievable using one of the top retrieved images, in practice there is a large performance gap from the oracle, and rerankers using frontier LVLMs do not close this gap -- leaving ample room for improvement by future methods. Code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches</title>
<link>https://arxiv.org/abs/2508.17397</link>
<guid>https://arxiv.org/abs/2508.17397</guid>
<content:encoded><![CDATA[
arXiv:2508.17397v1 Announce Type: new 
Abstract: This paper addresses the challenging problem of image enhancement in complex underwater scenes by proposing a solution based on deep learning. The proposed method skillfully integrates two deep convolutional neural network models, VGG19 and ResNet50, leveraging their powerful feature extraction capabilities to perform multi-scale and multi-level deep feature analysis of underwater images. By constructing a unified model, the complementary advantages of the two models are effectively integrated, achieving a more comprehensive and accurate image enhancement effect.To objectively evaluate the enhancement effect, this paper introduces image quality assessment metrics such as PSNR, UCIQE, and UIQM to quantitatively compare images before and after enhancement and deeply analyzes the performance of different models in different scenarios.Furthermore, to improve the practicality and stability of the underwater visual enhancement system, this paper also provides practical suggestions from aspects such as model optimization, multi-model fusion, and hardware selection, aiming to provide strong technical support for visual enhancement tasks in complex underwater environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling</title>
<link>https://arxiv.org/abs/2508.17404</link>
<guid>https://arxiv.org/abs/2508.17404</guid>
<content:encoded><![CDATA[
arXiv:2508.17404v1 Announce Type: new 
Abstract: Generating human videos with consistent motion from text prompts remains a significant challenge, particularly for whole-body or long-range motion. Existing video generation models prioritize appearance fidelity, resulting in unrealistic or physically implausible human movements with poor structural coherence. Additionally, most existing human video datasets primarily focus on facial or upper-body motions, or consist of vertically oriented dance videos, limiting the scope of corresponding generation methods to simple movements. To overcome these challenges, we propose MoCo, which decouples the process of human video generation into two components: structure generation and appearance generation. Specifically, our method first employs an efficient 3D structure generator to produce a human motion sequence from a text prompt. The remaining video appearance is then synthesized under the guidance of the generated structural sequence. To improve fine-grained control over sparse human structures, we introduce Human-Aware Dynamic Control modules and integrate dense tracking constraints during training. Furthermore, recognizing the limitations of existing datasets, we construct a large-scale whole-body human video dataset featuring complex and diverse motions. Extensive experiments demonstrate that MoCo outperforms existing approaches in generating realistic and structurally coherent human videos.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation</title>
<link>https://arxiv.org/abs/2508.17408</link>
<guid>https://arxiv.org/abs/2508.17408</guid>
<content:encoded><![CDATA[
arXiv:2508.17408v1 Announce Type: new 
Abstract: Although the Segment Anything Model (SAM) has advanced medical image segmentation, its Bayesian adaptation for uncertainty-aware segmentation remains hindered by three key issues: (1) instability in Bayesian fine-tuning of large pre-trained SAMs; (2) high computation cost due to SAM's massive parameters; (3) SAM's black-box design limits interpretability. To overcome these, we propose E-BayesSAM, an efficient framework combining Token-wise Variational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving interpretability. T-VBI innovatively reinterprets SAM's output tokens as dynamic probabilistic weights and reparameterizes them as latent variables without auxiliary training, enabling training-free VBI for uncertainty estimation. SO-KAN improves token prediction with learnable spline activations via self-supervised learning, providing insight to prune redundant tokens to boost efficiency and accuracy. Experiments on five ultrasound datasets demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image), (ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\% vs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four critical tokens governing SAM's decisions. By unifying efficiency, reliability, and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs, advancing deployment in safety-critical medical applications. The source code is available at https://github.com/mp31192/E-BayesSAM.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Leakage in Visual Datasets</title>
<link>https://arxiv.org/abs/2508.17416</link>
<guid>https://arxiv.org/abs/2508.17416</guid>
<content:encoded><![CDATA[
arXiv:2508.17416v1 Announce Type: new 
Abstract: We analyze data leakage in visual datasets. Data leakage refers to images in evaluation benchmarks that have been seen during training, compromising fair model evaluation. Given that large-scale datasets are often sourced from the internet, where many computer vision benchmarks are publicly available, our efforts are focused into identifying and studying this phenomenon. We characterize visual leakage into different types according to its modality, coverage, and degree. By applying image retrieval techniques, we unequivocally show that all the analyzed datasets present some form of leakage, and that all types of leakage, from severe instances to more subtle cases, compromise the reliability of model evaluation in downstream tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17417</link>
<guid>https://arxiv.org/abs/2508.17417</guid>
<content:encoded><![CDATA[
arXiv:2508.17417v1 Announce Type: new 
Abstract: Vision-language models (VLMs) pre-trained on web-scale data exhibit promising zero-shot generalization but often suffer from semantic misalignment due to domain gaps between pre-training and downstream tasks. Existing approaches primarily focus on text prompting with class-specific descriptions and visual-text adaptation via aligning cropped image regions with textual descriptions. However, they still face the issues of incomplete textual prompts and noisy visual prompts. In this paper, we propose a novel constrained prompt enhancement (CPE) method to improve visual-textual alignment by constructing comprehensive textual prompts and compact visual prompts from the semantic perspective. Specifically, our approach consists of two key components: Topology-Guided Synonymous Semantic Generation (TGSSG) and Category-Agnostic Discriminative Region Selection (CADRS). Textually, to address the issue of incomplete semantic expression in textual prompts, our TGSSG first generates synonymous semantic set for each category via large language models, and constructs comprehensive textual prompts based on semantic ambiguity entropy and persistent homology analysis. Visually, to mitigate the irrelevant visual noise introduced by random cropping, our CADRS identifies discriminative regions with activation maps outputted by a pre-trained vision model, effectively filtering out noisy regions and generating compact visual prompts. Given the comprehensive set of textual prompts and compact set of visual prompts, we introduce two set-to-set matching strategies based on test-time adaptation (TTA) and optimal transport (OT) to achieve effective visual-textual alignment, and so improve zero-shot generalization of VLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</title>
<link>https://arxiv.org/abs/2508.17427</link>
<guid>https://arxiv.org/abs/2508.17427</guid>
<content:encoded><![CDATA[
arXiv:2508.17427v1 Announce Type: new 
Abstract: Point cloud registration based on correspondences computes the rigid transformation that maximizes the number of inliers constrained within the noise threshold. Current state-of-the-art (SOTA) methods employing spatial compatibility graphs or branch-and-bound (BnB) search mainly focus on registration under high outlier ratios. However, graph-based methods require at least quadratic space and time complexity for graph construction, while multi-stage BnB search methods often suffer from inaccuracy due to local optima between decomposed stages. This paper proposes a geometric maximum overlapping registration framework via rotation-only BnB search. The rigid transformation is decomposed using Chasles' theorem into a translation along rotation axis and a 2D rigid transformation. The optimal rotation axis and angle are searched via BnB, with residual parameters formulated as range maximum query (RMQ) problems. Firstly, the top-k candidate rotation axes are searched within a hemisphere parameterized by cube mapping, and the translation along each axis is estimated through interval stabbing of the correspondences projected onto that axis. Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D RMQ of geometric overlapping for axis-aligned rectangles, which is solved deterministically in polynomial time using sweep line algorithm with segment tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets demonstrate superior accuracy and efficiency over SOTA methods, while the time complexity is polynomial and the space complexity increases linearly with the number of points, even in the worst case.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedKLPR: Personalized Federated Learning for Person Re-Identification with Adaptive Pruning</title>
<link>https://arxiv.org/abs/2508.17431</link>
<guid>https://arxiv.org/abs/2508.17431</guid>
<content:encoded><![CDATA[
arXiv:2508.17431v1 Announce Type: new 
Abstract: Person re-identification (Re-ID) is a fundamental task in intelligent surveillance and public safety. Federated learning (FL) offers a privacy-preserving solution by enabling collaborative model training without centralized data collection. However, applying FL to real-world re-ID systems faces two major challenges: statistical heterogeneity across clients due to non-IID data distributions, and substantial communication overhead caused by frequent transmission of large-scale models. To address these issues, we propose FedKLPR, a lightweight and communication-efficient federated learning framework for person re-identification. FedKLPR introduces four key components. First, the KL-Divergence Regularization Loss (KLL) constrains local models by minimizing the divergence from the global feature distribution, effectively mitigating the effects of statistical heterogeneity and improving convergence stability under non-IID conditions. Secondly, KL-Divergence-Prune Weighted Aggregation (KLPWA) integrates pruning ratio and distributional similarity into the aggregation process, thereby improving the robustness of the global model while significantly reducing communication overhead. Furthermore, sparse Activation Skipping (SAS) mitigates the dilution of critical parameters during the aggregation of pruned client models by excluding zero-valued weights from the update process. Finally, Cross-Round Recovery (CRR) introduces a dynamic pruning control mechanism that halts pruning when necessary, enabling deeper compression while maintaining model accuracy. Experimental results on eight benchmark datasets demonstrate that FedKLPR achieves significant communication reduction. Compared with the state-of-the-art, FedKLPR reduces 33\%-38\% communication cost on ResNet-50 and 20\%-40\% communication cost on ResNet-34, while maintaining model accuracy within 1\% degradation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinySR: Pruning Diffusion for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.17434</link>
<guid>https://arxiv.org/abs/2508.17434</guid>
<content:encoded><![CDATA[
arXiv:2508.17434v1 Announce Type: new 
Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</title>
<link>https://arxiv.org/abs/2508.17435</link>
<guid>https://arxiv.org/abs/2508.17435</guid>
<content:encoded><![CDATA[
arXiv:2508.17435v1 Announce Type: new 
Abstract: Despite the remarkable capabilities of text-to-image (T2I) generation models, real-world applications often demand fine-grained, iterative image editing that existing methods struggle to provide. Key challenges include granular instruction understanding, robust context preservation during modifications, and the lack of intelligent feedback mechanisms for iterative refinement. This paper introduces RefineEdit-Agent, a novel, training-free intelligent agent framework designed to address these limitations by enabling complex, iterative, and context-aware image editing. RefineEdit-Agent leverages the powerful planning capabilities of Large Language Models (LLMs) and the advanced visual understanding and evaluation prowess of Vision-Language Large Models (LVLMs) within a closed-loop system. Our framework comprises an LVLM-driven instruction parser and scene understanding module, a multi-level LLM-driven editing planner for goal decomposition, tool selection, and sequence generation, an iterative image editing module, and a crucial LVLM-driven feedback and evaluation loop. To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new benchmark featuring 500 initial images with complex, multi-turn editing instructions across nine visual dimensions. Extensive experiments demonstrate that RefineEdit-Agent significantly outperforms state-of-the-art baselines, achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and 3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of iterative refinement, backbone choices, tool usage, and robustness to instruction complexity further validate the efficacy of our agentic design in delivering superior edit fidelity and context preservation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Geometry and Appearance for Efficient Multi-View Surface Reconstruction and Rendering</title>
<link>https://arxiv.org/abs/2508.17436</link>
<guid>https://arxiv.org/abs/2508.17436</guid>
<content:encoded><![CDATA[
arXiv:2508.17436v1 Announce Type: new 
Abstract: This paper addresses the limitations of neural rendering-based multi-view surface reconstruction methods, which require an additional mesh extraction step that is inconvenient and would produce poor-quality surfaces with mesh aliasing, restricting downstream applications. Building on the explicit mesh representation and differentiable rasterization framework, this work proposes an efficient solution that preserves the high efficiency of this framework while significantly improving reconstruction quality and versatility. Specifically, we introduce a disentangled geometry and appearance model that does not rely on deep networks, enhancing learning and broadening applicability. A neural deformation field is constructed to incorporate global geometric context, enhancing geometry learning, while a novel regularization constrains geometric features passed to a neural shader to ensure its accuracy and boost shading. For appearance, a view-invariant diffuse term is separated and baked into mesh vertices, further improving rendering efficiency. Experimental results demonstrate that the proposed method achieves state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds, with reconstruction quality that is competitive with top-performing methods. Moreover, the method enables practical applications such as mesh and texture editing, showcasing its versatility and application potential. This combination of efficiency, competitive quality, and broad applicability makes our approach a valuable contribution to multi-view surface reconstruction and rendering.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels</title>
<link>https://arxiv.org/abs/2508.17437</link>
<guid>https://arxiv.org/abs/2508.17437</guid>
<content:encoded><![CDATA[
arXiv:2508.17437v1 Announce Type: new 
Abstract: Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Domain Gaps for Indoor 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.17439</link>
<guid>https://arxiv.org/abs/2508.17439</guid>
<content:encoded><![CDATA[
arXiv:2508.17439v1 Announce Type: new 
Abstract: As a fundamental task for indoor scene understanding, 3D object detection has been extensively studied, and the accuracy on indoor point cloud data has been substantially improved. However, existing researches have been conducted on limited datasets, where the training and testing sets share the same distribution. In this paper, we consider the task of adapting indoor 3D object detectors from one dataset to another, presenting a comprehensive benchmark with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator. Since indoor point cloud datasets are collected and constructed in different ways, the object detectors are likely to overfit to specific factors within each dataset, such as point cloud quality, bounding box layout and instance features. We conduct experiments across datasets on different adaptation scenarios including synthetic-to-real adaptation, point cloud quality adaptation, layout adaptation and instance feature adaptation, analyzing the impact of different domain gaps on 3D object detectors. We also introduce several approaches to improve adaptation performances, providing baselines for domain adaptive indoor 3D object detection, hoping that future works may propose detectors with stronger generalization ability across domains. Our project homepage can be found in https://jeremyzhao1998.github.io/DAVoteNet-release/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level LVLM Guidance for Untrimmed Video Action Recognition</title>
<link>https://arxiv.org/abs/2508.17442</link>
<guid>https://arxiv.org/abs/2508.17442</guid>
<content:encoded><![CDATA[
arXiv:2508.17442v1 Announce Type: new 
Abstract: Action recognition and localization in complex, untrimmed videos remain a formidable challenge in computer vision, largely due to the limitations of existing methods in capturing fine-grained actions, long-term temporal dependencies, and high-level semantic information from low-level visual features. This paper introduces the Event-Contextualized Video Transformer (ECVT), a novel architecture that leverages the advanced semantic understanding capabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT employs a dual-branch design, comprising a Video Encoding Branch for spatio-temporal feature extraction and a Cross-Modal Guidance Branch. The latter utilizes an LVLM to generate multi-granularity semantic descriptions, including Global Event Prompting for macro-level narrative and Temporal Sub-event Prompting for fine-grained action details. These multi-level textual cues are integrated into the video encoder's learning process through sophisticated mechanisms such as adaptive gating for high-level semantic fusion, cross-modal attention for fine-grained feature refinement, and an event graph module for temporal context calibration. Trained end-to-end with a comprehensive loss function incorporating semantic consistency and temporal calibration terms, ECVT significantly enhances the model's ability to understand video temporal structures and event logic. Extensive experiments on ActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves state-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3 and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Synthetic Dataset for Manometry Recognition in Robotic Applications</title>
<link>https://arxiv.org/abs/2508.17468</link>
<guid>https://arxiv.org/abs/2508.17468</guid>
<content:encoded><![CDATA[
arXiv:2508.17468v1 Announce Type: new 
Abstract: This work addresses the challenges of data scarcity and high acquisition costs for training robust object detection models in complex industrial environments, such as offshore oil platforms. The practical and economic barriers to collecting real-world data in these hazardous settings often hamper the development of autonomous inspection systems. To overcome this, in this work we propose and validate a hybrid data synthesis pipeline that combines procedural rendering with AI-driven video generation. Our methodology leverages BlenderProc to create photorealistic images with precise annotations and controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2 world-foundation model to synthesize physically plausible video sequences with temporal diversity, capturing rare viewpoints and adverse conditions. We demonstrate that a YOLO-based detection network trained on a composite dataset, blending real images with our synthetic data, achieves superior performance compared to models trained exclusively on real-world data. Notably, a 1:1 mixture of real and synthetic data yielded the highest accuracy, surpassing the real-only baseline. These findings highlight the viability of a synthetic-first approach as an efficient, cost-effective, and safe alternative for developing reliable perception systems in safety-critical and resource-constrained industrial applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.17472</link>
<guid>https://arxiv.org/abs/2508.17472</guid>
<content:encoded><![CDATA[
arXiv:2508.17472v1 Announce Type: new 
Abstract: We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis</title>
<link>https://arxiv.org/abs/2508.17478</link>
<guid>https://arxiv.org/abs/2508.17478</guid>
<content:encoded><![CDATA[
arXiv:2508.17478v1 Announce Type: new 
Abstract: In the field of multimodal medical data analysis, leveraging diverse types of data and understanding their hidden relationships continues to be a research focus. The main challenges lie in effectively modeling the complex interactions between heterogeneous data modalities with distinct characteristics while capturing both local and global dependencies across modalities. To address these challenges, this paper presents a two-stage multimodal prognosis model, GraphMMP, which is based on graph neural networks. The proposed model constructs feature graphs using mutual information and features a global fusion module built on Mamba, which significantly boosts prognosis performance. Empirical results show that GraphMMP surpasses existing methods on datasets related to liver prognosis and the METABRIC study, demonstrating its effectiveness in multimodal medical prognosis tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning</title>
<link>https://arxiv.org/abs/2508.17488</link>
<guid>https://arxiv.org/abs/2508.17488</guid>
<content:encoded><![CDATA[
arXiv:2508.17488v1 Announce Type: new 
Abstract: This paper tackles the critical challenge of optimizing multi-modal trackers by effectively adapting the pre-trained models for RGB data. Existing fine-tuning paradigms oscillate between excessive freedom and over-restriction, both leading to a suboptimal plasticity-stability trade-off. To mitigate this dilemma, we propose a novel sensitivity-aware regularized tuning framework, which delicately refines the learning process by incorporating intrinsic parameter sensitivities. Through a comprehensive investigation from pre-trained to multi-modal contexts, we identify that parameters sensitive to pivotal foundational patterns and cross-domain shifts are primary drivers of this issue. Specifically, we first analyze the tangent space of pre-trained weights to measure and orient prior sensitivities, dedicated to preserving generalization. Then, we further explore transfer sensitivities during the tuning phase, emphasizing adaptability and stability. By incorporating these sensitivities as regularization terms, our method significantly enhances the transferability across modalities. Extensive experiments showcase the superior performance of the proposed method, surpassing current state-of-the-art techniques across various multi-modal tracking. The source code and models will be publicly available at https://github.com/zhiwen-xdu/SRTrack.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice</title>
<link>https://arxiv.org/abs/2508.17502</link>
<guid>https://arxiv.org/abs/2508.17502</guid>
<content:encoded><![CDATA[
arXiv:2508.17502v1 Announce Type: new 
Abstract: Human social behaviors are inherently multimodal necessitating the development of powerful audiovisual models for their perception. In this paper, we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE), which is pre-trained on audiovisual social data. Specifically, we modify CAV-MAE to receive a larger number of frames as input and pre-train it on a large dataset of human social interaction (VoxCeleb2) in a self-supervised manner. We demonstrate the effectiveness of this model by finetuning and evaluating the model on different social and affective downstream tasks, namely, emotion recognition, laughter detection and apparent personality estimation. The model achieves state-of-the-art results on multimodal emotion recognition and laughter recognition and competitive results for apparent personality estimation, demonstrating the effectiveness of in-domain self-supervised pre-training. Code and model weight are available here https://github.com/HuBohy/SocialMAE.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers</title>
<link>https://arxiv.org/abs/2508.17509</link>
<guid>https://arxiv.org/abs/2508.17509</guid>
<content:encoded><![CDATA[
arXiv:2508.17509v1 Announce Type: new 
Abstract: Training AI models to understand images without costly labeled data remains a challenge. We combine two techniques--DINO (teacher-student learning) and Barlow Twins (redundancy reduction)--to create a model that learns better with fewer labels and less compute. While both DINO and Barlow Twins have independently demonstrated strong performance in self-supervised learning, each comes with limitations--DINO may be sensitive to certain augmentations, and Barlow Twins often requires batch sizes too large to fit on consumer hardware. By combining the redundancy-reduction objective of Barlow Twins with the self-distillation strategy of DINO, we aim to leverage their complementary strengths. We train a hybrid model on the MS COCO dataset using only 10\% of labeled data for linear probing, and evaluate its performance against standalone DINO and Barlow Twins implementations. Preliminary results show that the combined approach achieves comparable loss and classification accuracy to DINO while maintaining strong feature representations. Attention visualizations further suggest improved semantic segmentation capability in the hybrid model. This combined method offers a scalable, label-efficient alternative for training ViTs in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation</title>
<link>https://arxiv.org/abs/2508.17524</link>
<guid>https://arxiv.org/abs/2508.17524</guid>
<content:encoded><![CDATA[
arXiv:2508.17524v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but remains constrained by fragmented, multi-stage workflows encompassing acquisition, reconstruction, segmentation, detection, diagnosis, and reporting. While deep learning has achieved progress in individual tasks, existing approaches are often anatomy- or application-specific and lack generalizability across diverse clinical settings. Moreover, current pipelines rarely integrate imaging data with complementary language information that radiologists rely on in routine practice. Here, we introduce OmniMRI, a unified vision-language foundation model designed to generalize across the entire MRI workflow. OmniMRI is trained on a large-scale, heterogeneous corpus curated from 60 public datasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating image-only data, paired vision-text data, and instruction-response data. Its multi-stage training paradigm, comprising self-supervised vision pretraining, vision-language alignment, multimodal pretraining, and multi-task instruction tuning, progressively equips the model with transferable visual representations, cross-modal reasoning, and robust instruction-following capabilities. Qualitative results demonstrate OmniMRI's ability to perform diverse tasks within a single architecture, including MRI reconstruction, anatomical and pathological segmentation, abnormality detection, diagnostic suggestion, and radiology report generation. These findings highlight OmniMRI's potential to consolidate fragmented pipelines into a scalable, generalist framework, paving the way toward foundation models that unify imaging and clinical language for comprehensive, end-to-end MRI interpretation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks</title>
<link>https://arxiv.org/abs/2508.17537</link>
<guid>https://arxiv.org/abs/2508.17537</guid>
<content:encoded><![CDATA[
arXiv:2508.17537v1 Announce Type: new 
Abstract: We address the problem of estimating both translational and angular velocity of a camera from asynchronous point tracks, a formulation relevant to rolling shutter and event cameras. Since the original problem is non-polynomial, we propose a polynomial approximation, classify the resulting minimal problems, and determine their algebraic degrees. Furthermore, we develop minimal solvers for several problems with low degrees and evaluate them on synthetic and real datasets. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection</title>
<link>https://arxiv.org/abs/2508.17567</link>
<guid>https://arxiv.org/abs/2508.17567</guid>
<content:encoded><![CDATA[
arXiv:2508.17567v1 Announce Type: new 
Abstract: Modern computer vision models have proven to be highly useful for medical imaging classification and segmentation tasks, but the scarcity of medical imaging data often limits the efficacy of models trained from scratch. Transfer learning has emerged as a pivotal solution to this, enabling the fine-tuning of high-performance models on small data. Mei et al. (2022) found that pre-training CNNs on a large dataset of radiologist-labeled images (RadImageNet) enhanced model performance on downstream tasks compared to ImageNet pretraining. The present work extends Mei et al. (2022) by conducting a comprehensive investigation to determine optimal CNN architectures for breast lesion malignancy detection and ACL tear detection, as well as performing statistical analysis to compare the effect of RadImageNet and ImageNet pre-training on downstream model performance. Our findings suggest that 1-dimensional convolutional classifiers with skip connections, ResNet50 pre-trained backbones, and partial backbone unfreezing yields optimal downstream medical classification performance. Our best models achieve AUCs of 0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy detection, competitive with the results reported by Mei et al. (2022) and surpassing other previous works. We do not find evidence confirming RadImageNet pre-training to provide superior downstream performance for ACL tear and breast lesion classification tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation</title>
<link>https://arxiv.org/abs/2508.17568</link>
<guid>https://arxiv.org/abs/2508.17568</guid>
<content:encoded><![CDATA[
arXiv:2508.17568v1 Announce Type: new 
Abstract: Metamaterials are micro-architected structures whose geometry imparts highly tunable-often counter-intuitive-bulk properties. Yet their design is difficult because of geometric complexity and a non-trivial mapping from architecture to behaviour. We address these challenges with three complementary contributions. (i) MetaDSL: a compact, semantically rich domain-specific language that captures diverse metamaterial designs in a form that is both human-readable and machine-parsable. (ii) MetaDB: a curated repository of more than 150,000 parameterized MetaDSL programs together with their derivatives-three-dimensional geometry, multi-view renderings, and simulated elastic properties. (iii) MetaBench: benchmark suites that test three core capabilities of vision-language metamaterial assistants-structure reconstruction, property-driven inverse design, and performance prediction. We establish baselines by fine-tuning state-of-the-art vision-language models and deploy an omni-model within an interactive, CAD-like interface. Case studies show that our framework provides a strong first step toward integrated design and understanding of structure-representation-property relationships.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data</title>
<link>https://arxiv.org/abs/2508.17579</link>
<guid>https://arxiv.org/abs/2508.17579</guid>
<content:encoded><![CDATA[
arXiv:2508.17579v1 Announce Type: new 
Abstract: For simulation and training purposes, military organizations have made substantial investments in developing high-resolution 3D virtual environments through extensive imaging and 3D scanning. However, the dynamic nature of battlefield conditions-where objects may appear or vanish over time-makes frequent full-scale updates both time-consuming and costly. In response, we introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with only a small set of newly acquired images. Our approach starts with camera pose estimation to align new images with the existing 3D model, followed by change detection to pinpoint modifications in the scene. A 3D generative AI model is then used to create high-quality 3D assets of the new elements, which are seamlessly integrated into the existing 3D model. The IDU pipeline incorporates human guidance to ensure high accuracy in object identification and placement, with each update focusing on a single new object at a time. Experimental results confirm that our proposed IDU pipeline significantly reduces update time and labor, offering a cost-effective and targeted solution for maintaining up-to-date 3D models in rapidly evolving military scenarios.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</title>
<link>https://arxiv.org/abs/2508.17588</link>
<guid>https://arxiv.org/abs/2508.17588</guid>
<content:encoded><![CDATA[
arXiv:2508.17588v1 Announce Type: new 
Abstract: Generation-driven world models create immersive virtual environments but suffer slow inference due to the iterative nature of diffusion models. While recent advances have improved diffusion model efficiency, directly applying these techniques to world models introduces limitations such as quality degradation. In this paper, we present HERO, a training-free hierarchical acceleration framework tailored for efficient world models. Owing to the multi-modal nature of world models, we identify a feature coupling phenomenon, wherein shallow layers exhibit high temporal variability, while deeper layers yield more stable feature representations. Motivated by this, HERO adopts hierarchical strategies to accelerate inference: (i) In shallow layers, a patch-wise refresh mechanism efficiently selects tokens for recomputation. With patch-wise sampling and frequency-aware tracking, it avoids extra metric computation and remain compatible with FlashAttention. (ii) In deeper layers, a linear extrapolation scheme directly estimates intermediate features. This completely bypasses the computations in attention modules and feed-forward networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</title>
<link>https://arxiv.org/abs/2508.17595</link>
<guid>https://arxiv.org/abs/2508.17595</guid>
<content:encoded><![CDATA[
arXiv:2508.17595v1 Announce Type: new 
Abstract: Reasoning about fine-grained spatial relationships in warehouse-scale environments poses a significant challenge for existing vision-language models (VLMs), which often struggle to comprehend 3D layouts, object arrangements, and multimodal cues in real-world industrial settings. In this paper, we present TinyGiantVLM, a lightweight and modular two-stage framework designed for physical spatial reasoning, distinguishing itself from traditional geographic reasoning in complex logistics scenes. Our approach encodes both global and region-level features from RGB and depth modalities using pretrained visual backbones. To effectively handle the complexity of high-modality inputs and diverse question types, we incorporate a Mixture-of-Experts (MoE) fusion module, which dynamically combines spatial representations to support downstream reasoning tasks and improve convergence. Training is conducted in a two-phase strategy: the first phase focuses on generating free-form answers to enhance spatial reasoning ability, while the second phase uses normalized answers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our 64M-parameter base model achieved 5th place on the leaderboard with a score of 66.8861, demonstrating strong performance in bridging visual perception and spatial understanding in industrial environments. We further present an 80M-parameter variant with expanded MoE capacity, which demonstrates improved performance on spatial reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HotSpotter - Patterned Species Instance Recognition</title>
<link>https://arxiv.org/abs/2508.17605</link>
<guid>https://arxiv.org/abs/2508.17605</guid>
<content:encoded><![CDATA[
arXiv:2508.17605v1 Announce Type: new 
Abstract: We present HotSpotter, a fast, accurate algorithm for identifying individual animals against a labeled database. It is not species specific and has been applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We describe two approaches, both based on extracting and matching keypoints or "hotspots". The first tests each new query image sequentially against each database image, generating a score for each database image in isolation, and ranking the results. The second, building on recent techniques for instance recognition, matches the query image against the database using a fast nearest neighbor search. It uses a competitive scoring mechanism derived from the Local Naive Bayes Nearest Neighbor algorithm recently proposed for category recognition. We demonstrate results on databases of more than 1000 images, producing more accurate matches than published methods and matching each query image in just a few seconds.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores</title>
<link>https://arxiv.org/abs/2508.17613</link>
<guid>https://arxiv.org/abs/2508.17613</guid>
<content:encoded><![CDATA[
arXiv:2508.17613v1 Announce Type: new 
Abstract: Prognostic modeling is essential for forecasting future clinical scores and enabling early detection of Alzheimers disease (AD). While most existing methods focus on predicting the ADAS-Cog global score, they often overlook the predictive value of its 13 sub-scores, which reflect distinct cognitive domains. Some sub-scores may exert greater influence on determining global scores. Assigning higher loss weights to these clinically meaningful sub-scores can guide the model to focus on more relevant cognitive domains, enhancing both predictive accuracy and interpretability. In this study, we propose a weighted Vision Transformer (ViT)-based multi-task learning (MTL) framework to jointly predict the ADAS-Cog global score using baseline MRI scans and its 13 sub-scores at Month 24. Our framework integrates ViT as a feature extractor and systematically investigates the impact of sub-score-specific loss weighting on model performance. Results show that our proposed weighting strategies are group-dependent: strong weighting improves performance for MCI subjects with more heterogeneous MRI patterns, while moderate weighting is more effective for CN subjects with lower variability. Our findings suggest that uniform weighting underutilizes key sub-scores and limits generalization. The proposed framework offers a flexible, interpretable approach to AD prognosis using end-to-end MRI-based learning. (Github repo link will be provided after review)
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on</title>
<link>https://arxiv.org/abs/2508.17614</link>
<guid>https://arxiv.org/abs/2508.17614</guid>
<content:encoded><![CDATA[
arXiv:2508.17614v1 Announce Type: new 
Abstract: Virtual try-on systems have long been hindered by heavy reliance on human body masks, limited fine-grained control over garment attributes, and poor generalization to real-world, in-the-wild scenarios. In this paper, we propose JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-On), a novel framework that overcomes these limitations by integrating diffusion-based image generation with multi-modal conditional fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our approach directly incorporates diverse control signals -- such as the reference person image and the target garment image -- into the denoising process through dedicated conditional pathways that fuse features within the self-attention layers. This fusion is further enhanced with refined positional encodings and attention masks, enabling precise spatial alignment and improved garment-person integration. To address data scarcity and quality, we introduce a bidirectional generation strategy for dataset construction: one pipeline uses a mask-based model to generate realistic reference images, while a symmetric ``Try-Off'' model, trained in a self-supervised manner, recovers the corresponding garment images. The synthesized dataset undergoes rigorous manual curation, allowing iterative improvement in visual fidelity and diversity. Experiments demonstrate that JCo-MVTON achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Moreover, it shows strong generalization in real-world applications, surpassing commercial systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Interpretability in Alzheimer's Prediction via Joint Learning of ADAS-Cog Scores</title>
<link>https://arxiv.org/abs/2508.17619</link>
<guid>https://arxiv.org/abs/2508.17619</guid>
<content:encoded><![CDATA[
arXiv:2508.17619v1 Announce Type: new 
Abstract: Accurate prediction of clinical scores is critical for early detection and prognosis of Alzheimers disease (AD). While existing approaches primarily focus on forecasting the ADAS-Cog global score, they often overlook the predictive value of its sub-scores (13 items), which capture domain-specific cognitive decline. In this study, we propose a multi task learning (MTL) framework that jointly predicts the global ADAS-Cog score and its sub-scores (13 items) at Month 24 using baseline MRI and longitudinal clinical scores from baseline and Month 6. The main goal is to examine how each sub scores particularly those associated with MRI features contribute to the prediction of the global score, an aspect largely neglected in prior MTL studies. We employ Vision Transformer (ViT) and Swin Transformer architectures to extract imaging features, which are fused with longitudinal clinical inputs to model cognitive progression. Our results show that incorporating sub-score learning improves global score prediction. Subscore level analysis reveals that a small subset especially Q1 (Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) consistently dominates the predicted global score. However, some of these influential sub-scores exhibit high prediction errors, pointing to model instability. Further analysis suggests that this is caused by clinical feature dominance, where the model prioritizes easily predictable clinical scores over more complex MRI derived features. These findings emphasize the need for improved multimodal fusion and adaptive loss weighting to achieve more balanced learning. Our study demonstrates the value of sub score informed modeling and provides insights into building more interpretable and clinically robust AD prediction frameworks. (Github repo provided)
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</title>
<link>https://arxiv.org/abs/2508.17634</link>
<guid>https://arxiv.org/abs/2508.17634</guid>
<content:encoded><![CDATA[
arXiv:2508.17634v1 Announce Type: new 
Abstract: LiDAR scanning in outdoor scenes acquires accurate distance measurements over wide areas, producing large-scale point clouds. Application examples for this data include robotics, automotive vehicles, and land surveillance. During such applications, outlier objects from outside the training data will inevitably appear. Our research contributes a novel approach to open-set segmentation, leveraging the learnings of object defect-detection research. We also draw on the Mamba architecture's strong performance in utilising long-range dependencies and scalability to large data. Combining both, we create a reconstruction based approach for the task of outdoor scene open-set segmentation. We show that our approach improves performance not only when applied to our our own open-set segmentation method, but also when applied to existing methods. Furthermore we contribute a Mamba based architecture which is competitive with existing voxel-convolution based methods on challenging, large-scale pointclouds.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wound3DAssist: A Practical Framework for 3D Wound Assessment</title>
<link>https://arxiv.org/abs/2508.17635</link>
<guid>https://arxiv.org/abs/2508.17635</guid>
<content:encoded><![CDATA[
arXiv:2508.17635v1 Announce Type: new 
Abstract: Managing chronic wounds remains a major healthcare challenge, with clinical assessment often relying on subjective and time-consuming manual documentation methods. Although 2D digital videometry frameworks aided the measurement process, these approaches struggle with perspective distortion, a limited field of view, and an inability to capture wound depth, especially in anatomically complex or curved regions. To overcome these limitations, we present Wound3DAssist, a practical framework for 3D wound assessment using monocular consumer-grade videos. Our framework generates accurate 3D models from short handheld smartphone video recordings, enabling non-contact, automatic measurements that are view-independent and robust to camera motion. We integrate 3D reconstruction, wound segmentation, tissue classification, and periwound analysis into a modular workflow. We evaluate Wound3DAssist across digital models with known geometry, silicone phantoms, and real patients. Results show that the framework supports high-quality wound bed visualization, millimeter-level accuracy, and reliable tissue composition analysis. Full assessments are completed in under 20 minutes, demonstrating feasibility for real-world clinical use.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Pattern Detection via Template Matching and Regression</title>
<link>https://arxiv.org/abs/2508.17636</link>
<guid>https://arxiv.org/abs/2508.17636</guid>
<content:encoded><![CDATA[
arXiv:2508.17636v1 Announce Type: new 
Abstract: We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.17638</link>
<guid>https://arxiv.org/abs/2508.17638</guid>
<content:encoded><![CDATA[
arXiv:2508.17638v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects visual features and then concatenates them with text tokens to form a unified sequence input for Large Language Models (LLMs). However, this paradigm leads to a significant increase in the length of the input sequence, resulting in substantial computational overhead. Existing methods attempt to fuse visual information into the intermediate layers of LLMs, which alleviate the sequence length issue but often neglect the hierarchical semantic representations within the model and the fine-grained visual information available in the shallower visual encoding layers. To address this limitation, we propose DEHVF, an efficient vision-language fine-tuning method based on dynamic embedding and fusion of hierarchical visual features. Its core lies in leveraging the inherent hierarchical representation characteristics of visual encoders and language models. Through a lightweight hierarchical visual fuser, it dynamically selects and fuses hierarchical features corresponding to semantic granularity based on the internal representations of each layer in LLMs. The fused layer-related visual features are then projected and aligned before being directly embedded into the Feed-Forward Network (FFN) of the corresponding layer in LLMs. This approach not only avoids sequence expansion but also dynamically fuses multi-layer visual information. By fine-tuning only a small number of parameters, DEHVF achieves precise alignment and complementarity of cross-modal information at the same semantic granularity. We conducted experiments across various VL benchmarks, including visual question answering on ScienceQA and image captioning on COCO Captions. The results demonstrate that DEHVF achieves higher accuracy than existing parameter-efficient fine-tuning (PEFT) baselines while maintaining efficient training and inference.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyTver: A Novel Loss Function for Longitudinal Multiple Sclerosis Lesion Segmentation</title>
<link>https://arxiv.org/abs/2508.17639</link>
<guid>https://arxiv.org/abs/2508.17639</guid>
<content:encoded><![CDATA[
arXiv:2508.17639v1 Announce Type: new 
Abstract: Longitudinal Multiple Sclerosis Lesion Segmentation is a particularly challenging problem that involves both input and output imbalance in the data and segmentation. Therefore in order to develop models that are practical, one of the solutions is to develop better loss functions. Most models naively use either Dice loss or Cross-Entropy loss or their combination without too much consideration. However, one must select an appropriate loss function as the imbalance can be mitigated by selecting a proper loss function. In order to solve the imbalance problem, multiple loss functions were proposed that claimed to solve it. They come with problems of their own which include being too computationally complex due to hyperparameters as exponents or having detrimental performance in metrics other than region-based ones. We propose a novel hybrid loss called HyTver that achieves good segmentation performance while maintaining performance in other metrics. We achieve a Dice score of 0.659 while also ensuring that the distance-based metrics are comparable to other popular functions. In addition, we also evaluate the stability of the loss functions when used on a pre- trained model and perform extensive comparisons with other popular loss functions
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloraSyntropy-Net: Scalable Deep Learning with Novel FloraSyntropy Archive for Large-Scale Plant Disease Diagnosis</title>
<link>https://arxiv.org/abs/2508.17653</link>
<guid>https://arxiv.org/abs/2508.17653</guid>
<content:encoded><![CDATA[
arXiv:2508.17653v1 Announce Type: new 
Abstract: Early diagnosis of plant diseases is critical for global food safety, yet most AI solutions lack the generalization required for real-world agricultural diversity. These models are typically constrained to specific species, failing to perform accurately across the broad spectrum of cultivated plants. To address this gap, we first introduce the FloraSyntropy Archive, a large-scale dataset of 178,922 images across 35 plant species, annotated with 97 distinct disease classes. We establish a benchmark by evaluating numerous existing models on this archive, revealing a significant performance gap. We then propose FloraSyntropy-Net, a novel federated learning framework (FL) that integrates a Memetic Algorithm (MAO) for optimal base model selection (DenseNet201), a novel Deep Block for enhanced feature representation, and a client-cloning strategy for scalable, privacy-preserving training. FloraSyntropy-Net achieves a state-of-the-art accuracy of 96.38% on the FloraSyntropy benchmark. Crucially, to validate its generalization capability, we test the model on the unrelated multiclass Pest dataset, where it demonstrates exceptional adaptability, achieving 99.84% accuracy. This work provides not only a valuable new resource but also a robust and highly generalizable framework that advances the field towards practical, large-scale agricultural AI applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark</title>
<link>https://arxiv.org/abs/2508.17658</link>
<guid>https://arxiv.org/abs/2508.17658</guid>
<content:encoded><![CDATA[
arXiv:2508.17658v1 Announce Type: new 
Abstract: Complex tubular structures are essential in medical imaging and computer-assisted diagnosis, where their integrity enhances anatomical visualization and lesion detection. However, existing segmentation algorithms struggle with structural discontinuities, particularly in severe clinical cases such as coronary artery stenosis and vessel occlusions, which leads to undesired discontinuity and compromising downstream diagnostic accuracy. Therefore, it is imperative to reconnect discontinuous structures to ensure their completeness. In this study, we explore the tubular structure completion based on point cloud for the first time and establish a Point Cloud-based Coronary Artery Completion (PC-CAC) dataset, which is derived from real clinical data. This dataset provides a novel benchmark for tubular structure completion. Additionally, we propose TSRNet, a Tubular Structure Reconnection Network that integrates a detail-preservated feature extractor, a multiple dense refinement strategy, and a global-to-local loss function to ensure accurate reconnection while maintaining structural integrity. Comprehensive experiments on our PC-CAC and two additional public datasets (PC-ImageCAS and PC-PTR) demonstrate that our method consistently outperforms state-of-the-art approaches across multiple evaluation metrics, setting a new benchmark for point cloud-based tubular structure reconstruction. Our benchmark is available at https://github.com/YaoleiQi/PCCAC.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M^3-GloDets: Multi-Region and Multi-Scale Analysis of Fine-Grained Diseased Glomerular Detection</title>
<link>https://arxiv.org/abs/2508.17666</link>
<guid>https://arxiv.org/abs/2508.17666</guid>
<content:encoded><![CDATA[
arXiv:2508.17666v1 Announce Type: new 
Abstract: Accurate detection of diseased glomeruli is fundamental to progress in renal pathology and underpins the delivery of reliable clinical diagnoses. Although recent advances in computer vision have produced increasingly sophisticated detection algorithms, the majority of research efforts have focused on normal glomeruli or instances of global sclerosis, leaving the wider spectrum of diseased glomerular subtypes comparatively understudied. This disparity is not without consequence; the nuanced and highly variable morphological characteristics that define these disease variants frequently elude even the most advanced computational models. Moreover, ongoing debate surrounds the choice of optimal imaging magnifications and region-of-view dimensions for fine-grained glomerular analysis, adding further complexity to the pursuit of accurate classification and robust segmentation.
  To bridge these gaps, we present M^3-GloDet, a systematic framework designed to enable thorough evaluation of detection models across a broad continuum of regions, scales, and classes. Within this framework, we evaluate both long-standing benchmark architectures and recently introduced state-of-the-art models that have achieved notable performance, using an experimental design that reflects the diversity of region-of-interest sizes and imaging resolutions encountered in routine digital renal pathology. As the results, we found that intermediate patch sizes offered the best balance between context and efficiency. Additionally, moderate magnifications enhanced generalization by reducing overfitting. Through systematic comparison of these approaches on a multi-class diseased glomerular dataset, our aim is to advance the understanding of model strengths and limitations, and to offer actionable insights for the refinement of automated detection strategies and clinical workflows in the digital pathology domain.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2508.17667</link>
<guid>https://arxiv.org/abs/2508.17667</guid>
<content:encoded><![CDATA[
arXiv:2508.17667v1 Announce Type: new 
Abstract: In trustworthy medical diagnosis systems, integrating out-of-distribution (OOD) detection aims to identify unknown diseases in samples, thereby mitigating the risk of misdiagnosis. In this study, we propose a novel OOD detection framework based on vision-language models (VLMs), which integrates hierarchical visual information to cope with challenging unknown diseases that resemble known diseases. Specifically, a cross-scale visual fusion strategy is proposed to couple visual embeddings from multiple scales. This enriches the detailed representation of medical images and thus improves the discrimination of unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation strategy is proposed to benefit OOD detection maximally. Experimental evaluations on three public medical datasets support that the proposed framework achieves superior OOD detection performance compared to existing methods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing</title>
<link>https://arxiv.org/abs/2508.17686</link>
<guid>https://arxiv.org/abs/2508.17686</guid>
<content:encoded><![CDATA[
arXiv:2508.17686v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) struggle with long-form videos due to the quadratic complexity of attention mechanisms. We propose Language-Guided Temporal Token Pruning (LGTTP), which leverages temporal cues from queries to adaptively prune video tokens, preserving contextual continuity while reducing computational overhead. Unlike uniform pruning or keyframe selection, LGTTP retains higher token density in temporally relevant segments. Our model-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a 65% reduction in computation while preserving 97-99% of the original performance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on Charades-STA, it retains 99.6% of R@1. It excels on queries with explicit temporal markers and remains effective across general video understanding tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset</title>
<link>https://arxiv.org/abs/2508.17699</link>
<guid>https://arxiv.org/abs/2508.17699</guid>
<content:encoded><![CDATA[
arXiv:2508.17699v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (XAI) has become an essential component of medical imaging research, aiming to increase transparency and clinical trust in deep learning models. This study investigates brain hemorrhage diagnosis with a focus on explainability through Class Activation Mapping (CAM) techniques. A pipeline was developed to extract pixellevel segmentation and detection annotations from classification models using nine state-of-the-art CAM algorithms, applied across multiple network stages, and quantitatively evaluated on the Hemorica dataset, which uniquely provides both slice-level labels and high-quality segmentation masks. Metrics including Dice, IoU, and pixel-wise overlap were employed to benchmark CAM variants. Results show that the strongest localization performance occurred at stage 5 of EfficientNetV2S, with HiResCAM yielding the highest bounding-box alignment and AblationCAM achieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong accuracy given that models were trained solely for classification without segmentation supervision. To the best of current knowledge, this is among the f irst works to quantitatively compare CAM methods for brain hemorrhage detection, establishing a reproducible benchmark and underscoring the potential of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.17708</link>
<guid>https://arxiv.org/abs/2508.17708</guid>
<content:encoded><![CDATA[
arXiv:2508.17708v1 Announce Type: new 
Abstract: Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction</title>
<link>https://arxiv.org/abs/2508.17712</link>
<guid>https://arxiv.org/abs/2508.17712</guid>
<content:encoded><![CDATA[
arXiv:2508.17712v1 Announce Type: new 
Abstract: Dynamic garment reconstruction from monocular video is an important yet challenging task due to the complex dynamics and unconstrained nature of the garments. Recent advancements in neural rendering have enabled high-quality geometric reconstruction with image/video supervision. However, implicit representation methods that use volume rendering often provide smooth geometry and fail to model high-frequency details. While template reconstruction methods model explicit geometry, they use vertex displacement for deformation, which results in artifacts. Addressing these limitations, we propose NGD, a Neural Gradient-based Deformation method to reconstruct dynamically evolving textured garments from monocular videos. Additionally, we propose a novel adaptive remeshing strategy for modelling dynamically evolving surfaces like wrinkles and pleats of the skirt, leading to high-quality reconstruction. Finally, we learn dynamic texture maps to capture per-frame lighting and shadow effects. We provide extensive qualitative and quantitative evaluations to demonstrate significant improvements over existing SOTA methods and provide high-quality garment reconstructions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</title>
<link>https://arxiv.org/abs/2508.17714</link>
<guid>https://arxiv.org/abs/2508.17714</guid>
<content:encoded><![CDATA[
arXiv:2508.17714v1 Announce Type: new 
Abstract: Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users' actual needs for revisiting semantically coherent content scattered across long-form conversations. To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues. As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns. Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments. To this end, we propose F2RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards promoting semantic precision, relevance, and contextual coherence. To handle varying intra-fragment complexity, from locally dense to sparsely distributed, we introduce difficulty-aware curriculum sampling that ranks training instances by model-predicted difficulty and gradually exposes the model to harder samples. This boosts reasoning ability in long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain and real-domain settings, demonstrating superior retrieval performance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instant Preference Alignment for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2508.17718</link>
<guid>https://arxiv.org/abs/2508.17718</guid>
<content:encoded><![CDATA[
arXiv:2508.17718v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation has greatly enhanced creative expression, yet achieving preference-aligned generation in a real-time and training-free manner remains challenging. Previous methods often rely on static, pre-collected preferences or fine-tuning, limiting adaptability to evolving and nuanced user intents. In this paper, we highlight the need for instant preference-aligned T2I generation and propose a training-free framework grounded in multimodal large language model (MLLM) priors. Our framework decouples the task into two components: preference understanding and preference-guided generation. For preference understanding, we leverage MLLMs to automatically extract global preference signals from a reference image and enrich a given prompt using structured instruction design. Our approach supports broader and more fine-grained coverage of user preferences than existing methods. For preference-guided generation, we integrate global keyword-based control and local region-aware cross-attention modulation to steer the diffusion model without additional training, enabling precise alignment across both global attributes and local elements. The entire framework supports multi-round interactive refinement, facilitating real-time and context-aware image generation. Extensive experiments on the Viper dataset and our collected benchmark demonstrate that our method outperforms prior approaches in both quantitative metrics and human evaluations, and opens up new possibilities for dialog-based generation and MLLM-diffusion integration.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</title>
<link>https://arxiv.org/abs/2508.17726</link>
<guid>https://arxiv.org/abs/2508.17726</guid>
<content:encoded><![CDATA[
arXiv:2508.17726v1 Announce Type: new 
Abstract: Human Action Anomaly Detection (HAAD) aims to identify anomalous actions given only normal action data during training. Existing methods typically follow a one-model-per-category paradigm, requiring separate training for each action category and a large number of normal samples. These constraints hinder scalability and limit applicability in real-world scenarios, where data is often scarce or novel categories frequently appear. To address these limitations, we propose a unified framework for HAAD that is compatible with few-shot scenarios. Our method constructs a category-agnostic representation space via contrastive learning, enabling AD by comparing test samples with a given small set of normal examples (referred to as the support set). To improve inter-category generalization and intra-category robustness, we introduce a generative motion augmentation strategy harnessing a diffusion-based foundation model for creating diverse and realistic training samples. Notably, to the best of our knowledge, our work is the first to introduce such a strategy specifically tailored to enhance contrastive learning for action AD. Extensive experiments on the HumanAct12 dataset demonstrate the state-of-the-art effectiveness of our approach under both seen and unseen category settings, regarding training efficiency and model scalability for few-shot HAAD.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning</title>
<link>https://arxiv.org/abs/2508.17728</link>
<guid>https://arxiv.org/abs/2508.17728</guid>
<content:encoded><![CDATA[
arXiv:2508.17728v1 Announce Type: new 
Abstract: Cervical cancer remains a significant global health concern and a leading cause of cancer-related deaths among women. Early detection through Pap smear tests is essential to reduce mortality rates; however, the manual examination is time consuming and prone to human error. This study proposes a deep learning framework that integrates U-Net for segmentation and a classification model to enhance diagnostic performance. The Herlev Pap Smear Dataset, a publicly available cervical cell dataset, was utilized for training and evaluation. The impact of segmentation on classification performance was evaluated by comparing the model trained on segmented images and another trained on non-segmented images. Experimental results showed that the use of segmented images marginally improved the model performance on precision (about 0.41 percent higher) and F1-score (about 1.30 percent higher), which suggests a slightly more balanced classification performance. While segmentation helps in feature extraction, the results showed that its impact on classification performance appears to be limited. The proposed framework offers a supplemental tool for clinical applications, which may aid pathologists in early diagnosis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp Segmentation</title>
<link>https://arxiv.org/abs/2508.17729</link>
<guid>https://arxiv.org/abs/2508.17729</guid>
<content:encoded><![CDATA[
arXiv:2508.17729v1 Announce Type: new 
Abstract: Automated colonic polyp segmentation is crucial for assisting doctors in screening of precancerous polyps and diagnosis of colorectal neoplasms. Although existing methods have achieved promising results, polyp segmentation remains hindered by the following limitations,including: (1) significant variation in polyp shapes and sizes, (2) indistinct boundaries between polyps and adjacent tissues, and (3) small-sized polyps are easily overlooked during the segmentation process. Driven by these practical difficulties, an innovative architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD module. The CMD module, serving as an innovative decoder, introduces a cross-scanning method to reduce blurry boundaries. The MSA module adopts a multi-branch parallel structure to enhance the recognition ability for polyps with diverse geometries and scale distributions. The FD module establishes dependencies among all decoder features to alleviate the under-detection of polyps with small-scale features. Experimental results show that CMFDNet outperforms six SOTA methods used for comparison, especially on ETIS and ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and 1.55%, respectively.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning</title>
<link>https://arxiv.org/abs/2508.17746</link>
<guid>https://arxiv.org/abs/2508.17746</guid>
<content:encoded><![CDATA[
arXiv:2508.17746v1 Announce Type: new 
Abstract: Estimating the 3D pose of a drone is important for anti-drone systems, but existing methods struggle with the unique challenges of drone keypoint detection. Drone propellers serve as keypoints but are difficult to detect due to their high visual similarity and diversity of poses. To address these challenges, we propose DroneKey, a framework that combines a 2D keypoint detector and a 3D pose estimator specifically designed for drones. In the keypoint detection stage, we extract two key-representations (intermediate and compact) from each transformer encoder layer and optimally combine them using a gated sum. We also introduce a pose-adaptive Mahalanobis distance in the loss function to ensure stable keypoint predictions across extreme poses. We built new datasets of drone 2D keypoints and 3D pose to train and evaluate our method, which have been publicly released. Experiments show that our method achieves an AP of 99.68% (OKS) in keypoint detection, outperforming existing methods. Ablation studies confirm that the pose-adaptive Mahalanobis loss function improves keypoint prediction stability and accuracy. Additionally, improvements in the encoder design enable real-time processing at 44 FPS. For 3D pose estimation, our method achieved an MAE-angle of 10.62{\deg}, an RMSE of 0.221m, and an MAE-absolute of 0.076m, demonstrating high accuracy and reliability. The code and dataset are available at https://github.com/kkanuseobin/DroneKey.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Global to Local: Social Bias Transfer in CLIP</title>
<link>https://arxiv.org/abs/2508.17750</link>
<guid>https://arxiv.org/abs/2508.17750</guid>
<content:encoded><![CDATA[
arXiv:2508.17750v1 Announce Type: new 
Abstract: The recycling of contrastive language-image pre-trained (CLIP) models as backbones for a large number of downstream tasks calls for a thorough analysis of their transferability implications, especially their well-documented reproduction of social biases and human stereotypes. How do such biases, learned during pre-training, propagate to downstream applications like visual question answering or image captioning? Do they transfer at all?
  We investigate this phenomenon, referred to as bias transfer in prior literature, through a comprehensive empirical analysis. Firstly, we examine how pre-training bias varies between global and local views of data, finding that bias measurement is highly dependent on the subset of data on which it is computed. Secondly, we analyze correlations between biases in the pre-trained models and the downstream tasks across varying levels of pre-training bias, finding difficulty in discovering consistent trends in bias transfer. Finally, we explore why this inconsistency occurs, showing that under the current paradigm, representation spaces of different pre-trained CLIPs tend to converge when adapted for downstream tasks. We hope this work offers valuable insights into bias behavior and informs future research to promote better bias mitigation practices.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.17760</link>
<guid>https://arxiv.org/abs/2508.17760</guid>
<content:encoded><![CDATA[
arXiv:2508.17760v1 Announce Type: new 
Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model's understanding of the concept of interactive "actions" more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Anomaly Detection in Industrial Environments via Meta-Learning</title>
<link>https://arxiv.org/abs/2508.17789</link>
<guid>https://arxiv.org/abs/2508.17789</guid>
<content:encoded><![CDATA[
arXiv:2508.17789v1 Announce Type: new 
Abstract: Anomaly detection is fundamental for ensuring quality control and operational efficiency in industrial environments, yet conventional approaches face significant challenges when training data contains mislabeled samples-a common occurrence in real-world scenarios. This paper presents RAD, a robust anomaly detection framework that integrates Normalizing Flows with Model-Agnostic Meta-Learning to address the critical challenge of label noise in industrial settings. Our approach employs a bi-level optimization strategy where meta-learning enables rapid adaptation to varying noise conditions, while uncertainty quantification guides adaptive L2 regularization to maintain model stability. The framework incorporates multiscale feature processing through pretrained feature extractors and leverages the precise likelihood estimation capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance, achieving I-AUROC scores of 95.4% and 94.6% respectively under clean conditions, while maintaining robust detection capabilities above 86.8% and 92.1% even when 50% of training samples are mislabeled. The results highlight RAD's exceptional resilience to noisy training conditions and its ability to detect subtle anomalies across diverse industrial scenarios, making it a practical solution for real-world anomaly detection applications where perfect data curation is challenging.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketchpose: Learning to Segment Cells with Partial Annotations</title>
<link>https://arxiv.org/abs/2508.17798</link>
<guid>https://arxiv.org/abs/2508.17798</guid>
<content:encoded><![CDATA[
arXiv:2508.17798v1 Announce Type: new 
Abstract: The most popular networks used for cell segmentation (e.g. Cellpose, Stardist, HoverNet,...) rely on a prediction of a distance map. It yields unprecedented accuracy but hinges on fully annotated datasets. This is a serious limitation to generate training sets and perform transfer learning. In this paper, we propose a method that still relies on the distance map and handles partially annotated objects. We evaluate the performance of the proposed approach in the contexts of frugal learning, transfer learning and regular learning on regular databases. Our experiments show that it can lead to substantial savings in time and resources without sacrificing segmentation quality. The proposed algorithm is embedded in a user-friendly Napari plugin.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models</title>
<link>https://arxiv.org/abs/2508.17807</link>
<guid>https://arxiv.org/abs/2508.17807</guid>
<content:encoded><![CDATA[
arXiv:2508.17807v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) typically process a significantly larger number of visual tokens compared to text tokens due to the inherent redundancy in visual signals. Visual token pruning is a promising direction to reduce the computational cost of VLMs by eliminating redundant visual tokens. The text-visual attention score is a widely adopted criterion for visual token pruning as it reflects the relevance of visual tokens to the text input. However, many sequence models exhibit a recency bias, where tokens appearing later in the sequence exert a disproportionately large influence on the model's output. In VLMs, this bias manifests as inflated attention scores for tokens corresponding to the lower regions of the image, leading to suboptimal pruning that disproportionately retains tokens from the image bottom. In this paper, we present an extremely simple yet effective approach to alleviate the recency bias in visual token pruning. We propose a straightforward reweighting mechanism that adjusts the attention scores of visual tokens according to their spatial positions in the image. Our method, termed Position-reweighted Visual Token Pruning, is a plug-and-play solution that can be seamlessly incorporated into existing visual token pruning frameworks without any changes to the model architecture or extra training. Extensive experiments on LVLMs demonstrate that our method improves the performance of visual token pruning with minimal computational overhead.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</title>
<link>https://arxiv.org/abs/2508.17816</link>
<guid>https://arxiv.org/abs/2508.17816</guid>
<content:encoded><![CDATA[
arXiv:2508.17816v1 Announce Type: new 
Abstract: During raw-data acquisition in CT imaging, diverse factors can degrade the collected sinograms, with undersampling and noise leading to severe artifacts and noise in reconstructed images and compromising diagnostic accuracy. Conventional correction methods rely on manually designed algorithms or fixed empirical parameters, but these approaches often lack generalizability across heterogeneous artifact types. To address these limitations, we propose UniSino, a foundation model for universal CT sinogram standardization. Unlike existing foundational models that operate in image domain, UniSino directly standardizes data in the projection domain, which enables stronger generalization across diverse undersampling scenarios. Its training framework incorporates the physical characteristics of sinograms, enhancing generalization and enabling robust performance across multiple subtasks spanning four benchmark datasets. Experimental results demonstrate thatUniSino achieves superior reconstruction quality both single and mixed undersampling case, demonstrating exceptional robustness and generalization in sinogram enhancement for CT imaging. The code is available at: https://github.com/yqx7150/UniSino.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration</title>
<link>https://arxiv.org/abs/2508.17817</link>
<guid>https://arxiv.org/abs/2508.17817</guid>
<content:encoded><![CDATA[
arXiv:2508.17817v1 Announce Type: new 
Abstract: Existing multi-modal fusion methods typically apply static frame-based image fusion techniques directly to video fusion tasks, neglecting inherent temporal dependencies and leading to inconsistent results across frames. To address this limitation, we propose the first video fusion framework that explicitly incorporates temporal modeling with visual-semantic collaboration to simultaneously ensure visual fidelity, semantic accuracy, and temporal consistency. First, we introduce a visual-semantic interaction module consisting of a semantic branch and a visual branch, with Dinov2 and VGG19 employed for targeted distillation, allowing simultaneous enhancement of both the visual and semantic representations. Second, we pioneer integrate the video degradation enhancement task into the video fusion pipeline by constructing a temporal cooperative module, which leverages temporal dependencies to facilitate weak information recovery. Third, to ensure temporal consistency, we embed a temporal-enhanced mechanism into the network and devise a temporal loss to guide the optimization process. Finally, we introduce two innovative evaluation metrics tailored for video fusion, aimed at assessing the temporal consistency of the generated fused videos. Extensive experimental results on public video datasets demonstrate the superiority of our method. Our code is released at https://github.com/Meiqi-Gong/TemCoCo.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.17827</link>
<guid>https://arxiv.org/abs/2508.17827</guid>
<content:encoded><![CDATA[
arXiv:2508.17827v1 Announce Type: new 
Abstract: Industrial and medical anomaly detection faces critical challenges from data scarcity and prohibitive annotation costs, particularly in evolving manufacturing and healthcare settings. To address this, we propose CoZAD, a novel zero-shot anomaly detection framework that integrates soft confident learning with meta-learning and contrastive feature representation. Unlike traditional confident learning that discards uncertain samples, our method assigns confidence-based weights to all training data, preserving boundary information while emphasizing prototypical normal patterns. The framework quantifies data uncertainty through IQR-based thresholding and model uncertainty via covariance based regularization within a Model-Agnostic Meta-Learning. Contrastive learning creates discriminative feature spaces where normal patterns form compact clusters, enabling rapid domain adaptation. Comprehensive evaluation across 10 datasets spanning industrial and medical domains demonstrates state-of-the-art performance, outperforming existing methods on 6 out of 7 industrial benchmarks with notable improvements on texture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and pixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates dependence on vision-language alignments or model ensembles, making it valuable for resourceconstrained environments requiring rapid deployment.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</title>
<link>https://arxiv.org/abs/2508.17832</link>
<guid>https://arxiv.org/abs/2508.17832</guid>
<content:encoded><![CDATA[
arXiv:2508.17832v1 Announce Type: new 
Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior design, embodied intelligence, and scene understanding. While existing methods have made progress in coarse-scale furniture arrangement, they struggle to capture fine-grained object placements, limiting the realism and utility of generated environments. This gap hinders immersive virtual experiences and detailed scene comprehension for embodied AI applications. To address these issues, we propose Hierarchical Layout Generation (HLG), a novel method for fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine hierarchical approach, refining scene layouts from large-scale furniture placement to intricate object arrangements. Specifically, our fine-grained layout alignment module constructs a hierarchical layout through vertical and horizontal decoupling, effectively decomposing complex 3D indoor scenes into multiple levels of granularity. Additionally, our trainable layout optimization network addresses placement issues, such as incorrect positioning, orientation errors, and object intersections, ensuring structurally coherent and physically plausible scene generation. We demonstrate the effectiveness of our approach through extensive experiments, showing superior performance in generating realistic indoor scenes compared to existing methods. This work advances the field of scene generation and opens new possibilities for applications requiring detailed 3D environments. We will release our code upon publication to encourage future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection</title>
<link>https://arxiv.org/abs/2508.17843</link>
<guid>https://arxiv.org/abs/2508.17843</guid>
<content:encoded><![CDATA[
arXiv:2508.17843v1 Announce Type: new 
Abstract: The difficulty of pixel-level annotation has significantly hindered the development of the Camouflaged Object Detection (COD) field. To save on annotation costs, previous works leverage the semi-supervised COD framework that relies on a small number of labeled data and a large volume of unlabeled data. We argue that there is still significant room for improvement in the effective utilization of unlabeled data. To this end, we introduce a Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection (SCOUT). It includes an Adaptive Data Augment and Selection (ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable data for annotation through an adversarial augment and sampling strategy. The TFM module further leverages the selected valuable data by combining camouflage-related knowledge and text-visual interaction. To adapt to this work, we build a new dataset, namely RefTextCOD. Extensive experiments show that the proposed method surpasses previous semi-supervised methods in the COD field and achieves state-of-the-art performance. Our code will be released at https://github.com/Heartfirey/SCOUT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Data Augmentation for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2508.17844</link>
<guid>https://arxiv.org/abs/2508.17844</guid>
<content:encoded><![CDATA[
arXiv:2508.17844v1 Announce Type: new 
Abstract: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data. We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Our proposed approach uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images. Generated samples undergo dynamic quality validation through a latentspace segmentation network that ensures accurate localization while enabling single-step inference. The text prompts, derived from medical literature, guide the generation of diverse abnormality types without requiring manual annotation. Our validation mechanism filters synthetic samples based on spatial accuracy, maintaining quality while operating efficiently through direct latent estimation. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternating Training-based Label Smoothing Enhances Prompt Generalization</title>
<link>https://arxiv.org/abs/2508.17846</link>
<guid>https://arxiv.org/abs/2508.17846</guid>
<content:encoded><![CDATA[
arXiv:2508.17846v1 Announce Type: new 
Abstract: Recent advances in pre-trained vision-language models have demonstrated remarkable zero-shot generalization capabilities. To further enhance these models' adaptability to various downstream tasks, prompt tuning has emerged as a parameter-efficient fine-tuning method. However, despite its efficiency, the generalization ability of prompt remains limited. In contrast, label smoothing (LS) has been widely recognized as an effective regularization technique that prevents models from becoming over-confident and improves their generalization. This inspires us to explore the integration of LS with prompt tuning. However, we have observed that the vanilla LS even weakens the generalization ability of prompt tuning. To address this issue, we propose the Alternating Training-based Label Smoothing (ATLaS) method, which alternately trains with standard one-hot labels and soft labels generated by LS to supervise the prompt tuning. Moreover, we introduce two types of efficient offline soft labels, including Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide inter-class or instance-class relationships for prompt tuning. The theoretical properties of the proposed ATLaS method are analyzed. Extensive experiments demonstrate that the proposed ATLaS method, combined with CSL and ISL, consistently enhances the generalization performance of prompt tuning. Moreover, the proposed ATLaS method exhibits high compatibility with prevalent prompt tuning methods, enabling seamless integration into existing methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Box-Level Class-Balanced Sampling for Active Object Detection</title>
<link>https://arxiv.org/abs/2508.17849</link>
<guid>https://arxiv.org/abs/2508.17849</guid>
<content:encoded><![CDATA[
arXiv:2508.17849v1 Announce Type: new 
Abstract: Training deep object detectors demands expensive bounding box annotation. Active learning (AL) is a promising technique to alleviate the annotation burden. Performing AL at box-level for object detection, i.e., selecting the most informative boxes to label and supplementing the sparsely-labelled image with pseudo labels, has been shown to be more cost-effective than selecting and labelling the entire image. In box-level AL for object detection, we observe that models at early stage can only perform well on majority classes, making the pseudo labels severely class-imbalanced. We propose a class-balanced sampling strategy to select more objects from minority classes for labelling, so as to make the final training data, \ie, ground truth labels obtained by AL and pseudo labels, more class-balanced to train a better model. We also propose a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo labels. We evaluate our method on public benchmarking datasets and show that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</title>
<link>https://arxiv.org/abs/2508.17857</link>
<guid>https://arxiv.org/abs/2508.17857</guid>
<content:encoded><![CDATA[
arXiv:2508.17857v1 Announce Type: new 
Abstract: In this study, we introduce a novel method called group-wise \textbf{VI}sual token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue of inefficient inference stemming from excessive visual tokens in multimoal large language models (MLLMs). Compared with previous token pruning approaches, our method can preserve more visual information while compressing visual tokens. We first propose a graph-based visual token aggregation (VTA) module. VTA treats each visual token as a node, forming a graph based on semantic similarity among visual tokens. It then aggregates information from removed tokens into kept tokens based on this graph, producing a more compact visual token representation. Additionally, we introduce a group-wise token selection strategy (GTS) to divide visual tokens into kept and removed ones, guided by text tokens from the final layers of each group. This strategy progressively aggregates visual information, enhancing the stability of the visual information extraction process. We conduct comprehensive experiments on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate the efficacy of VISA. Our method consistently outperforms previous methods, achieving a superior trade-off between model performance and inference speed. The code is available at https://github.com/mobiushy/VISA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering</title>
<link>https://arxiv.org/abs/2508.17860</link>
<guid>https://arxiv.org/abs/2508.17860</guid>
<content:encoded><![CDATA[
arXiv:2508.17860v1 Announce Type: new 
Abstract: The advancement of Multimodal Large Language Models (MLLMs) has driven significant progress in Visual Question Answering (VQA), evolving from Single to Multi Image VQA (MVQA). However, the increased number of images in MVQA inevitably introduces substantial visual redundancy that is irrelevant to question answering, negatively impacting both accuracy and efficiency. To address this issue, existing methods lack flexibility in controlling the number of compressed visual tokens and tend to produce discrete visual fragments, which hinder MLLMs' ability to comprehend images holistically. In this paper, we propose a straightforward yet universal Adaptive Visual Anchoring strategy, which can be seamlessly integrated into existing MLLMs, offering significant accuracy improvements through adaptive compression. Meanwhile, to balance the results derived from both global and compressed visual input, we further introduce a novel collaborative decoding mechanism, enabling optimal performance. Extensive experiments validate the effectiveness of our method, demonstrating consistent performance improvements across various MLLMs. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera Pose Refinement via 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.17876</link>
<guid>https://arxiv.org/abs/2508.17876</guid>
<content:encoded><![CDATA[
arXiv:2508.17876v1 Announce Type: new 
Abstract: Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2508.17877</link>
<guid>https://arxiv.org/abs/2508.17877</guid>
<content:encoded><![CDATA[
arXiv:2508.17877v1 Announce Type: new 
Abstract: The rapid advancement of generative models has led to a growing prevalence of highly realistic AI-generated images, posing significant challenges for digital forensics and content authentication. Conventional detection methods mainly rely on deep learning models that extract global features, which often overlook subtle structural inconsistencies and demand substantial computational resources. To address these limitations, we propose a hybrid detection framework that combines a fine-tuned Vision Transformer (ViT) with a novel edge-based image processing module. The edge-based module computes variance from edge-difference maps generated before and after smoothing, exploiting the observation that AI-generated images typically exhibit smoother textures, weaker edges, and reduced noise compared to real images. When applied as a post-processing step on ViT predictions, this module enhances sensitivity to fine-grained structural cues while maintaining computational efficiency. Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets demonstrate that the proposed framework achieves superior detection performance across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on CIFAKE, surpassing widely adopted state-of-the-art models. These results establish the proposed method as a lightweight, interpretable, and effective solution for both still images and video frames, making it highly suitable for real-world applications in automated content verification and digital forensics.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</title>
<link>https://arxiv.org/abs/2508.17885</link>
<guid>https://arxiv.org/abs/2508.17885</guid>
<content:encoded><![CDATA[
arXiv:2508.17885v1 Announce Type: new 
Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniAPO: Unified Multimodal Automated Prompt Optimization</title>
<link>https://arxiv.org/abs/2508.17890</link>
<guid>https://arxiv.org/abs/2508.17890</guid>
<content:encoded><![CDATA[
arXiv:2508.17890v1 Announce Type: new 
Abstract: Prompting is fundamental to unlocking the full potential of large language models. To automate and enhance this process, automatic prompt optimization (APO) has been developed, demonstrating effectiveness primarily in text-only input scenarios. However, extending existing APO methods to multimodal tasks, such as video-language generation introduces two core challenges: (i) visual token inflation, where long visual token sequences restrict context capacity and result in insufficient feedback signals; (ii) a lack of process-level supervision, as existing methods focus on outcome-level supervision and overlook intermediate supervision, limiting prompt optimization. We present UniAPO: Unified Multimodal Automated Prompt Optimization, the first framework tailored for multimodal APO. UniAPO adopts an EM-inspired optimization process that decouples feedback modeling and prompt refinement, making the optimization more stable and goal-driven. To further address the aforementioned challenges, we introduce a short-long term memory mechanism: historical feedback mitigates context limitations, while historical prompts provide directional guidance for effective prompt optimization. UniAPO achieves consistent gains across text, image, and video benchmarks, establishing a unified framework for efficient and transferable prompt optimization.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Practical Models for Isolated Word Visual Speech Recognition</title>
<link>https://arxiv.org/abs/2508.17894</link>
<guid>https://arxiv.org/abs/2508.17894</guid>
<content:encoded><![CDATA[
arXiv:2508.17894v1 Announce Type: new 
Abstract: Visual speech recognition (VSR) systems decode spoken words from an input sequence using only the video data. Practical applications of such systems include medical assistance as well as human-machine interactions. A VSR system is typically employed in a complementary role in cases where the audio is corrupt or not available. In order to accurately predict the spoken words, these architectures often rely on deep neural networks in order to extract meaningful representations from the input sequence. While deep architectures achieve impressive recognition performance, relying on such models incurs significant computation costs which translates into increased resource demands in terms of hardware requirements and results in limited applicability in real-world scenarios where resources might be constrained. This factor prevents wider adoption and deployment of speech recognition systems in more practical applications. In this work, we aim to alleviate this issue by developing architectures for VSR that have low hardware costs. Following the standard two-network design paradigm, where one network handles visual feature extraction and another one utilizes the extracted features to classify the entire sequence, we develop lightweight end-to-end architectures by first benchmarking efficient models from the image classification literature, and then adopting lightweight block designs in a temporal convolution network backbone. We create several unified models with low resource requirements but strong recognition performance. Experiments on the largest public database for English words demonstrate the effectiveness and practicality of our developed models. Code and trained models will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</title>
<link>https://arxiv.org/abs/2508.17916</link>
<guid>https://arxiv.org/abs/2508.17916</guid>
<content:encoded><![CDATA[
arXiv:2508.17916v1 Announce Type: new 
Abstract: Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant domain adaptability limitations and semantic perception deficiencies when applied to endoscopy. In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Furthermore, we design a mask-guided smoothness loss to enforce depth consistency within anatomical tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. This work contributes to augmenting surgeons' spatial perception during minimally invasive procedures, thereby enhancing surgical precision and safety, with crucial implications for augmented reality and navigation systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation</title>
<link>https://arxiv.org/abs/2508.17924</link>
<guid>https://arxiv.org/abs/2508.17924</guid>
<content:encoded><![CDATA[
arXiv:2508.17924v1 Announce Type: new 
Abstract: Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces a novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with a 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops</title>
<link>https://arxiv.org/abs/2508.17932</link>
<guid>https://arxiv.org/abs/2508.17932</guid>
<content:encoded><![CDATA[
arXiv:2508.17932v1 Announce Type: new 
Abstract: Human video comprehension demonstrates dynamic coordination between reasoning and visual attention, adaptively focusing on query-relevant details. However, current long-form video question answering systems employ rigid pipelines that decouple reasoning from perception, leading to either information loss through premature visual abstraction or computational inefficiency through exhaustive processing. The core limitation lies in the inability to adapt visual extraction to specific reasoning requirements, different queries demand fundamentally different visual evidence from the same video content. In this work, we present CAVIA, a training-free framework that revolutionizes video understanding through reasoning, perception coordination. Unlike conventional approaches where visual processing operates independently of reasoning, CAVIA creates a closed-loop system where reasoning continuously guides visual extraction based on identified information gaps. CAVIA introduces three innovations: (1) hierarchical reasoning, guided localization to precise frames; (2) cross-modal semantic bridging for targeted extraction; (3) confidence-driven iterative synthesis. CAVIA achieves state-of-the-art performance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA (76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic reasoning-perception coordination provides a scalable paradigm for video understanding.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets</title>
<link>https://arxiv.org/abs/2508.17961</link>
<guid>https://arxiv.org/abs/2508.17961</guid>
<content:encoded><![CDATA[
arXiv:2508.17961v1 Announce Type: new 
Abstract: This study aims to investigate the effect of various beam geometries and dimensions of input data on the sparse-sampling streak artifact correction task with U-Nets for clinical CT scans as a means of incorporating the volumetric context into artifact reduction tasks to improve model performance. A total of 22 subjects were retrospectively selected (01.2016-12.2018) from the Technical University of Munich's research hospital, TUM Klinikum rechts der Isar. Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel, fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and 3D U-Nets were trained and validated on 14, and tested on 8 subjects, respectively. For the dimensionality study, in addition to the 512x512 2D CT images, the CT scans were further pre-processed to generate a so-called '2.5D', and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data refers to individual 64-voxel blocks. An axial, coronal, and sagittal cut through the center of each block resulted in three 64x64 2D patches that were rearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance was assessed with the mean squared error (MSE) and structural similarity index measure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices results in the best MSE and SSIM values, outperforming the 2.5D and 3D input data dimensions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</title>
<link>https://arxiv.org/abs/2508.17972</link>
<guid>https://arxiv.org/abs/2508.17972</guid>
<content:encoded><![CDATA[
arXiv:2508.17972v1 Announce Type: new 
Abstract: Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM) problem by directly regressing camera poses and 3D scene structures from input images. They demonstrate impressive performance in handling images under extreme viewpoint changes. However, these methods struggle to handle a large number of input images. To address this problem, we introduce SAIL-Recon, a feed-forward Transformer for large scale SfM, by augmenting the scene regression network with visual localization capabilities. Specifically, our method first computes a neural scene representation from a subset of anchor images. The regression network is then fine-tuned to reconstruct all input images conditioned on this neural scene representation. Comprehensive experiments show that our method not only scales efficiently to large-scale scenes, but also achieves state-of-the-art results on both camera pose estimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and Tanks & Temples. We will publish our model and code. Code and models are publicly available at: https://hkust-sail.github.io/ sail-recon/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.17975</link>
<guid>https://arxiv.org/abs/2508.17975</guid>
<content:encoded><![CDATA[
arXiv:2508.17975v1 Announce Type: new 
Abstract: The use of computer vision in automotive is a trending research in which safety and security are a primary concern. In particular, for autonomous driving, preventing road accidents requires highly accurate object detection under diverse conditions. To address this issue, recently the International Organization for Standardization (ISO) released the 8800 norm, providing structured frameworks for managing associated AI relevant risks. However, challenging scenarios such as adverse weather or low lighting often introduce data drift, leading to degraded model performance and potential safety violations. In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Our dual mode framework utilized YOLO version 8 for swift detection and incorporated a five-layer CNN for verification. The system functioned in sequence and improved the detection accuracy by more than 90\% when tested with drift-augmented road images. The focus was to demonstrate how such a hybrid model can provide better road safety when working together in a hybrid structure.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</title>
<link>https://arxiv.org/abs/2508.17976</link>
<guid>https://arxiv.org/abs/2508.17976</guid>
<content:encoded><![CDATA[
arXiv:2508.17976v1 Announce Type: new 
Abstract: The increasing sophistication of image manipulation techniques demands robust forensic solutions that can both reliably detect alterations and precisely localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show promise by leveraging world knowledge and semantic understanding for context-aware detection, yet they struggle with perceiving subtle, low-level forensic artifacts crucial for accurate manipulation localization. This paper presents a novel Propose-Rectify framework that effectively bridges semantic reasoning with forensic-specific analysis. In the proposal stage, our approach utilizes a forensic-adapted LLaVA model to generate initial manipulation analysis and preliminary localization of suspicious regions based on semantic understanding and contextual reasoning. In the rectification stage, we introduce a Forensics Rectification Module that systematically validates and refines these initial proposals through multi-scale forensic feature analysis, integrating technical evidence from several specialized filters. Additionally, we present an Enhanced Segmentation Module that incorporates critical forensic cues into SAM's encoded image embeddings, thereby overcoming inherent semantic biases to achieve precise delineation of manipulated regions. By synergistically combining advanced multimodal reasoning with established forensic methodologies, our framework ensures that initial semantic proposals are systematically validated and enhanced through concrete technical evidence, resulting in comprehensive detection accuracy and localization precision. Extensive experimental validation demonstrates state-of-the-art performance across diverse datasets with exceptional robustness and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.18007</link>
<guid>https://arxiv.org/abs/2508.18007</guid>
<content:encoded><![CDATA[
arXiv:2508.18007v1 Announce Type: new 
Abstract: Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any labels even when the training set may contain anomalous samples. To achieve FUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based on teacher-student framework into the FUAD setting. However, due to the presence of anomalies in the training data, traditional KD methods risk enabling the student to learn the teacher's representation of anomalies under FUAD setting, thereby resulting in poor anomaly detection performance. To address this issue, we propose a novel Cross-Domain Distillation (CDD) framework based on the widely studied reverse distillation (RD) paradigm. Specifically, we design a Domain-Specific Training, which divides the training set into multiple domains with lower anomaly ratios and train a domain-specific student for each. Cross-Domain Knowledge Aggregation is then performed, where pseudo-normal features generated by domain-specific students collaboratively guide a global student to learn generalized normal representations across all samples. Experimental results on noisy versions of the MVTec AD and VisA datasets demonstrate that our method achieves significant performance improvements over the baseline, validating its effectiveness under FUAD setting.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria</title>
<link>https://arxiv.org/abs/2508.18012</link>
<guid>https://arxiv.org/abs/2508.18012</guid>
<content:encoded><![CDATA[
arXiv:2508.18012v1 Announce Type: new 
Abstract: Neural networks in assistive technology for visually impaired leverage artificial intelligence's capacity to recognize patterns in complex data. They are used for converting visual data into auditory or tactile representations, helping the visually impaired understand their surroundings. The primary aim of this research is to explore the potential of artificial neural networks to facilitate the differentiation of various forms of cash for individuals with visual impairments. In this study, we built a custom dataset of 3,468 images, which was subsequently used to train an SSD neural network model. The proposed system can accurately identify Nigerian cash, thereby streamlining commercial transactions. The performance of the system in terms of accuracy was assessed, and the Mean Average Precision score was over 90%. We believe that our system has the potential to make a substantial contribution to the field of assistive technology while also improving the quality of life of visually challenged persons in Nigeria and beyond.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Continual Visual Anomaly Detection in the Medical Domain</title>
<link>https://arxiv.org/abs/2508.18013</link>
<guid>https://arxiv.org/abs/2508.18013</guid>
<content:encoded><![CDATA[
arXiv:2508.18013v1 Announce Type: new 
Abstract: Visual Anomaly Detection (VAD) seeks to identify abnormal images and precisely localize the corresponding anomalous regions, relying solely on normal data during training. This approach has proven essential in domains such as manufacturing and, more recently, in the medical field, where accurate and explainable detection is critical. Despite its importance, the impact of evolving input data distributions over time has received limited attention, even though such changes can significantly degrade model performance. In particular, given the dynamic and evolving nature of medical imaging data, Continual Learning (CL) provides a natural and effective framework to incrementally adapt models while preserving previously acquired knowledge. This study explores for the first time the application of VAD models in a CL scenario for the medical field. In this work, we utilize a CL version of the well-established PatchCore model, called PatchCoreCL, and evaluate its performance using BMAD, a real-world medical imaging dataset with both image-level and pixel-level annotations. Our results demonstrate that PatchCoreCL is an effective solution, achieving performance comparable to the task-specific models, with a forgetting value less than a 1%, highlighting the feasibility and potential of CL for adaptive VAD in medical imaging.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction</title>
<link>https://arxiv.org/abs/2508.18031</link>
<guid>https://arxiv.org/abs/2508.18031</guid>
<content:encoded><![CDATA[
arXiv:2508.18031v1 Announce Type: new 
Abstract: Craniofacial reconstruction in forensics is one of the processes to identify victims of crime and natural disasters. Identifying an individual from their remains plays a crucial role when all other identification methods fail. Traditional methods for this task, such as clay-based craniofacial reconstruction, require expert domain knowledge and are a time-consuming process. At the same time, other probabilistic generative models like the statistical shape model or the Basel face model fail to capture the skull and face cross-domain attributes. Looking at these limitations, we propose a generic framework for craniofacial reconstruction from 2D X-ray images. Here, we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune the generator and discriminator parts to generate more realistic images in two distinct domains, which are the skull and face of an individual. This is the first time where 2D X-rays are being used as a representation of the skull by generative models for craniofacial reconstruction. We have evaluated the quality of generated faces using FID, IS, and SSIM scores. Finally, we have proposed a retrieval framework where the query is the generated face image and the gallery is the database of real faces. By experimental results, we have found that this can be an effective tool for forensic science.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.18032</link>
<guid>https://arxiv.org/abs/2508.18032</guid>
<content:encoded><![CDATA[
arXiv:2508.18032v1 Announce Type: new 
Abstract: Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation</title>
<link>https://arxiv.org/abs/2508.18050</link>
<guid>https://arxiv.org/abs/2508.18050</guid>
<content:encoded><![CDATA[
arXiv:2508.18050v1 Announce Type: new 
Abstract: Camouflaged Object Segmentation (COS) poses a significant challenge due to the intrinsic high similarity between targets and backgrounds, demanding models capable of profound holistic understanding beyond superficial cues. Prevailing methods, often limited by shallow feature representation, inadequate reasoning mechanisms, and weak cross-modal integration, struggle to achieve this depth of cognition, resulting in prevalent issues like incomplete target separation and imprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed Giant-emphasizing holistic observation, omnidirectional focus, and intensive scrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought framework underpinned by cross-modal synergy and omnidirectional reasoning within Vision-Language Models (VLMs). ArgusCogito orchestrates three cognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive prior through global reasoning with cross-modal fusion (RGB, depth, semantic maps), enabling holistic scene understanding and enhanced target-background disambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning and focused reasoning, guided by semantic priors from Conjecture, enabling precise target localization and region-of-interest refinement. (3) Sculpting: Progressively sculpts high-fidelity segmentation masks by integrating cross-modal information and iteratively generating dense positive/negative point prompts within focused regions, emulating Argus' intensive scrutiny. Extensive evaluations on four challenging COS benchmarks and three Medical Image Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves state-of-the-art (SOTA) performance, validating the framework's exceptional efficacy, superior generalization capability, and robustness.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</title>
<link>https://arxiv.org/abs/2508.18067</link>
<guid>https://arxiv.org/abs/2508.18067</guid>
<content:encoded><![CDATA[
arXiv:2508.18067v1 Announce Type: new 
Abstract: Semantic segmentation of remote sensing (RS) images is pivotal for comprehensive Earth observation, but the demand for interpreting new object categories, coupled with the high expense of manual annotation, poses significant challenges. Although open-vocabulary semantic segmentation (OVSS) offers a promising solution, existing frameworks designed for natural images are insufficient for the unique complexities of RS data. They struggle with vast scale variations and fine-grained details, and their adaptation often relies on extensive, costly annotations. To address this critical gap, this paper introduces SegEarth-OV, the first framework for annotation-free open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp, a universal upsampler that robustly restores high-resolution spatial details from coarse features, correcting distorted target shapes without any task-specific post-training. We also present a simple yet effective Global Bias Alleviation operation to subtract the inherent global context from patch features, significantly enhancing local semantic fidelity. These components empower SegEarth-OV to effectively harness the rich semantics of pre-trained VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the framework's universality to other challenging RS modalities like SAR images, where large-scale VLMs are unavailable and expensive to create, we introduce AlignEarth, which is a distillation-based strategy and can efficiently transfer semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the need to build SAR foundation models from scratch and enabling universal OVSS across diverse sensor types. Extensive experiments on both optical and SAR datasets validate that SegEarth-OV can achieve dramatic improvements over the SOTA methods, establishing a robust foundation for annotation-free and open-world Earth observation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventTracer: Fast Path Tracing-based Event Stream Rendering</title>
<link>https://arxiv.org/abs/2508.18071</link>
<guid>https://arxiv.org/abs/2508.18071</guid>
<content:encoded><![CDATA[
arXiv:2508.18071v1 Announce Type: new 
Abstract: Simulating event streams from 3D scenes has become a common practice in event-based vision research, as it meets the demand for large-scale, high temporal frequency data without setting up expensive hardware devices or undertaking extensive data collections. Yet existing methods in this direction typically work with noiseless RGB frames that are costly to render, and therefore they can only achieve a temporal resolution equivalent to 100-300 FPS, far lower than that of real-world event data. In this work, we propose EventTracer, a path tracing-based rendering pipeline that simulates high-fidelity event sequences from complex 3D scenes in an efficient and physics-aware manner. Specifically, we speed up the rendering process via low sample-per-pixel (SPP) path tracing, and train a lightweight event spiking network to denoise the resulting RGB videos into realistic event sequences. To capture the physical properties of event streams, the network is equipped with a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at a speed of about 4 minutes per second of 720p video, and it inherits the merit of accurate spatiotemporal modeling from its path tracing backbone. We show in two downstream tasks that EventTracer captures better scene details and demonstrates a greater similarity to real-world event data than other event simulators, which establishes it as a promising tool for creating large-scale event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based vision, and boosting various application scenarios such as robotics, autonomous driving, and VRAR.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering</title>
<link>https://arxiv.org/abs/2508.18075</link>
<guid>https://arxiv.org/abs/2508.18075</guid>
<content:encoded><![CDATA[
arXiv:2508.18075v1 Announce Type: new 
Abstract: Open-set few-shot hyperspectral image (HSI) classification aims to classify image pixels by using few labeled pixels per class, where the pixels to be classified may be not all from the classes that have been seen. To address the open-set HSI classification challenge, current methods focus mainly on distinguishing the unknown class samples from the known class samples and rejecting them to increase the accuracy of identifying known class samples. They fails to further identify or discovery the unknow classes among the samples. This paper proposes a prototype learning and clustering method for discoverying unknown classes in HSIs under the few-shot environment. Using few labeled samples, it strives to develop the ability of infering the prototypes of unknown classes while distinguishing unknown classes from known classes. Once the unknown class samples are rejected by the learned known class classifier, the proposed method can further cluster the unknown class samples into different classes according to their distance to the inferred unknown class prototypes. Compared to existing state-of-the-art methods, extensive experiments on four benchmark HSI datasets demonstrate that our proposed method exhibits competitive performance in open-set few-shot HSI classification tasks. All the codes are available at \href{https://github.com/KOBEN-ff/OpenFUCD-main} {https://github.com/KOBEN-ff/OpenFUCD-main}
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Pre-trained Diffusion Models in Solving the Schr\"odinger Bridge Problem</title>
<link>https://arxiv.org/abs/2508.18095</link>
<guid>https://arxiv.org/abs/2508.18095</guid>
<content:encoded><![CDATA[
arXiv:2508.18095v1 Announce Type: new 
Abstract: This paper aims to unify Score-based Generative Models (SGMs), also known as Diffusion models, and the Schr\"odinger Bridge (SB) problem through three reparameterization techniques: Iterative Proportional Mean-Matching (IPMM), Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional Flow-Matching (IPFM). These techniques significantly accelerate and stabilize the training of SB-based models. Furthermore, the paper introduces novel initialization strategies that use pre-trained SGMs to effectively train SB-based models. By using SGMs as initialization, we leverage the advantages of both SB-based models and SGMs, ensuring efficient training of SB-based models and further improving the performance of SGMs. Extensive experiments demonstrate the significant effectiveness and improvements of the proposed methods. We believe this work contributes to and paves the way for future research on generative models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines</title>
<link>https://arxiv.org/abs/2508.18136</link>
<guid>https://arxiv.org/abs/2508.18136</guid>
<content:encoded><![CDATA[
arXiv:2508.18136v1 Announce Type: new 
Abstract: The urgent need for renewable energy expansion, particularly wind power, is hindered by conflicts with wildlife conservation. To address this, we developed BirdRecorder, an advanced AI-based anti-collision system to protect endangered birds, especially the red kite (Milvus milvus). Integrating robotics, telemetry, and high-performance AI algorithms, BirdRecorder aims to detect, track, and classify avian species within a range of 800 m to minimize bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and software architectures to enable real-time image processing. Leveraging Single Shot Detector (SSD) for detection, combined with specialized hardware acceleration and tracking algorithms, our system achieves high detection precision while maintaining the speed necessary for real-time decision-making. By combining these components, BirdRecorder outperforms existing approaches in both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the BirdRecorder system. By bridging the gap between renewable energy expansion and wildlife conservation, BirdRecorder contributes to a more sustainable coexistence of technology and nature.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability</title>
<link>https://arxiv.org/abs/2508.18154</link>
<guid>https://arxiv.org/abs/2508.18154</guid>
<content:encoded><![CDATA[
arXiv:2508.18154v1 Announce Type: new 
Abstract: Class Activation Maps (CAMs) are one of the important methods for visualizing regions used by deep learning models. Yet their robustness to different noise remains underexplored. In this work, we evaluate and report the resilience of various CAM methods for different noise perturbations across multiple architectures and datasets. By analyzing the influence of different noise types on CAM explanations, we assess the susceptibility to noise and the extent to which dataset characteristics may impact explanation stability. The findings highlight considerable variability in noise sensitivity for various CAMs. We propose a robustness metric for CAMs that captures two key properties: consistency and responsiveness. Consistency reflects the ability of CAMs to remain stable under input perturbations that do not alter the predicted class, while responsiveness measures the sensitivity of CAMs to changes in the prediction caused by such perturbations. The metric is evaluated empirically across models, different perturbations, and datasets along with complementary statistical tests to exemplify the applicability of our proposed approach.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpotEdit: Evaluating Visually-Guided Image Editing Methods</title>
<link>https://arxiv.org/abs/2508.18159</link>
<guid>https://arxiv.org/abs/2508.18159</guid>
<content:encoded><![CDATA[
arXiv:2508.18159v1 Announce Type: new 
Abstract: Visually-guided image editing, where edits are conditioned on both visual cues and textual prompts, has emerged as a powerful paradigm for fine-grained, controllable content generation. Although recent generative models have shown remarkable capabilities, existing evaluations remain simple and insufficiently representative of real-world editing challenges. We present SpotEdit, a comprehensive benchmark designed to systematically assess visually-guided image editing methods across diverse diffusion, autoregressive, and hybrid generative models, uncovering substantial performance disparities. To address a critical yet underexplored challenge, our benchmark includes a dedicated component on hallucination, highlighting how leading models, such as GPT-4o, often hallucinate the existence of a visual cue and erroneously perform the editing task. Our code and benchmark are publicly released at https://github.com/SaraGhazanfari/SpotEdit.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</title>
<link>https://arxiv.org/abs/2508.18177</link>
<guid>https://arxiv.org/abs/2508.18177</guid>
<content:encoded><![CDATA[
arXiv:2508.18177v1 Announce Type: new 
Abstract: This study proposes the dual technological innovation framework, including a cross-modal differ entiated quantization framework for vision-language models (VLMs) and a scene-aware vectorized
  memory multi-agent system for visually impaired assistance. The modular framework was developed
  implementing differentiated processing strategies, effectively reducing memory requirements from
  38GB to 16GB while maintaining model performance. The multi-agent architecture combines
  scene classification, vectorized memory, and multimodal interaction, enabling persistent storage
  and efficient retrieval of scene memories. Through perception-memory-reasoning workflows, the
  system provides environmental information beyond the current view using historical memories.
  Experiments show the quantized 19B-parameter model only experiences a 2.05% performance drop
  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9), outperforming smaller
  models with equivalent memory requirements like the Molmo-7B series. The system maintains
  response latency between 2.83-3.52 seconds from scene analysis to initial speech output, substantially
  faster than non-streaming methods. This research advances computational efficiency and assistive
  technology, offering visually impaired users comprehensive real-time assistance in scene perception,
  text recognition, and navigation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning</title>
<link>https://arxiv.org/abs/2508.18186</link>
<guid>https://arxiv.org/abs/2508.18186</guid>
<content:encoded><![CDATA[
arXiv:2508.18186v1 Announce Type: new 
Abstract: Large annotated datasets are vital for training segmentation models, but pixel-level labeling is time-consuming, error-prone, and often requires scarce expert annotators, especially in medical imaging. In contrast, coarse annotations are quicker, cheaper, and easier to produce, even by non-experts. In this paper, we propose to use coarse drawings from both positive (target) and negative (background) classes in the image, even with noisy pixels, to train a convolutional neural network (CNN) for semantic segmentation. We present a method for learning the true segmentation label distributions from purely noisy coarse annotations using two coupled CNNs. The separation of the two CNNs is achieved by high fidelity with the characters of the noisy training annotations. We propose to add a complementary label learning that encourages estimating negative label distribution. To illustrate the properties of our method, we first use a toy segmentation dataset based on MNIST. We then present the quantitative results of experiments using publicly available datasets: Cityscapes dataset for multi-class segmentation, and retinal images for medical applications. In all experiments, our method outperforms state-of-the-art methods, particularly in the cases where the ratio of coarse annotations is small compared to the given dense annotations.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding</title>
<link>https://arxiv.org/abs/2508.18187</link>
<guid>https://arxiv.org/abs/2508.18187</guid>
<content:encoded><![CDATA[
arXiv:2508.18187v1 Announce Type: new 
Abstract: Memory decay makes it harder for the human brain to recognize visual objects and retain details. Consequently, recorded brain signals become weaker, uncertain, and contain poor visual context over time. This paper presents one of the first vision-learning approaches to address this problem. First, we statistically and experimentally demonstrate the existence of inconsistency in brain signals and its impact on the Vision-Brain Understanding (VBU) model. Our findings show that brain signal representations shift over recording sessions, leading to compounding bias, which poses challenges for model learning and degrades performance. Then, we propose a new Bias-Mitigation Continual Learning (BRAIN) approach to address these limitations. In this approach, the model is trained in a continual learning setup and mitigates the growing bias from each learning step. A new loss function named De-bias Contrastive Learning is also introduced to address the bias problem. In addition, to prevent catastrophic forgetting, where the model loses knowledge from previous sessions, the new Angular-based Forgetting Mitigation approach is introduced to preserve learned knowledge in the model. Finally, the empirical experiments demonstrate that our approach achieves State-of-the-Art (SOTA) performance across various benchmarks, surpassing prior and non-continual learning methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain and Monitor Deep Learning Models for Computer Vision using Obz AI</title>
<link>https://arxiv.org/abs/2508.18188</link>
<guid>https://arxiv.org/abs/2508.18188</guid>
<content:encoded><![CDATA[
arXiv:2508.18188v1 Announce Type: new 
Abstract: Deep learning has transformed computer vision (CV), achieving outstanding performance in classification, segmentation, and related tasks. Such AI-based CV systems are becoming prevalent, with applications spanning from medical imaging to surveillance. State of the art models such as convolutional neural networks (CNNs) and vision transformers (ViTs) are often regarded as ``black boxes,'' offering limited transparency into their decision-making processes. Despite a recent advancement in explainable AI (XAI), explainability remains underutilized in practical CV deployments. A primary obstacle is the absence of integrated software solutions that connect XAI techniques with robust knowledge management and monitoring frameworks. To close this gap, we have developed Obz AI, a comprehensive software ecosystem designed to facilitate state-of-the-art explainability and observability for vision AI systems. Obz AI provides a seamless integration pipeline, from a Python client library to a full-stack analytics dashboard. With Obz AI, a machine learning engineer can easily incorporate advanced XAI methodologies, extract and analyze features for outlier detection, and continuously monitor AI models in real time. By making the decision-making mechanisms of deep models interpretable, Obz AI promotes observability and responsible deployment of computer vision systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance</title>
<link>https://arxiv.org/abs/2508.18213</link>
<guid>https://arxiv.org/abs/2508.18213</guid>
<content:encoded><![CDATA[
arXiv:2508.18213v1 Announce Type: new 
Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry of hand-held objects from monocular RGB images by leveraging hand-object interaction as geometric guidance. Our method conditions a latent diffusion model on an inpainted object appearance and uses inference-time guidance to optimize the object reconstruction, while simultaneously ensuring plausible hand-object interactions. Unlike prior methods that rely on extensive post-processing or produce low-quality reconstructions, our approach directly generates high-quality object geometry during the diffusion process by introducing guidance with an optimization-in-the-loop design. Specifically, we guide the diffusion model by applying supervision to the velocity field while simultaneously optimizing the transformations of both the hand and the object being reconstructed. This optimization is driven by multi-modal geometric cues, including normal and depth alignment, silhouette consistency, and 2D keypoint reprojection. We further incorporate signed distance field supervision and enforce contact and non-intersection constraints to ensure physical plausibility of hand-object interaction. Our method yields accurate, robust and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GM-Skip: Metric-Guided Transformer Block Skipping for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.18227</link>
<guid>https://arxiv.org/abs/2508.18227</guid>
<content:encoded><![CDATA[
arXiv:2508.18227v1 Announce Type: new 
Abstract: Transformer-based Vision-Language Models (VLMs) have achieved impressive performance on tasks such as image captioning, object recognition, and visual reasoning, but their high computational cost hinders deployment in latency-sensitive applications like autonomous driving. We introduce GM-Skip, a flexible and metric-adaptive framework for Transformer block skipping that accelerates VLM inference while preserving output quality. GM-Skip features a greedy, metric-guided block selection strategy that uses metric feedback (e.g., accuracy, CIDEr) to identify redundant layers, along with a reverse-order deletion mechanism that preserves early foundational blocks to avoid performance collapse. To support diverse deployment needs, it incorporates a tunable trade-off between sparsity and performance via a score-sparsity balance objective. Experiments across multiple tasks and datasets, including COCO and CODA, show that GM-Skip consistently improves inference speed while maintaining task performance. On the COCO dataset, GM-Skip improves single-object classification accuracy on the Person category from 19.1 percent to 87.3 percent while skipping more than 40 percent of Transformer blocks. In real-world deployment, it achieves up to 45.4 percent latency reduction on single-object detection when integrated into an autonomous vehicle running Autoware.Universe, validating the effectiveness of its skip configurations and confirming its practical value in accelerating real-world inference.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.18235</link>
<guid>https://arxiv.org/abs/2508.18235</guid>
<content:encoded><![CDATA[
arXiv:2508.18235v1 Announce Type: new 
Abstract: Text-to-image diffusion models have revolutionized generative AI, but their vulnerability to backdoor attacks poses significant security risks. Adversaries can inject imperceptible textual triggers into training data, causing models to generate manipulated outputs. Although text-based backdoor defenses in classification models are well-explored, generative models lack effective mitigation techniques against. We address this by selectively erasing the model's learned associations between adversarial text triggers and poisoned outputs, while preserving overall generation quality. Our approach, Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG), uses knowledge distillation to guide the model in correcting responses to poisoned prompts while maintaining image quality by exploiting the fact that the backdoored model still produces clean outputs in the absence of triggers. Using the cross-attention mechanism, SKD-CAG neutralizes backdoor influences at the attention level, ensuring the targeted removal of adversarial effects. Extensive experiments show that our method outperforms existing approaches, achieving removal accuracy 100\% for pixel backdoors and 93\% for style-based attacks, without sacrificing robustness or image fidelity. Our findings highlight targeted unlearning as a promising defense to secure generative models. Code and model weights can be found at https://github.com/Mystic-Slice/Sealing-The-Backdoor .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Evaluation of AI-Generated Content with Language-Grounded Sparse Encoders</title>
<link>https://arxiv.org/abs/2508.18236</link>
<guid>https://arxiv.org/abs/2508.18236</guid>
<content:encoded><![CDATA[
arXiv:2508.18236v1 Announce Type: new 
Abstract: While the quality of AI-generated contents, such as synthetic images, has become remarkably high, current evaluation metrics provide only coarse-grained assessments, failing to identify specific strengths and weaknesses that researchers and practitioners need for model selection and development, further limiting the scientific understanding and commercial deployment of these generative models. To address this, we introduce Language-Grounded Sparse Encoders (LanSE), a novel architecture that creates interpretable evaluation metrics by identifying interpretable visual patterns and automatically describing them in natural language. Through large-scale human evaluation (more than 11,000 annotations) and large multimodal model (LMM) based analysis, LanSE demonstrates reliable capabilities to detect interpretable visual patterns in synthetic images with more than 93\% accuracy in natural images. LanSE further provides a fine-grained evaluation framework that quantifies four key dimensions of generation quality, prompt match, visual realism, physical plausibility, and content diversity. LanSE reveals nuanced model differences invisible to existing metrics, for instance, FLUX's superior physical plausibility and SDXL-medium's strong content diversity, while aligning with human judgments. By bridging interpretability with practical evaluation needs, LanSE offers all users of generative AI models a powerful tool for model selection, quality control of synthetic content, and model improvement. These capabilities directly address the need for public confidence and safety in AI-generated content, both critical for the future of generative AI applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorFormer: A Transformer for Real-time Monocular 3D Human Pose Estimation with Versatile Geometric Priors</title>
<link>https://arxiv.org/abs/2508.18238</link>
<guid>https://arxiv.org/abs/2508.18238</guid>
<content:encoded><![CDATA[
arXiv:2508.18238v1 Announce Type: new 
Abstract: This paper proposes a new lightweight Transformer-based lifter that maps short sequences of human 2D joint positions to 3D poses using a single camera. The proposed model takes as input geometric priors including segment lengths and camera intrinsics and is designed to operate in both calibrated and uncalibrated settings. To this end, a masking mechanism enables the model to ignore missing priors during training and inference. This yields a single versatile network that can adapt to different deployment scenarios, from fully calibrated lab environments to in-the-wild monocular videos without calibration.  The model was trained using 3D keypoints from AMASS dataset with corresponding 2D synthetic data generated by sampling random camera poses and intrinsics. It was then compared to an expert model trained, only on complete priors, and the validation was done by conducting an ablation study. Results show that both, camera and segment length priors, improve performance and that the versatile model outperforms the expert, even when all priors are available, and maintains high accuracy when priors are missing. Overall the average 3D joint center positions estimation accuracy was as low as 36mm improving state of the art by half a centimeter and at a much lower computational cost. Indeed, the proposed model runs in 380$\mu$s on GPU and 1800$\mu$s on CPU, making it suitable for deployment on embedded platforms and low-power devices.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</title>
<link>https://arxiv.org/abs/2508.18242</link>
<guid>https://arxiv.org/abs/2508.18242</guid>
<content:encoded><![CDATA[
arXiv:2508.18242v1 Announce Type: new 
Abstract: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</title>
<link>https://arxiv.org/abs/2508.18264</link>
<guid>https://arxiv.org/abs/2508.18264</guid>
<content:encoded><![CDATA[
arXiv:2508.18264v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as a maximum coverage problem. Afterward, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87x speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title>
<link>https://arxiv.org/abs/2508.18265</link>
<guid>https://arxiv.org/abs/2508.18265</guid>
<content:encoded><![CDATA[
arXiv:2508.18265v1 Announce Type: new 
Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.18271</link>
<guid>https://arxiv.org/abs/2508.18271</guid>
<content:encoded><![CDATA[
arXiv:2508.18271v1 Announce Type: new 
Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
<link>https://arxiv.org/abs/2508.08237</link>
<guid>https://arxiv.org/abs/2508.08237</guid>
<content:encoded><![CDATA[
arXiv:2508.08237v2 Announce Type: cross 
Abstract: The emergence of audio-visual foundation models underscores the importance of reliably assessing their multi-modal understanding. The VGGSound dataset is commonly used as a benchmark for evaluation audio-visual classification. However, our analysis identifies several limitations of VGGSound, including incomplete labelling, partially overlapping classes, and misaligned modalities. These lead to distorted evaluations of auditory and visual capabilities. To address these limitations, we introduce VGGSounder, a comprehensively re-annotated, multi-label test set that extends VGGSound and is specifically designed to evaluate audio-visual foundation models. VGGSounder features detailed modality annotations, enabling precise analyses of modality-specific performance. Furthermore, we reveal model limitations by analysing performance degradation when adding another input modality with our new modality confusion metric.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting User Grasp Intentions in Virtual Reality</title>
<link>https://arxiv.org/abs/2508.16582</link>
<guid>https://arxiv.org/abs/2508.16582</guid>
<content:encoded><![CDATA[
arXiv:2508.16582v1 Announce Type: cross 
Abstract: Predicting user intentions in virtual reality (VR) is crucial for creating immersive experiences, particularly in tasks involving complex grasping motions where accurate haptic feedback is essential. In this work, we leverage time-series data from hand movements to evaluate both classification and regression approaches across 810 trials with varied object types, sizes, and manipulations. Our findings reveal that classification models struggle to generalize across users, leading to inconsistent performance. In contrast, regression-based approaches, particularly those using Long Short Term Memory (LSTM) networks, demonstrate more robust performance, with timing errors within 0.25 seconds and distance errors around 5-20 cm in the critical two-second window before a grasp. Despite these improvements, predicting precise hand postures remains challenging. Through a comprehensive analysis of user variability and model interpretability, we explore why certain models fail and how regression models better accommodate the dynamic and complex nature of user behavior in VR. Our results underscore the potential of machine learning models to enhance VR interactions, particularly through adaptive haptic feedback, and lay the groundwork for future advancements in real-time prediction of user actions in VR.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative Shanshui: Real-time Interactive Ink Painting Synthesis</title>
<link>https://arxiv.org/abs/2508.16612</link>
<guid>https://arxiv.org/abs/2508.16612</guid>
<content:encoded><![CDATA[
arXiv:2508.16612v1 Announce Type: cross 
Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis approach that reinterprets classical Chinese landscape ink painting, i.e., shanshui, to engage with ecological crises in the Anthropocene. Negative Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences and integrates it with gaze-driven inpainting, frame interpolation; it enables dynamic morphing animations in response to the viewer's gaze and presents as an interactive virtual reality (VR) experience. The paper describes the complete technical pipeline, covering the system framework, optimization strategies, gaze-based interaction, and multimodal deployment in an art festival. Further analysis of audience feedback collected during its public exhibition highlights how participants variously engaged with the work through empathy, ambivalence, and critical reflection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D latent diffusion models for parameterizing and history matching multiscenario facies systems</title>
<link>https://arxiv.org/abs/2508.16621</link>
<guid>https://arxiv.org/abs/2508.16621</guid>
<content:encoded><![CDATA[
arXiv:2508.16621v1 Announce Type: cross 
Abstract: Geological parameterization procedures entail the mapping of a high-dimensional geomodel to a low-dimensional latent variable. These parameterizations can be very useful for history matching because the number of variables to be calibrated is greatly reduced, and the mapping can be constructed such that geological realism is automatically preserved. In this work, a parameterization method based on generative latent diffusion models (LDMs) is developed for 3D channel-levee-mud systems. Geomodels with variable scenario parameters, specifically mud fraction, channel orientation, and channel width, are considered. A perceptual loss term is included during training to improve geological realism. For any set of scenario parameters, an (essentially) infinite number of realizations can be generated, so our LDM parameterizes over a very wide model space. New realizations constructed using the LDM procedure are shown to closely resemble reference geomodels, both visually and in terms of one- and two-point spatial statistics. Flow response distributions, for a specified set of injection and production wells, are also shown to be in close agreement between the two sets of models. The parameterization method is applied for ensemble-based history matching, with model updates performed in the LDM latent space, for cases involving geological scenario uncertainty. For three synthetic true models corresponding to different geological scenarios, we observe clear uncertainty reduction in both production forecasts and geological scenario parameters. The overall method is additionally shown to provide posterior geomodels consistent with the synthetic true model in each case.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting brain tumour enhancement from non-contrast MR imaging with artificial intelligence</title>
<link>https://arxiv.org/abs/2508.16650</link>
<guid>https://arxiv.org/abs/2508.16650</guid>
<content:encoded><![CDATA[
arXiv:2508.16650v1 Announce Type: cross 
Abstract: Brain tumour imaging assessment typically requires both pre- and post-contrast MRI, but gadolinium administration is not always desirable, such as in frequent follow-up, renal impairment, allergy, or paediatric patients. We aimed to develop and validate a deep learning model capable of predicting brain tumour contrast enhancement from non-contrast MRI sequences alone. We assembled 11089 brain MRI studies from 10 international datasets spanning adult and paediatric populations with various neuro-oncological states, including glioma, meningioma, metastases, and post-resection appearances. Deep learning models (nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing tumour using only non-contrast T1-, T2-, and T2/FLAIR-weighted images. Performance was evaluated on 1109 held-out test patients using patient-level detection metrics and voxel-level segmentation accuracy. Model predictions were compared against 11 expert radiologists who each reviewed 100 randomly selected patients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5% sensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement volume predictions strongly correlated with ground truth (R2 0.859). The model outperformed expert radiologists, who achieved 69.8% accuracy, 75.9% sensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3 (acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had Dice over 0.7 (excellent detection). Deep learning can identify contrast-enhancing brain tumours from non-contrast MRI with clinically relevant performance. These models show promise as screening tools and may reduce gadolinium dependence in neuro-oncology imaging. Future work should evaluate clinical utility alongside radiology experts.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context</title>
<link>https://arxiv.org/abs/2508.16655</link>
<guid>https://arxiv.org/abs/2508.16655</guid>
<content:encoded><![CDATA[
arXiv:2508.16655v1 Announce Type: cross 
Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient monitoring (RPM) emerged as a promising solution for managing heart failure. However, the heart rate can fluctuate significantly due to various factors, and without correlating it to the patient's actual physical activity, it becomes difficult to assess whether changes are significant. Although Artificial Intelligence (AI) models may enhance the accuracy and contextual understanding of remote heart rate monitoring, the integration of activity data is still rarely addressed. In this paper, we propose a Transformer model combined with a Laplace diffusion technique to model heart rate fluctuations driven by physical activity of the patient. Unlike prior models that treat activity as secondary, our approach conditions the entire modeling process on activity context using specialized embeddings and attention mechanisms to prioritize activity specific historical patents. The model captures both long-term patterns and activity-specific heart rate dynamics by incorporating contextualized embeddings and dedicated encoder. The Transformer model was validated on a real-world dataset collected from 29 patients over a 4-month period. Experimental results show that our model outperforms current state-of-the-art methods, achieving a 43% reduction in mean absolute error compared to the considered baseline models. Moreover, the coefficient of determination R2 is 0.97 indicating the model predicted heart rate is in strong agreement with actual heart rate values. These findings suggest that the proposed model is a practical and effective tool for supporting both healthcare providers and remote patient monitoring systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainPath: Generating Subject-Specific Brain Aging Trajectories</title>
<link>https://arxiv.org/abs/2508.16667</link>
<guid>https://arxiv.org/abs/2508.16667</guid>
<content:encoded><![CDATA[
arXiv:2508.16667v1 Announce Type: cross 
Abstract: Quantifying and forecasting individual brain aging trajectories is critical for understanding neurodegenerative disease and the heterogeneity of aging, yet current approaches remain limited. Most models predict chronological age, an imperfect surrogate for biological aging, or generate synthetic MRIs that enhance data diversity but fail to capture subject-specific trajectories. Here, we present BrainPath, a 3D generative framework that learns longitudinal brain aging dynamics during training and, at inference, predicts anatomically faithful MRIs at arbitrary timepoints from a single baseline scan. BrainPath integrates an age calibration loss, a swap learning strategy, and an age perceptual loss to preserve subtle, biologically meaningful variations. Across held-out ADNI and an independent NACC dataset, BrainPath outperforms state-of-the-art reference models in structural similarity (SSIM), mean squared error (MSE), peak signal-to-noise ratio (PSNR), and MRI age-difference accuracy, while capturing realistic and temporally consistent aging patterns. Beyond methodological innovation, BrainPath enables personalized mapping of brain aging, synthetic follow-up scan prediction, and trajectory-based analyses, providing a foundation for precision modeling of brain aging and supporting research into neurodegeneration and aging interventions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Transferability Estimation Metrics for Surgical Phase Recognition</title>
<link>https://arxiv.org/abs/2508.16730</link>
<guid>https://arxiv.org/abs/2508.16730</guid>
<content:encoded><![CDATA[
arXiv:2508.16730v1 Announce Type: cross 
Abstract: Fine-tuning pre-trained models has become a cornerstone of modern machine learning, allowing practitioners to achieve high performance with limited labeled data. In surgical video analysis, where expert annotations are especially time-consuming and costly, identifying the most suitable pre-trained model for a downstream task is both critical and challenging. Source-independent transferability estimation (SITE) offers a solution by predicting how well a model will fine-tune on target data using only its embeddings or outputs, without requiring full retraining. In this work, we formalize SITE for surgical phase recognition and provide the first comprehensive benchmark of three representative metrics, LogME, H-Score, and TransRate, on two diverse datasets (RAMIE and AutoLaparo). Our results show that LogME, particularly when aggregated by the minimum per-subset score, aligns most closely with fine-tuning accuracy; H-Score yields only weak predictive power; and TransRate often inverses true model rankings. Ablation studies show that when candidate models have similar performances, transferability estimates lose discriminative power, emphasizing the importance of maintaining model diversity or using additional validation. We conclude with practical guidelines for model selection and outline future directions toward domain-specific metrics, theoretical foundations, and interactive benchmarking tools.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Multimodal Representation Learning for Biological Taxonomies</title>
<link>https://arxiv.org/abs/2508.16744</link>
<guid>https://arxiv.org/abs/2508.16744</guid>
<content:encoded><![CDATA[
arXiv:2508.16744v1 Announce Type: cross 
Abstract: Taxonomic classification in biodiversity research involves organizing biological specimens into structured hierarchies based on evidence, which can come from multiple modalities such as images and genetic information. We investigate whether hyperbolic networks can provide a better embedding space for such hierarchical models. Our method embeds multimodal inputs into a shared hyperbolic space using contrastive and a novel stacked entailment-based objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding achieves competitive performance with Euclidean baselines, and outperforms all other models on unseen species classification using DNA barcodes. However, fine-grained classification and open-world generalization remain challenging. Our framework offers a structure-aware foundation for biodiversity modelling, with potential applications to species discovery, ecological monitoring, and conservation efforts.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UM3: Unsupervised Map to Map Matching</title>
<link>https://arxiv.org/abs/2508.16874</link>
<guid>https://arxiv.org/abs/2508.16874</guid>
<content:encoded><![CDATA[
arXiv:2508.16874v1 Announce Type: cross 
Abstract: Map-to-map matching is a critical task for aligning spatial data across heterogeneous sources, yet it remains challenging due to the lack of ground truth correspondences, sparse node features, and scalability demands. In this paper, we propose an unsupervised graph-based framework that addresses these challenges through three key innovations. First, our method is an unsupervised learning approach that requires no training data, which is crucial for large-scale map data where obtaining labeled training samples is challenging. Second, we introduce pseudo coordinates that capture the relative spatial layout of nodes within each map, which enhances feature discriminability and enables scale-invariant learning. Third, we design an mechanism to adaptively balance feature and geometric similarity, as well as a geometric-consistent loss function, ensuring robustness to noisy or incomplete coordinate data. At the implementation level, to handle large-scale maps, we develop a tile-based post-processing pipeline with overlapping regions and majority voting, which enables parallel processing while preserving boundary coherence. Experiments on real-world datasets demonstrate that our method achieves state-of-the-art accuracy in matching tasks, surpassing existing methods by a large margin, particularly in high-noise and large-scale scenarios. Our framework provides a scalable and practical solution for map alignment, offering a robust and efficient alternative to traditional approaches.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.16882</link>
<guid>https://arxiv.org/abs/2508.16882</guid>
<content:encoded><![CDATA[
arXiv:2508.16882v1 Announce Type: cross 
Abstract: Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise diagnosis and effective treatment planning. However, traditional single-modality imaging methods often fall short of capturing the complex anatomical and pathological features of these tumors. In this study, we present an innovative multi-modality representation learning framework based on the `Align-Disentangle-Fusion' mechanism that seamlessly integrates 2D White Light Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation performance. A cornerstone of our approach is multi-scale distribution alignment, which mitigates modality discrepancies by aligning features across multiple transformer layers. Furthermore, a progressive feature disentanglement strategy is developed with the designed preliminary disentanglement and disentangle-aware contrastive learning to effectively separate modality-specific and shared features, enabling robust multimodal contrastive learning and efficient semantic fusion. Comprehensive experiments on multiple datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving superior accuracy across diverse real clinical scenarios.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network</title>
<link>https://arxiv.org/abs/2508.16897</link>
<guid>https://arxiv.org/abs/2508.16897</guid>
<content:encoded><![CDATA[
arXiv:2508.16897v1 Announce Type: cross 
Abstract: Contrast-enhanced computed tomography (CT) imaging is essential for diagnosing and monitoring thoracic diseases, including aortic pathologies. However, contrast agents pose risks such as nephrotoxicity and allergic-like reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT angiography (CTA) images without contrast administration would be transformative, enhancing patient safety and accessibility while reducing healthcare costs. In this study, we propose the first bridge diffusion-based solution for synthesizing contrast-enhanced CTA images from non-contrast CT scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM), leveraging its ability to model complex mappings while maintaining consistency across slices. Unlike conventional slice-wise synthesis methods, our framework preserves full 3D anatomical integrity while operating in a high-resolution 2D fashion, allowing seamless volumetric interpretation under a low memory budget. To ensure robust spatial alignment, we implement a comprehensive preprocessing pipeline that includes resampling, registration using the Symmetric Normalization method, and a sophisticated dilated segmentation mask to extract the aorta and surrounding structures. We create two datasets from the Coltea-Lung dataset: one containing only the aorta and another including both the aorta and heart, enabling a detailed analysis of anatomical context. We compare our approach against baseline methods on both datasets, demonstrating its effectiveness in preserving vascular structures while enhancing contrast fidelity.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation</title>
<link>https://arxiv.org/abs/2508.16911</link>
<guid>https://arxiv.org/abs/2508.16911</guid>
<content:encoded><![CDATA[
arXiv:2508.16911v1 Announce Type: cross 
Abstract: We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark dataset designed for text-controlled and music-conditioned 3D duet dance motion generation. Our dataset comprises 620 minutes of high-quality motion capture data performed by professional dancers, synchronized with music, and detailed with over 10K fine-grained natural language descriptions. The annotations capture a rich movement vocabulary, detailing spatial relationships, body movements, and rhythm, making MDD the first dataset to seamlessly integrate human motions, music, and text for duet dance generation. We introduce two novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a textual prompt, both the leader and follower dance motion are generated (2) Text-to-Dance Accompaniment, where given music, textual prompt, and the leader's motion, the follower's motion is generated in a cohesive, text-aligned manner. We include baseline evaluations on both tasks to support future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation</title>
<link>https://arxiv.org/abs/2508.16930</link>
<guid>https://arxiv.org/abs/2508.16930</guid>
<content:encoded><![CDATA[
arXiv:2508.16930v1 Announce Type: cross 
Abstract: Recent advances in video generation produce visually realistic content, yet the absence of synchronized audio severely compromises immersion. To address key challenges in video-to-audio generation, including multimodal data scarcity, modality imbalance and limited audio quality in existing methods, we propose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that synthesizes high-fidelity audio precisely aligned with visual dynamics and semantic context. Our approach incorporates three core innovations: (1) a scalable data pipeline curating 100k-hour multimodal datasets through automated annotation; (2) a representation alignment strategy using self-supervised audio features to guide latent diffusion training, efficiently improving audio quality and generation stability; (3) a novel multimodal diffusion transformer resolving modal competition, containing dual-stream audio-video fusion through joint attention, and textual semantic injection via cross-attention. Comprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new state-of-the-art performance across audio fidelity, visual-semantic alignment, temporal alignment and distribution matching. The demo page is available at: https://szczesnys.github.io/hunyuanvideo-foley/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions</title>
<link>https://arxiv.org/abs/2508.16950</link>
<guid>https://arxiv.org/abs/2508.16950</guid>
<content:encoded><![CDATA[
arXiv:2508.16950v1 Announce Type: cross 
Abstract: Neural networks often contain polysemantic neurons that respond to multiple, sometimes unrelated, features, complicating mechanistic interpretability. We introduce the Polysemanticity Index (PSI), a null-calibrated metric that quantifies when a neuron's top activations decompose into semantically distinct clusters. PSI multiplies three independently calibrated components: geometric cluster quality (S), alignment to labeled categories (Q), and open-vocabulary semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with Tiny-ImageNet images, PSI identifies neurons whose activation sets split into coherent, nameable prototypes, and reveals strong depth trends: later layers exhibit substantially higher PSI than earlier layers. We validate our approach with robustness checks (varying hyperparameters, random seeds, and cross-encoder text heads), breadth analyses (comparing class-only vs. open-vocabulary concepts), and causal patch-swap interventions. In particular, aligned patch replacements increase target-neuron activation significantly more than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI thus offers a principled and practical lever for discovering, quantifying, and studying polysemantic units in neural networks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebSight: A Vision-First Architecture for Robust Web Agents</title>
<link>https://arxiv.org/abs/2508.16987</link>
<guid>https://arxiv.org/abs/2508.16987</guid>
<content:encoded><![CDATA[
arXiv:2508.16987v1 Announce Type: cross 
Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to interact with web environments purely through visual perception, eliminating dependence on HTML or DOM-based inputs. Central to our approach we introduce our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI element interaction, trained using LoRA on a web-focused subset of the Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent architecture, comprising planning, reasoning, vision-action, and verification agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks benchmark, outperforming several larger generalist models while maintaining lower latency. The full WebSight agent achieves a 68.0% success rate on the WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly 97.14% of the time, indicating high precision. Together, WebSight and WebSight-7B establish a new standard for interpretable, robust, and efficient visual web navigation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Deep Learning-based Point Cloud Denoising</title>
<link>https://arxiv.org/abs/2508.17011</link>
<guid>https://arxiv.org/abs/2508.17011</guid>
<content:encoded><![CDATA[
arXiv:2508.17011v1 Announce Type: cross 
Abstract: Accurate 3D geometry acquisition is essential for a wide range of applications, such as computer graphics, autonomous driving, robotics, and augmented reality. However, raw point clouds acquired in real-world environments are often corrupted with noise due to various factors such as sensor, lighting, material, environment etc, which reduces geometric fidelity and degrades downstream performance. Point cloud denoising is a fundamental problem, aiming to recover clean point sets while preserving underlying structures. Classical optimization-based methods, guided by hand-crafted filters or geometric priors, have been extensively studied but struggle to handle diverse and complex noise patterns. Recent deep learning approaches leverage neural network architectures to learn distinctive representations and demonstrate strong outcomes, particularly on complex and large-scale point clouds. Provided these significant advances, this survey provides a comprehensive and up-to-date review of deep learning-based point cloud denoising methods up to August 2025. We organize the literature from two perspectives: (1) supervision level (supervised vs. unsupervised), and (2) modeling perspective, proposing a functional taxonomy that unifies diverse approaches by their denoising principles. We further analyze architectural trends both structurally and chronologically, establish a unified benchmark with consistent training settings, and evaluate methods in terms of denoising quality, surface fidelity, point distribution, and computational efficiency. Finally, we discuss open challenges and outline directions for future research in this rapidly evolving field.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SACA: Selective Attention-Based Clustering Algorithm</title>
<link>https://arxiv.org/abs/2508.17150</link>
<guid>https://arxiv.org/abs/2508.17150</guid>
<content:encoded><![CDATA[
arXiv:2508.17150v1 Announce Type: cross 
Abstract: Clustering algorithms are widely used in various applications, with density-based methods such as Density-Based Spatial Clustering of Applications with Noise (DBSCAN) being particularly prominent. These algorithms identify clusters in high-density regions while treating sparser areas as noise. However, reliance on user-defined parameters often poses optimization challenges that require domain expertise. This paper presents a novel density-based clustering method inspired by the concept of selective attention, which minimizes the need for user-defined parameters under standard conditions. Initially, the algorithm operates without requiring user-defined parameters. If parameter adjustment is needed, the method simplifies the process by introducing a single integer parameter that is straightforward to tune. The approach computes a threshold to filter out the most sparsely distributed points and outliers, forms a preliminary cluster structure, and then reintegrates the excluded points to finalize the results. Experimental evaluations on diverse data sets highlight the accessibility and robust performance of the method, providing an effective alternative for density-based clustering tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</title>
<link>https://arxiv.org/abs/2508.17180</link>
<guid>https://arxiv.org/abs/2508.17180</guid>
<content:encoded><![CDATA[
arXiv:2508.17180v1 Announce Type: cross 
Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Architectures for Medical Image Denoising: A Comparative Study of CNN-DAE, CADTra, and DCMIEDNet</title>
<link>https://arxiv.org/abs/2508.17223</link>
<guid>https://arxiv.org/abs/2508.17223</guid>
<content:encoded><![CDATA[
arXiv:2508.17223v1 Announce Type: cross 
Abstract: Medical imaging modalities are inherently susceptible to noise contamination that degrades diagnostic utility and clinical assessment accuracy. This paper presents a comprehensive comparative evaluation of three state-of-the-art deep learning architectures for MRI brain image denoising: CNN-DAE, CADTra, and DCMIEDNet. We systematically evaluate these models across multiple Gaussian noise intensities ($\sigma = 10, 15, 25$) using the Figshare MRI Brain Dataset. Our experimental results demonstrate that DCMIEDNet achieves superior performance at lower noise levels, with PSNR values of $32.921 \pm 2.350$ dB and $30.943 \pm 2.339$ dB for $\sigma = 10$ and $15$ respectively. However, CADTra exhibits greater robustness under severe noise conditions ($\sigma = 25$), achieving the highest PSNR of $27.671 \pm 2.091$ dB. All deep learning approaches significantly outperform traditional wavelet-based methods, with improvements ranging from 5-8 dB across tested conditions. This study establishes quantitative benchmarks for medical image denoising and provides insights into architecture-specific strengths for varying noise intensities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Learning for Generalization of Hyperbolic Neural Networks</title>
<link>https://arxiv.org/abs/2508.17232</link>
<guid>https://arxiv.org/abs/2508.17232</guid>
<content:encoded><![CDATA[
arXiv:2508.17232v1 Announce Type: cross 
Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in representing real-world data with hierarchical structures via exploiting the geometric properties of hyperbolic spaces characterized by negative curvatures. Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may cause HNNs to converge to suboptimal parameters, degrading overall performance. So far, the theoretical foundation of the effect of curvatures on HNNs has not been developed. In this paper, we derive a PAC-Bayesian generalization bound of HNNs, highlighting the role of curvatures in the generalization of HNNs via their effect on the smoothness of the loss landscape. Driven by the derived bound, we propose a sharpness-aware curvature learning method to smooth the loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized through a bi-level optimization process. Then, we introduce an implicit differentiation algorithm that efficiently solves the bi-level optimization by approximating gradients of curvatures. We present the approximation error and convergence analyses of the proposed method, showing that the approximation error is upper-bounded, and the proposed method can converge by bounding gradients of HNNs. Experiments on four settings: classification, learning from long-tailed data, learning from noisy data, and few-shot learning show that our method can improve the performance of HNNs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing</title>
<link>https://arxiv.org/abs/2508.17326</link>
<guid>https://arxiv.org/abs/2508.17326</guid>
<content:encoded><![CDATA[
arXiv:2508.17326v1 Announce Type: cross 
Abstract: Echocardiography plays a central role in cardiac imaging, offering dynamic views of the heart that are essential for diagnosis and monitoring. However, image quality can be significantly degraded by haze arising from multipath reverberations, particularly in difficult-to-image patients. In this work, we propose a semantic-guided, diffusion-based dehazing algorithm developed for the MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method integrates a pixel-wise noise model, derived from semantic segmentation of hazy inputs into a diffusion posterior sampling framework guided by a generative prior trained on clean ultrasound data. Quantitative evaluation on the challenge dataset demonstrates strong performance across contrast and fidelity metrics. Code for the submitted algorithm is available at https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions</title>
<link>https://arxiv.org/abs/2508.17342</link>
<guid>https://arxiv.org/abs/2508.17342</guid>
<content:encoded><![CDATA[
arXiv:2508.17342v1 Announce Type: cross 
Abstract: Generating coherent and diverse human dances from music signals has gained tremendous progress in animating virtual avatars. While existing methods support direct dance synthesis, they fail to recognize that enabling users to edit dance movements is far more practical in real-world choreography scenarios. Moreover, the lack of high-quality dance datasets incorporating iterative editing also limits addressing this challenge. To achieve this goal, we first construct DanceRemix, a large-scale multi-turn editable dance dataset comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In addition, we propose a novel framework for iterative and editable dance generation coherently aligned with given music signals, namely DanceEditor. Considering the dance motion should be both musical rhythmic and enable iterative editing by user descriptions, our framework is built upon a prediction-then-editing paradigm unifying multi-modal conditions. At the initial prediction stage, our framework improves the authority of generated results by directly modeling dance movements from tailored, aligned music. Moreover, at the subsequent iterative editing stages, we incorporate text descriptions as conditioning information to draw the editable results through a specifically designed Cross-modality Editing Module (CEM). Specifically, CEM adaptively integrates the initial prediction with music and text prompts as temporal motion cues to guide the synthesized sequences. Thereby, the results display music harmonics while preserving fine-grained semantic alignment with text descriptions. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected DanceRemix dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShaLa: Multimodal Shared Latent Space Modelling</title>
<link>https://arxiv.org/abs/2508.17376</link>
<guid>https://arxiv.org/abs/2508.17376</guid>
<content:encoded><![CDATA[
arXiv:2508.17376v1 Announce Type: cross 
Abstract: This paper presents a novel generative framework for learning shared latent representations across multimodal data. Many advanced multimodal methods focus on capturing all combinations of modality-specific details across inputs, which can inadvertently obscure the high-level semantic concepts that are shared across modalities. Notably, Multimodal VAEs with low-dimensional latent variables are designed to capture shared representations, enabling various tasks such as joint multimodal synthesis and cross-modal inference. However, multimodal VAEs often struggle to design expressive joint variational posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses these challenges by integrating a novel architectural inference model and a second-stage expressive diffusion prior, which not only facilitates effective inference of shared latent representation but also significantly improves the quality of downstream multimodal synthesis. We validate ShaLa extensively across multiple benchmarks, demonstrating superior coherence and synthesis quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales to many more modalities while prior multimodal VAEs have fallen short in capturing the increasing complexity of the shared latent space.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</title>
<link>https://arxiv.org/abs/2508.17389</link>
<guid>https://arxiv.org/abs/2508.17389</guid>
<content:encoded><![CDATA[
arXiv:2508.17389v1 Announce Type: cross 
Abstract: Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation</title>
<link>https://arxiv.org/abs/2508.17466</link>
<guid>https://arxiv.org/abs/2508.17466</guid>
<content:encoded><![CDATA[
arXiv:2508.17466v1 Announce Type: cross 
Abstract: Quadruped robots have emerged as highly efficient and versatile platforms, excelling in navigating complex and unstructured terrains where traditional wheeled robots might fail. Equipping these robots with manipulator arms unlocks the advanced capability of loco-manipulation to perform complex physical interaction tasks in areas ranging from industrial automation to search-and-rescue missions. However, achieving precise and adaptable grasping in such dynamic scenarios remains a significant challenge, often hindered by the need for extensive real-world calibration and pre-programmed grasp configurations. This paper introduces a deep learning framework designed to enhance the grasping capabilities of quadrupeds equipped with arms, focusing on improved precision and adaptability. Our approach centers on a sim-to-real methodology that minimizes reliance on physical data collection. We developed a pipeline within the Genesis simulation environment to generate a synthetic dataset of grasp attempts on common objects. By simulating thousands of interactions from various perspectives, we created pixel-wise annotated grasp-quality maps to serve as the ground truth for our model. This dataset was used to train a custom CNN with a U-Net-like architecture that processes multi-modal input from an onboard RGB and depth cameras, including RGB images, depth maps, segmentation masks, and surface normal maps. The trained model outputs a grasp-quality heatmap to identify the optimal grasp point. We validated the complete framework on a four-legged robot. The system successfully executed a full loco-manipulation task: autonomously navigating to a target object, perceiving it with its sensors, predicting the optimal grasp pose using our model, and performing a precise grasp. This work proves that leveraging simulated training with advanced sensing offers a scalable and effective solution for object handling.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2508.17600</link>
<guid>https://arxiv.org/abs/2508.17600</guid>
<content:encoded><![CDATA[
arXiv:2508.17600v1 Announce Type: cross 
Abstract: Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation</title>
<link>https://arxiv.org/abs/2508.17643</link>
<guid>https://arxiv.org/abs/2508.17643</guid>
<content:encoded><![CDATA[
arXiv:2508.17643v1 Announce Type: cross 
Abstract: Event cameras offer microsecond latency, high dynamic range, and low power consumption, making them ideal for real-time robotic perception under challenging conditions such as motion blur, occlusion, and illumination changes. However, despite their advantages, synthetic event-based vision remains largely unexplored in mainstream robotics simulators. This lack of simulation setup hinders the evaluation of event-driven approaches for robotic manipulation and navigation tasks. This work presents an open-source, user-friendly v2e robotics operating system (ROS) package for Gazebo simulation that enables seamless event stream generation from RGB camera feeds. The package is used to investigate event-based robotic policies (ERP) for real-time navigation and manipulation. Two representative scenarios are evaluated: (1) object following with a mobile robot and (2) object detection and grasping with a robotic manipulator. Transformer-based ERPs are trained by behavior cloning and compared to RGB-based counterparts under various operating conditions. Experimental results show that event-guided policies consistently deliver competitive advantages. The results highlight the potential of event-driven perception to improve real-time robotic navigation and manipulation, providing a foundation for broader integration of event cameras into robotic policy learning. The GitHub repo for the dataset and code: https://eventbasedvision.github.io/SEBVS/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities</title>
<link>https://arxiv.org/abs/2508.17648</link>
<guid>https://arxiv.org/abs/2508.17648</guid>
<content:encoded><![CDATA[
arXiv:2508.17648v1 Announce Type: cross 
Abstract: Urban climate resilience requires more than high-resolution data; it demands systems that embed data collection, interpretation, and action within the daily lives of citizens. This chapter presents a scalable, citizen-centric framework that reimagines environmental infrastructure through participatory sensing, open analytics, and prescriptive urban planning tools. Applied in Pune, India, the framework comprises three interlinked modules: (1) a smartphone-based measurement toolkit enhanced by AI segmentation to extract tree height, canopy diameter, and trunk girth; (2) a percentile-based model using satellite-derived Land Surface Temperature to calculate localized cooling through two new metrics, Cooling Efficacy and Ambient Heat Relief; and (3) an eco-routing engine that guides mobility using a Static Environmental Quality score, based on tree density, species diversity, and cumulative carbon sequestration. Together, these modules form a closed feedback loop where citizens generate actionable data and benefit from personalized, sustainable interventions. This framework transforms open data from a passive repository into an active platform for shared governance and environmental equity. In the face of growing ecological inequality and data centralization, this chapter presents a replicable model for citizen-driven urban intelligence, reframing planning as a co-produced, climate-resilient, and radically local practice.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Feature Adapter for Efficient Adversarial Training</title>
<link>https://arxiv.org/abs/2508.17680</link>
<guid>https://arxiv.org/abs/2508.17680</guid>
<content:encoded><![CDATA[
arXiv:2508.17680v1 Announce Type: cross 
Abstract: Adversarial training (AT) with projected gradient descent is the most popular method to improve model robustness under adversarial attacks. However, computational overheads become prohibitively large when AT is applied to large backbone models. AT is also known to have the issue of robust overfitting. This paper contributes to solving both problems simultaneously towards building more trustworthy foundation models. In particular, we propose a new adapter-based approach for efficient AT directly in the feature space. We show that the proposed adapter-based approach can improve the inner-loop convergence quality by eliminating robust overfitting. As a result, it significantly increases computational efficiency and improves model accuracy by generalizing adversarial robustness to unseen attacks. We demonstrate the effectiveness of the new adapter-based approach in different backbone architectures and in AT at scale.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2508.17768</link>
<guid>https://arxiv.org/abs/2508.17768</guid>
<content:encoded><![CDATA[
arXiv:2508.17768v1 Announce Type: cross 
Abstract: Automated segmentation of BUS images is important for precise lesion delineation and tumor characterization, but is challenged by inherent artifacts and dataset inconsistencies. In this work, we evaluate the use of a modified Residual Encoder U-Net for breast ultrasound segmentation, with a focus on uncertainty quantification. We identify and correct for data duplication in the BUSI dataset, and use a deduplicated subset for more reliable estimates of generalization performance. Epistemic uncertainty is quantified using Monte Carlo dropout, deep ensembles, and their combination. Models are benchmarked on both in-distribution and out-of-distribution datasets to demonstrate how they generalize to unseen cross-domain data. Our approach achieves state-of-the-art segmentation accuracy on the Breast-Lesion-USG dataset with in-distribution validation, and provides calibrated uncertainty estimates that effectively signal regions of low model confidence. Performance declines and increased uncertainty observed in out-of-distribution evaluation highlight the persistent challenge of domain shift in medical imaging, and the importance of integrated uncertainty modeling for trustworthy clinical deployment. \footnote{Code available at: https://github.com/toufiqmusah/nn-uncertainty.git}
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.17811</link>
<guid>https://arxiv.org/abs/2508.17811</guid>
<content:encoded><![CDATA[
arXiv:2508.17811v1 Announce Type: cross 
Abstract: Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model</title>
<link>https://arxiv.org/abs/2508.17922</link>
<guid>https://arxiv.org/abs/2508.17922</guid>
<content:encoded><![CDATA[
arXiv:2508.17922v1 Announce Type: cross 
Abstract: Affordance is crucial for intelligent robots in the context of object manipulation. In this paper, we argue that affordance should be task-/instruction-dependent, which is overlooked by many previous works. That is, different instructions can lead to different manipulation regions and directions even for the same object. According to this observation, we present a new dataset comprising fifteen thousand object-instruction-affordance triplets. All scenes in the dataset are from an egocentric viewpoint, designed to approximate the perspective of a human-like robot. Furthermore, we investigate how to enable large multimodal models (LMMs) to serve as affordance predictors by implementing a ``search against verifiers'' pipeline. An LMM is asked to progressively predict affordances, with the output at each step being verified by itself during the iterative process, imitating a reasoning process. Experiments show that our method not only unlocks new instruction-oriented affordance prediction capabilities, but also achieves outstanding performance broadly.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</title>
<link>https://arxiv.org/abs/2508.17930</link>
<guid>https://arxiv.org/abs/2508.17930</guid>
<content:encoded><![CDATA[
arXiv:2508.17930v1 Announce Type: cross 
Abstract: Recently, detection of label errors and improvement of label quality in datasets for supervised learning tasks has become an increasingly important goal in both research and industry. The consequences of incorrectly annotated data include reduced model performance, biased benchmark results, and lower overall accuracy. Current state-of-the-art label error detection methods often focus on a single computer vision task and, consequently, a specific type of dataset, containing, for example, either bounding boxes or pixel-wise annotations. Furthermore, previous methods are not learning-based. In this work, we overcome this research gap. We present a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. In a nutshell, our approach - learning to detect label errors by making them - works as follows: we inject different kinds of label errors into the ground truth. Then, the detection of label errors, across all mentioned primary tasks, is framed as an instance segmentation problem based on a composite input. In our experiments, we compare the label error detection performance of our method with various baselines and state-of-the-art approaches of each task's domain on simulated label errors across multiple tasks, datasets, and base models. This is complemented by a generalization study on real-world label errors. Additionally, we release 459 real label errors identified in the Cityscapes dataset and provide a benchmark for real label error detection in Cityscapes.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Feature Imputing - A Technique for Error-resilient Semantic Communication</title>
<link>https://arxiv.org/abs/2508.17957</link>
<guid>https://arxiv.org/abs/2508.17957</guid>
<content:encoded><![CDATA[
arXiv:2508.17957v1 Announce Type: cross 
Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for achieving unprecedented communication efficiency in sixth-generation (6G) networks by leveraging artificial intelligence (AI) to extract and transmit the underlying meanings of source data. However, deploying SemCom over digital systems presents new challenges, particularly in ensuring robustness against transmission errors that may distort semantically critical content. To address this issue, this paper proposes a novel framework, termed generative feature imputing, which comprises three key techniques. First, we introduce a spatial error concentration packetization strategy that spatially concentrates feature distortions by encoding feature elements based on their channel mappings, a property crucial for both the effectiveness and reduced complexity of the subsequent techniques. Second, building on this strategy, we propose a generative feature imputing method that utilizes a diffusion model to efficiently reconstruct missing features caused by packet losses. Finally, we develop a semantic-aware power allocation scheme that enables unequal error protection by allocating transmission power according to the semantic importance of each packet. Experimental results demonstrate that the proposed framework outperforms conventional approaches, such as Deep Joint Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions, achieving higher semantic accuracy and lower Learned Perceptual Image Patch Similarity (LPIPS) scores.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning</title>
<link>https://arxiv.org/abs/2508.17965</link>
<guid>https://arxiv.org/abs/2508.17965</guid>
<content:encoded><![CDATA[
arXiv:2508.17965v1 Announce Type: cross 
Abstract: Livestreaming has become increasingly prevalent in modern visual communication, where automatic camera quality tuning is essential for delivering superior user Quality of Experience (QoE). Such tuning requires accurate blind image quality assessment (BIQA) to guide parameter optimization decisions. Unfortunately, the existing BIQA models typically only predict an overall coarse-grained quality score, which cannot provide fine-grained perceptual guidance for precise camera parameter tuning. To bridge this gap, we first establish FGLive-10K, a comprehensive fine-grained BIQA database containing 10,185 high-resolution images captured under varying camera parameter configurations across diverse livestreaming scenarios. The dataset features 50,925 multi-attribute quality annotations and 19,234 fine-grained pairwise preference annotations. Based on FGLive-10K, we further develop TuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which integrates human-aware feature extraction and graph-based camera parameter fusion. Extensive experiments and comparisons demonstrate that TuningIQA significantly outperforms state-of-the-art BIQA methods in both score regression and fine-grained quality ranking, achieving superior performance when deployed for livestreaming camera tuning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title>
<link>https://arxiv.org/abs/2508.17969</link>
<guid>https://arxiv.org/abs/2508.17969</guid>
<content:encoded><![CDATA[
arXiv:2508.17969v1 Announce Type: cross 
Abstract: This paper introduces a holistic perception system for internal and external monitoring of autonomous vehicles, with the aim of demonstrating a novel AI-leveraged self-adaptive framework of advanced vehicle technologies and solutions that optimize perception and experience on-board. Internal monitoring system relies on a multi-camera setup designed for predicting and identifying driver and occupant behavior through facial recognition, exploiting in addition a large language model as virtual assistant. Moreover, the in-cabin monitoring system includes AI-empowered smart sensors that measure air-quality and perform thermal comfort analysis for efficient on and off-boarding. On the other hand, external monitoring system perceives the surrounding environment of vehicle, through a LiDAR-based cost-efficient semantic segmentation approach, that performs highly accurate and efficient super-resolution on low-quality raw 3D point clouds. The holistic perception framework is developed in the context of EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on a real electric vehicle provided by ALKE. Experimental validation and evaluation at the integration site of Joint Research Centre at Ispra, Italy, highlights increased performance and efficiency of the modular blocks of the proposed perception architecture.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology Aware Neural Interpolation of Scalar Fields</title>
<link>https://arxiv.org/abs/2508.17995</link>
<guid>https://arxiv.org/abs/2508.17995</guid>
<content:encoded><![CDATA[
arXiv:2508.17995v1 Announce Type: cross 
Abstract: This paper presents a neural scheme for the topology-aware interpolation of time-varying scalar fields. Given a time-varying sequence of persistence diagrams, along with a sparse temporal sampling of the corresponding scalar fields, denoted as keyframes, our interpolation approach aims at "inverting" the non-keyframe diagrams to produce plausible estimations of the corresponding, missing data. For this, we rely on a neural architecture which learns the relation from a time value to the corresponding scalar field, based on the keyframe examples, and reliably extends this relation to the non-keyframe time steps. We show how augmenting this architecture with specific topological losses exploiting the input diagrams both improves the geometrical and topological reconstruction of the non-keyframe time steps. At query time, given an input time value for which an interpolation is desired, our approach instantaneously produces an output, via a single propagation of the time input through the network. Experiments interpolating 2D and 3D time-varying datasets show our approach superiority, both in terms of data and topological fitting, with regard to reference interpolation schemes.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</title>
<link>https://arxiv.org/abs/2508.18025</link>
<guid>https://arxiv.org/abs/2508.18025</guid>
<content:encoded><![CDATA[
arXiv:2508.18025v1 Announce Type: cross 
Abstract: Autonomous planetary exploration missions are critically dependent on real-time, accurate environmental perception for navigation and hazard avoidance. However, deploying deep learning models on the resource-constrained computational hardware of planetary exploration platforms remains a significant challenge. This paper introduces the Adaptive Quantized Planetary Crater Detection System (AQ-PCDSys), a novel framework specifically engineered for real-time, onboard deployment in the computationally constrained environments of space exploration missions. AQ-PCDSys synergistically integrates a Quantized Neural Network (QNN) architecture, trained using Quantization-Aware Training (QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture significantly optimizes model size and inference latency suitable for real-time onboard deployment in space exploration missions, while preserving high accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive Weighting Mechanism (AWM) to dynamically prioritize the most relevant and reliable sensor modality based on planetary ambient conditions. This approach enhances detection robustness across diverse planetary landscapes. Paired with Multi-Scale Detection Heads specifically designed for robust and efficient detection of craters across a wide range of sizes, AQ-PCDSys provides a computationally efficient, reliable and accurate solution for planetary crater detection, a critical capability for enabling the next generation of autonomous planetary landing, navigation, and scientific exploration.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.18179</link>
<guid>https://arxiv.org/abs/2508.18179</guid>
<content:encoded><![CDATA[
arXiv:2508.18179v1 Announce Type: cross 
Abstract: Evaluating whether vision-language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce SEAM, a benchmark that pairs semantically equivalent inputs across four domains that have existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notation and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</title>
<link>https://arxiv.org/abs/2508.18249</link>
<guid>https://arxiv.org/abs/2508.18249</guid>
<content:encoded><![CDATA[
arXiv:2508.18249v1 Announce Type: cross 
Abstract: Traversability estimation is critical for enabling robots to navigate across diverse terrains and environments. While recent self-supervised learning methods achieve promising results, they often fail to capture the characteristics of non-traversable regions. Moreover, most prior works concentrate on a single modality, overlooking the complementary strengths offered by integrating heterogeneous sensory modalities for more robust traversability estimation. To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. First, our annotation pipeline integrates footprint, LiDAR, and camera data as prompts for a vision foundation model, generating traversability labels that account for both semantic and geometric cues. Then, leveraging these labels, we train a dual-stream network that jointly learns from different modalities in a decoupled manner, enhancing its capacity to recognize diverse traversability patterns. In addition, we incorporate sparse LiDAR-based supervision to mitigate the noise introduced by pseudo labels. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. The proposed automatic labeling method consistently achieves around 88% IoU across diverse datasets. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Adversarial Domain Adaptation</title>
<link>https://arxiv.org/abs/1911.02054</link>
<guid>https://arxiv.org/abs/1911.02054</guid>
<content:encoded><![CDATA[
arXiv:1911.02054v3 Announce Type: replace 
Abstract: Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition</title>
<link>https://arxiv.org/abs/2209.11750</link>
<guid>https://arxiv.org/abs/2209.11750</guid>
<content:encoded><![CDATA[
arXiv:2209.11750v2 Announce Type: replace 
Abstract: Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Face Restoration: A Survey</title>
<link>https://arxiv.org/abs/2211.02831</link>
<guid>https://arxiv.org/abs/2211.02831</guid>
<content:encoded><![CDATA[
arXiv:2211.02831v2 Announce Type: replace 
Abstract: Face Restoration (FR) aims to restore High-Quality (HQ) faces from Low-Quality (LQ) input images, which is a domain-specific image restoration problem in the low-level computer vision area. The early face restoration methods mainly use statistical priors and degradation models, which are difficult to meet the requirements of real-world applications in practice. In recent years, face restoration has witnessed great progress after stepping into the deep learning era. However, there are few works to systematically study the deep learning based face restoration methods. Thus, in this paper, we provide a comprehensive survey of recent advances in deep learning techniques for face restoration. Specifically, we first summarize different problem formulations and analyze the characteristics of face images. Second, we discuss the challenges of face restoration. With regard to these challenges, we present a comprehensive review of recent FR methods, including prior-based methods and deep-learning methods. Then, we explore developed techniques in the task of FR covering network architectures, loss functions, and benchmark datasets. We also conduct a systematic benchmark evaluation on representative methods. Finally, we discuss the future directions including network designs, metrics, benchmark datasets, applications, etc. We also provide an open source repository for all the discussed methods, which is available at https://github.com/TaoWangzj/Awesome-Face-Restoration.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering</title>
<link>https://arxiv.org/abs/2311.18561</link>
<guid>https://arxiv.org/abs/2311.18561</guid>
<content:encoded><![CDATA[
arXiv:2311.18561v3 Announce Type: replace 
Abstract: Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent and large scene representation learning with sparse training data, we introduce a novel temporal smoothing mechanism and a position-aware adaptive control strategy respectively. Extensive experiments on Waymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 900-fold acceleration in rendering over the best alternative.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptRR: Diffusion Models as Prompt Generators for Single Image Reflection Removal</title>
<link>https://arxiv.org/abs/2402.02374</link>
<guid>https://arxiv.org/abs/2402.02374</guid>
<content:encoded><![CDATA[
arXiv:2402.02374v2 Announce Type: replace 
Abstract: Existing single image reflection removal (SIRR) methods using deep learning tend to miss key low-frequency (LF) and high-frequency (HF) differences in images, affecting their effectiveness in removing reflections. To address this problem, this paper proposes a novel prompt-guided reflection removal (PromptRR) framework that uses frequency information as new visual prompts for better reflection performance. Specifically, the proposed framework decouples the reflection removal process into the prompt generation and subsequent prompt-guided restoration. For the prompt generation, we first propose a prompt pre-training strategy to train a frequency prompt encoder that encodes the ground-truth image into LF and HF prompts. Then, we adopt diffusion models (DMs) as prompt generators to generate the LF and HF prompts estimated by the pre-trained frequency prompt encoder. For the prompt-guided restoration, we integrate specially generated prompts into the PromptFormer network, employing a novel Transformer-based prompt block to effectively steer the model toward enhanced reflection removal. The results on commonly used benchmarks show that our method outperforms state-of-the-art approaches. The codes and models are available at https://github.com/TaoWangzj/PromptRR.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imperceptible Protection against Style Imitation from Diffusion Models</title>
<link>https://arxiv.org/abs/2403.19254</link>
<guid>https://arxiv.org/abs/2403.19254</guid>
<content:encoded><![CDATA[
arXiv:2403.19254v3 Announce Type: replace 
Abstract: Recent progress in diffusion models has profoundly enhanced the fidelity of image generation, but it has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we introduce a visually improved protection method while preserving its protection capability. To this end, we devise a perceptual map to highlight areas sensitive to human eyes, guided by instance-aware refinement, which refines the protection intensity accordingly. We also introduce a difficulty-aware protection by predicting how difficult the artwork is to protect and dynamically adjusting the intensity based on this. Lastly, we integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout</title>
<link>https://arxiv.org/abs/2404.00412</link>
<guid>https://arxiv.org/abs/2404.00412</guid>
<content:encoded><![CDATA[
arXiv:2404.00412v2 Announce Type: replace 
Abstract: Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at https://github.com/ayanban011/SVGCraft.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-Down Guidance for Learning Object-Centric Representations</title>
<link>https://arxiv.org/abs/2405.10598</link>
<guid>https://arxiv.org/abs/2405.10598</guid>
<content:encoded><![CDATA[
arXiv:2405.10598v3 Announce Type: replace 
Abstract: Humans' innate ability to decompose scenes into objects allows for efficient understanding, predicting, and planning. In light of this, Object-Centric Learning (OCL) attempts to endow networks with similar capabilities, learning to represent scenes with the composition of objects. However, existing OCL models only learn through reconstructing the input images, which does not assist the model in distinguishing objects, resulting in suboptimal object-centric representations. This flaw limits current object-centric models to relatively simple downstream tasks. To address this issue, we draw on humans' top-down vision pathway and propose Top-Down Guided Network (TDGNet), which includes a top-down pathway to improve object-centric representations. During training, the top-down pathway constructs guidance with high-level object-centric representations to optimize low-level grid features output by the backbone. While during inference, it refines object-centric representations by detecting and solving conflicts between low- and high-level features. We show that TDGNet outperforms current object-centric models on multiple datasets of varying complexity. In addition, we expand the downstream task scope of object-centric representations by applying TDGNet to the field of robotics, validating its effectiveness in downstream tasks including video prediction and visual planning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenUnify: Scaling Up Autoregressive Pretraining for Neuron Segmentation</title>
<link>https://arxiv.org/abs/2405.16847</link>
<guid>https://arxiv.org/abs/2405.16847</guid>
<content:encoded><![CDATA[
arXiv:2405.16847v2 Announce Type: replace 
Abstract: Neuron segmentation from electron microscopy (EM) volumes is crucial for understanding brain circuits, yet the complex neuronal structures in high-resolution EM images present significant challenges. EM data exhibits unique characteristics including high noise levels, anisotropic voxel dimensions, and ultra-long spatial dependencies that make traditional vision models inadequate. Inspired by autoregressive pretraining in language models, we propose TokenUnify, a hierarchical predictive coding framework that captures multi-scale dependencies through three complementary learning objectives. TokenUnify integrates random token prediction, next-token prediction, and next-all token prediction to create a comprehensive representational space with emergent properties. From an information-theoretic perspective, these three tasks are complementary and provide optimal coverage of visual data structure, with our approach reducing autoregressive error accumulation from O(K) to O(sqrt(K)) for sequences of length K. We also introduce a large-scale EM dataset with 1.2 billion annotated voxels, offering ideal long-sequence visual data with spatial continuity. Leveraging the Mamba architecture's linear-time sequence modeling capabilities, TokenUnify achieves a 44% performance improvement on downstream neuron segmentation and outperforms MAE by 25%. Our approach demonstrates superior scaling properties as model size increases, effectively bridging the gap between pretraining strategies for language and vision models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixRO: Pixel-Distributed Rotational Odometry with Gaussian Belief Propagation</title>
<link>https://arxiv.org/abs/2406.09726</link>
<guid>https://arxiv.org/abs/2406.09726</guid>
<content:encoded><![CDATA[
arXiv:2406.09726v2 Announce Type: replace 
Abstract: Images are the standard input for most computer vision algorithms. However, their processing often reduces to parallelizable operations applied locally and independently to individual pixels. Yet, many of these low-level raw pixel readings only provide redundant or noisy information for specific high-level tasks, leading to inefficiencies in both energy consumption during their transmission off-sensor and computational resources in their subsequent processing.
  As novel sensors featuring advanced in-pixel processing capabilities emerge, we envision a paradigm shift toward performing increasingly complex visual processing directly in-pixel, reducing computational overhead downstream. We advocate for synthesizing high-level cues at the pixel level, enabling their off-sensor transmission to directly support downstream tasks more effectively than raw pixel readings.
  This paper conceptualizes a novel photometric rotation estimation algorithm to be distributed at pixel level, where each pixel estimates the global motion of the camera by exchanging information with other pixels to achieve global consensus. We employ a probabilistic formulation and leverage Gaussian Belief Propagation (GBP) for decentralized inference using messaging-passing. The proposed proposed technique is evaluated on real-world public datasets and we offer a in-depth analysis of the practicality of applying GBP to distributed rotation estimation at pixel level.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Feature Distillation with Object-Centric Priors</title>
<link>https://arxiv.org/abs/2406.18742</link>
<guid>https://arxiv.org/abs/2406.18742</guid>
<content:encoded><![CDATA[
arXiv:2406.18742v5 Announce Type: replace 
Abstract: Grounding natural language to the physical world is a ubiquitous topic with a wide range of applications in computer vision and robotics. Recently, 2D vision-language models such as CLIP have been widely popularized, due to their impressive capabilities for open-vocabulary grounding in 2D images. Recent works aim to elevate 2D CLIP features to 3D via feature distillation, but either learn neural fields that are scene-specific and hence lack generalization, or focus on indoor room scan data that require access to multiple camera views, which is not practical in robot manipulation scenarios. Additionally, related methods typically fuse features at pixel-level and assume that all camera views are equally informative. In this work, we show that this approach leads to sub-optimal 3D features, both in terms of grounding accuracy, as well as segmentation crispness. To alleviate this, we propose a multi-view feature fusion strategy that employs object-centric priors to eliminate uninformative views based on semantic information, and fuse features at object-level via instance segmentation masks. To distill our object-centric 3D features, we generate a large-scale synthetic multi-view dataset of cluttered tabletop scenes, spawning 15k scenes from over 3300 unique object instances, which we make publicly available. We show that our method reconstructs 3D CLIP features with improved grounding capacity and spatial consistency, while doing so from single-view RGB-D, thus departing from the assumption of multiple camera views at test time. Finally, we show that our approach can generalize to novel tabletop domains and be re-purposed for 3D instance segmentation without fine-tuning, and demonstrate its utility for language-guided robotic grasping in clutter.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Evaluative AI: A Hypothesis-Driven Tool with Concept-Based Explanations and Weight of Evidence</title>
<link>https://arxiv.org/abs/2407.04710</link>
<guid>https://arxiv.org/abs/2407.04710</guid>
<content:encoded><![CDATA[
arXiv:2407.04710v3 Announce Type: replace 
Abstract: This paper presents Visual Evaluative AI, a decision aid that provides positive and negative evidence from image data for a given hypothesis. This tool finds high-level human concepts in an image and generates the Weight of Evidence (WoE) for each hypothesis in the decision-making process. We apply and evaluate this tool in the skin cancer domain by building a web-based application that allows users to upload a dermatoscopic image, select a hypothesis and analyse their decisions by evaluating the provided evidence. Further, we demonstrate the effectiveness of Visual Evaluative AI on different concept-based explanation approaches.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion</title>
<link>https://arxiv.org/abs/2407.06673</link>
<guid>https://arxiv.org/abs/2407.06673</guid>
<content:encoded><![CDATA[
arXiv:2407.06673v2 Announce Type: replace 
Abstract: Transformers have captured growing attention in computer vision, thanks to its large capacity and global processing capabilities. However, transformers are data hungry, and their ability to generalize is constrained compared to Convolutional Neural Networks (ConvNets), especially when trained with limited data due to the absence of the built-in spatial inductive biases present in ConvNets. In this paper, we strive to optimally combine the strengths of both convolution and transformers for image classification tasks. Towards this end, we present a novel lightweight hybrid network that pairs Convolution with Transformers via Representation Learning Fusion and Multi-Level Feature Cross-Attention named CTRL-F. Our network comprises a convolution branch and a novel transformer module named multi-level feature cross-attention (MFCA). The MFCA module operates on multi-level feature representations obtained at different convolution stages. It processes small patch tokens and large patch tokens extracted from these multi-level feature representations via two separate transformer branches, where both branches communicate and exchange knowledge through cross-attention mechanism. We fuse the local responses acquired from the convolution path with the global responses acquired from the MFCA module using novel representation fusion techniques dubbed adaptive knowledge fusion (AKF) and collaborative knowledge fusion (CKF). Experiments demonstrate that our CTRL-F variants achieve state-of-the-art performance, whether trained from scratch on large data or even with low-data regime. For Instance, CTRL-F achieves top-1 accuracy of 82.24% and 99.91% when trained from scratch on Oxford-102 Flowers and PlantVillage datasets respectively, surpassing state-of-the-art models which showcase the robustness of our model on image classification tasks. Code at: https://github.com/hosamsherif/CTRL-F
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Clinical Knowledge Graphs and Gradient-Based Neural Systems for Enhanced Melanoma Diagnosis via the 7-Point Checklist</title>
<link>https://arxiv.org/abs/2407.16822</link>
<guid>https://arxiv.org/abs/2407.16822</guid>
<content:encoded><![CDATA[
arXiv:2407.16822v3 Announce Type: replace 
Abstract: The 7-point checklist (7PCL) is a widely used diagnostic tool in dermoscopy for identifying malignant melanoma by assigning point values to seven specific attributes. However, the traditional 7PCL is limited to distinguishing between malignant melanoma and melanocytic Nevi, and falls short in scenarios where multiple skin diseases with appearances similar to melanoma coexist. To address this limitation, we propose a novel diagnostic framework that integrates a clinical knowledge-based topological graph (CKTG) with a gradient diagnostic strategy featuring a data-driven weighting system (GD-DDW). The CKTG captures both the internal and external relationships among the 7PCL attributes, while the GD-DDW emulates dermatologists' diagnostic processes, prioritizing visual observation before making predictions. Additionally, we introduce a multimodal feature extraction approach leveraging a dual-attention mechanism to enhance feature extraction through cross-modal interaction and unimodal collaboration. This method incorporates meta-information to uncover interactions between clinical data and image features, ensuring more accurate and robust predictions. Our approach, evaluated on the EDRA dataset, achieved an average AUC of 88.6%, demonstrating superior performance in melanoma detection and feature prediction. This integrated system provides data-driven benchmarks for clinicians, significantly enhancing the precision of melanoma diagnosis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Framework to Rule Them All: Unifying Multimodal Tasks with LLM Neural-Tuning</title>
<link>https://arxiv.org/abs/2408.03001</link>
<guid>https://arxiv.org/abs/2408.03001</guid>
<content:encoded><![CDATA[
arXiv:2408.03001v3 Announce Type: replace 
Abstract: Large-scale models have exhibited remarkable capabilities across diverse domains, including automated medical services and intelligent customer support. However, as most large models are trained on single-modality corpora, enabling them to effectively process and understand multimodal signals remains a significant challenge. Current research often focuses on designing task-specific or scenario-specific tuning strategies, which limits the scalability and versatility. To address this limitation, we propose a unified framework that concurrently handles multiple tasks and modalities. In this framework, all modalities and tasks are represented as unified tokens and trained using a single, consistent approach. To enable efficient multitask processing, we introduce a novel tuning strategy termed neural tuning, inspired by the concept of sparse distributed representation in the human brain, where only specific subsets of neurons are activated for each task. Furthermore, to advance research in multimodal and multitask learning, we present a new benchmark, MMUD, which includes samples annotated with multiple task labels spanning reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. By applying neural tuning to pretrained large models on the MMUD benchmark, we demonstrate the ability to handle multiple tasks simultaneously in a streamlined and efficient manner. All models, code, and datasets will be released publicly upon publication, fostering further research and innovation in this field.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set Deepfake Detection: A Parameter-Efficient Adaptation Method with Forgery Style Mixture</title>
<link>https://arxiv.org/abs/2408.12791</link>
<guid>https://arxiv.org/abs/2408.12791</guid>
<content:encoded><![CDATA[
arXiv:2408.12791v2 Announce Type: replace 
Abstract: Open-set face forgery detection poses significant security threats and presents substantial challenges for existing detection models. These detectors primarily have two limitations: they cannot generalize across unknown forgery domains and inefficiently adapt to new data. To address these issues, we introduce an approach that is both general and parameter-efficient for face forgery detection. It builds on the assumption that different forgery source domains exhibit distinct style statistics. Previous methods typically require fully fine-tuning pre-trained networks, consuming substantial time and computational resources. In turn, we design a forgery-style mixture formulation that augments the diversity of forgery source domains, enhancing the model's generalizability across unseen domains. Drawing on recent advancements in vision transformers (ViT) for face forgery detection, we develop a parameter-efficient ViT-based detection model that includes lightweight forgery feature extraction modules and enables the model to extract global and local forgery clues simultaneously. We only optimize the inserted lightweight modules during training, maintaining the original ViT structure with its pre-trained ImageNet weights. This training strategy effectively preserves the informative pre-trained knowledge while flexibly adapting the model to the task of Deepfake detection. Extensive experimental results demonstrate that the designed model achieves state-of-the-art generalizability with significantly reduced trainable parameters, representing an important step toward open-set Deepfake detection in the wild.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModalPrompt: Towards Efficient Multimodal Continual Instruction Tuning with Dual-Modality Guided Prompt</title>
<link>https://arxiv.org/abs/2410.05849</link>
<guid>https://arxiv.org/abs/2410.05849</guid>
<content:encoded><![CDATA[
arXiv:2410.05849v2 Announce Type: replace 
Abstract: Large Multimodal Models (LMMs) exhibit remarkable multi-tasking ability by learning mixed instruction datasets. However, novel tasks would be encountered sequentially in dynamic world, which urges for equipping LMMs with multimodal continual instruction learning (MCIT) ability especially for diverse and challenging generative tasks. Existing MCIT methods do not fully exploit the unique attribute of LMMs and often gain performance at the expense of efficiency. In this paper, we propose a novel prompt learning framework for MCIT to effectively alleviate forgetting of previous knowledge while managing computational complexity with natural image-text supervision. Concretely, we learn prompts for each task and exploit efficient prompt fusion for knowledge transfer and prompt selection for complexity management with dual-modality guidance. Extensive experiments demonstrate that our approach achieves substantial +14.26% performance gain on MCIT benchmarks with remarkable $\times$ 1.42 inference speed free from growing computation. Code is available at https://github.com/AuroraZengfh/ModalPrompt.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARLA2Real: a tool for reducing the sim2real appearance gap in CARLA simulator</title>
<link>https://arxiv.org/abs/2410.18238</link>
<guid>https://arxiv.org/abs/2410.18238</guid>
<content:encoded><![CDATA[
arXiv:2410.18238v4 Announce Type: replace 
Abstract: Simulators are indispensable for research in autonomous systems such as self-driving cars, autonomous robots, and drones. Despite significant progress in various simulation aspects, such as graphical realism, an evident gap persists between the virtual and real-world environments. Since the ultimate goal is to deploy the autonomous systems in the real world, reducing the sim2real gap is of utmost importance. In this paper, we employ a state-of-the-art approach to enhance the photorealism of simulated data, aligning them with the visual characteristics of real-world datasets. Based on this, we developed CARLA2Real, an easy-to-use, publicly available tool (plug-in) for the widely used and open-source CARLA simulator. This tool enhances the output of CARLA in near real-time, achieving a frame rate of 13 FPS, translating it to the visual style and realism of real-world datasets such as Cityscapes, KITTI, and Mapillary Vistas. By employing the proposed tool, we generated synthetic datasets from both the simulator and the enhancement model outputs, including their corresponding ground truth annotations for tasks related to autonomous driving. Then, we performed a number of experiments to evaluate the impact of the proposed approach on feature extraction and semantic segmentation methods when trained on the enhanced synthetic data. The results demonstrate that the sim2real appearance gap is significant and can indeed be reduced by the introduced approach. Comparisons with a state-of-the-art image-to-image translation approach are also provided. The tool, pre-trained models, and associated data for this work are available for download at: https://github.com/stefanos50/CARLA2Real.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LumiSculpt: Enabling Consistent Portrait Lighting in Video Generation</title>
<link>https://arxiv.org/abs/2410.22979</link>
<guid>https://arxiv.org/abs/2410.22979</guid>
<content:encoded><![CDATA[
arXiv:2410.22979v2 Announce Type: replace 
Abstract: Lighting plays a pivotal role in ensuring the naturalness and aesthetic quality of video generation. However, the impact of lighting is deeply coupled with other factors of videos, e.g., objects and scenes. Thus, it remains challenging to disentangle and model coherent lighting conditions independently, limiting the flexibility to control lighting in video generation. In this paper, inspired by the established controllable T2I models, we propose LumiSculpt, which enables precise and consistent lighting control in T2V generation models. LumiSculpt equips the video generation with new interactive capabilities, allowing the input of reference image sequences with customized lighting conditions. Furthermore, the core learnable plug-and-play module of LumiSculpt facilitates direct control over the intensity, position and trajectory of an assumed light source in video diffusion models. To effectively train LumiSculpt and address the issue of insufficient lighting data, we construct LumiHuman, a new lightweight and flexible dataset for portrait lighting of images and videos. Experimental results demonstrate that LumiSculpt achieves precise and high-quality lighting control in video generation. The analysis demonstrates the flexibility of LumiHuman.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2410.23266</link>
<guid>https://arxiv.org/abs/2410.23266</guid>
<content:encoded><![CDATA[
arXiv:2410.23266v2 Announce Type: replace 
Abstract: Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection</title>
<link>https://arxiv.org/abs/2411.08402</link>
<guid>https://arxiv.org/abs/2411.08402</guid>
<content:encoded><![CDATA[
arXiv:2411.08402v5 Announce Type: replace 
Abstract: Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weather-robust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%/6.70% in foggy/snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: https://github.com/ylwhxht/V2X-R.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation</title>
<link>https://arxiv.org/abs/2411.16789</link>
<guid>https://arxiv.org/abs/2411.16789</guid>
<content:encoded><![CDATA[
arXiv:2411.16789v2 Announce Type: replace 
Abstract: Sign language translation (SLT) is a challenging task that involves translating sign language images into spoken language. For SLT models to perform this task successfully, they must bridge the modality gap and identify subtle variations in sign language components to understand their meanings accurately. To address these challenges, we propose a novel gloss-free SLT framework called Multimodal Sign Language Translation (MMSLT), which leverages the representational capabilities of off-the-shelf multimodal large language models (MLLMs). Specifically, we use MLLMs to generate detailed textual descriptions of sign language components. Then, through our proposed multimodal-language pre-training module, we integrate these description features with sign video features to align them within the spoken sentence space. Our approach achieves state-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily, highlighting the potential of MLLMs to be utilized effectively in SLT. Code is available at https://github.com/hwjeon98/MMSLT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Shadow Art</title>
<link>https://arxiv.org/abs/2411.19161</link>
<guid>https://arxiv.org/abs/2411.19161</guid>
<content:encoded><![CDATA[
arXiv:2411.19161v2 Announce Type: replace 
Abstract: Shadow art is a captivating form of sculptural expression where the projection of a sculpture in a specific direction reveals a desired shape with high precision. In this work, we introduce Neural Shadow Art, which leverages implicit occupancy function representation to significantly expand the possibilities of shadow art. This representation enables the design of high-quality, 3D-printable geometric models with arbitrary topologies at any resolution, surpassing previous voxel- and mesh-based methods. Our method provides a more flexible framework, enabling projections to match input binary images under various light directions and screen orientations, without requiring light sources to be perpendicular to the screens. Furthermore, we allow rigid transformations of the projected geometries relative to the input binary images and simultaneously optimize light directions and screen orientations to ensure that the projections closely resemble the target images, especially when dealing with inputs of complex topologies. In addition, our model promotes surface smoothness and reduces material usage. This is particularly advantageous for efficient industrial production and enhanced artistic effect by generating compelling shadow art that avoids trivial, intersecting cylindrical structures. In summary, we propose a more flexible representation for shadow art, significantly improving projection accuracy while simultaneously meeting industrial requirements and delivering awe-inspiring artistic effects.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</title>
<link>https://arxiv.org/abs/2412.04380</link>
<guid>https://arxiv.org/abs/2412.04380</guid>
<content:encoded><![CDATA[
arXiv:2412.04380v3 Announce Type: replace 
Abstract: 3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents that demand to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through the local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Our EmbodiedOcc outperforms existing methods by a large margin and accomplishes the embodied occupancy prediction with high accuracy and efficiency. Code: https://github.com/YkiWu/EmbodiedOcc.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Text Embedding Leakage in Diffusion-based Image Editing</title>
<link>https://arxiv.org/abs/2412.04715</link>
<guid>https://arxiv.org/abs/2412.04715</guid>
<content:encoded><![CDATA[
arXiv:2412.04715v4 Announce Type: replace 
Abstract: Text-based image editing, powered by generative diffusion models, lets users modify images through natural-language prompts and has dramatically simplified traditional workflows. Despite these advances, current methods still suffer from a critical problem: attribute leakage, where edits meant for specific objects unintentionally affect unrelated regions or other target objects. Our analysis reveals the root cause as the semantic entanglement inherent in End-of-Sequence (EOS) embeddings generated by autoregressive text encoders, which indiscriminately aggregate attributes across prompts. To address this issue, we introduce Attribute-Leakage-free Editing (ALE), a framework that tackles attribute leakage at its source. ALE combines Object-Restricted Embeddings (ORE) to disentangle text embeddings, Region-Guided Blending for Cross-Attention Masking (RGB-CAM) for spatially precise attention, and Background Blending (BB) to preserve non-edited content. To quantitatively evaluate attribute leakage across various editing methods, we propose the Attribute-Leakage Evaluation Benchmark (ALE-Bench), featuring comprehensive editing scenarios and new metrics. Extensive experiments show that ALE reduces attribute leakage by large margins, thereby enabling accurate, multi-object, text-driven image editing while faithfully preserving non-target content.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2412.13195</link>
<guid>https://arxiv.org/abs/2412.13195</guid>
<content:encoded><![CDATA[
arXiv:2412.13195v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models excel at generating photorealistic images but often fail to render accurate spatial relationships. We identify two core issues underlying this common failure: 1) the ambiguous nature of data concerning spatial relationships in existing datasets, and 2) the inability of current text encoders to accurately interpret the spatial semantics of input descriptions. We propose CoMPaSS, a versatile framework that enhances spatial understanding in T2I models. It first addresses data ambiguity with the Spatial Constraints-Oriented Pairing (SCOP) data engine, which curates spatially-accurate training data via principled constraints. To leverage these priors, CoMPaSS also introduces the Token ENcoding ORdering (TENOR) module, which preserves crucial token ordering information lost by text encoders, thereby reinforcing the prompt's linguistic structure. Extensive experiments on four popular T2I models (UNet and MMDiT-based) show CoMPaSS sets a new state of the art on key spatial benchmarks, with substantial relative gains on VISOR (+98%), T2I-CompBench Spatial (+67%), and GenEval Position (+131%). Code is available at https://github.com/blurgyy/CoMPaSS.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchid: Image Latent Diffusion for Joint Appearance and Geometry Generation</title>
<link>https://arxiv.org/abs/2501.13087</link>
<guid>https://arxiv.org/abs/2501.13087</guid>
<content:encoded><![CDATA[
arXiv:2501.13087v2 Announce Type: replace 
Abstract: We introduce Orchid, a unified latent diffusion model that learns a joint appearance-geometry prior to generate color, depth, and surface normal images in a single diffusion process. This unified approach is more efficient and coherent than current pipelines that use separate models for appearance and geometry. Orchid is versatile - it directly generates color, depth, and normal images from text, supports joint monocular depth and normal estimation with color-conditioned finetuning, and seamlessly inpaints large 3D regions by sampling from the joint distribution. It leverages a novel Variational Autoencoder (VAE) that jointly encodes RGB, relative depth, and surface normals into a shared latent space, combined with a latent diffusion model that denoises these latents. Our extensive experiments demonstrate that Orchid delivers competitive performance against SOTA task-specific methods for geometry prediction, even surpassing them in normal-prediction accuracy and depth-normal consistency. It also inpaints color-depth-normal images jointly, with more qualitative realism than existing multi-step methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Generation Without Guidance</title>
<link>https://arxiv.org/abs/2501.15420</link>
<guid>https://arxiv.org/abs/2501.15420</guid>
<content:encoded><![CDATA[
arXiv:2501.15420v2 Announce Type: replace 
Abstract: Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCN: Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2501.16289</link>
<guid>https://arxiv.org/abs/2501.16289</guid>
<content:encoded><![CDATA[
arXiv:2501.16289v4 Announce Type: replace 
Abstract: Although LiDAR sensors have become indispensable for autonomous vehicles (AVs) due to their ability to provide accurate 3D scene understanding and robust perception under adverse weather conditions, the properties of LiDAR point clouds vary widely across sensor configurations and data acquisition domains, leading to severe performance degradation when models are transferred between heterogeneous sensors or from simulation to the real world. To address this challenge, we propose the Multi-view Structural Convolution Network (MSCN), a novel architecture designed to achieve domain-invariant recognition across diverse LiDAR configurations and environments. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Furthermore, we incorporate an unseen domain generation strategy to mitigate domain gaps during training. Extensive experiments demonstrate that MSCN consistently outperforms state-of-the-art point cloud classification methods across all domain change scenarios. These results highlight MSCN as a scalable solution for deploying LiDAR-based perception systems of AVs. Our code is available at https://github.com/MLMLab/MSCN.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</title>
<link>https://arxiv.org/abs/2502.15969</link>
<guid>https://arxiv.org/abs/2502.15969</guid>
<content:encoded><![CDATA[
arXiv:2502.15969v4 Announce Type: replace 
Abstract: Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow</title>
<link>https://arxiv.org/abs/2502.17288</link>
<guid>https://arxiv.org/abs/2502.17288</guid>
<content:encoded><![CDATA[
arXiv:2502.17288v4 Announce Type: replace 
Abstract: Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation</title>
<link>https://arxiv.org/abs/2503.09878</link>
<guid>https://arxiv.org/abs/2503.09878</guid>
<content:encoded><![CDATA[
arXiv:2503.09878v2 Announce Type: replace 
Abstract: Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis</title>
<link>https://arxiv.org/abs/2503.13211</link>
<guid>https://arxiv.org/abs/2503.13211</guid>
<content:encoded><![CDATA[
arXiv:2503.13211v2 Announce Type: replace 
Abstract: Advancements in AI for medical imaging offer significant potential. However, their applications are constrained by the limited availability of data and the reluctance of medical centers to share it due to patient privacy concerns. Generative models present a promising solution by creating synthetic data as a substitute for real patient data. However, medical images are typically high-dimensional, and current state-of-the-art methods are often impractical for computational resource-constrained healthcare environments. These models rely on data sub-sampling, raising doubts about their feasibility and real-world applicability. Furthermore, many of these models are evaluated on quantitative metrics that alone can be misleading in assessing the image quality and clinical meaningfulness of the generated images. To address this, we introduce MedLoRD, a generative diffusion model designed for computational resource-constrained environments. MedLoRD is capable of generating high-dimensional medical volumes with resolutions up to 512$\times$512$\times$256, utilizing GPUs with only 24GB VRAM, which are commonly found in standard desktop workstations. MedLoRD is evaluated across multiple modalities, including Coronary Computed Tomography Angiography and Lung Computed Tomography datasets. Extensive evaluations through radiological evaluation, relative regional volume analysis, adherence to conditional masks, and downstream tasks show that MedLoRD generates high-fidelity images closely adhering to segmentation mask conditions, surpassing the capabilities of current state-of-the-art generative models for medical image synthesis in computational resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios</title>
<link>https://arxiv.org/abs/2503.21893</link>
<guid>https://arxiv.org/abs/2503.21893</guid>
<content:encoded><![CDATA[
arXiv:2503.21893v2 Announce Type: replace 
Abstract: Object detection models often struggle with class imbalance, where rare categories appear significantly less frequently than common ones. Existing sampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and Instance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting sample frequencies based on image and instance counts. However, these methods are based on linear adjustments, which limit their effectiveness in long-tailed distributions. This work introduces Exponentially Weighted Instance-Aware Repeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential scaling to better differentiate between rare and frequent classes. E-IRFS adjusts sampling probabilities using an exponential function applied to the geometric mean of image and instance frequencies, ensuring a more adaptive rebalancing strategy. We evaluate E-IRFS on a dataset derived from the Fireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11 object detection models to identify fire, smoke, people and lakes in emergency scenarios. The results show that E-IRFS improves detection performance by 22\% over the baseline and outperforms RFS and IRFS, particularly for rare categories. The analysis also highlights that E-IRFS has a stronger effect on lightweight models with limited capacity, as these models rely more on data sampling strategies to address class imbalance. The findings demonstrate that E-IRFS improves rare object detection in resource-constrained environments, making it a suitable solution for real-time applications such as UAV-based emergency monitoring. The code is available at: https://github.com/futurians/E-IRFS.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up Questions</title>
<link>https://arxiv.org/abs/2503.24180</link>
<guid>https://arxiv.org/abs/2503.24180</guid>
<content:encoded><![CDATA[
arXiv:2503.24180v2 Announce Type: replace 
Abstract: Graphical user interfaces (GUI) automation agents are emerging as powerful tools, enabling humans to accomplish increasingly complex tasks on smart devices. However, users often inadvertently omit key information when conveying tasks, which hinders agent performance in the current agent paradigm that does not support immediate user intervention. To address this issue, we introduce a $\textbf{Self-Correction GUI Navigation}$ task that incorporates interactive information completion capabilities within GUI agents. We developed the $\textbf{Navi-plus}$ dataset with GUI follow-up question-answer pairs, alongside a $\textbf{Dual-Stream Trajectory Evaluation}$ method to benchmark this new capability. Our results show that agents equipped with the ability to ask GUI follow-up questions can fully recover their performance when faced with ambiguous user tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T*: Re-thinking Temporal Search for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2504.02259</link>
<guid>https://arxiv.org/abs/2504.02259</guid>
<content:encoded><![CDATA[
arXiv:2504.02259v3 Announce Type: replace 
Abstract: Efficiently understanding long-form videos remains a significant challenge in computer vision. In this work, we revisit temporal search paradigms for long-form video understanding and address a fundamental issue pertaining to all state-of-the-art (SOTA) long-context vision-language models (VLMs). Our contributions are twofold: First, we frame temporal search as a Long Video Haystack problem: finding a minimal set of relevant frames (e.g., one to five) from tens of thousands based on specific queries. Upon this formulation, we introduce LV-Haystack, the first dataset with 480 hours of videos, 15,092 human-annotated instances for both training and evaluation aiming to improve temporal search quality and efficiency. Results on LV-Haystack highlight a significant research gap in temporal search capabilities, with current SOTA search methods only achieving 2.1% temporal F1 score on the Longvideobench subset. Next, inspired by visual search in images, we propose a lightweight temporal search framework, T* that reframes costly temporal search as spatial search. T* leverages powerful visual localization techniques commonly used in images and introduces an adaptive zooming-in mechanism that operates across both temporal and spatial dimensions. Extensive experiments show that integrating T* with existing methods significantly improves SOTA long-form video understanding. Under an inference budget of 32 frames, T* improves GPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-OV-72B's performance from 56.5% to 62.4% on the Longvideobench XL subset. Our code, benchmark, and models are provided in the Supplementary material.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2504.08736</link>
<guid>https://arxiv.org/abs/2504.08736</guid>
<content:encoded><![CDATA[
arXiv:2504.08736v2 Announce Type: replace 
Abstract: In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to $\bf{3 \space billion}$ parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEL: A Novel Lipschitz Continuity-constrained Ensemble Learning Model for EEG-based Emotion Recognition</title>
<link>https://arxiv.org/abs/2504.09156</link>
<guid>https://arxiv.org/abs/2504.09156</guid>
<content:encoded><![CDATA[
arXiv:2504.09156v2 Announce Type: replace 
Abstract: The accurate and efficient recognition of emotional states in oneself and others is critical, as impairments in this ability can lead to significant psychosocial difficulties. While electroencephalography (EEG) offers a powerful tool for emotion detection, current EEG-based emotion recognition (EER) methods face key limitations: insufficient model stability, limited accuracy in processing high-dimensional nonlinear EEG signals, and poor robustness against intra-subject variability and signal noise. To address these challenges, we introduce LEL (Lipschitz continuity-constrained Ensemble Learning), a novel framework that enhances EEG-based emotion recognition. By integrating Lipschitz continuity constraints, LEL ensures greater model stability and improves generalization, thereby reducing sensitivity to signal variability and noise while significantly boosting the model's overall accuracy and robustness. Its ensemble learning strategy optimizes overall performance by fusing decisions from multiple classifiers to reduce single-model bias and variance. Experimental results on three public benchmark datasets (EAV, FACED and SEED) demonstrated the LEL's state-of-the-art performance, achieving average recognition accuracies of 76.43%, 83.00% and 87.22%, respectively. The official implementation codes are released at https://github.com/NZWANG/LEL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AffordanceSAM: Segment Anything Once More in Affordance Grounding</title>
<link>https://arxiv.org/abs/2504.15650</link>
<guid>https://arxiv.org/abs/2504.15650</guid>
<content:encoded><![CDATA[
arXiv:2504.15650v2 Announce Type: replace 
Abstract: Building a generalized affordance grounding model to identify actionable regions on objects is vital for real-world applications. Existing methods to train the model can be divided into weakly and fully supervised ways. However, the former method requires a complex training framework design and can not infer new actions without an auxiliary prior. While the latter often struggle with limited annotated data and components trained from scratch despite being simpler. This study focuses on fully supervised affordance grounding and overcomes its limitations by proposing AffordanceSAM, which extends SAM's generalization capacity in segmentation to affordance grounding. Specifically, we design an affordance-adaption module and curate a coarse-to-fine annotated dataset called C2F-Aff to thoroughly transfer SAM's robust performance to affordance in a three-stage training manner. Experimental results confirm that AffordanceSAM achieves state-of-the-art (SOTA) performance on the AGD20K benchmark and exhibits strong generalized capacity.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnimateAnywhere: Rouse the Background in Human Image Animation</title>
<link>https://arxiv.org/abs/2504.19834</link>
<guid>https://arxiv.org/abs/2504.19834</guid>
<content:encoded><![CDATA[
arXiv:2504.19834v2 Announce Type: replace 
Abstract: Human image animation aims to generate human videos of given characters and backgrounds that adhere to the desired pose sequence. However, existing methods focus more on human actions while neglecting the generation of background, which typically leads to static results or inharmonious movements. The community has explored camera pose-guided animation tasks, yet preparing the camera trajectory is impractical for most entertainment applications and ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing the background in human image animation without requirements on camera trajectories. In particular, based on our key insight that the movement of the human body often reflects the motion of the background, we introduce a background motion learner (BML) to learn background motions from human pose sequences. To encourage the model to learn more accurate cross-frame correspondences, we further deploy an epipolar constraint on the 3D attention map. Specifically, the mask used to suppress geometrically unreasonable attention is carefully constructed by combining an epipolar mask and the current 3D attention map. Extensive experiments demonstrate that our AnimateAnywhere effectively learns the background motion from human pose sequences, achieving state-of-the-art performance in generating human animation results with vivid and realistic backgrounds. The source code and model will be available at https://github.com/liuxiaoyu1104/AnimateAnywhere.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-Learner: Texturing Mesh with Spherical Harmonics</title>
<link>https://arxiv.org/abs/2504.19938</link>
<guid>https://arxiv.org/abs/2504.19938</guid>
<content:encoded><![CDATA[
arXiv:2504.19938v3 Announce Type: replace 
Abstract: In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</title>
<link>https://arxiv.org/abs/2504.20970</link>
<guid>https://arxiv.org/abs/2504.20970</guid>
<content:encoded><![CDATA[
arXiv:2504.20970v2 Announce Type: replace 
Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient-based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications. The implementation is available at: github.com/meterdogan07/SVD-LS.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.00568</link>
<guid>https://arxiv.org/abs/2505.00568</guid>
<content:encoded><![CDATA[
arXiv:2505.00568v3 Announce Type: replace 
Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. Pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. This behavior is especially valuable in medical imaging, where annotations are often scarce. However, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. In practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. Consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. Therefore, we introduce BM-MAE, a masked image modeling pre-training strategy tailored for multimodal MRI data. The same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. This allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. Extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. Additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. Code and trained models are available at: https://github.com/Lucas-rbnt/BM-MAE
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
<link>https://arxiv.org/abs/2505.01571</link>
<guid>https://arxiv.org/abs/2505.01571</guid>
<content:encoded><![CDATA[
arXiv:2505.01571v3 Announce Type: replace 
Abstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities - including RGB, synthetic thermal, and estimated depth videos - and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 75 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment. The foundation model's architecture (code) and weights are available at: https://github.com/GkikasStefanos/PainFormer.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.02005</link>
<guid>https://arxiv.org/abs/2505.02005</guid>
<content:encoded><![CDATA[
arXiv:2505.02005v2 Announce Type: replace 
Abstract: Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decompose scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released at https://github.com/MiZhenxing/Switch-NeRF.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIN-NBV: A View Introspection Network for Next-Best-View Selection</title>
<link>https://arxiv.org/abs/2505.06219</link>
<guid>https://arxiv.org/abs/2505.06219</guid>
<content:encoded><![CDATA[
arXiv:2505.06219v3 Announce Type: replace 
Abstract: Next Best View (NBV) algorithms aim to maximize 3D scene acquisition quality using minimal resources, e.g. number of acquisitions, time taken, or distance traversed. Prior methods often rely on coverage maximization as a proxy for reconstruction quality, but for complex scenes with occlusions and finer details, this is not always sufficient and leads to poor reconstructions. Our key insight is to train an acquisition policy that directly optimizes for reconstruction quality rather than just coverage. To achieve this, we introduce the View Introspection Network (VIN): a lightweight neural network that predicts the Relative Reconstruction Improvement (RRI) of a potential next viewpoint without making any new acquisitions. We use this network to power a simple, yet effective, sequential samplingbased greedy NBV policy. Our approach, VIN-NBV, generalizes to unseen object categories, operates without prior scene knowledge, is adaptable to resource constraints, and can handle occlusions. We show that our RRI fitness criterion leads to a ~30% gain in reconstruction quality over a coverage-based criterion using the same greedy strategy. Furthermore, VIN-NBV also outperforms deep reinforcement learning methods, Scan-RL and GenNBV, by ~40%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</title>
<link>https://arxiv.org/abs/2505.15123</link>
<guid>https://arxiv.org/abs/2505.15123</guid>
<content:encoded><![CDATA[
arXiv:2505.15123v2 Announce Type: replace 
Abstract: Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion</title>
<link>https://arxiv.org/abs/2505.15313</link>
<guid>https://arxiv.org/abs/2505.15313</guid>
<content:encoded><![CDATA[
arXiv:2505.15313v2 Announce Type: replace 
Abstract: Human facial images encode a rich spectrum of information, encompassing both stable identity-related traits and mutable attributes such as pose, expression, and emotion. While recent advances in image generation have enabled high-quality identity-conditional face synthesis, precise control over non-identity attributes remains challenging, and disentangling identity from these mutable factors is particularly difficult. To address these limitations, we propose a novel identity-conditional diffusion model that introduces two lightweight control modules designed to independently manipulate facial pose, expression, and emotion without compromising identity preservation. These modules are embedded within the cross-attention layers of the base diffusion model, enabling precise attribute control with minimal parameter overhead. Furthermore, our tailored training strategy, which leverages cross-attention between the identity feature and each non-identity control feature, encourages identity features to remain orthogonal to control signals, enhancing controllability and diversity. Quantitative and qualitative evaluations, along with perceptual user studies, demonstrate that our method surpasses existing approaches in terms of control accuracy over pose, expression, and emotion, while also improving generative diversity under identity-only conditioning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for Precise Underwater Instance Segmentation</title>
<link>https://arxiv.org/abs/2505.15581</link>
<guid>https://arxiv.org/abs/2505.15581</guid>
<content:encoded><![CDATA[
arXiv:2505.15581v3 Announce Type: replace 
Abstract: With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at https://github.com/LiamLian0727/UIIS10K.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title>
<link>https://arxiv.org/abs/2506.01608</link>
<guid>https://arxiv.org/abs/2506.01608</guid>
<content:encoded><![CDATA[
arXiv:2506.01608v2 Announce Type: replace 
Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model</title>
<link>https://arxiv.org/abs/2506.02452</link>
<guid>https://arxiv.org/abs/2506.02452</guid>
<content:encoded><![CDATA[
arXiv:2506.02452v2 Announce Type: replace 
Abstract: While diffusion models advance text-to-motion generation, their static semantic conditioning ignores temporal-frequency demands: early denoising requires structural semantics for motion foundations while later stages need localized details for text alignment. This mismatch mirrors biological morphogenesis where developmental phases demand distinct genetic programs. Inspired by epigenetic regulation governing morphological specialization, we propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture. ANT orchestrates semantic granularity through: **(i) Semantic Temporally Adaptive (STA) Module:** Automatically partitions denoising into low-frequency structural planning and high-frequency refinement via spectral analysis. **(ii) Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts conditional to unconditional ratio enhancing efficiency while maintaining fidelity. Extensive experiments show that ANT can be applied to various baselines, significantly improving model performance, and achieving state-of-the-art semantic alignment on StableMoFusion.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WetCat: Enabling Automated Skill Assessment in Wet-Lab Cataract Surgery Videos</title>
<link>https://arxiv.org/abs/2506.08896</link>
<guid>https://arxiv.org/abs/2506.08896</guid>
<content:encoded><![CDATA[
arXiv:2506.08896v2 Announce Type: replace 
Abstract: To meet the growing demand for systematic surgical training, wetlab environments have become indispensable platforms for hands-on practice in ophthalmology. Yet, traditional wetlab training depends heavily on manual performance evaluations, which are labor-intensive, time-consuming, and often subject to variability. Recent advances in computer vision offer promising avenues for automated skill assessment, enhancing both the efficiency and objectivity of surgical education. Despite notable progress in ophthalmic surgical datasets, existing resources predominantly focus on real surgeries or isolated tasks, falling short of supporting comprehensive skill evaluation in controlled wetlab settings. To address these limitations, we introduce WetCat, the first dataset of wetlab cataract surgery videos specifically curated for automated skill assessment. WetCat comprises high-resolution recordings of surgeries performed by trainees on artificial eyes, featuring comprehensive phase annotations and semantic segmentations of key anatomical structures. These annotations are meticulously designed to facilitate skill assessment during the critical capsulorhexis and phacoemulsification phases, adhering to standardized surgical skill assessment frameworks. By focusing on these essential phases, WetCat enables the development of interpretable, AI-driven evaluation tools aligned with established clinical metrics. This dataset lays a strong foundation for advancing objective, scalable surgical education and sets a new benchmark for automated workflow analysis and skill assessment in ophthalmology training. The dataset and annotations are publicly available in Synapse https://www.synapse.org/Synapse:syn66401174/files.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffS-NOCS: 3D Point Cloud Reconstruction through Coloring Sketches to NOCS Maps Using Diffusion Models</title>
<link>https://arxiv.org/abs/2506.12835</link>
<guid>https://arxiv.org/abs/2506.12835</guid>
<content:encoded><![CDATA[
arXiv:2506.12835v2 Announce Type: replace 
Abstract: Reconstructing a 3D point cloud from a given conditional sketch is challenging. Existing methods often work directly in 3D space, but domain variability and difficulty in reconstructing accurate 3D structures from 2D sketches remain significant obstacles. Moreover, ideal models should also accept prompts for control, in addition with the sparse sketch, posing challenges in multi-modal fusion. We propose DiffS-NOCS (Diffusion-based Sketch-to-NOCS Map), which leverages ControlNet with a modified multi-view decoder to generate NOCS maps with embedded 3D structure and position information in 2D space from sketches. The 3D point cloud is reconstructed by combining multiple NOCS maps from different views. To enhance sketch understanding, we integrate a viewpoint encoder for extracting viewpoint features. Additionally, we design a feature-level multi-view aggregation network as the denoising module, facilitating cross-view information exchange and improving 3D consistency in NOCS map generation. Experiments on ShapeNet demonstrate that DiffS-NOCS achieves controllable and fine-grained point cloud reconstruction aligned with sketches.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification</title>
<link>https://arxiv.org/abs/2506.15565</link>
<guid>https://arxiv.org/abs/2506.15565</guid>
<content:encoded><![CDATA[
arXiv:2506.15565v2 Announce Type: replace 
Abstract: Ultra-high Spatial Resolution (UHSR) Land Cover Classification is increasingly important for urban analysis, enabling fine-scale planning, ecological monitoring, and infrastructure management. It identifies land cover types on sub-meter remote sensing imagery, capturing details such as building outlines, road networks, and distinct boundaries. However, most existing methods focus on 1 m imagery and rely heavily on large-scale annotations, while UHSR data remain scarce and difficult to annotate, limiting practical applicability. To address these challenges, we introduce Baltimore Atlas, a UHSR land cover classification framework that reduces reliance on large-scale training data and delivers high-accuracy results. Baltimore Atlas builds on three key ideas: (1) Baltimore Atlas Dataset, a 0.3 m resolution dataset based on aerial imagery of Baltimore City; (2) FreqWeaver Adapter, a parameter-efficient adapter that transfers SAM2 to this domain, leveraging foundation model knowledge to reduce training data needs while enabling fine-grained detail and structural modeling; (3) Uncertainty-Aware Teacher Student Framework, a semi-supervised framework that exploits unlabeled data to further reduce training dependence and improve generalization across diverse scenes. Using only 5.96% of total model parameters, our approach achieves a 1.78% IoU improvement over existing parameter-efficient tuning strategies and a 3.44% IoU gain compared to state-of-the-art high-resolution remote sensing segmentation methods on the Baltimore Atlas Dataset.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</title>
<link>https://arxiv.org/abs/2506.15610</link>
<guid>https://arxiv.org/abs/2506.15610</guid>
<content:encoded><![CDATA[
arXiv:2506.15610v3 Announce Type: replace 
Abstract: Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection. Specifically, given streaming posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual foundation model (VFM) for single-view 3D object detection by bounding boxes, coupled with CLIP to capture open-vocabulary semantics of detected objects. To fuse all detected bounding boxes across different views into a unified one, we employ an association module for correspondences of multi-views and an optimization module to fuse the 3D bounding boxes of the same instance predicted in multi-views. The association module utilizes 3D Non-Maximum Suppression (NMS) and a box correspondence matching module, while the optimization module uses an IoU-guided efficient random optimization technique based on particle filtering to enforce multi-view consistency of the 3D bounding boxes while minimizing computational complexity. Extensive experiments on ScanNetV2 and CA-1M datasets demonstrate that our method achieves state-of-the-art performance among online methods. Benefiting from this novel reconstruction-free paradigm for 3D object detection, our method exhibits great generalization abilities in various scenarios, enabling real-time perception even in environments exceeding 1000 square meters.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Temporal Sentence Grounding via Causal Inference</title>
<link>https://arxiv.org/abs/2507.04958</link>
<guid>https://arxiv.org/abs/2507.04958</guid>
<content:encoded><![CDATA[
arXiv:2507.04958v2 Announce Type: replace 
Abstract: Temporal Sentence Grounding (TSG) aims to identify relevant moments in an untrimmed video that semantically correspond to a given textual query. Despite existing studies having made substantial progress, they often overlook the issue of spurious correlations between video and textual queries. These spurious correlations arise from two primary factors: (1) inherent biases in the textual data, such as frequent co-occurrences of specific verbs or phrases, and (2) the model's tendency to overfit to salient or repetitive patterns in video content. Such biases mislead the model into associating textual cues with incorrect visual moments, resulting in unreliable predictions and poor generalization to out-of-distribution examples. To overcome these limitations, we propose a novel TSG framework, causal intervention and counterfactual reasoning that utilizes causal inference to eliminate spurious correlations and enhance the model's robustness. Specifically, we first formulate the TSG task from a causal perspective with a structural causal model. Then, to address unobserved confounders reflecting textual biases toward specific verbs or phrases, a textual causal intervention is proposed, utilizing do-calculus to estimate the causal effects. Furthermore, visual counterfactual reasoning is performed by constructing a counterfactual scenario that focuses solely on video features, excluding the query and fused multi-modal features. This allows us to debias the model by isolating and removing the influence of the video from the overall effect. Experiments on public datasets demonstrate the superiority of the proposed method. The code is available at https://github.com/Tangkfan/CICR.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency Regulation for Exposure Bias Mitigation in Diffusion Models</title>
<link>https://arxiv.org/abs/2507.10072</link>
<guid>https://arxiv.org/abs/2507.10072</guid>
<content:encoded><![CDATA[
arXiv:2507.10072v3 Announce Type: replace 
Abstract: Diffusion models exhibit impressive generative capabilities but are significantly impacted by exposure bias. In this paper, we make a key observation: the energy of predicted noisy samples in the reverse process continuously declines compared to perturbed samples in the forward process. Building on this, we identify two important findings: 1) The reduction in energy follows distinct patterns in the low-frequency and high-frequency subbands; 2) The subband energy of reverse-process reconstructed samples is consistently lower than that of forward-process ones, and both are lower than the original data samples. Based on the first finding, we introduce a dynamic frequency regulation mechanism utilizing wavelet transforms, which separately adjusts the low- and high-frequency subbands. Leveraging the second insight, we derive the rigorous mathematical form of exposure bias. It is worth noting that, our method is training-free and plug-and-play, significantly improving the generative quality of various diffusion models and frameworks with negligible computational cost. The source code is available at https://github.com/kunzhan/wpp.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks</title>
<link>https://arxiv.org/abs/2507.10239</link>
<guid>https://arxiv.org/abs/2507.10239</guid>
<content:encoded><![CDATA[
arXiv:2507.10239v2 Announce Type: replace 
Abstract: Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition</title>
<link>https://arxiv.org/abs/2507.16287</link>
<guid>https://arxiv.org/abs/2507.16287</guid>
<content:encoded><![CDATA[
arXiv:2507.16287v2 Announce Type: replace 
Abstract: Few-shot action recognition (FSAR) aims to classify human actions in videos with only a small number of labeled samples per category. The scarcity of training data has driven recent efforts to incorporate additional modalities, particularly text. However, the subtle variations in human posture, motion dynamics, and the object interactions that occur during different phases, are critical inherent knowledge of actions that cannot be fully exploited by action labels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a novel framework that goes beyond label semantics by leveraging Large Language Models (LLMs) to dissect the essential representational characteristics hidden beneath action labels. Guided by the prior knowledge encoded in LLM, LGA effectively captures rich spatiotemporal cues in few-shot scenarios. Specifically, for text, we prompt an off-the-shelf LLM to anatomize labels into sequences of atomic action descriptions, focusing on the three core elements of action (subject, motion, object). For videos, a Visual Anatomy Module segments actions into atomic video phases to capture the sequential structure of actions. A fine-grained fusion strategy then integrates textual and visual features at the atomic level, resulting in more generalizable prototypes. Finally, we introduce a Multimodal Matching mechanism, comprising both video-video and video-text matching, to ensure robust few-shot classification. Experimental results demonstrate that LGA achieves state-of-the-art performance across multipe FSAR benchmarks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Hybrid Captioner for Improved Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2507.17047</link>
<guid>https://arxiv.org/abs/2507.17047</guid>
<content:encoded><![CDATA[
arXiv:2507.17047v2 Announce Type: replace 
Abstract: Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging</title>
<link>https://arxiv.org/abs/2507.17412</link>
<guid>https://arxiv.org/abs/2507.17412</guid>
<content:encoded><![CDATA[
arXiv:2507.17412v2 Announce Type: replace 
Abstract: The increasing volume of medical images poses challenges for radiologists in retrieving relevant cases. Content-based image retrieval (CBIR) systems offer potential for efficient access to similar cases, yet lack standardized evaluation and comprehensive studies. Building on prior studies for tumor characterization via CBIR, this study advances CBIR research for volumetric medical images through three key contributions: (1) a framework eliminating reliance on pre-segmented data and organ-specific datasets, aligning with large and unstructured image archiving systems, i.e. PACS in clinical practice; (2) introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's contextualized late interaction mechanism for 3D medical imaging; (3) comprehensive evaluation across four tumor sites using three feature extractors and three database configurations. Our evaluations highlight the significant advantages of C-MIR. We demonstrate the successful adaptation of the late interaction principle to volumetric medical images, enabling effective context-aware re-ranking. A key finding is C-MIR's ability to effectively localize the region of interest, eliminating the need for pre-segmentation of datasets and offering a computationally efficient alternative to systems relying on expensive data enrichment steps. C-MIR demonstrates promising improvements in tumor flagging, achieving improved performance, particularly for colon and lung tumors (p<0.05). C-MIR also shows potential for improving tumor staging, warranting further exploration of its capabilities. Ultimately, our work seeks to bridge the gap between advanced retrieval techniques and their practical applications in healthcare, paving the way for improved diagnostic processes.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization</title>
<link>https://arxiv.org/abs/2507.20562</link>
<guid>https://arxiv.org/abs/2507.20562</guid>
<content:encoded><![CDATA[
arXiv:2507.20562v2 Announce Type: replace 
Abstract: Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pr$^2$R: Information-Fused and Style-Aware Privacy-Preserving Replay for Lifelong Person Re-Identification</title>
<link>https://arxiv.org/abs/2508.01587</link>
<guid>https://arxiv.org/abs/2508.01587</guid>
<content:encoded><![CDATA[
arXiv:2508.01587v2 Announce Type: replace 
Abstract: Lifelong person re-identification (LReID) aims to incrementally accumulate knowledge across a sequence of tasks under domain shifts. Recently, replay-based methods have demonstrated strong effectiveness in LReID by rehearsing past samples stored in an auxiliary memory. However, storing historical exemplars raises concerns over data privacy. To avoid this, exemplar-free approaches attempt to match the distribution of past data without storing raw samples. Despite being privacy-friendly, these methods often suffer from performance degradation due to the forgetting of specific past knowledge representations. To this end, we propose to fuse information from sequential data into the pixel space in the replay memory, enabling Privacy-Preserving Replay (Pr$^2$R). More specifically, by distilling the training characteristics of multiple real images into a single image, the fused samples undergo pixel-level changes. This not only protects the privacy of the original data but also makes the replay samples more representative for sequential tasks. During the style replay phase, we align the current domain to the previous one while simultaneously adapting the replay samples to match the style of the current domain. This dual-alignment strategy effectively mitigates both class-incremental challenges and forgetting caused by domain shifts. Extensive experiments on multiple benchmarks show that the proposed method significantly improves replay effectiveness while preserving data privacy. Specifically, Pr$^2$R achieves 4% and 6% higher accuracy on sequential tasks compared to the current state-of-the-art and other replay-based methods, respectively.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VPN: Visual Prompt Navigation</title>
<link>https://arxiv.org/abs/2508.01766</link>
<guid>https://arxiv.org/abs/2508.01766</guid>
<content:encoded><![CDATA[
arXiv:2508.01766v3 Announce Type: replace 
Abstract: While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis</title>
<link>https://arxiv.org/abs/2508.03775</link>
<guid>https://arxiv.org/abs/2508.03775</guid>
<content:encoded><![CDATA[
arXiv:2508.03775v2 Announce Type: replace 
Abstract: Automated experimentation with real time data analysis in scanning transmission electron microscopy (STEM) often require end-to-end framework. The four-dimensional scanning transmission electron microscopy (4D-STEM) with high-throughput data acquisition has been constrained by the critical bottleneck results from data preprocessing. Pervasive noise, beam center drift, and elliptical distortions during high-throughput acquisition inevitably corrupt diffraction patterns, systematically biasing quantitative measurements. Yet, conventional correction algorithms are often material-specific and fail to provide a robust, generalizable solution. In this work, we present 4D-PreNet, an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net and ResNet architectures to simultaneously perform denoising, center correction, and elliptical distortion calibration. The network is trained on large, simulated datasets encompassing a wide range of noise levels, drift magnitudes, and distortion types, enabling it to generalize effectively to experimental data acquired under varying conditions. Quantitative evaluations demonstrate that our pipeline reduces mean squared error by up to 50% during denoising and achieves sub-pixel center localization in the center detection task, with average errors below 0.04 pixels. The outputs are bench-marked against traditional algorithms, highlighting improvements in both noise suppression and restoration of diffraction patterns, thereby facilitating high-throughput, reliable 4D-STEM real-time analysis for automated characterization.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.06259</link>
<guid>https://arxiv.org/abs/2508.06259</guid>
<content:encoded><![CDATA[
arXiv:2508.06259v3 Announce Type: replace 
Abstract: Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method. Code: https://github.com/zhangquanchen/SIFThinker.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</title>
<link>https://arxiv.org/abs/2508.07493</link>
<guid>https://arxiv.org/abs/2508.07493</guid>
<content:encoded><![CDATA[
arXiv:2508.07493v2 Announce Type: replace 
Abstract: Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical Autoregressive Model for Robotic Manipulation without Action Pretraining</title>
<link>https://arxiv.org/abs/2508.09822</link>
<guid>https://arxiv.org/abs/2508.09822</guid>
<content:encoded><![CDATA[
arXiv:2508.09822v2 Announce Type: replace 
Abstract: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras</title>
<link>https://arxiv.org/abs/2508.09912</link>
<guid>https://arxiv.org/abs/2508.09912</guid>
<content:encoded><![CDATA[
arXiv:2508.09912v2 Announce Type: replace 
Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier-Guided Attention Upsampling for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.10616</link>
<guid>https://arxiv.org/abs/2508.10616</guid>
<content:encoded><![CDATA[
arXiv:2508.10616v2 Announce Type: replace 
Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Visual Anomaly Detection for Task Execution Monitoring</title>
<link>https://arxiv.org/abs/2107.14206</link>
<guid>https://arxiv.org/abs/2107.14206</guid>
<content:encoded><![CDATA[
arXiv:2107.14206v2 Announce Type: replace-cross 
Abstract: Execution monitoring is essential for robots to detect and respond to failures. Since it is impossible to enumerate all failures for a given task, we learn from successful executions of the task to detect visual anomalies during runtime. Our method learns to predict the motions that occur during the nominal execution of a task, including camera and robot body motion. A probabilistic U-Net architecture is used to learn to predict optical flow, and the robot's kinematics and 3D model are used to model camera and body motion. The errors between the observed and predicted motion are used to calculate an anomaly score. We evaluate our method on a dataset of a robot placing a book on a shelf, which includes anomalies such as falling books, camera occlusions, and robot disturbances. We find that modeling camera and body motion, in addition to the learning-based optical flow prediction, results in an improvement of the area under the receiver operating characteristic curve from 0.752 to 0.804, and the area under the precision-recall curve from 0.467 to 0.549.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach</title>
<link>https://arxiv.org/abs/2401.09671</link>
<guid>https://arxiv.org/abs/2401.09671</guid>
<content:encoded><![CDATA[
arXiv:2401.09671v3 Announce Type: replace-cross 
Abstract: Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Handover Failure Detection Dataset and Baselines</title>
<link>https://arxiv.org/abs/2402.18319</link>
<guid>https://arxiv.org/abs/2402.18319</guid>
<content:encoded><![CDATA[
arXiv:2402.18319v2 Announce Type: replace-cross 
Abstract: An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Quality Assessment and Example-Guided Image Processing by Disentangling Picture Appearance from Content</title>
<link>https://arxiv.org/abs/2404.13484</link>
<guid>https://arxiv.org/abs/2404.13484</guid>
<content:encoded><![CDATA[
arXiv:2404.13484v2 Announce Type: replace-cross 
Abstract: The deep learning revolution has strongly impacted low-level image processing tasks such as style/domain transfer, enhancement/restoration, and visual quality assessments. Despite often being treated separately, the aforementioned tasks share a common theme of understanding, editing, or enhancing the appearance of input images without modifying the underlying content. We leverage this observation to develop a novel disentangled representation learning method that decomposes inputs into content and appearance features. The model is trained in a self-supervised manner and we use the learned features to develop a new quality prediction model named DisQUE. We demonstrate through extensive evaluations that DisQUE achieves state-of-the-art accuracy across quality prediction tasks and distortion types. Moreover, we demonstrate that the same features may also be used for image processing tasks such as HDR tone mapping, where the desired output characteristics may be tuned using example input-output pairs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroTune: Efficient Federated Learning for Large Heterogeneous Models</title>
<link>https://arxiv.org/abs/2411.16796</link>
<guid>https://arxiv.org/abs/2411.16796</guid>
<content:encoded><![CDATA[
arXiv:2411.16796v2 Announce Type: replace-cross 
Abstract: While large pre-trained models have achieved impressive performance across AI tasks, their deployment in privacy-sensitive and distributed environments remains challenging. Federated learning (FL) offers a viable solution by enabling decentralized fine-tuning without data sharing, but real-world applications face significant obstacles due to heterogeneous client resources in compute and memory. To address this, we propose HeteroTune, a novel federated fine-tuning paradigm for large, heterogeneous models operating under limited communication and computation budgets. The core of our method lies in a novel architecture, DeMA (Dense Mixture of Adapters), which enables flexible and efficient aggregation of heterogeneous models by preserving their full representational capacity while facilitating seamless cross-model knowledge fusion. We further introduce CMGA (Cross-Model Gradient Alignment), a lightweight yet effective mechanism that enhances training stability by harmonizing gradient directions across heterogeneous client models during aggregation, mitigating update conflicts and promoting more consistent convergence in federated settings. We provide both theoretical analysis and empirical evidence showing that HeteroTune achieves state-of-the-art performance and efficiency across diverse tasks and model architectures. For example, on LLaMA models, it reduces communication overhead by 99.5%, cuts peak memory usage by ~50%, and improves performance by 4.61%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Field Matching: an Electrostatic Paradigm to Generate and Transfer Data</title>
<link>https://arxiv.org/abs/2502.02367</link>
<guid>https://arxiv.org/abs/2502.02367</guid>
<content:encoded><![CDATA[
arXiv:2502.02367v3 Announce Type: replace-cross 
Abstract: We propose Electrostatic Field Matching (EFM), a novel method that is suitable for both generative modeling and distribution transfer tasks. Our approach is inspired by the physics of an electrical capacitor. We place source and target distributions on the capacitor plates and assign them positive and negative charges, respectively. Then we learn the electrostatic field of the capacitor using a neural network approximator. To map the distributions to each other, we start at one plate of the capacitor and move the samples along the learned electrostatic field lines until they reach the other plate. We theoretically justify that this approach provably yields the distribution transfer. In practice, we demonstrate the performance of our EFM in toy and image data experiments. Our code is available at https://github.com/justkolesov/FieldMatching
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising, segmentation and volumetric rendering of optical coherence tomography angiography (OCTA) image using deep learning techniques: a review</title>
<link>https://arxiv.org/abs/2502.14935</link>
<guid>https://arxiv.org/abs/2502.14935</guid>
<content:encoded><![CDATA[
arXiv:2502.14935v2 Announce Type: replace-cross 
Abstract: Optical coherence tomography angiography (OCTA) is a non-invasive imaging technique widely used to study vascular structures and micro-circulation dynamics in the retina and choroid. OCTA has been widely used in clinics for diagnosing ocular disease and monitoring its progression, because OCTA is safer and faster than dye-based angiography while retaining the ability to characterize micro-scale structures. However, OCTA data contains many inherent noises from the devices and acquisition protocols and suffers from various types of artifacts, which impairs diagnostic accuracy and repeatability. Deep learning (DL) based imaging analysis models are able to automatically detect and remove artifacts and noises, and enhance the quality of image data. It is also a powerful tool for segmentation and identification of normal and pathological structures in the images. Thus, the value of OCTA imaging can be significantly enhanced by the DL-based approaches for interpreting and performing measurements and predictions on the OCTA data. In this study, we reviewed literature on the DL models for OCTA images in the latest five years. In particular, we focused on discussing the current problems in the OCTA data and the corresponding design principles of the DL models. We also reviewed the state-of-art DL models for 3D volumetric reconstruction of the vascular networks and pathological structures such as the edema and distorted optic disc. In addition, the publicly available dataset of OCTA images are summarized at the end of this review. Overall, this review can provide valuable insights for engineers to develop novel DL models by utilizing the characteristics of OCTA signals and images. The pros and cons of each DL methods and their applications discussed in this review can be helpful to assist technicians and clinicians to use proper DL models for fundamental research and disease screening.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real</title>
<link>https://arxiv.org/abs/2502.17894</link>
<guid>https://arxiv.org/abs/2502.17894</guid>
<content:encoded><![CDATA[
arXiv:2502.17894v2 Announce Type: replace-cross 
Abstract: Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a pre-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate the strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average real-world success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Posterior Estimation for Cataloging Astronomical Images with Spatially Varying Backgrounds and Point Spread Functions</title>
<link>https://arxiv.org/abs/2503.00156</link>
<guid>https://arxiv.org/abs/2503.00156</guid>
<content:encoded><![CDATA[
arXiv:2503.00156v2 Announce Type: replace-cross 
Abstract: Neural posterior estimation (NPE), a type of amortized variational inference, is a computationally efficient means of constructing probabilistic catalogs of light sources from astronomical images. To date, NPE has not been used to perform inference in models with spatially varying covariates. However, ground-based astronomical images have spatially varying sky backgrounds and point spread functions (PSFs), and accounting for this variation is essential for constructing accurate catalogs of imaged light sources. In this work, we introduce a method of performing NPE with spatially varying backgrounds and PSFs. In this method, we generate synthetic catalogs and semi-synthetic images for these catalogs using randomly sampled PSF and background estimates from existing surveys. Using this data, we train a neural network, which takes an astronomical image and representations of its background and PSF as input, to output a probabilistic catalog. Our experiments with Sloan Digital Sky Survey data demonstrate the effectiveness of NPE in the presence of spatially varying backgrounds and PSFs for light source detection, star/galaxy separation, and flux measurement.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</title>
<link>https://arxiv.org/abs/2503.05652</link>
<guid>https://arxiv.org/abs/2503.05652</guid>
<content:encoded><![CDATA[
arXiv:2503.05652v2 Announce Type: replace-cross 
Abstract: Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</title>
<link>https://arxiv.org/abs/2504.13386</link>
<guid>https://arxiv.org/abs/2504.13386</guid>
<content:encoded><![CDATA[
arXiv:2504.13386v3 Announce Type: replace-cross 
Abstract: In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at https://thunder.is.tue.mpg.de/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing</title>
<link>https://arxiv.org/abs/2505.01263</link>
<guid>https://arxiv.org/abs/2505.01263</guid>
<content:encoded><![CDATA[
arXiv:2505.01263v2 Announce Type: replace-cross 
Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. Existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. To address these issues, we propose a large language model (LLM) based flow matching architecture for dubbing, named FlowDubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the in-context sequence from movie scripts and reference audio. Then, the proposed semantic-aware learning focuses on capturing LLM semantic knowledge at the phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves acoustic quality in two aspects, which introduces an LLM-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?</title>
<link>https://arxiv.org/abs/2505.10924</link>
<guid>https://arxiv.org/abs/2505.10924</guid>
<content:encoded><![CDATA[
arXiv:2505.10924v3 Announce Type: replace-cross 
Abstract: Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v5 Announce Type: replace-cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2505.16258</link>
<guid>https://arxiv.org/abs/2505.16258</guid>
<content:encoded><![CDATA[
arXiv:2505.16258v2 Announce Type: replace-cross 
Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs presents unique challenges, often requiring task-specific fine-tuning and extensive reasoning steps. However, current Chain-of-Thought approaches do not efficiently leverage the same cognitive processes that enable humans to identify sarcasm. We present IRONIC, an in-context learning framework that leverages Multi-modal Coherence Relations to analyze referential, analogical and pragmatic image-text linkages. Our experiments show that IRONIC achieves state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across different baselines. This demonstrates the need for incorporating linguistic and cognitive insights into the design of multi-modal reasoning strategies. Our code is available at: https://github.com/aashish2000/IRONIC
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning for Generative AI: From LLMs to MLLMs and Beyond</title>
<link>https://arxiv.org/abs/2506.13045</link>
<guid>https://arxiv.org/abs/2506.13045</guid>
<content:encoded><![CDATA[
arXiv:2506.13045v4 Announce Type: replace-cross 
Abstract: The rapid advancement of generative models has empowered modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models are fundamentally constrained by \emph{catastrophic forgetting}, \ie~a persistent challenge where models experience performance degradation on previously learned tasks when adapting to new tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative AI in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative AI models, encompassing large language models, multimodal large language models, vision-language-action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, thereby providing deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models</title>
<link>https://arxiv.org/abs/2506.16627</link>
<guid>https://arxiv.org/abs/2506.16627</guid>
<content:encoded><![CDATA[
arXiv:2506.16627v2 Announce Type: replace-cross 
Abstract: Neural signed-distance fields (SDFs) are a versatile backbone for neural geometry representation, but enforcing CAD-style developability usually requires Gaussian-curvature penalties with full Hessian evaluation and second-order differentiation, which are costly in memory and time. We introduce an off-diagonal Weingarten loss that regularizes only the mixed shape operator term that represents the gap between principal curvatures and flattens the surface. We present two variants: a finite-difference version using six SDF evaluations plus one gradient, and an auto-diff version using a single Hessian-vector product. Both converge to the exact mixed term and preserve the intended geometric properties without assembling the full Hessian. On the ABC benchmarks the losses match or exceed Hessian-based baselines while cutting GPU memory and training time by roughly a factor of two. The method is drop-in and framework-agnostic, enabling scalable curvature-aware SDF learning for engineering-grade shape reconstruction. Our code is available at https://flatcad.github.io/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Blur: A Fluid Perspective on Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2506.16827</link>
<guid>https://arxiv.org/abs/2506.16827</guid>
<content:encoded><![CDATA[
arXiv:2506.16827v2 Announce Type: replace-cross 
Abstract: We propose a novel PDE-driven corruption process for generative image synthesis based on advection-diffusion processes which generalizes existing PDE-based approaches. Our forward pass formulates image corruption via a physically motivated PDE that couples directional advection with isotropic diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet, Fourier). We implement this PDE numerically through a GPU-accelerated custom Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence, we generate stochastic velocity fields that introduce coherent motion and capture multi-scale mixing. In the generative process, a neural network learns to reverse the advection-diffusion operator thus constituting a novel generative model. We discuss how previous methods emerge as specific cases of our operator, demonstrating that our framework generalizes prior PDE-based corruption techniques. We illustrate how advection improves the diversity and quality of the generated images while keeping the overall color palette unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and deep generative modeling, offering a fresh perspective on physically informed image corruption processes for diffusion-based synthesis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience</title>
<link>https://arxiv.org/abs/2507.09024</link>
<guid>https://arxiv.org/abs/2507.09024</guid>
<content:encoded><![CDATA[
arXiv:2507.09024v3 Announce Type: replace-cross 
Abstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets. CNeuroMod-THINGS meets this need by capturing neural representations for a wide set of semantic concepts using well-characterized images in a new densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS exploits synergies between two existing projects: the THINGS initiative (THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has developed a common set of thoroughly annotated images broadly sampling natural and man-made objects which is used to acquire a growing collection of large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring hundreds of hours of fMRI data from a core set of participants during controlled and naturalistic tasks, including visual tasks like movie watching and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each completed 33-36 sessions of a continuous recognition paradigm using approximately 4000 images from the THINGS stimulus set spanning 720 categories. We report behavioural and neuroimaging metrics that showcase the quality of the data. By bridging together large existing resources, CNeuroMod-THINGS expands our capacity to model broad slices of the human visual experience.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images</title>
<link>https://arxiv.org/abs/2507.12698</link>
<guid>https://arxiv.org/abs/2507.12698</guid>
<content:encoded><![CDATA[
arXiv:2507.12698v2 Announce Type: replace-cross 
Abstract: Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - https://tehraninasab.github.io/pixelperfect-megamed.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</title>
<link>https://arxiv.org/abs/2508.07165</link>
<guid>https://arxiv.org/abs/2508.07165</guid>
<content:encoded><![CDATA[
arXiv:2508.07165v2 Announce Type: replace-cross 
Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable versatility, enabling the distinct visualization of different tissue types. Nevertheless, the inherent heterogeneity among MRI sequences poses significant challenges to the generalization capability of deep learning models. These challenges undermine model performance when faced with varying acquisition parameters, thereby severely restricting their clinical utility. In this study, we present PRISM, a foundation model PRe-trained with large-scale multI-Sequence MRI. We collected a total of 64 datasets from both public and private sources, encompassing a wide range of whole-body anatomical structures, with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI scans from 34 datasets (8 public and 26 private) were curated to construct the largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a novel pretraining paradigm that disentangles anatomically invariant features from sequence-specific variations in MRI, while preserving high-level semantic representations. We established a benchmark comprising 44 downstream tasks, including disease diagnosis, image segmentation, registration, progression prediction, and report generation. These tasks were evaluated on 32 public datasets and 5 private cohorts. PRISM consistently outperformed both non-pretrained models and existing foundation models, achieving first-rank results in 39 out of 44 downstream benchmarks with statistical significance improvements. These results underscore its ability to learn robust and generalizable representations across unseen data acquired under diverse MRI protocols. PRISM provides a scalable framework for multi-sequence MRI analysis, thereby enhancing the translational potential of AI in radiology. It delivers consistent performance across diverse imaging protocols, reinforcing its clinical applicability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2508.07903</link>
<guid>https://arxiv.org/abs/2508.07903</guid>
<content:encoded><![CDATA[
arXiv:2508.07903v2 Announce Type: replace-cross 
Abstract: Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise</title>
<link>https://arxiv.org/abs/2508.10383</link>
<guid>https://arxiv.org/abs/2508.10383</guid>
<content:encoded><![CDATA[
<div> Keywords: Image segmentation, Label noise, Data augmentation, NSegment+, Semantic segmentation

Summary:
NSegment+ is a novel augmentation framework introduced in this study to address subtle label imperfections in image segmentation. The method decouples image and label transformations to enhance model performance. By applying controlled elastic deformations only to segmentation labels, NSegment+ encourages models to learn robust representations of object structures despite minor label inconsistencies. Extensive experiments on various datasets show consistent improvements in mean Intersection over Union (mIoU) scores. NSegment+ achieves mIoU gains of up to +3.39 on PASCAL VOC, highlighting the importance of addressing implicit label noise. When combined with other training techniques such as CutMix and Label Smoothing, the performance gains can be further amplified. This study underscores the significance of addressing realistic noise in image segmentation tasks to improve model generalization capacity and overall performance.<br /><br />Summary: <div>
arXiv:2508.10383v2 Announce Type: replace 
Abstract: While previous studies on image segmentation focus on handling severe (or explicit) label noise, real-world datasets also exhibit subtle (or implicit) label imperfections. These arise from inherent challenges, such as ambiguous object boundaries and annotator variability. Although not explicitly present, such mild and latent noise can still impair model performance. Typical data augmentation methods, which apply identical transformations to the image and its label, risk amplifying these subtle imperfections and limiting the model's generalization capacity. In this paper, we introduce NSegment+, a novel augmentation framework that decouples image and label transformations to address such realistic noise for semantic segmentation. By introducing controlled elastic deformations only to segmentation labels while preserving the original images, our method encourages models to focus on learning robust representations of object structures despite minor label inconsistencies. Extensive experiments demonstrate that NSegment+ consistently improves performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even without bells and whistles, highlighting the importance of addressing implicit label noise. These gains can be further amplified when combined with other training tricks, including CutMix and Label Smoothing.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Driven 3D Hand Motion Generation from Sign Language Data</title>
<link>https://arxiv.org/abs/2508.15902</link>
<guid>https://arxiv.org/abs/2508.15902</guid>
<content:encoded><![CDATA[
<div> Keyword: generative model, 3D hand motions, natural language descriptions, sign language dataset, HandMDM <br />
Summary: 
This article introduces a new approach to training a generative model of 3D hand motions based on natural language descriptions. The authors use a large-scale sign language video dataset and translate noisy pseudo-annotations into hand motion descriptions using a language model that incorporates a dictionary of sign attributes and motion-script cues. This data enables the training of a text-conditioned hand motion diffusion model, HandMDM, which can generate hand motions based on textual input. The model is robust across domains, including unseen sign categories from the same sign language, different sign languages, and non-sign hand movements. The authors conduct extensive experiments to evaluate the model's performance in various scenarios. They also plan to release their trained models and data to support further research in this emerging field. <br /><br />Summary: <div>
arXiv:2508.15902v1 Announce Type: new 
Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on natural language descriptions specifying motion characteristics such as handshapes, locations, finger/hand/arm movements. To this end, we automatically build pairs of 3D hand motions and their associated textual labels with unprecedented scale. Specifically, we leverage a large-scale sign language video dataset, along with noisy pseudo-annotated sign categories, which we translate into hand motion descriptions via an LLM that utilizes a dictionary of sign attributes, as well as our complementary motion-script cues. This data enables training a text-conditioned hand motion diffusion model HandMDM, that is robust across domains such as unseen sign categories from the same sign language, but also signs from another sign language and non-sign hand movements. We contribute extensive experimental investigation of these scenarios and will make our trained models and data publicly available to support future research in this relatively new field.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos</title>
<link>https://arxiv.org/abs/2508.15903</link>
<guid>https://arxiv.org/abs/2508.15903</guid>
<content:encoded><![CDATA[
<div> Keyword: action recognition, long-term videos, large vision-language models, event sequences, visual event representations <br />
Summary:
This paper introduces VT-LVLM-AR, a framework for fine-grained action recognition in long-term videos. It utilizes a Video-to-Event Mapper (VTEM) to efficiently transform raw video into visual event sequences, which are then processed by an LVLM-based Action Reasoning module adapted using Prompt Tuning. The framework achieves state-of-the-art performance on the NTU RGB+D and NTU RGB+D 120 datasets, surpassing existing methods. Ablation studies confirm the importance of VTEM's components and the effectiveness of Prompt Tuning. Human evaluations show the interpretability of visual event representations. This work demonstrates the potential of LVLMs for robust and interpretable video action understanding through efficient model adaptation and video-to-language translation. <br /><br />Summary: <div>
arXiv:2508.15903v1 Announce Type: new 
Abstract: Human action recognition in long-term videos, characterized by complex backgrounds and subtle action differences, poses significant challenges for traditional deep learning models due to computational overhead, difficulty in capturing long-range temporal dependencies, and limited semantic understanding. While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have shown remarkable capabilities in multi-modal understanding and reasoning, their direct application to continuous video streams for fine-grained action recognition remains an open problem. This paper introduces VT-LVLM-AR (Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a novel framework designed to bridge this gap. VT-LVLM-AR comprises a Video-to-Event Mapper (VTEM) that efficiently transforms raw video into compact, semantically rich, and temporally coherent "visual event sequences" through lightweight spatio-temporal feature extraction, adaptive temporal pooling, and conceptual quantization with an event coherence bias. These visual event sequences are then fed into an LVLM-based Action Reasoning module, specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR consistently achieves state-of-the-art performance, surpassing existing methods (e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the critical contributions of VTEM's components and the efficacy of Prompt Tuning, while human evaluations underscore the interpretability of our visual event representations. This work highlights the immense potential of leveraging LVLMs for robust and interpretable video action understanding through effective video-to-language translation and efficient model adaptation.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping</title>
<link>https://arxiv.org/abs/2508.15904</link>
<guid>https://arxiv.org/abs/2508.15904</guid>
<content:encoded><![CDATA[
<div> Rare cancers, pathology, vision-language, multi-instance learning, diagnosis <br />
<br />
Summary: PathPT is introduced as a framework to improve the diagnostic accuracy of rare cancers by leveraging vision-language pathology models and multi-instance learning techniques. It addresses limitations in current methods by incorporating spatially-aware visual aggregation and task-specific prompt tuning, allowing for fine-grained tile-level guidance based on histopathological semantics. PathPT outperforms existing models in subtyping accuracy and cancerous region grounding ability across various rare cancer datasets. This advancement in AI-assisted diagnosis offers a scalable solution for improving diagnostic accuracy in settings with limited access to specialized expertise. <div>
arXiv:2508.15904v1 Announce Type: new 
Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic challenges due to limited expert availability-especially in pediatric oncology, where they represent over 70% of cases. While pathology vision-language (VL) foundation models show promising zero-shot capabilities for common cancer subtyping, their clinical performance for rare cancers remains limited. Existing multi-instance learning (MIL) methods rely only on visual features, overlooking cross-modal knowledge and compromising interpretability critical for rare cancer diagnosis. To address this limitation, we propose PathPT, a novel framework that fully exploits the potential of vision-language pathology foundation models through spatially-aware visual aggregation and task-specific prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision into fine-grained tile-level guidance by leveraging the zero-shot capabilities of VL models, thereby preserving localization on cancerous regions and enabling cross-modal reasoning through prompts aligned with histopathological semantics. We benchmark PathPT on eight rare cancer datasets(four adult and four pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer datasets, evaluating four state-of-the-art VL models and four MIL frameworks under three few-shot settings. Results show that PathPT consistently delivers superior performance, achieving substantial gains in subtyping accuracy and cancerous region grounding ability. This work advances AI-assisted diagnosis for rare cancers, offering a scalable solution for improving subtyping accuracy in settings with limited access to specialized expertise.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Ship Detection with Vision-Language Integration</title>
<link>https://arxiv.org/abs/2508.15930</link>
<guid>https://arxiv.org/abs/2508.15930</guid>
<content:encoded><![CDATA[
<div> Keywords: ship detection, remote sensing imagery, Vision-Language Models, multi-scale adaptive sliding window, Semantic-Aware Ship Detection
Summary: 
Ship detection in remote sensing imagery is essential for various applications, but existing methods lack fine-grained semantic information capture. To address this, a novel detection framework combining Vision-Language Models with a multi-scale adaptive sliding window approach is proposed. ShipSem-VL, a specialized dataset, is introduced to facilitate Semantic-Aware Ship Detection (SASD). The framework is evaluated through three tasks, demonstrating its effectiveness in advancing SASD. The combination of VLMs and a multi-scale approach improves the fine-grained ship attribute capture in complex scenarios. The ShipSem-VL dataset enhances the semantic information available for ship detection. The comprehensive evaluation of the framework showcases its performance in multiple aspects of SASD. <div>
arXiv:2508.15930v1 Announce Type: new 
Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging applications, such as maritime activity monitoring, shipping logistics, and environmental studies. However, existing methods often struggle to capture fine-grained semantic information, limiting their effectiveness in complex scenarios. To address these challenges, we propose a novel detection framework that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture fine-grained ship attributes. We evaluate our framework through three well-defined tasks, providing a comprehensive analysis of its performance and demonstrating its effectiveness in advancing SASD from multiple perspectives.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Retrieval of Specific Cows from Unlabeled Videos</title>
<link>https://arxiv.org/abs/2508.15945</link>
<guid>https://arxiv.org/abs/2508.15945</guid>
<content:encoded><![CDATA[
<div> AutoCattloger, Cattlog, dairy cows, eidetic cow recognizer, CowFinder<br />
<br />
AutoCattloger, Cattlog, and CowFinder are components of a new automated video system for cataloging and identifying cows in a dairy herd. The system is designed to work with a single input video clip per cow and can identify individuals in continuous video streams without using deep learning. By utilizing the eidetic cow recognizer, the system is able to identify cows in unlabeled and unsegmented video footage, specifically in the holding area of a milking parlor where cows are walking freely. This hands-free system offers a valuable solution for dairy farmers looking to track and manage their herd without the need for manual cataloging or identification processes. The innovative approach presented in this work demonstrates the potential for automated video systems to revolutionize the way dairy herds are monitored and managed.<br /><br />Summary: <div>
arXiv:2508.15945v1 Announce Type: new 
Abstract: Few automated video systems are described in the open literature that enable hands-free cataloging and identification (ID) of cows in a dairy herd. In this work, we describe our system, composed of an AutoCattloger, which builds a Cattlog of dairy cows in a herd with a single input video clip per cow, an eidetic cow recognizer which uses no deep learning to ID cows, and a CowFinder, which IDs cows in a continuous stream of video. We demonstrate its value in finding individuals in unlabeled, unsegmented videos of cows walking unconstrained through the holding area of a milking parlor.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Different Geo Priors for Image Classification</title>
<link>https://arxiv.org/abs/2508.15946</link>
<guid>https://arxiv.org/abs/2508.15946</guid>
<content:encoded><![CDATA[
<div> Spatial Implicit Neural Representations, species distribution models, visual classification, iNaturalist observations, Geo Priors
Summary:
- The study evaluates SINR models as a geographical prior for visual species classification using iNaturalist data.
- Different model configurations and handling of predictions for unseen species are explored.
- Factors important for Geo Prior effectiveness are identified, distinct from accurate range mapping.
<br /><br />Summary: The study examines the use of Spatial Implicit Neural Representations (SINR) models as a geographical prior for visually classifying species using iNaturalist observations. Different model configurations and strategies for handling predictions for species not included in training data are considered. The analysis highlights factors that contribute to the effectiveness of these models as Geo Priors, highlighting differences from traditional range mapping methods. <div>
arXiv:2508.15946v1 Announce Type: new 
Abstract: Species distribution models encode spatial patterns of species occurrence making them effective priors for vision-based species classification when location information is available. In this study, we evaluate various SINR (Spatial Implicit Neural Representations) models as a geographical prior for visual classification of species from iNaturalist observations. We explore the impact of different model configurations and adjust how we handle predictions for species not included in Geo Prior training. Our analysis reveals factors that contribute to the effectiveness of these models as Geo Priors, factors that may differ from making accurate range maps.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning with Adaptive Superpixel Coding</title>
<link>https://arxiv.org/abs/2508.15959</link>
<guid>https://arxiv.org/abs/2508.15959</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, vision models, Transformers, self-supervised, superpixel coding

Summary:
The article introduces a new self-supervised model called Adaptive Superpixel Coding (ASC) based on Transformers for vision tasks. Traditional Vision Transformers rely on fixed-size patch partitioning, but ASC uses adaptive superpixel layers that adjust dynamically to image content. The model overcomes limitations of existing methods and outperforms them on standard image downstream task benchmarks. Key insights of ASC's effectiveness are analyzed, highlighting the benefits of adaptive superpixels. The approach is shown to be effective in overcoming domain-specific assumptions and grid structures typically used in vision models. ASC demonstrates superior performance compared to widely-used alternatives, showcasing the potential for adaptive superpixel layers in deep learning vision models.<br /><br />Summary: <div>
arXiv:2508.15959v1 Announce Type: new 
Abstract: Deep learning vision models are typically tailored for specific modalities and often rely on domain-specific assumptions, such as the grid structures used by nearly all existing vision models. In this work, we propose a self-supervised model based on Transformers, which we call Adaptive Superpixel Coding (ASC). The key insight of our model is to overcome the limitations of traditional Vision Transformers, which depend on fixed-size and non-adaptive patch partitioning. Instead, ASC employs adaptive superpixel layers that dynamically adjust to the underlying image content. We analyze key properties of the approach that make it effective, and find that our method outperforms widely-used alternatives on standard image downstream task benchmarks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification</title>
<link>https://arxiv.org/abs/2508.15960</link>
<guid>https://arxiv.org/abs/2508.15960</guid>
<content:encoded><![CDATA[
<div> adaptation, vision-language models, fine-grained classification, renal pathology, few-shot learning

Summary: 
Vision-language models (VLMs) are being explored for fine-grained glomerular classification in renal pathology, a challenging task due to subtle morphological variations among subtypes. The Glo-VLMs framework leverages pathology images and clinical text prompts to facilitate joint representation learning. Various VLM architectures and adaptation strategies were assessed under a few-shot learning paradigm to understand their impact on model performance. Despite limited labeled data, fine-tuning VLMs achieved high accuracy, macro-AUC, and F1-score, demonstrating effective adaptation for medical image classification even with minimal supervision. This research clarifies the potential of large pretrained models for specialized clinical applications. <div>
arXiv:2508.15960v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have shown considerable potential in digital pathology, yet their effectiveness remains limited for fine-grained, disease-specific classification tasks such as distinguishing between glomerular subtypes. The subtle morphological variations among these subtypes, combined with the difficulty of aligning visual patterns with precise clinical terminology, make automated diagnosis in renal pathology particularly challenging. In this work, we explore how large pretrained VLMs can be effectively adapted to perform fine-grained glomerular classification, even in scenarios where only a small number of labeled examples are available. In this work, we introduce Glo-VLMs, a systematic framework designed to explore the adaptation of VLMs to fine-grained glomerular classification in data-constrained settings. Our approach leverages curated pathology images alongside clinical text prompts to facilitate joint image-text representation learning for nuanced renal pathology subtypes. By assessing various VLMs architectures and adaptation strategies under a few-shot learning paradigm, we explore how both the choice of method and the amount of labeled data impact model performance in clinically relevant scenarios. To ensure a fair comparison, we evaluate all models using standardized multi-class metrics, aiming to clarify the practical requirements and potential of large pretrained models for specialized clinical research applications. As a result, fine-tuning the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with only 8 shots per class, demonstrating that even with highly limited supervision, foundation models can be effectively adapted for fine-grained medical image classification.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</title>
<link>https://arxiv.org/abs/2508.15973</link>
<guid>https://arxiv.org/abs/2508.15973</guid>
<content:encoded><![CDATA[
<div> weakly supervised learning, multi-task learning, self-supervised learning, supervised contrastive learning, few-shot learning
Summary: 
This manuscript discusses label-efficient learning in computer vision and remote sensing, focusing on methods that can effectively learn from limited or partially annotated data while leveraging abundant unlabeled data. It covers weakly supervised learning for object discovery, multi-task learning for improved performance on object detection and semantic segmentation, self-supervised and supervised contrastive learning for scene classification in remote sensing, and few-shot learning for hierarchical scene classification using class hierarchies. The research addresses challenges unique to Earth observation data such as multi-modality and spatial resolution variability. Extensive experimental results across different datasets demonstrate the effectiveness of the proposed methods. The manuscript also outlines future research directions aimed at scaling and enhancing label-efficient learning for real-world applications.<br /><br /> <div>
arXiv:2508.15973v1 Announce Type: new 
Abstract: This manuscript presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Panoptic Segmentation of Environmental UAV Images : Litter Beach</title>
<link>https://arxiv.org/abs/2508.15985</link>
<guid>https://arxiv.org/abs/2508.15985</guid>
<content:encoded><![CDATA[
<div> monitoring, marine litter, UAVs, convolutional neural networks, segmentation

Summary:
The paper discusses the efficient use of convolutional neural networks (CNN) for monitoring marine litter using UAVs. Unlike satellite images, UAVs offer higher resolution and better adaptability in local areas. However, traditional CNN models struggle with the heterogeneous nature of beach sand, leading to inaccurate inferences. To address this, the paper proposes utilizing instance-based segmentation and panoptic segmentation methods, which exhibit high accuracy with minimal samples. These models prove to be more robust and less susceptible to environmental factors such as sand color reflections, human activity, shadows, and natural elements like algae and dunes. By incorporating these advanced CNN-based segmentation techniques, the monitoring of marine litter can be significantly improved, aiding in the global efforts to combat this environmental challenge.<br /><br />Summary: <div>
arXiv:2508.15985v1 Announce Type: new 
Abstract: Convolutional neural networks (CNN) have been used efficiently in several fields, including environmental challenges. In fact, CNN can help with the monitoring of marine litter, which has become a worldwide problem. UAVs have higher resolution and are more adaptable in local areas than satellite images, making it easier to find and count trash. Since the sand is heterogeneous, a basic CNN model encounters plenty of inferences caused by reflections of sand color, human footsteps, shadows, algae present, dunes, holes, and tire tracks. For these types of images, other CNN models, such as CNN-based segmentation methods, may be more appropriate. In this paper, we use an instance-based segmentation method and a panoptic segmentation method that show good accuracy with just a few samples. The model is more robust and less
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset</title>
<link>https://arxiv.org/abs/2508.15986</link>
<guid>https://arxiv.org/abs/2508.15986</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label deep learning, retinal disease classification, synthetic dataset, performance benchmark, meta-ensemble model

Summary:
This study focused on developing deep learning models for retinal disease classification using a newly released synthetic dataset, SynFundus-1M, with over one million fundus images. Six modern architectures were trained and a meta-ensemble model was created, achieving high performance with a macro-average AUC of 0.9973 on the internal validation set. The models showed strong generalization to real-world clinical datasets, with AUC values of 0.7972 on a diabetic retinopathy dataset, 0.9126 on a glaucoma dataset, and a macro-AUC of 0.8800 on a multi-label retinal fundus dataset. This study establishes a robust baseline for future research on large-scale synthetic datasets and demonstrates the potential of using synthetic data to accelerate the development of AI systems in ophthalmology.<br /><br />Summary: This study utilized a synthetic dataset to train deep learning models for retinal disease classification, achieving high performance and strong generalization to real-world clinical datasets. The models showed promising results and provide a foundation for further research in using synthetic data to accelerate the development of comprehensive AI systems in ophthalmology. <div>
arXiv:2508.15986v1 Announce Type: new 
Abstract: The development of multi-label deep learning models for retinal disease classification is often hindered by the scarcity of large, expertly annotated clinical datasets due to patient privacy concerns and high costs. The recent release of SynFundus-1M, a high-fidelity synthetic dataset with over one million fundus images, presents a novel opportunity to overcome these barriers. To establish a foundational performance benchmark for this new resource, we developed an end-to-end deep learning pipeline, training six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a meta-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a macro-average Area Under the Receiver Operating Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated strong generalization to three diverse, real-world clinical datasets, achieving an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset. This work provides a robust baseline for future research on large-scale synthetic datasets and establishes that models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images, offering a viable pathway to accelerate the development of comprehensive AI systems in ophthalmology.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production</title>
<link>https://arxiv.org/abs/2508.15988</link>
<guid>https://arxiv.org/abs/2508.15988</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language production, diversity, Latent Diffusion Model, digital avatars, non-manual attributes

Summary:
The article introduces a novel approach to Sign Language Production (SLP) that leverages Latent Diffusion Model (LDM) to create photorealistic digital avatars from reference images. The proposed method includes a sign feature aggregation module that explicitly models non-manual features (such as facial expressions) and manual features (such as hand movements). This module ensures the preservation of linguistic content while incorporating diversity by using reference images with different ethnic backgrounds. Experiments conducted on the YouTube-SL-25 sign language dataset demonstrate that the proposed pipeline achieves superior visual quality compared to existing methods, with significant improvements in perceptual metrics. Overall, the approach addresses the challenge of capturing diversity in sign representation while maintaining visual quality and accurately modeling non-manual attributes like emotions. 

<br /><br />Summary: <div>
arXiv:2508.15988v1 Announce Type: new 
Abstract: The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions</title>
<link>https://arxiv.org/abs/2508.16016</link>
<guid>https://arxiv.org/abs/2508.16016</guid>
<content:encoded><![CDATA[
<div> Instance segmentation, deep learning, disaster response, aerial imagery, real-time detection
<br />
Summary: 
The article introduces DRespNeT, a high-resolution dataset for aerial instance segmentation of post-earthquake environments. It provides detailed polygon-level annotations from high-definition aerial footage for improved search-and-rescue operations. The dataset includes 28 classes such as structurally compromised buildings, access points, debris levels, rescue personnel, vehicles, and civilian visibility. DRespNeT's fine-grained annotation enables differentiation between accessible and obstructed areas, enhancing operational planning. Performance evaluations with YOLOv8-seg show significant improvements in real-time situational awareness. The optimized YOLOv8-DRN model achieves a 92.7% mAP50 with 27 FPS on an RTX-4090 GPU, meeting operational requirements. The dataset and models support SAR teams and robotic systems, improving human-robot collaboration, emergency response, and survivor outcomes. 
<br /> <div>
arXiv:2508.16016v1 Announce Type: new 
Abstract: Recent advancements in computer vision and deep learning have enhanced disaster-response capabilities, particularly in the rapid assessment of earthquake-affected urban environments. Timely identification of accessible entry points and structural obstacles is essential for effective search-and-rescue (SAR) operations. To address this need, we introduce DRespNeT, a high-resolution dataset specifically developed for aerial instance segmentation of post-earthquake structural environments. Unlike existing datasets, which rely heavily on satellite imagery or coarse semantic labeling, DRespNeT provides detailed polygon-level instance segmentation annotations derived from high-definition (1080p) aerial footage captured in disaster zones, including the 2023 Turkiye earthquake and other impacted regions. The dataset comprises 28 operationally critical classes, including structurally compromised buildings, access points such as doors, windows, and gaps, multiple debris levels, rescue personnel, vehicles, and civilian visibility. A distinctive feature of DRespNeT is its fine-grained annotation detail, enabling differentiation between accessible and obstructed areas, thereby improving operational planning and response efficiency. Performance evaluations using YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate significant gains in real-time situational awareness and decision-making. Our optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27 FPS on an RTX-4090 GPU for multi-target detection, meeting real-time operational requirements. The dataset and models support SAR teams and robotic systems, providing a foundation for enhancing human-robot collaboration, streamlining emergency response, and improving survivor outcomes.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralMeshing: Complete Object Mesh Extraction from Casual Captures</title>
<link>https://arxiv.org/abs/2508.16026</link>
<guid>https://arxiv.org/abs/2508.16026</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric models, object reconstruction, Structure-from-Motion, fiducial marker, automated system 

Summary: 
An automated system has been developed for generating geometric models of objects from multiple videos without the need for commercial 3D scanners. The system requires at least one known point specified in each video frame, which can be automatically determined using fiducial markers like checkerboards or Augmented Reality markers. Using Structure-from-Motion techniques, the frames are then positioned in world space. By utilizing multiple videos and merging the results, a complete object mesh can be generated without the need for hole filling. The code for this system is available on GitHub for reference. This system provides a cost-effective and efficient method for extracting complete geometric models of objects encountered in daily life. <br /><br />Summary: <div>
arXiv:2508.16026v1 Announce Type: new 
Abstract: How can we extract complete geometric models of objects that we encounter in our daily life, without having access to commercial 3D scanners? In this paper we present an automated system for generating geometric models of objects from two or more videos. Our system requires the specification of one known point in at least one frame of each video, which can be automatically determined using a fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The remaining frames are automatically positioned in world space by using Structure-from-Motion techniques. By using multiple videos and merging results, a complete object mesh can be generated, without having to rely on hole filling. Code for our system is available from https://github.com/FlorisE/NeuralMeshing.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</title>
<link>https://arxiv.org/abs/2508.16030</link>
<guid>https://arxiv.org/abs/2508.16030</guid>
<content:encoded><![CDATA[
<div> dataset, radar, camera, perception, multi-vehicle<br />
Summary:<br />
The article introduces CoVeRaP, a new cooperative dataset consisting of radar, camera, and GPS data from multiple vehicles. A cooperative-perception framework is proposed, with options for middle- and late-fusion, using a PointNet-style encoder with self-attention to combine different sensor cues. The framework achieves improved detection performance, especially with middle fusion and intensity encoding, showing up to 9 times higher mean Average Precision at IoU 0.9 compared to single-vehicle baselines. CoVeRaP establishes a benchmark for multi-vehicle FMCW-radar perception, highlighting the benefits of radar sharing for enhanced detection robustness. The dataset and code are publicly available to facilitate further research. <br />  <div>
arXiv:2508.16030v1 Announce Type: new 
Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse, noisy point clouds constrain 3-D object detection. We therefore release CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and GPS streams from multiple vehicles across diverse manoeuvres. Built on this data, we propose a unified cooperative-perception framework with middle- and late-fusion options. Its baseline network employs a multi-branch PointNet-style encoder enhanced with self-attention to fuse spatial, Doppler, and intensity cues into a common latent space, which a decoder converts into 3-D bounding boxes and per-point depth confidence. Experiments show that middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the first reproducible benchmark for multi-vehicle FMCW-radar perception and demonstrates that affordable radar sharing markedly improves detection robustness. Dataset and code are publicly available to encourage further research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet-Enhanced PaDiM for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.16034</link>
<guid>https://arxiv.org/abs/2508.16034</guid>
<content:encoded><![CDATA[
<div> Wavelet-Enhanced PaDiM, Anomaly Detection, Industrial Images, Convolutional Neural Networks, Discrete Wavelet Transform<br />
<br />
Summary: <br />
Wavelet-Enhanced PaDiM (WE-PaDiM) integrates Discrete Wavelet Transform (DWT) analysis with multi-layer CNN features to improve anomaly detection and localization in industrial images. By applying DWT to feature maps from multiple backbone layers and selecting specific frequency subbands, WE-PaDiM enhances feature selection based on relevant frequency content. Evaluations on the MVTec AD dataset with various backbones show strong performance in anomaly detection and localization. The method achieves high Image-AUC and Pixel-AUC scores across 15 categories with optimized configurations. Analysis reveals that wavelet choices impact performance trade-offs, with simpler wavelets and detail subbands enhancing localization, while approximation bands improve image-level detection. WE-PaDiM offers a competitive and interpretable alternative to random feature selection in PaDiM, delivering robust results suitable for industrial inspection efficiently. <br /> <div>
arXiv:2508.16034v1 Announce Type: new 
Abstract: Anomaly detection and localization in industrial images are essential for automated quality inspection. PaDiM, a prominent method, models the distribution of normal image features extracted by pre-trained Convolutional Neural Networks (CNNs) but reduces dimensionality through random channel selection, potentially discarding structured information. We propose Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform (DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM applies 2D DWT to feature maps from multiple backbone layers, selects specific frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates them channel-wise before modeling with PaDiM's multivariate Gaussian framework. This DWT-before-concatenation strategy provides a principled method for feature selection based on frequency content relevant to anomalies, leveraging multi-scale wavelet information as an alternative to random selection. We evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones (ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in anomaly detection and localization, yielding average results of 99.32% Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized configurations. Our analysis shows that wavelet choices affect performance trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH) often enhance localization, while approximation bands (LL) improve image-level detection. WE-PaDiM thus offers a competitive and interpretable alternative to random feature selection in PaDiM, achieving robust results suitable for industrial inspection with comparable efficiency.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expandable Residual Approximation for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.16050</link>
<guid>https://arxiv.org/abs/2508.16050</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge distillation, lightweight models, residual knowledge, Multi-Branched Residual Network, Teacher Weight Integration

Summary:
ERA is a new knowledge distillation method that aims to transfer knowledge from a large-scale teacher model to a lightweight student model more effectively by decomposing the residual knowledge into multiple steps. It utilizes a Multi-Branched Residual Network (MBRNet) to achieve this decomposition and introduces a Teacher Weight Integration (TWI) strategy to address the capacity disparity between teacher and student models. Experimental results demonstrate that ERA improves accuracy on ImageNet classification and AP on MS COCO object detection benchmarks, outperforming existing methods across various computer vision tasks. The ERA method is open-source and available for access along with the codes and models on GitHub. <br /><br />Summary: <div>
arXiv:2508.16050v1 Announce Type: new 
Abstract: Knowledge distillation (KD) aims to transfer knowledge from a large-scale teacher model to a lightweight one, significantly reducing computational and storage requirements. However, the inherent learning capacity gap between the teacher and student often hinders the sufficient transfer of knowledge, motivating numerous studies to address this challenge. Inspired by the progressive approximation principle in the Stone-Weierstrass theorem, we propose Expandable Residual Approximation (ERA), a novel KD method that decomposes the approximation of residual knowledge into multiple steps, reducing the difficulty of mimicking the teacher's representation through a divide-and-conquer approach. Specifically, ERA employs a Multi-Branched Residual Network (MBRNet) to implement this residual knowledge decomposition. Additionally, a Teacher Weight Integration (TWI) strategy is introduced to mitigate the capacity disparity by reusing the teacher's head weights. Extensive experiments show that ERA improves the Top-1 accuracy on the ImageNet classification benchmark by 1.41% and the AP on the MS COCO object detection benchmark by 1.40, as well as achieving leading performance across computer vision tasks. Codes and models are available at https://github.com/Zhaoyi-Yan/ERA.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals</title>
<link>https://arxiv.org/abs/2508.16062</link>
<guid>https://arxiv.org/abs/2508.16062</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, animals, deep learning, RGB images, motion

Summary:
This paper explores the use of deep learning techniques for non-intrusive 3D reconstruction of animals based on RGB images or videos. It discusses various methods and categorizes them based on input modalities, representation of animal geometry and motion, reconstruction techniques, and training mechanisms. The paper also evaluates the performance of key methods, highlighting their strengths and limitations. Current challenges and future research directions in the field are identified. The aim is to provide insights into the latest developments in this emerging field of research and explore its potential applications in biology, livestock management, animal conservation, digital entertainment, and VR/AR technologies.<br /><br />Summary: <div>
arXiv:2508.16062v1 Announce Type: new 
Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a long-standing problem, which has a wide range of applications, from biology, livestock management, and animal conservation and welfare to content creation in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally, 3D models of real animals are obtained using 3D scanners. These, however, are intrusive, often prohibitively expensive, and difficult to deploy in the natural environment of the animals. In recent years, we have seen a significant surge in deep learning-based techniques that enable the 3D reconstruction, in a non-intrusive manner, of the shape and motion of dynamic objects just from their RGB image and/or video observations. Several papers have explored their application and extension to various types of animals. This paper surveys the latest developments in this emerging and growing field of research. It categorizes and discusses the state-of-the-art methods based on their input modalities, the way the 3D geometry and motion of animals are represented, the type of reconstruction techniques they use, and the training mechanisms they adopt. It also analyzes the performance of some key methods, discusses their strengths and limitations, and identifies current challenges and directions for future research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.16069</link>
<guid>https://arxiv.org/abs/2508.16069</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud object detection, Transformer-based models, State Space Models, Voxel Diffusion Module, benchmark datasets<br />
Summary: <br />
- A novel Voxel Diffusion Module (VDM) is proposed to enhance voxel-level representation and diffusion in point cloud data.
- VDM incorporates sparse 3D convolutions, submanifold sparse convolutions, and residual connections to improve spatial context and feature representation.
- VDM output feature maps are downsampled for computational efficiency, allowing seamless integration into Transformer- or SSM-based detection models.
- Experimental results demonstrate that VDM consistently improves detection accuracy over baseline models on benchmark datasets such as Waymo, nuScenes, Argoverse 2, and ONCE.
- VDM-SSMs achieve state-of-the-art performance on the mentioned datasets, with 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE. Our code will be publicly available. <br /><br />Summary:  <div>
arXiv:2508.16069v1 Announce Type: new 
Abstract: Recent advances in point cloud object detection have increasingly adopted Transformer-based and State Space Models (SSMs), demonstrating strong performance. However, voxelbased representations in these models require strict consistency in input and output dimensions due to their serialized processing, which limits the spatial diffusion capability typically offered by convolutional operations. This limitation significantly affects detection accuracy. Inspired by CNN-based object detection architectures, we propose a novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and diffusion in point cloud data. VDM is composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. To ensure computational efficiency, the output feature maps are downsampled to one-fourth of the original input resolution. VDM serves two primary functions: (1) diffusing foreground voxel features through sparse 3D convolutions to enrich spatial context, and (2) aggregating fine-grained spatial information to strengthen voxelwise feature representation. The enhanced voxel features produced by VDM can be seamlessly integrated into mainstream Transformer- or SSM-based detection models for accurate object classification and localization, highlighting the generalizability of our method. We evaluate VDM on several benchmark datasets by embedding it into both Transformerbased and SSM-based models. Experimental results show that our approach consistently improves detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new stateof-the-art performance across all datasets. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble learning of foundation models for precision oncology</title>
<link>https://arxiv.org/abs/2508.16085</link>
<guid>https://arxiv.org/abs/2508.16085</guid>
<content:encoded><![CDATA[
<div> framework, pathology, ensemble learning, clinical applications, precision oncology
Summary:
The article introduces ELF, an Ensemble Learning of Foundation models framework for pathology, integrating five state-of-the-art models to create unified slide-level representations. ELF is trained on a large dataset of 53,699 whole-slide images from 20 anatomical sites, using ensemble learning to capture diverse information efficiently. Unlike tile-level models, ELF's slide-level architecture is advantageous in contexts with limited data, such as therapeutic response prediction. The framework is evaluated for disease classification, biomarker detection, and response prediction to various anticancer therapies, consistently outperforming existing models and demonstrating superior accuracy and robustness. The study emphasizes the effectiveness of ensemble learning in pathology models and suggests ELF as a scalable and generalizable solution for advancing AI-assisted precision oncology. 
<br /><br />Summary: <div>
arXiv:2508.16085v1 Announce Type: new 
Abstract: Histopathology is essential for disease diagnosis and treatment decision-making. Recent advances in artificial intelligence (AI) have enabled the development of pathology foundation models that learn rich visual representations from large-scale whole-slide images (WSIs). However, existing models are often trained on disparate datasets using varying strategies, leading to inconsistent performance and limited generalizability. Here, we introduce ELF (Ensemble Learning of Foundation models), a novel framework that integrates five state-of-the-art pathology foundation models to generate unified slide-level representations. Trained on 53,699 WSIs spanning 20 anatomical sites, ELF leverages ensemble learning to capture complementary information from diverse models while maintaining high data efficiency. Unlike traditional tile-level models, ELF's slide-level architecture is particularly advantageous in clinical contexts where data are limited, such as therapeutic response prediction. We evaluated ELF across a wide range of clinical applications, including disease classification, biomarker detection, and response prediction to major anticancer therapies, cytotoxic chemotherapy, targeted therapy, and immunotherapy, across multiple cancer types. ELF consistently outperformed all constituent foundation models and existing slide-level models, demonstrating superior accuracy and robustness. Our results highlight the power of ensemble learning for pathology foundation models and suggest ELF as a scalable and generalizable solution for advancing AI-assisted precision oncology.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-flow Feedback Multi-scale Progressive Generative Adversarial Network</title>
<link>https://arxiv.org/abs/2508.16089</link>
<guid>https://arxiv.org/abs/2508.16089</guid>
<content:encoded><![CDATA[
<div> MSPG-SEN, Generative Adversarial Network, Adaptive Perception-Behavioral Feedback Loop, Two-Flow Dynamic Residual Network, Dynamic Embedded Attention Mechanism

Summary: 
The paper introduces a novel two-flow feedback multi-scale progressive GAN model, MSPG-SEN, enhancing image quality while simplifying the training process and reducing costs. It achieves state-of-the-art results on various datasets. The adaptive perception-behavioral feedback loop (APFL) improves robustness and training stability. The globally connected two-flow dynamic residual network enhances training efficiency and generalization. The dynamic embedded attention mechanism (DEMA) captures global-local information, enhancing feature separation and expression with minimal computing resources. The proposed model showcases strong cross-task capability and flexibility. <div>
arXiv:2508.16089v1 Announce Type: new 
Abstract: Although diffusion model has made good progress in the field of image generation, GAN\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\cite{liu2021comparing}, SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\% with INJK With strong cross-task capability.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation via Feature Refinement</title>
<link>https://arxiv.org/abs/2508.16124</link>
<guid>https://arxiv.org/abs/2508.16124</guid>
<content:encoded><![CDATA[
<div> adaptation, unsupervised, domain, feature refinement, distribution shift

Summary:
- The article introduces DAFR2, a framework for unsupervised domain adaptation that combines adaptation of Batch Normalization statistics, feature distillation, and hypothesis transfer.
- DAFR2 aligns feature distributions at statistical and representational levels to create robust and domain-invariant feature spaces that generalize across similar domains without target labels.
- Extensive experiments on benchmark datasets show that DAFR2 outperforms prior methods in robustness to corruption.
- Theoretical and empirical analyses demonstrate improved feature alignment, increased mutual information between domains, and reduced sensitivity to input perturbations. 

Summary: <div>
arXiv:2508.16124v1 Announce Type: new 
Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet effective framework for unsupervised domain adaptation under distribution shift. The proposed method synergistically combines three key components: adaptation of Batch Normalization statistics using unlabeled target data, feature distillation from a source-trained model and hypothesis transfer. By aligning feature distributions at the statistical and representational levels, DAFR2 produces robust and domain-invariant feature spaces that generalize across similar domains without requiring target labels, complex architectures or sophisticated training objectives. Extensive experiments on benchmark datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C, demonstrate that the proposed algorithm outperforms prior methods in robustness to corruption. Theoretical and empirical analyses further reveal that our method achieves improved feature alignment, increased mutual information between the domains and reduced sensitivity to input perturbations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration</title>
<link>https://arxiv.org/abs/2508.16138</link>
<guid>https://arxiv.org/abs/2508.16138</guid>
<content:encoded><![CDATA[
<div> Keywords: computed tomography, joint motion, 4D imaging, orthopedic care, deep learning

Summary: 
A new 4D joint analysis platform has been proposed to capture dynamic, weight-bearing joint motion using dual robotic arm cone-beam CT (CBCT) system. The platform combines upright scanning trajectory with deep learning-based preprocessing and 3D-2D projection. It offers accurate kinematic assessment with sub-voxel accuracy and a high success rate in simulation studies. Clinical evaluation shows accurate quantification of tibial plateau motion and medial-lateral variance in post-total knee arthroplasty (TKA) patients. This low-dose imaging platform enables fast and precise dynamic joint imaging, providing opportunities for biomechanical research, precise diagnostics, and personalized orthopedic care.<br /><br />Summary: <div>
arXiv:2508.16138v1 Announce Type: new 
Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic, weight-bearing joint motion. Functional evaluation, particularly after surgical intervention, requires four-dimensional (4D) imaging, but current methods are limited by excessive radiation exposure or incomplete spatial information from 2D techniques. We propose an integrated 4D joint analysis platform that combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a programmable, gantry-free trajectory optimized for upright scanning; (2) a hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using deep learning-based preprocessing, 3D-2D projection, and iterative optimization; and (3) a clinically validated framework for quantitative kinematic assessment. In simulation studies, the method achieved sub-voxel accuracy (0.235 mm) with a 99.18 percent success rate, outperforming conventional and state-of-the-art registration approaches. Clinical evaluation further demonstrated accurate quantification of tibial plateau motion and medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D CBCT platform enables fast, accurate, and low-dose dynamic joint imaging, offering new opportunities for biomechanical research, precision diagnostics, and personalized orthopedic care.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection</title>
<link>https://arxiv.org/abs/2508.16140</link>
<guid>https://arxiv.org/abs/2508.16140</guid>
<content:encoded><![CDATA[
<div> Keywords: cervical cells, Thinprep Cytologic Test (TCT) images, hypergraph-based cell detection network, Multi-level Fusion Sub-network (MLF-SNet), Cross-level Feature Fusion Strategy

Summary: 
The study focuses on automatic detection of abnormal cervical cells from Thinprep Cytologic Test (TCT) images. Current algorithms lack the ability to effectively model spatial correlation features, which are crucial for accurate diagnosis. The proposed hypergraph-based cell detection network integrates different types of features, combining spatial correlations and deep discriminative features. The Multi-level Fusion Sub-network (MLF-SNet) enhances feature extraction capabilities. A Cross-level Feature Fusion Strategy with Hypergraph Computation module (CLFFS-HC) is introduced to integrate mixed features. Experiments on publicly available datasets show that the method significantly improves the performance of cervical abnormal cell detection. The approach provides a promising advancement in the development of intelligent computer-aided diagnostic systems for cervical abnormalities. <br /><br />Summary: <div>
arXiv:2508.16140v1 Announce Type: new 
Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test (TCT) images is a critical component in the development of intelligent computer-aided diagnostic systems. However, existing algorithms typically fail to effectively model the correlations of visual features, while these spatial correlation features actually contain critical diagnostic information. Furthermore, no detection algorithm has the ability to integrate inter-correlation features of cells with intra-discriminative features of cells, lacking a fusion strategy for the end-to-end detection model. In this work, we propose a hypergraph-based cell detection network that effectively fuses different types of features, combining spatial correlation features and deep discriminative features. Specifically, we use a Multi-level Fusion Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation module (CLFFS-HC), to integrate mixed features. Finally, we conducted experiments on three publicly available datasets, and the results demonstrate that our method significantly improves the performance of cervical abnormal cell detection.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.16157</link>
<guid>https://arxiv.org/abs/2508.16157</guid>
<content:encoded><![CDATA[
<div> approach, Adaptive Prompt Tuning, semantic alignment, anomaly detection, pre-trained Vision-Language Models<br />
<br />
Summary: 
The paper introduces Adaptive Prompt Tuning (APT), a novel framework for anomaly detection using Pre-trained Vision-Language Models (VLMs). APT overcomes limitations of traditional prompt-based methods by self-generating anomaly samples with noise perturbations for few-shot learning. The system trains learnable prompts to capture context-specific anomalies in various scenarios. To prevent overfitting to synthetic noise, a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) is proposed to align prompts with general anomaly semantics while incorporating diverse synthetic anomalies. APT not only improves pixel-wise anomaly detection but also achieves state-of-the-art performance on multiple benchmark datasets without the need for human-designed prompts. This approach establishes a robust and versatile solution for real-world anomaly detection.<br /><br /> <div>
arXiv:2508.16157v1 Announce Type: new 
Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in detecting anomalies. However, previous approaches are fundamentally limited by their reliance on human-designed prompts and the lack of accessible anomaly samples, leading to significant gaps in context-specific anomaly understanding. In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning with semantic alignment for anomaly detection (APT), a groundbreaking prior knowledge-free, few-shot framework and overcomes the limitations of traditional prompt-based approaches. APT uses self-generated anomaly samples with noise perturbations to train learnable prompts that capture context-dependent anomalies in different scenarios. To prevent overfitting to synthetic noise, we propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively aligns the prompts with general anomaly semantics while incorporating diverse synthetic anomaly. Our system not only advances pixel-wise anomaly detection, but also achieves state-of-the-art performance on multiple benchmark datasets without requiring prior knowledge for prompt crafting, establishing a robust and versatile solution for real-world anomaly detection.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.16158</link>
<guid>https://arxiv.org/abs/2508.16158</guid>
<content:encoded><![CDATA[
<div> keywords: vision-language models, super-resolution, regional attention, fine-grained information, T2I models<br />
Summary:<br />
The article introduces a new method called Regional Attention Guided Super-Resolution (RAGSR) to address challenges in generating clear and accurate regional details in single-image super-resolution (SISR). By extracting localized fine-grained information and utilizing a regional attention mechanism, RAGSR enhances detail and overall visually coherent SR results. The method localizes object regions in an image, assigns fine-grained captions to each region, and leverages regional guided attention to effectively integrate text and image information. This approach overcomes limitations faced by traditional SISR techniques and demonstrates superior performance in generating perceptually authentic visual details while maintaining contextual consistency. Experimental results on benchmark datasets validate the effectiveness of the RAGSR method. <br /><br />Summary: <div>
arXiv:2508.16158v1 Announce Type: new 
Abstract: The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</title>
<link>https://arxiv.org/abs/2508.16159</link>
<guid>https://arxiv.org/abs/2508.16159</guid>
<content:encoded><![CDATA[
<div> Meta-learning, support-query pairs, identical network design, heterogeneous network, weakly-supervised few-shot semantic segmentation<br />
<br />
Summary: This study introduces a novel heterogeneous network design to address the over-semantic homogenization issue in meta-learning. By utilizing heterogeneous visual aggregation (HA) modules and heterogeneous transfer (HT) module, the proposed model enhances complementarity while preserving semantic commonality. The inclusion of heterogeneous CLIP (HC) textual information further boosts the generalization capability of multimodal models. In weakly-supervised few-shot semantic segmentation (WFSS) task, the proposed TLG model outperforms existing state-of-the-art models with significantly fewer parameters. Specifically, TLG achieves a 13.2% improvement on Pascal-5\textsuperscript{i} and a 9.7% improvement on COCO-20\textsuperscript{i}. Remarkably, TLG is the first weakly supervised model to surpass fully supervised models under the same backbone architectures. The code for the model is available on GitHub for further exploration and development. <br /><br /> <div>
arXiv:2508.16159v1 Announce Type: new 
Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\% improvement on Pascal-5\textsuperscript{i} and a 9.7\% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FTIO: Frequent Temporally Integrated Objects</title>
<link>https://arxiv.org/abs/2508.16183</link>
<guid>https://arxiv.org/abs/2508.16183</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Object Segmentation, Unsupervised VOS, Frequent Temporally Integrated Objects, Object selection, Temporal inconsistencies<br />
<br />
Summary: <br />
Predicting and tracking objects in real-world scenarios is a challenging task in Video Object Segmentation (VOS). Unsupervised VOS (UVOS) faces the additional difficulty of finding initial object segmentation, leading to uncertainty in object proposals. The proposed Frequent Temporally Integrated Objects (FTIO) framework addresses these challenges by implementing a combined criterion for improved object selection and correcting temporal inconsistencies through a three-stage method. This approach enhances object selection, especially for small or complex objects, and integrates missing object mask regions to improve overall performance in multi-object UVOS. Experimental results demonstrate that FTIO achieves superior performance, making it a promising solution for real-world VOS applications. <div>
arXiv:2508.16183v1 Announce Type: new 
Abstract: Predicting and tracking objects in real-world scenarios is a critical challenge in Video Object Segmentation (VOS) tasks. Unsupervised VOS (UVOS) has the additional challenge of finding an initial segmentation of salient objects, which affects the entire process and keeps a permanent uncertainty about the object proposals. Moreover, deformation and fast motion can lead to temporal inconsistencies. To address these problems, we propose Frequent Temporally Integrated Objects (FTIO), a post-processing framework with two key components. First, we introduce a combined criterion to improve object selection, mitigating failures common in UVOS--particularly when objects are small or structurally complex--by extracting frequently appearing salient objects. Second, we present a three-stage method to correct temporal inconsistencies by integrating missing object mask regions. Experimental results demonstrate that FTIO achieves state-of-the-art performance in multi-object UVOS. Code is available at: https://github.com/MohammadMohammadzadehKalati/FTIO
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
<link>https://arxiv.org/abs/2508.16201</link>
<guid>https://arxiv.org/abs/2508.16201</guid>
<content:encoded><![CDATA[
<div> Keywords: Video large language models, Speculative decoding, Video token pruning, Efficient speculation, Video understanding benchmarks

Summary:
SpecVLM is a new speculative decoding framework designed for Video large language models (Vid-LLMs) to improve efficiency without compromising accuracy. By leveraging a two-stage video token pruning process, SpecVLM can prune up to 90% of video tokens while maintaining performance. The framework uses attention signals from a verifier model to select informative tokens in Stage I and prunes redundant tokens in a uniform manner in Stage II. Extensive experiments on various video understanding benchmarks show that SpecVLM achieves significant decoding speedup, with up to 2.68$\times$ speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B. Overall, SpecVLM demonstrates effectiveness and robustness in accelerating the decoding stage of Vid-LLMs while preserving the quality of video content understanding. 

<br /><br />Summary: 
- SpecVLM introduces a training-free speculative decoding framework for Vid-LLMs. 
- It enables efficient speculation through video token pruning, reducing memory and computational overhead.
- SpecVLM performs a two-stage pruning process guided by attention signals from a verifier model.
- Extensive experiments show significant decoding speedup on video understanding benchmarks.
- The framework achieves up to 2.68$\times$ speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B. <div>
arXiv:2508.16201v1 Announce Type: new 
Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</title>
<link>https://arxiv.org/abs/2508.16207</link>
<guid>https://arxiv.org/abs/2508.16207</guid>
<content:encoded><![CDATA[
<div> Keywords: driver monitoring, deep learning, perspective, adaptation, video

Summary:<br /><br />

The study explores the challenge of adapting image foundation models to driver monitoring when faced with changes in camera perspective. Different probing strategies and foundation models, including DINOv2 and CLIP, are evaluated for their effectiveness in handling unseen viewpoints without further adaptation. A new method called T-Mask is introduced, which leverages temporal token masking to improve cross-view accuracy without adding parameters. The results show that T-Mask outperforms strong probing baselines and parameter-efficient fine-tuning methods, particularly benefiting underrepresented secondary activities. This research demonstrates the potential of lightweight probing methods in enhancing fine-grained driver observation, especially in cross-view and low-data settings. The importance of temporal token selection in leveraging foundation models for building robust driver monitoring systems is highlighted. Code and models for T-Mask are available to support further research endeavors. <div>
arXiv:2508.16207v1 Announce Type: new 
Abstract: Changes of camera perspective are a common obstacle in driver monitoring. While deep learning and pretrained foundation models show strong potential for improved generalization via lightweight adaptation of the final layers ('probing'), their robustness to unseen viewpoints remains underexplored. We study this challenge by adapting image foundation models to driver monitoring using a single training view, and evaluating them directly on unseen perspectives without further adaptation. We benchmark simple linear probes, advanced probing strategies, and compare two foundation models (DINOv2 and CLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning. Building on these insights, we introduce \textsc{T-Mask} -- a new image-to-video probing method that leverages temporal token masking and emphasizes more dynamic video regions. Benchmarked on the public Drive\&amp;Act dataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ over strong probing baselines and $+8.0\%$ over PEFT methods, without adding any parameters. It proves particularly effective for underrepresented secondary activities, boosting recognition by $+5.42\%$ under the trained view and $+1.36\%$ under cross-view settings. This work provides encouraging evidence that adapting foundation models with lightweight probing methods like \textsc{T-Mask} has strong potential in fine-grained driver observation, especially in cross-view and low-data settings. These results highlight the importance of temporal token selection when leveraging foundation models to build robust driver monitoring systems. Code and models will be made available at https://github.com/th-nesh/T-MASK to support ongoing research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers</title>
<link>https://arxiv.org/abs/2508.16211</link>
<guid>https://arxiv.org/abs/2508.16211</guid>
<content:encoded><![CDATA[
<div> ODE, feature caching, acceleration, image generation, video generation

Summary:
FoCa introduces a new approach to feature caching in Diffusion Transformers (DiTs) to improve efficiency in image and video generation tasks. By treating feature caching as a feature-ODE solving problem, FoCa enables robust integration of historical features under large skipping intervals. This method significantly enhances acceleration ratios while maintaining high generation quality. Experimental results across various tasks show FoCa's effectiveness, achieving near-lossless speedups on FLUX, HunyuanVideo, Inf-DiT, and DiT without additional training. FoCa demonstrates superior performance compared to existing caching strategies, especially under aggressive acceleration scenarios. The approach not only reduces computational costs but also ensures stable generation quality in high-speed inference scenarios, making it a promising technique for accelerating high-fidelity image and video generation models. 

<br /><br />Summary: <div>
arXiv:2508.16211v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models</title>
<link>https://arxiv.org/abs/2508.16212</link>
<guid>https://arxiv.org/abs/2508.16212</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, Transformer architectures, generative tasks, caching strategies, real-time deployment 

Summary: 
OmniCache introduces a training-free acceleration method for diffusion models, addressing the high computational cost associated with Transformer architectures in generative tasks. By strategically distributing cache reuse across the entire sampling process and dynamically estimating noise during cache reuse, OmniCache enhances efficiency while maintaining generative quality. The approach differs from existing methods by focusing on sampling trajectories of DIT models and utilizing cached computations effectively throughout the diffusion trajectory. Experimental results show that OmniCache accelerates the sampling process and offers a practical solution for efficient deployment of diffusion-based generative models. <div>
arXiv:2508.16212v1 Announce Type: new 
Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedOmni-45{\deg}: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine</title>
<link>https://arxiv.org/abs/2508.16213</link>
<guid>https://arxiv.org/abs/2508.16213</guid>
<content:encoded><![CDATA[
<div> faithfulness, reliability, safety-performance, benchmarks, medical LLMs <br />
Summary: <br />
The article introduces the MedOmni-45 Degrees benchmark and workflow to evaluate the reliability and safety-performance trade-offs of large language models (LLMs) used in medical decision-support. This benchmark focuses on Chain-of-Thought (CoT) faithfulness, sycophancy, and manipulative hint conditions. It includes 1,804 reasoning-focused medical questions across six specialties, evaluated using seven LLMs. The benchmark combines three metrics - Accuracy, CoT-Faithfulness, and Anti-Sycophancy - into a composite score visualized on a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B model performs closest to a balanced safety and accuracy score. MedOmni-45 Degrees aims to expose reasoning vulnerabilities in medical LLMs and guide the development of safer models. <br /> <div>
arXiv:2508.16213v1 Announce Type: new 
Abstract: With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2508.16217</link>
<guid>https://arxiv.org/abs/2508.16217</guid>
<content:encoded><![CDATA[
<div> Adversarial Protection, Diffusion-based Inpainting Models, PromptFlare, Cross-Attention Mechanism, Unauthorized Image Manipulations

Summary:
PromptFlare is a new method designed to protect images from unauthorized modifications using diffusion-based inpainting models. It leverages the cross-attention mechanism to target shared, invariant token prompts, injecting noise to divert the model's focus and neutralize the effect of prompts. This approach outperforms previous methods by achieving state-of-the-art performance on the EditBench dataset while reducing computational overhead and GPU memory usage. By exploiting prompt embeddings, PromptFlare effectively prevents malicious actors from manipulating images while maintaining high-quality results. The code for PromptFlare is available on GitHub, providing a robust and efficient solution to safeguard against potential misuse of diffusion models. 

<br /><br />Summary: <div>
arXiv:2508.16217v1 Announce Type: new 
Abstract: The success of diffusion models has enabled effortless, high-quality image modifications that precisely align with users' intentions, thereby raising concerns about their potential misuse by malicious actors. Previous studies have attempted to mitigate such misuse through adversarial attacks. However, these approaches heavily rely on image-level inconsistencies, which pose fundamental limitations in addressing the influence of textual prompts. In this paper, we propose PromptFlare, a novel adversarial protection method designed to protect images from malicious modifications facilitated by diffusion-based inpainting models. Our approach leverages the cross-attention mechanism to exploit the intrinsic properties of prompt embeddings. Specifically, we identify and target shared token of prompts that is invariant and semantically uninformative, injecting adversarial noise to suppress the sampling process. The injected noise acts as a cross-attention decoy, diverting the model's focus away from meaningful prompt-image alignments and thereby neutralizing the effect of prompt. Extensive experiments on the EditBench dataset demonstrate that our method achieves state-of-the-art performance across various metrics while significantly reducing computational overhead and GPU memory usage. These findings highlight PromptFlare as a robust and efficient protection against unauthorized image manipulations. The code is available at https://github.com/NAHOHYUN-SKKU/PromptFlare.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Visual Foundation Models Robustness</title>
<link>https://arxiv.org/abs/2508.16225</link>
<guid>https://arxiv.org/abs/2508.16225</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Foundation Models, computer vision, network robustness, adversarial attacks, benchmarking metrics

Summary:
Visual Foundation Models (VFMs) are essential in computer vision for tasks like object detection, image classification, and medical analysis. They leverage deep learning models like LeNet-5 and ResNet to achieve superior performance. Network robustness is crucial in VFMs to handle dynamic environments influenced by factors like lighting and sensor characteristics. Empirical defenses and robust training help enhance network robustness against challenges like distributional shifts and adversarial attacks. However, there are challenges associated with these defense mechanisms, including network properties and components. A thorough analysis is required through ablation studies and benchmarking metrics to evaluate network robustness effectively. <div>
arXiv:2508.16225v1 Announce Type: new 
Abstract: Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision, powering systems for diverse tasks such as object detection, image classification, segmentation, pose estimation, and motion tracking. VFMs are capitalizing on seminal innovations in deep learning models, such as LeNet-5, AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver superior performance across a range of critical computer vision applications. These include security-sensitive domains like biometric verification, autonomous vehicle perception, and medical image analysis, where robustness is essential to fostering trust between technology and the end-users. This article investigates network robustness requirements crucial in computer vision systems to adapt effectively to dynamic environments influenced by factors such as lighting, weather conditions, and sensor characteristics. We examine the prevalent empirical defenses and robust training employed to enhance vision network robustness against real-world challenges such as distributional shifts, noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we provide a comprehensive analysis of the challenges associated with these defense mechanisms, including network properties and components to guide ablation studies and benchmarking metrics to evaluate network robustness.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing</title>
<link>https://arxiv.org/abs/2508.16230</link>
<guid>https://arxiv.org/abs/2508.16230</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal creative writing, FlexMUSE, T2I module, modality semantic alignment gating, ArtMUSE

Summary:
FlexMUSE introduces a new approach for multi-modal creative writing (MMCW) by incorporating a T2I module that allows optional visual input. It focuses on aligning semantics between the modalities of the output by implementing modality semantic alignment gating (msaGate) to restrict textual input and attention-based cross-modality fusion for semantic enhancement. The model also features modality semantic creative direct preference optimization (mscDPO) to enhance writing creativity by utilizing rejected samples. Additionally, a dataset called ArtMUSE, consisting of around 3k text-image pairs, is introduced to advance MMCW research. FlexMUSE demonstrates promising results in terms of consistency, creativity, and coherence, showcasing its potential for enhancing multi-modal generative tasks. 

<br /><br />Summary: <div>
arXiv:2508.16230v1 Announce Type: new 
Abstract: Multi-modal creative writing (MMCW) aims to produce illustrated articles. Unlike common multi-modal generative (MMG) tasks such as storytelling or caption generation, MMCW is an entirely new and more abstract challenge where textual and visual contexts are not strictly related to each other. Existing methods for related tasks can be forcibly migrated to this track, but they require specific modality inputs or costly training, and often suffer from semantic inconsistencies between modalities. Therefore, the main challenge lies in economically performing MMCW with flexible interactive patterns, where the semantics between the modalities of the output are more aligned. In this work, we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE promotes creativity and emphasizes the unification between modalities by proposing the modality semantic alignment gating (msaGate) to restrict the textual input. Besides, an attention-based cross-modality fusion is proposed to augment the input features for semantic enhancement. The modality semantic creative direct preference optimization (mscDPO) within FlexMUSE is designed by extending the rejected samples to facilitate the writing creativity. Moreover, to advance the MMCW, we expose a dataset called ArtMUSE which contains with around 3k calibrated text-image pairs. FlexMUSE achieves promising results, demonstrating its consistency, creativity and coherence.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation</title>
<link>https://arxiv.org/abs/2508.16239</link>
<guid>https://arxiv.org/abs/2508.16239</guid>
<content:encoded><![CDATA[
<div> dataset, electron micrograph, deep learning, instance segmentation, materials science
<br />
UniEM-3M is introduced as a large-scale and multimodal electron micrograph dataset for instance-level understanding in materials science. It contains high-resolution EMs, millions of segmentation labels, and attribute-disentangled textual descriptions. A text-to-image diffusion model trained on the dataset is released for data augmentation and distribution representation. UniEM-Net is presented as a strong baseline model for instance segmentation, outperforming other advanced methods. The release of a partial dataset, generative model, and benchmark will accelerate progress in automated materials analysis.
<br /><br />Summary: UniEM-3M is a significant contribution to materials science with its large-scale EM dataset and advanced models. The dataset includes high-resolution electron micrographs and instance segmentation labels, along with textual descriptions. A text-to-image diffusion model and UniEM-Net baseline model are released for data augmentation and benchmarking, showing superior performance in instance segmentation. This comprehensive release will facilitate advancements in automated materials analysis. <div>
arXiv:2508.16239v1 Announce Type: new 
Abstract: Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring GUI Elements through Vision Language Models: Towards Action Space Generation</title>
<link>https://arxiv.org/abs/2508.16271</link>
<guid>https://arxiv.org/abs/2508.16271</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, graphical user interface, coordinate generation, IoU-Augmented Maximum Likelihood, data augmentation

Summary: 
Multimodal large language models (MLLMs) play a critical role in improving human-computer interaction, particularly in graphical user interface (GUI) elements structuring. However, MLLMs face challenges in accurately generating UI element coordinates due to the semantic gap in representing numerical UI coordinates in language spaces. To address this, the study introduces an IoU-Augmented Maximum Likelihood (IAML) training approach that incorporates IoU-based coordinate sampling to enhance training data and mitigate exposure bias. The novel pipeline for data augmentation significantly improves MLLMs' performance in generating UI element coordinates, crucial for GUI understanding. Experimental results validate the superiority of the IAML training approach over traditional methods, showcasing its potential to enhance the capabilities of MLLMs in processing user instructions based on screen contents. <div>
arXiv:2508.16271v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction. In this paper we focus on the application of MLLMs in the field of graphical user interface (GUI) elements structuring, where they assist in processing user instructions based on screen contents. Despite the promise of MLLMs, their performance in precisely generating UI element coordinates, a critical aspect of GUI understanding, is hindered by the nature of next-token prediction training. This challenge arises from the semantic void surrounding numerical UI coordinates in language representation spaces, necessitating a substantial and diverse dataset to bolster visual module capabilities. To address these limitations, we introduce an IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our approach involves a novel pipeline for IoU-based coordinate sampling to augment the training data, which considers the proximity to ground truth coordinates. This data augmentation strategy is then employed to fine-tune MLLMs under the IAML paradigm, which is designed to mitigate the exposure bias problem inherent in traditional maximum likelihood estimation. Through extensive experiments, we demonstrate the superior performance of our IAML training approach over traditional training paradigms.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization</title>
<link>https://arxiv.org/abs/2508.16272</link>
<guid>https://arxiv.org/abs/2508.16272</guid>
<content:encoded><![CDATA[
<div> Annotation system, Global coverage, Multi-feature, Deep learning models, Remote sensing

Summary:
IRSAMap is a new global remote sensing dataset designed for large-scale, high-resolution land cover mapping using object-based vector modeling. It addresses challenges in existing datasets by providing comprehensive vector annotations for 10 typical objects across 79 regions on six continents. The dataset combines manual and AI-based methods for efficient and consistent annotation and offers adaptability for various tasks such as pixel-level classification and building outline extraction. IRSAMap serves as a benchmark for transitioning from pixel-based to object-based approaches in geographic feature automation, facilitating global geographic information updates and digital twin construction. The dataset is publicly available for use in advancing deep learning models in the field of remote sensing. 

<br /><br />Summary: <div>
arXiv:2508.16272v1 Announce Type: new 
Abstract: With the enhancement of remote sensing image resolution and the rapid advancement of deep learning, land cover mapping is transitioning from pixel-level segmentation to object-based vector modeling. This shift demands more from deep learning models, requiring precise object boundaries and topological consistency. However, existing datasets face three main challenges: limited class annotations, small data scale, and lack of spatial structural information. To overcome these issues, we introduce IRSAMap, the first global remote sensing dataset for large-scale, high-resolution, multi-feature land cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive vector annotation system with over 1.8 million instances of 10 typical objects (e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an intelligent annotation workflow combining manual and AI-based methods to improve efficiency and consistency; 3) global coverage across 79 regions in six continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks like pixel-level classification, building outline extraction, road centerline extraction, and panoramic segmentation. IRSAMap provides a standardized benchmark for the shift from pixel-based to object-based approaches, advancing geographic feature automation and collaborative modeling. It is valuable for global geographic information updates and digital twin construction. The dataset is publicly available at https://github.com/ucas-dlg/IRSAMap
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Small Methane Plume Segmentation in Satellite Imagery</title>
<link>https://arxiv.org/abs/2508.16282</link>
<guid>https://arxiv.org/abs/2508.16282</guid>
<content:encoded><![CDATA[
<div> methane plumes, Sentinel-2 imagery, deep learning, U-Net, ResNet34 encoder;<br />
Summary:<br />
This paper presents a new deep learning approach using U-Net with a ResNet34 encoder to detect methane plumes in Sentinel-2 imagery. By integrating dual spectral enhancement techniques, the model surpasses traditional methods by detecting small plumes down to 400 m2. The proposed method achieves a 78.39% F1-score on the validation set, showing superior sensitivity and precision for automated methane monitoring, especially for small plumes. This innovative solution contributes to mitigating climate change by improving the detection and monitoring of methane emissions, a potent greenhouse gas. <div>
arXiv:2508.16282v1 Announce Type: new 
Abstract: This paper tackles the challenging problem of detecting methane plumes, a potent greenhouse gas, using Sentinel-2 imagery. This contributes to the mitigation of rapid climate change. We propose a novel deep learning solution based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement techniques (Varon ratio and Sanchez regression) to optimise input features for heightened sensitivity. A key achievement is the ability to detect small plumes down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing traditional methods limited to larger plumes. Experiments show our approach achieves a 78.39% F1-score on the validation set, demonstrating superior performance in sensitivity and precision over existing remote sensing techniques for automated methane monitoring, especially for small plumes.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents</title>
<link>https://arxiv.org/abs/2508.16284</link>
<guid>https://arxiv.org/abs/2508.16284</guid>
<content:encoded><![CDATA[
<div> Keywords: document forgery detection, convolutional transformer, auxiliary noiseprint features, EdgeDoc, ICCV 2025 DeepID Challenge

Summary: EdgeDoc is a new approach for detecting and localizing document forgeries, specifically targeting the threat they pose to KYC processes and remote onboarding systems. The architecture of EdgeDoc combines a lightweight convolutional transformer with auxiliary noiseprint features extracted from images, improving its ability to identify subtle manipulations. The system performed well in the ICCV 2025 DeepID Challenge, achieving third place and demonstrating its competitiveness. Experimental results on the FantasyID dataset show that EdgeDoc surpasses baseline methods, proving its effectiveness in practical scenarios. The project page provides more information on EdgeDoc's capabilities and performance. <br /><br />Summary: <div>
arXiv:2508.16284v1 Announce Type: new 
Abstract: The widespread availability of tools for manipulating images and documents has made it increasingly easy to forge digital documents, posing a serious threat to Know Your Customer (KYC) processes and remote onboarding systems. Detecting such forgeries is essential to preserving the integrity and security of these services. In this work, we present EdgeDoc, a novel approach for the detection and localization of document forgeries. Our architecture combines a lightweight convolutional transformer with auxiliary noiseprint features extracted from the images, enhancing its ability to detect subtle manipulations. EdgeDoc achieved third place in the ICCV 2025 DeepID Challenge, demonstrating its competitiveness. Experimental results on the FantasyID dataset show that our method outperforms baseline approaches, highlighting its effectiveness in realworld scenarios. Project page : https://www.idiap. ch/paper/edgedoc/
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment</title>
<link>https://arxiv.org/abs/2508.16291</link>
<guid>https://arxiv.org/abs/2508.16291</guid>
<content:encoded><![CDATA[
<div> Keywords: Figure skating, Technical Element Score, Program Component Score, Mamba pyramid network, Action quality assessment 

Summary: 
The article proposes a novel approach for predicting Technical Element Score (TES) and Program Component Score (PCS) in figure skating competitions. The challenges faced by existing methods are addressed by a two-stream Mamba pyramid network that aligns with judging criteria. The network separates the visual-feature based TES evaluation stream from the audio-visual-feature based PCS evaluation stream. A multi-level fusion mechanism is introduced in the PCS evaluation stream to enhance scoring by fusing visual and auditory cues. The TES evaluation stream utilizes a multi-scale Mamba pyramid and TES head to effectively localize and evaluate action elements with various temporal scales. The method is efficient in handling lengthy competition videos, capturing long-range dependencies with linear computational complexity. Experimental results demonstrate state-of-the-art performance on the FineFS benchmark. Overall, the proposed framework provides accurate predictions for TES and PCS in figure skating competitions. 

<br /><br />Summary: <div>
arXiv:2508.16291v1 Announce Type: new 
Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations in figure skating demand precise assessment of athletic actions and artistic interpretation, respectively. Existing methods face three major challenges. Firstly, video and audio cues are regarded as common features for both TES and PCS predictions in previous works without considering the prior evaluation criterion of figure skating. Secondly, action elements in competitions are separated in time, TES should be derived from each element's score, but existing methods try to give an overall TES prediction without evaluating each action element. Thirdly, lengthy competition videos make it difficult and inefficient to handle long-range contexts. To address these challenges, we propose a two-stream Mamba pyramid network that aligns with actual judging criteria to predict TES and PCS by separating visual-feature based TES evaluation stream from audio-visual-feature based PCS evaluation stream. In the PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee that video-based features remain unaffected when assessing TES, and enhance PCS estimation by fusing visual and auditory cues across each contextual level of the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and TES head we proposed effectively address the challenges of localizing and evaluating action elements with various temporal scales and give score predictions. With Mamba's superior ability to capture long-range dependencies and its linear computational complexity, our method is ideal for handling lengthy figure skating videos. Comprehensive experimentation demonstrates that our framework attains state-of-the-art performance on the FineFS benchmark. Our source code is available at https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Hybrid Technique for Efficient Digitization of Handwritten Marksheets</title>
<link>https://arxiv.org/abs/2508.16295</link>
<guid>https://arxiv.org/abs/2508.16295</guid>
<content:encoded><![CDATA[
<div> OpenCV, table detection, PaddleOCR, handwritten text recognition, YOLOv8<br />
Summary:<br />
This article introduces a hybrid method for digitizing handwritten marksheets, combining OpenCV for table detection and PaddleOCR for recognizing handwritten text. The system efficiently detects table structures with OpenCV, achieving high accuracy. YOLOv8 and Modified YOLOv8 are utilized for handwritten text recognition within tables, outperforming PaddleOCR. The proposed model achieves 92.72% accuracy, reducing the need for manual work in digitizing academic and administrative documents. This research significantly advances document automation, particularly in handwritten document understanding, by providing reliable methods to scale and integrate technologies involved.<br /> <div>
arXiv:2508.16295v1 Announce Type: new 
Abstract: The digitization of handwritten marksheets presents huge challenges due to the different styles of handwriting and complex table structures in such documents like marksheets. This work introduces a hybrid method that integrates OpenCV for table detection and PaddleOCR for recognizing sequential handwritten text. The image processing capabilities of OpenCV efficiently detects rows and columns which enable computationally lightweight and accurate table detection. Additionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text recognition within the detected table structures alongside PaddleOCR which further enhance the system's versatility. The proposed model achieves high accuracy on our custom dataset which is designed to represent different and diverse handwriting styles and complex table layouts. Experimental results demonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent, outperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This efficiency reduces the necessity for manual work which makes this a practical and fast solution for digitizing academic as well as administrative documents. This research serves the field of document automation, particularly handwritten document understanding, by providing operational and reliable methods to scale, enhance, and integrate the technologies involved.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension</title>
<link>https://arxiv.org/abs/2508.16300</link>
<guid>https://arxiv.org/abs/2508.16300</guid>
<content:encoded><![CDATA[
<div> framework, multimodal learning, cross-modal relation, hierarchical interactive attention, multitasking<br />
<br />
Summary: 
The article introduces the MM-ORIENT framework for multimodal learning, addressing noise within individual modalities. It utilizes cross-modal relation graphs to reconstruct monomodal features for acquiring multimodal representations without explicit interactions, reducing noise effects. The Hierarchical Interactive Monomadal Attention (HIMA) component focuses on important information within each modality, aiding in multitasking by learning discriminative features before fusion. The framework effectively comprehends multimodal content for multiple tasks, as demonstrated through extensive experimental evaluation on three datasets. <div>
arXiv:2508.16300v1 Announce Type: new 
Abstract: A major challenge in multimodal learning is the presence of noise within individual modalities. This noise inherently affects the resulting multimodal representations, especially when these representations are obtained through explicit interactions between different modalities. Moreover, the multimodal fusion techniques while aiming to achieve a strong joint representation, can neglect valuable discriminative information within the individual modalities. To this end, we propose a Multimodal-Multitask framework with crOss-modal Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective for multiple tasks. The proposed approach acquires multimodal representations cross-modally without explicit interaction between different modalities, reducing the noise effect at the latent stage. To achieve this, we propose cross-modal relation graphs that reconstruct monomodal features to acquire multimodal representations. The features are reconstructed based on the node neighborhood, where the neighborhood is decided by the features of a different modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA) to focus on pertinent information within a modality. While cross-modal relation graphs help comprehend high-order relationships between two modalities, HIMA helps in multitasking by learning discriminative features of individual modalities before late-fusing them. Finally, extensive experimental evaluation on three datasets demonstrates that the proposed approach effectively comprehends multimodal content for multiple tasks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers</title>
<link>https://arxiv.org/abs/2508.16311</link>
<guid>https://arxiv.org/abs/2508.16311</guid>
<content:encoded><![CDATA[
<div> Transformer models, Multi-Head Self-Attention, edge deployment, information redundancy, Shannon entropy <br />
Summary: Transformer models with Multi-Head Self-Attention (MHSA) face challenges like high computational complexity and memory demands, hindering deployment at the edge. This study proposes an approach called Entropy Attention Maps (EAM) to accelerate model inference by freezing and quantizing low-entropy attention maps. Analysis based on Shannon entropy indicates that low-entropy attention heads contribute less information, leading to targeted compression strategies. EAM achieves comparable or higher accuracy with less than or equal to 20% sparsity in attention maps on ImageNet-1k dataset. The approach shows competitive performance even beyond 20% sparsity for Transformer models like DeiT and Swin. This work highlights the potential of leveraging information redundancy in attention maps to optimize Transformer model efficiency. <br /><br />Summary: <div>
arXiv:2508.16311v1 Announce Type: new 
Abstract: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the edge. In this work, we analyze and exploit information redundancy in attention maps to accelerate model inference. By quantifying the information captured by each attention head using Shannon entropy, our analysis reveals that attention heads with lower entropy, i.e., exhibiting more deterministic behavior, tend to contribute less information, motivating targeted compression strategies. Relying on these insights, we propose Entropy Attention Maps (EAM), a model that freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation. Empirical validation on ImageNet-1k shows that EAM achieves similar or higher accuracy at $\leq$20\% sparsity in attention maps and competitive performance beyond this level for the DeiT and Swin Transformer models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision encoders should be image size agnostic and task driven</title>
<link>https://arxiv.org/abs/2508.16317</link>
<guid>https://arxiv.org/abs/2508.16317</guid>
<content:encoded><![CDATA[
<div> Keywords: vision encoders, image size agnostic, task-driven, efficiency, computational complexity

Summary: 
Vision encoders of the future should prioritize task-driven approaches over fixed image sizes, drawing inspiration from the efficiency seen in biological vision. The focus should be on adapting computational complexity based on the specific task at hand rather than the sheer size of the image. Nature demonstrates the ability to efficiently process vast amounts of visual data by dynamically allocating energy based on task requirements. This approach contrasts with current vision encoders that lack such adaptability. A proof-of-concept solution for image classification showcases the feasibility and potential of this task-driven, dynamic approach. This shift towards task-driven vision encoders holds promise for improving efficiency and effectiveness in processing visual data.<br /><br />Summary: <div>
arXiv:2508.16317v1 Announce Type: new 
Abstract: This position paper argues that the next generation of vision encoders should be image size agnostic and task driven. The source of our inspiration is biological. Not a structural aspect of biological vision, but a behavioral trait -- efficiency. We focus on a couple of ways in which vision in nature is efficient, but modern vision encoders not. We -- humans and animals -- deal with vast quantities of visual data, and need to be smart where we focus our limited energy -- it depends on the task. It is our belief that vision encoders should be dynamic and the computational complexity should depend on the task at hand rather than the size of the image. We, also, provide concrete first steps towards our vision -- a proof-of-concept solution for image classification. Despite classification being not very representative for what we are trying to achieve, it shows that our approach is feasible and promising.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Mechanism in Randomized Time Warping</title>
<link>https://arxiv.org/abs/2508.16366</link>
<guid>https://arxiv.org/abs/2508.16366</guid>
<content:encoded><![CDATA[
<div> Keywords: Randomized Time Warping, self-attention mechanism, Transformers, motion recognition, Dynamic Time Warping 

Summary: 
Randomized Time Warping (RTW) has been identified as a form of self-attention mechanism, similar to that used in Transformers for motion recognition tasks. While RTW is an extension of Dynamic Time Warping (DTW) for matching sequential patterns, it focuses on identifying optimal contribution weights for each element in the input sequence to generate discriminative features. Despite differences in operation, RTW and self-attention weights exhibit high correlation. RTW attention processes the entire input sequence, while self-attention focuses on a local subset due to computational constraints. This distinction gives RTW an edge over Transformers, as evidenced by a 5% performance boost on the Something-Something V2 dataset. <div>
arXiv:2508.16366v1 Announce Type: new 
Abstract: This paper reveals that we can interpret the fundamental function of Randomized Time Warping (RTW) as a type of self-attention mechanism, a core technology of Transformers in motion recognition. The self-attention is a mechanism that enables models to identify and weigh the importance of different parts of an input sequential pattern. On the other hand, RTW is a general extension of Dynamic Time Warping (DTW), a technique commonly used for matching and comparing sequential patterns. In essence, RTW searches for optimal contribution weights for each element of the input sequential patterns to produce discriminative features. Although the two approaches look different, these contribution weights can be interpreted as self-attention weights. In fact, the two weight patterns look similar, producing a high average correlation of 0.80 across the ten smallest canonical angles. However, they work in different ways: RTW attention operates on an entire input sequential pattern, while self-attention focuses on only a local view which is a subset of the input sequential pattern because of the computational costs of the self-attention matrix. This targeting difference leads to an advantage of RTW against Transformer, as demonstrated by the 5\% performance improvement on the Something-Something V2 dataset.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection</title>
<link>https://arxiv.org/abs/2508.16397</link>
<guid>https://arxiv.org/abs/2508.16397</guid>
<content:encoded><![CDATA[
<div> detect defects, steel manufacturing, real-time, deep learning, lightweight

Summary:<br />
The article introduces GMBINet, a lightweight framework for real-time surface defect detection in the steel manufacturing industry. Existing deep learning methods suffer from high computational complexity, limiting their deployment in industrial environments. GMBINet addresses these challenges by enhancing multiscale feature extraction and interaction through Group Multiscale Bidirectional Interactive (GMBI) modules. These modules ensure scale-agnostic computational complexity and improve cross-scale interaction without introducing additional overhead. Experiments show that GMBINet achieves competitive accuracy with real-time speeds on both GPU and CPU. Evaluations on the NEU-CLS dataset demonstrate its strong generalization ability, indicating potential for broader industrial vision applications. The dataset and code for GMBINet are publicly available for further research and development. <br />Summary: <div>
arXiv:2508.16397v1 Announce Type: new 
Abstract: Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2508.16408</link>
<guid>https://arxiv.org/abs/2508.16408</guid>
<content:encoded><![CDATA[
<div> Sensor fusion, autonomous robots, adverse weather conditions, multimodal data, transformer decoder <br />
Summary:<br />
This study introduces a novel sensor fusion approach designed for autonomous robots operating in adverse weather conditions. The method combines RGB, LiDAR, NIR gated camera, and radar sensor data using attentive blending schemes and learned refinement on the Bird's Eye View (BEV) plane. A transformer decoder is utilized to predict detections, weighting modalities based on distance and visibility. The approach improves multimodal sensor fusion reliability in challenging weather scenarios, enhancing average precision by 17.2 AP for vulnerable pedestrians in long distances and foggy scenes. By bridging the gap between ideal conditions and real-world edge cases, the proposed method enhances object detection and decision-making capabilities for autonomous vehicles. The project page for further information can be accessed at https://light.princeton.edu/samfusion/ <br /> <div>
arXiv:2508.16408v1 Announce Type: new 
Abstract: Multimodal sensor fusion is an essential capability for autonomous robots, enabling object detection and decision-making in the presence of failing or uncertain inputs. While recent fusion methods excel in normal environmental conditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or obstructions due to soiling. We introduce a novel multi-sensor fusion approach tailored to adverse weather conditions. In addition to fusing RGB and LiDAR sensors, which are employed in recent autonomous driving literature, our sensor fusion stack is also capable of learning from NIR gated camera and radar modalities to tackle low light and inclement weather. We fuse multimodal sensor data through attentive, depth-based blending schemes, with learned refinement on the Bird's Eye View (BEV) plane to combine image and range features effectively. Our detections are predicted by a transformer decoder that weighs modalities based on distance and visibility. We demonstrate that our method improves the reliability of multimodal sensor fusion in autonomous vehicles under challenging weather conditions, bridging the gap between ideal conditions and real-world edge cases. Our approach improves average precision by 17.2 AP compared to the next best method for vulnerable pedestrians in long distances and challenging foggy scenes. Our project page is available at https://light.princeton.edu/samfusion/
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction</title>
<link>https://arxiv.org/abs/2508.16433</link>
<guid>https://arxiv.org/abs/2508.16433</guid>
<content:encoded><![CDATA[
<div> sparse, uncalibrated images, human-centric scenarios, 3D geometry, multi-view images, dense scene geometry 

Summary: 

HAMSt3R is introduced as an extension of MASt3R for recovering joint human and scene 3D geometry from sparse, uncalibrated multi-view images. It utilizes DUNE, an image encoder incorporating knowledge from scene geometry and human body estimation models. HAMSt3R includes network heads for segmenting people, predicting dense correspondences, and estimating depth in human-centric environments to achieve comprehensive 3D reconstruction. The method generates a dense point map with human semantic information in 3D, efficiently without complex optimization pipelines. Evaluation on challenging benchmarks shows effective human reconstruction and strong performance in general 3D tasks, bridging the gap between human and scene understanding in 3D vision. <div>
arXiv:2508.16433v1 Announce Type: new 
Abstract: Recovering the 3D geometry of a scene from a sparse set of uncalibrated images is a long-standing problem in computer vision. While recent learning-based approaches such as DUSt3R and MASt3R have demonstrated impressive results by directly predicting dense scene geometry, they are primarily trained on outdoor scenes with static environments and struggle to handle human-centric scenarios. In this work, we introduce HAMSt3R, an extension of MASt3R for joint human and scene 3D reconstruction from sparse, uncalibrated multi-view images. First, we exploit DUNE, a strong image encoder obtained by distilling, among others, the encoders from MASt3R and from a state-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better understanding of scene geometry and human bodies. Our method then incorporates additional network heads to segment people, estimate dense correspondences via DensePose, and predict depth in human-centric environments, enabling a more comprehensive 3D reconstruction. By leveraging the outputs of our different heads, HAMSt3R produces a dense point map enriched with human semantic information in 3D. Unlike existing methods that rely on complex optimization pipelines, our approach is fully feed-forward and efficient, making it suitable for real-world applications. We evaluate our model on EgoHumans and EgoExo4D, two challenging benchmarks con taining diverse human-centric scenarios. Additionally, we validate its generalization to traditional multi-view stereo and multi-view pose regression tasks. Our results demonstrate that our method can reconstruct humans effectively while preserving strong performance in general 3D reconstruction tasks, bridging the gap between human and scene understanding in 3D vision.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</title>
<link>https://arxiv.org/abs/2508.16465</link>
<guid>https://arxiv.org/abs/2508.16465</guid>
<content:encoded><![CDATA[
arXiv:2508.16465v1 Announce Type: new 
Abstract: Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arbitrary-Scale 3D Gaussian Super-Resolution</title>
<link>https://arxiv.org/abs/2508.16467</link>
<guid>https://arxiv.org/abs/2508.16467</guid>
<content:encoded><![CDATA[
arXiv:2508.16467v1 Announce Type: new 
Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation</title>
<link>https://arxiv.org/abs/2508.16512</link>
<guid>https://arxiv.org/abs/2508.16512</guid>
<content:encoded><![CDATA[
arXiv:2508.16512v1 Announce Type: new 
Abstract: Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called "world models". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open World Detection: A Survey</title>
<link>https://arxiv.org/abs/2508.16527</link>
<guid>https://arxiv.org/abs/2508.16527</guid>
<content:encoded><![CDATA[
arXiv:2508.16527v1 Announce Type: new 
Abstract: For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up today's state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground/background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-RAG: Retrieval Augmented Multiview Diffusion</title>
<link>https://arxiv.org/abs/2508.16577</link>
<guid>https://arxiv.org/abs/2508.16577</guid>
<content:encoded><![CDATA[
arXiv:2508.16577v1 Announce Type: new 
Abstract: Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonious Color Pairings: Insights from Human Preference and Natural Hue Statistics</title>
<link>https://arxiv.org/abs/2508.15777</link>
<guid>https://arxiv.org/abs/2508.15777</guid>
<content:encoded><![CDATA[
arXiv:2508.15777v1 Announce Type: cross 
Abstract: While color harmony has long been studied in art and design, a clear consensus remains elusive, as most models are grounded in qualitative insights or limited datasets. In this work, we present a quantitative, data-driven study of color pairing preferences using controlled hue-based palettes in the HSL color space. Participants evaluated combinations of thirteen distinct hues, enabling us to construct a preference matrix and define a combinability index for each color. Our results reveal that preferences are highly hue dependent, challenging the assumption of universal harmony rules proposed in the literature. Yet, when averaged over hues, statistically meaningful patterns of aesthetic preference emerge, with certain hue separations perceived as more harmonious. Strikingly, these patterns align with hue distributions found in natural landscapes, pointing to a statistical correspondence between human color preferences and the structure of color in nature. Together, these findings offer a quantitative framework for studying color harmony and its potential perceptual and ecological underpinnings.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Residual Finite Scalar Quantization for Neural Compression</title>
<link>https://arxiv.org/abs/2508.15860</link>
<guid>https://arxiv.org/abs/2508.15860</guid>
<content:encoded><![CDATA[
arXiv:2508.15860v1 Announce Type: cross 
Abstract: Finite Scalar Quantization (FSQ) has emerged as a promising alternative to Vector Quantization (VQ) in neural compression, offering simplified training and improved stability. However, naive application of FSQ in residual quantization frameworks suffers from the \textbf{residual magnitude decay problem}, where subsequent FSQ layers receive progressively weaker signals, severely limiting their effectiveness. We propose \textbf{Robust Residual Finite Scalar Quantization (RFSQ)}, a general framework that addresses this fundamental limitation through two novel conditioning strategies: learnable scaling factors and invertible layer normalization. Our approach maintains the simplicity of FSQ while enabling effective multi-stage residual quantization. Comprehensive experiments on ImageNet demonstrate that RFSQ variants significantly outperform strong baselines including VQ-EMA, FSQ, and LFQ, achieving up to 45\% improvement in perceptual loss and 28.7\% reduction in L1 reconstruction error. The proposed LayerNorm strategy shows the most consistent improvements across different configurations, establishing RFSQ as a superior quantization method for neural compression.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation</title>
<link>https://arxiv.org/abs/2508.15972</link>
<guid>https://arxiv.org/abs/2508.15972</guid>
<content:encoded><![CDATA[
arXiv:2508.15972v1 Announce Type: cross 
Abstract: Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI Based Fuzzy Logic and Spatial Statistics for Unsupervised Microscopy Segmentation</title>
<link>https://arxiv.org/abs/2508.15979</link>
<guid>https://arxiv.org/abs/2508.15979</guid>
<content:encoded><![CDATA[
arXiv:2508.15979v1 Announce Type: cross 
Abstract: Brightfield microscopy imaging of unstained live cells remains a persistent challenge due to low contrast, temporal changes in specimen phenotypes, irregular illumination, and the absence of training labels. While deep learning (DL) methods (e.g., Cellpose 3.0) achieve state-of-the-art (SOTA) performance, they require extensive labeled data and heavy computational resources, and they often fail under uneven illumination. We present the first unsupervised segmentation framework combining spatial standard deviation from local mean (SSDLM), fuzzy logic, adjusted variograms, Moran's I, and cumulative squared shift of nodal intensity (CSSNI) to address these limitations. Unlike deep learning models, our approach requires no annotations or retraining and operates through a user-friendly GUI tailored for non-programming users. The robustness and generality were validated on three datasets, including cross-domain data. We benchmark our method against 2023--2024 SOTA models, including Cellpose 3.0 and StarDist, using a dataset of unstained myoblast images. Our method achieves a significant improvement in segmentation performance, with an IoU increase of up to 48\% and statistically validated superiority ($p < 0.01$, Wilcoxon signed-rank test). Expert evaluation from two biologists further supports the segmentation quality (Cohen's $\kappa > 0.75$). The proposed algorithm is lightweight, interpretable, and computationally efficient, offering a practical and effective alternative for cell segmentation in label-free microscopy. The code, the dataset, and the results are available for reproducibility*.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System</title>
<link>https://arxiv.org/abs/2508.15990</link>
<guid>https://arxiv.org/abs/2508.15990</guid>
<content:encoded><![CDATA[
arXiv:2508.15990v1 Announce Type: cross 
Abstract: Accurately perceiving an object's pose and shape is essential for precise grasping and manipulation. Compared to common vision-based methods, tactile sensing offers advantages in precision and immunity to occlusion when tracking and reconstructing objects in contact. This makes it particularly valuable for in-hand and other high-precision manipulation tasks. In this work, we present GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to estimate object pose over long periods and reconstruct object shapes with high fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses tactile-derived surface normals and curvatures for robust tracking and loop closure. It can track object motion in real time with low error and minimal drift, and reconstruct shapes with submillimeter accuracy, even for low-texture objects such as wooden tools. GelSLAM extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, and we believe it will serve as a foundation for many precise manipulation tasks involving interaction with objects in hand. The video demo is available on our website: https://joehjhuang.github.io/gelslam.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Attention Multimodal Fusion for Breast Cancer Diagnosis: Integrating Mammography and Clinical Data with Explainability</title>
<link>https://arxiv.org/abs/2508.16000</link>
<guid>https://arxiv.org/abs/2508.16000</guid>
<content:encoded><![CDATA[
arXiv:2508.16000v1 Announce Type: cross 
Abstract: A precise assessment of the risk of breast lesions can greatly lower it and assist physicians in choosing the best course of action. To categorise breast lesions, the majority of current computer-aided systems only use characteristics from mammograms. Although this method is practical, it does not completely utilise clinical reports' valuable information to attain the best results. When compared to utilising mammography alone, will clinical features greatly enhance the categorisation of breast lesions? How may clinical features and mammograms be combined most effectively? In what ways may explainable AI approaches improve the interpretability and reliability of models used to diagnose breast cancer? To answer these basic problems, a comprehensive investigation is desperately needed. In order to integrate mammography and categorical clinical characteristics, this study examines a number of multimodal deep networks grounded on feature concatenation, co-attention, and cross-attention. The model achieved an AUC-ROC of 0.98, accuracy of 0.96, F1-score of 0.94, precision of 0.92, and recall of 0.95 when tested on publicly accessible datasets (TCGA and CBIS-DDSM).
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinically-Informed Preprocessing Improves Stroke Segmentation in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2508.16004</link>
<guid>https://arxiv.org/abs/2508.16004</guid>
<content:encoded><![CDATA[
arXiv:2508.16004v1 Announce Type: cross 
Abstract: Stroke is among the top three causes of death worldwide, and accurate identification of ischemic stroke lesion boundaries from imaging is critical for diagnosis and treatment. The main imaging modalities used include magnetic resonance imaging (MRI), particularly diffusion weighted imaging (DWI), and computed tomography (CT)-based techniques such as non-contrast CT (NCCT), contrast-enhanced CT angiography (CTA), and CT perfusion (CTP). DWI is the gold standard for the identification of lesions but has limited applicability in low-resource settings due to prohibitive costs. CT-based imaging is currently the most practical imaging method in low-resource settings due to low costs and simplified logistics, but lacks the high specificity of MRI-based methods in monitoring ischemic insults. Supervised deep learning methods are the leading solution for automated ischemic stroke lesion segmentation and provide an opportunity to improve diagnostic quality in low-resource settings by incorporating insights from DWI when segmenting from CT. Here, we develop a series of models which use CT images taken upon arrival as inputs to predict follow-up lesion volumes annotated from DWI taken 2-9 days later. Furthermore, we implement clinically motivated preprocessing steps and show that the proposed pipeline results in a 38% improvement in Dice score over 10 folds compared to a nnU-Net model trained with the baseline preprocessing. Finally, we demonstrate that through additional preprocessing of CTA maps to extract vessel segmentations, we further improve our best model by 21% over 5 folds.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet-Space Super-Resolution for Real-Time Rendering</title>
<link>https://arxiv.org/abs/2508.16024</link>
<guid>https://arxiv.org/abs/2508.16024</guid>
<content:encoded><![CDATA[
arXiv:2508.16024v1 Announce Type: cross 
Abstract: We investigate the use of wavelet-space feature decomposition in neural super-resolution for rendering pipelines. Building on the DFASR framework, we introduce a wavelet-domain representation that separates low- and high-frequency details before reconstruction, enabling the network to better preserve fine textures while maintaining structural consistency. Unlike RGB-space regression, our approach leverages the stationary wavelet transform (SWT) to avoid spatial down-sampling, ensuring alignment across subbands and preserving shift invariance. The model predicts wavelet coefficients conditioned on spatial G-buffers and temporally warped history frames, which are then recombined through inverse wavelet synthesis. We conduct a comprehensive ablation study across wavelet families, transform types, and architectural variants, showing that incorporating SWT improves PSNR by up to 1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX 4090( 11ms), the relative overhead remains modest, suggesting that on higher-end GPUs our method would also remain real-time capable. Taken together, our results suggest that wavelet-domain representations are a principled and effective way to enhance perceptual quality in neural upscaling for graphics applications.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation</title>
<link>https://arxiv.org/abs/2508.16076</link>
<guid>https://arxiv.org/abs/2508.16076</guid>
<content:encoded><![CDATA[
arXiv:2508.16076v1 Announce Type: cross 
Abstract: Sign Language (SL) enables two-way communication for the deaf and hard-of-hearing community, yet many sign languages remain under-resourced in the AI space. Sign Language Instruction Generation (SLIG) produces step-by-step textual instructions that enable non-SL users to imitate and learn SL gestures, promoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG dataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced SLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to appear in the VLM pre-training data. To enhance zero-shot performance, we introduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL parameters, like hand shape, motion, and orientation, directly into the textual prompts. Subsuming standard sign parameters into the prompt makes the instructions more structured and reproducible than free-form natural text from vanilla prompting. We envision that our work would promote inclusivity and advancement in SL learning systems for the under-resourced communities.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight and Fast Real-time Image Enhancement via Decomposition of the Spatial-aware Lookup Tables</title>
<link>https://arxiv.org/abs/2508.16121</link>
<guid>https://arxiv.org/abs/2508.16121</guid>
<content:encoded><![CDATA[
arXiv:2508.16121v1 Announce Type: cross 
Abstract: The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation</title>
<link>https://arxiv.org/abs/2508.16188</link>
<guid>https://arxiv.org/abs/2508.16188</guid>
<content:encoded><![CDATA[
arXiv:2508.16188v1 Announce Type: cross 
Abstract: We present an Audio-Visual Language Model (AVLM) for expressive speech generation by integrating full-face visual cues into a pre-trained expressive speech model. We explore multiple visual encoders and multimodal fusion strategies during pre-training to identify the most effective integration approach. Subsequent fine-tuning on emotion recognition and expressive dialogue tasks yields substantial gains over speech-only baselines (e.g., +5 F1 in emotion recognition). AVLM highlights the value of expressive visual information in guiding speech generation and offers a foundation for end-to-end multimodal conversational systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-enabled virtual multiplexed immunostaining of label-free tissue for vascular invasion assessment</title>
<link>https://arxiv.org/abs/2508.16209</link>
<guid>https://arxiv.org/abs/2508.16209</guid>
<content:encoded><![CDATA[
arXiv:2508.16209v1 Announce Type: cross 
Abstract: Immunohistochemistry (IHC) has transformed clinical pathology by enabling the visualization of specific proteins within tissue sections. However, traditional IHC requires one tissue section per stain, exhibits section-to-section variability, and incurs high costs and laborious staining procedures. While multiplexed IHC (mIHC) techniques enable simultaneous staining with multiple antibodies on a single slide, they are more tedious to perform and are currently unavailable in routine pathology laboratories. Here, we present a deep learning-based virtual multiplexed immunostaining framework to simultaneously generate ERG and PanCK, in addition to H&amp;E virtual staining, enabling accurate localization and interpretation of vascular invasion in thyroid cancers. This virtual mIHC technique is based on the autofluorescence microscopy images of label-free tissue sections, and its output images closely match the histochemical staining counterparts (ERG, PanCK and H&amp;E) of the same tissue sections. Blind evaluation by board-certified pathologists demonstrated that virtual mIHC staining achieved high concordance with the histochemical staining results, accurately highlighting epithelial cells and endothelial cells. Virtual mIHC conducted on the same tissue section also allowed the identification and localization of small vessel invasion. This multiplexed virtual IHC approach can significantly improve diagnostic accuracy and efficiency in the histopathological evaluation of vascular invasion, potentially eliminating the need for traditional staining protocols and mitigating issues related to tissue loss and heterogeneity.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Validated Learning for Particle Separation: A Correctness-Based Self-Training Framework Without Human Labels</title>
<link>https://arxiv.org/abs/2508.16224</link>
<guid>https://arxiv.org/abs/2508.16224</guid>
<content:encoded><![CDATA[
arXiv:2508.16224v1 Announce Type: cross 
Abstract: Non-destructive 3D imaging of large multi-particulate samples is essential for quantifying particle-level properties, such as size, shape, and spatial distribution, across applications in mining, materials science, and geology. However, accurate instance segmentation of particles in tomographic data remains challenging due to high morphological variability and frequent particle contact, which limit the effectiveness of classical methods like watershed algorithms. While supervised deep learning approaches offer improved performance, they rely on extensive annotated datasets that are labor-intensive, error-prone, and difficult to scale. In this work, we propose self-validated learning, a novel self-training framework for particle instance segmentation that eliminates the need for manual annotations. Our method leverages implicit boundary detection and iteratively refines the training set by identifying particles that can be consistently matched across reshuffled scans of the same sample. This self-validation mechanism mitigates the impact of noisy pseudo-labels, enabling robust learning from unlabeled data. After just three iterations, our approach accurately segments over 97% of the total particle volume and identifies more than 54,000 individual particles in tomographic scans of quartz fragments. Importantly, the framework also enables fully autonomous model evaluation without the need for ground truth annotations, as confirmed through comparisons with state-of-the-art instance segmentation techniques. The method is integrated into the Biomedisa image analysis platform (https://github.com/biomedisa/biomedisa/).
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Diagnostic Quality Flat-Panel Detector CT Imaging Using Diffusion Models</title>
<link>https://arxiv.org/abs/2508.16252</link>
<guid>https://arxiv.org/abs/2508.16252</guid>
<content:encoded><![CDATA[
arXiv:2508.16252v1 Announce Type: cross 
Abstract: Patients undergoing a mechanical thrombectomy procedure usually have a multi-detector CT (MDCT) scan before and after the intervention. The image quality of the flat panel detector CT (FDCT) present in the intervention room is generally much lower than that of a MDCT due to significant artifacts. However, using only FDCT images could improve patient management as the patient would not need to be moved to the MDCT room. Several studies have evaluated the potential use of FDCT imaging alone and the time that could be saved by acquiring the images before and/or after the intervention only with the FDCT. This study proposes using a denoising diffusion probabilistic model (DDPM) to improve the image quality of FDCT scans, making them comparable to MDCT scans. Clinicans evaluated FDCT, MDCT, and our model's predictions for diagnostic purposes using a questionnaire. The DDPM eliminated most artifacts and improved anatomical visibility without reducing bleeding detection, provided that the input FDCT image quality is not too low. Our code can be found on github.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RotaTouille: Rotation Equivariant Deep Learning for Contours</title>
<link>https://arxiv.org/abs/2508.16359</link>
<guid>https://arxiv.org/abs/2508.16359</guid>
<content:encoded><![CDATA[
arXiv:2508.16359v1 Announce Type: cross 
Abstract: Contours or closed planar curves are common in many domains. For example, they appear as object boundaries in computer vision, isolines in meteorology, and the orbits of rotating machinery. In many cases when learning from contour data, planar rotations of the input will result in correspondingly rotated outputs. It is therefore desirable that deep learning models be rotationally equivariant. In addition, contours are typically represented as an ordered sequence of edge points, where the choice of starting point is arbitrary. It is therefore also desirable for deep learning methods to be equivariant under cyclic shifts. We present RotaTouille, a deep learning framework for learning from contour data that achieves both rotation and cyclic shift equivariance through complex-valued circular convolution. We further introduce and characterize equivariant non-linearities, coarsening layers, and global pooling layers to obtain invariant representations for downstream tasks. Finally, we demonstrate the effectiveness of RotaTouille through experiments in shape classification, reconstruction, and contour regression.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents</title>
<link>https://arxiv.org/abs/2508.16414</link>
<guid>https://arxiv.org/abs/2508.16414</guid>
<content:encoded><![CDATA[
arXiv:2508.16414v1 Announce Type: cross 
Abstract: Understanding how prenatal exposure to psychoactive substances such as cannabis shapes adolescent brain organization remains a critical challenge, complicated by the complexity of multimodal neuroimaging data and the limitations of conventional analytic methods. Existing approaches often fail to fully capture the complementary features embedded within structural and functional connectomes, constraining both biological insight and predictive performance. To address this, we introduced NeuroKoop, a novel graph neural network-based framework that integrates structural and functional brain networks utilizing neural Koopman operator-driven latent space fusion. By leveraging Koopman theory, NeuroKoop unifies node embeddings derived from source-based morphometry (SBM) and functional network connectivity (FNC) based brain graphs, resulting in enhanced representation learning and more robust classification of prenatal drug exposure (PDE) status. Applied to a large adolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant baselines and revealed salient structural-functional connections, advancing our understanding of the neurodevelopmental impact of PDE.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding MGMT Methylation: A Step Towards Precision Medicine in Glioblastoma</title>
<link>https://arxiv.org/abs/2508.16424</link>
<guid>https://arxiv.org/abs/2508.16424</guid>
<content:encoded><![CDATA[
arXiv:2508.16424v1 Announce Type: cross 
Abstract: Glioblastomas, constituting over 50% of malignant brain tumors, are highly aggressive brain tumors that pose substantial treatment challenges due to their rapid progression and resistance to standard therapies. The methylation status of the O-6-Methylguanine-DNA Methyltransferase (MGMT) gene is a critical biomarker for predicting patient response to treatment, particularly with the alkylating agent temozolomide. However, accurately predicting MGMT methylation status using non-invasive imaging techniques remains challenging due to the complex and heterogeneous nature of glioblastomas, that includes, uneven contrast, variability within lesions, and irregular enhancement patterns. This study introduces the Convolutional Autoencoders for MGMT Methylation Status Prediction (CAMP) framework, which is based on adaptive sparse penalties to enhance predictive accuracy. The CAMP framework operates in two phases: first, generating synthetic MRI slices through a tailored autoencoder that effectively captures and preserves intricate tissue and tumor structures across different MRI modalities; second, predicting MGMT methylation status using a convolutional neural network enhanced by adaptive sparse penalties. The adaptive sparse penalty dynamically adjusts to variations in the data, such as contrast differences and tumor locations in MR images. Our method excels in MRI image synthesis, preserving brain tissue, fat, and individual tumor structures across all MRI modalities. Validated on benchmark datasets, CAMP achieved an accuracy of 0.97, specificity of 0.98, and sensitivity of 0.97, significantly outperforming existing methods. These results demonstrate the potential of the CAMP framework to improve the interpretation of MRI data and contribute to more personalized treatment strategies for glioblastoma patients.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Embedding Recomposition for Incremental Learning</title>
<link>https://arxiv.org/abs/2508.16463</link>
<guid>https://arxiv.org/abs/2508.16463</guid>
<content:encoded><![CDATA[
arXiv:2508.16463v1 Announce Type: cross 
Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization</title>
<link>https://arxiv.org/abs/2508.16479</link>
<guid>https://arxiv.org/abs/2508.16479</guid>
<content:encoded><![CDATA[
arXiv:2508.16479v1 Announce Type: cross 
Abstract: Histopathology remains the gold standard for cancer diagnosis and prognosis. With the advent of transcriptome profiling, multi-modal learning combining transcriptomics with histology offers more comprehensive information. However, existing multi-modal approaches are challenged by intrinsic multi-modal heterogeneity, insufficient multi-scale integration, and reliance on paired data, restricting clinical applicability. To address these challenges, we propose a disentangled multi-modal framework with four contributions: 1) To mitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into tumor and microenvironment subspaces using a disentangled multi-modal fusion module, and introduce a confidence-guided gradient coordination strategy to balance subspace optimization. 2) To enhance multi-scale integration, we propose an inter-magnification gene-expression consistency strategy that aligns transcriptomic signals across WSI magnifications. 3) To reduce dependency on paired data, we propose a subspace knowledge distillation strategy enabling transcriptome-agnostic inference through a WSI-only student model. 4) To improve inference efficiency, we propose an informative token aggregation module that suppresses WSI redundancy while preserving subspace semantics. Extensive experiments on cancer diagnosis, prognosis, and survival prediction demonstrate our superiority over state-of-the-art methods across multiple settings. Code is available at https://github.com/helenypzhang/Disentangled-Multimodal-Learning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine</title>
<link>https://arxiv.org/abs/2508.16553</link>
<guid>https://arxiv.org/abs/2508.16553</guid>
<content:encoded><![CDATA[
arXiv:2508.16553v1 Announce Type: cross 
Abstract: In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.16557</link>
<guid>https://arxiv.org/abs/2508.16557</guid>
<content:encoded><![CDATA[
arXiv:2508.16557v1 Announce Type: cross 
Abstract: Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</title>
<link>https://arxiv.org/abs/2508.16568</link>
<guid>https://arxiv.org/abs/2508.16568</guid>
<content:encoded><![CDATA[
arXiv:2508.16568v1 Announce Type: cross 
Abstract: Foundation models (FMs) exhibit remarkable generalization but require adaptation to downstream tasks, particularly in privacy-sensitive applications. Due to data privacy regulations, cloud-based FMs cannot directly access private edge data, limiting their adaptation. Federated learning (FL) provides a privacy-aware alternative, but existing FL approaches overlook the constraints imposed by edge devices -- namely, limited computational resources and the scarcity of labeled data. To address these challenges, we introduce Practical Semi-Supervised Federated Learning (PSSFL), where edge devices hold only unlabeled, low-resolution data, while the server has limited labeled, high-resolution data. In this setting, we propose the Federated Mixture of Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox tackles computational and resolution mismatch challenges via a sparse Mixture-of-Experts architecture, employing a spatial router to align features across resolutions and a Soft-Mixture strategy to stabilize semi-supervised learning. We take object detection as a case study, and experiments on real-world autonomous driving datasets demonstrate that FedMox effectively adapts FMs under PSSFL, significantly improving performance with constrained memory costs on edge devices. Our work paves the way for scalable and privacy-preserving FM adaptation in federated scenarios.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer</title>
<link>https://arxiv.org/abs/2508.16569</link>
<guid>https://arxiv.org/abs/2508.16569</guid>
<content:encoded><![CDATA[
arXiv:2508.16569v1 Announce Type: cross 
Abstract: The non-invasive assessment of increasingly incidentally discovered renal masses is a critical challenge in urologic oncology, where diagnostic uncertainty frequently leads to the overtreatment of benign or indolent tumors. In this study, we developed and validated RenalCLIP using a dataset of 27,866 CT scans from 8,809 patients across nine Chinese medical centers and the public TCIA cohort, a visual-language foundation model for characterization, diagnosis and prognosis of renal mass. The model was developed via a two-stage pre-training strategy that first enhances the image and text encoders with domain-specific knowledge before aligning them through a contrastive learning objective, to create robust representations for superior generalization and diagnostic precision. RenalCLIP achieved better performance and superior generalizability across 10 core tasks spanning the full clinical workflow of kidney cancer, including anatomical assessment, diagnostic classification, and survival prediction, compared with other state-of-the-art general-purpose CT foundation models. Especially, for complicated task like recurrence-free survival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726, representing a substantial improvement of approximately 20% over the leading baselines. Furthermore, RenalCLIP's pre-training imparted remarkable data efficiency; in the diagnostic classification task, it only needs 20% training data to achieve the peak performance of all baseline models even after they were fully fine-tuned on 100% of the data. Additionally, it achieved superior performance in report generation, image-text retrieval and zero-shot diagnosis tasks. Our findings establish that RenalCLIP provides a robust tool with the potential to enhance diagnostic accuracy, refine prognostic stratification, and personalize the management of patients with kidney cancer.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Correspondence Matching for Generalizable Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2304.12294</link>
<guid>https://arxiv.org/abs/2304.12294</guid>
<content:encoded><![CDATA[
arXiv:2304.12294v2 Announce Type: replace 
Abstract: We present a new generalizable NeRF method that is able to directly generalize to new unseen scenarios and perform novel view synthesis with as few as two source views. The key to our approach lies in the explicitly modeled correspondence matching information, so as to provide the geometry prior to the prediction of NeRF color and density for volume rendering. The explicit correspondence matching is quantified with the cosine similarity between image features sampled at the 2D projections of a 3D point on different views, which is able to provide reliable cues about the surface geometry. Unlike previous methods where image features are extracted independently for each view, we consider modeling the cross-view interactions via Transformer cross-attention, which greatly improves the feature matching quality. Our method achieves state-of-the-art results on different evaluation settings, with the experiments showing a strong correlation between our learned cosine feature similarity and volume density, demonstrating the effectiveness and superiority of our proposed method. The code and model are on our project page: https://donydchen.github.io/matchnerf
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIB-KD: Teaching Inductive Bias for Efficient Vision Transformer Distillation and Compression</title>
<link>https://arxiv.org/abs/2310.00369</link>
<guid>https://arxiv.org/abs/2310.00369</guid>
<content:encoded><![CDATA[
arXiv:2310.00369v4 Announce Type: replace 
Abstract: With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalising prospect of unified information processing across visual and textual domains due to the lack of inherent inductive biases in ViTs. ViTs require enormous datasets for training. We introduce an innovative ensemble-based distillation approach that distils inductive bias from complementary lightweight teacher models to make their applications practical. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to jointly instruct the student transformer. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework LIB-KD also involves precomputing and keeping logits in advance, essentially the unnormalized predictions of the model. This optimisation can accelerate the distillation process by eliminating the need for repeated forward passes during knowledge distillation, significantly reducing the computational burden and enhancing efficiency.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric-Aware Low-Light Image and Video Enhancement via Depth Guidance</title>
<link>https://arxiv.org/abs/2312.15855</link>
<guid>https://arxiv.org/abs/2312.15855</guid>
<content:encoded><![CDATA[
arXiv:2312.15855v2 Announce Type: replace 
Abstract: Low-Light Enhancement (LLE) is aimed at improving the quality of photos/videos captured under low-light conditions. It is worth noting that most existing LLE methods do not take advantage of geometric modeling. We believe that incorporating geometric information can enhance LLE performance, as it provides insights into the physical structure of the scene that influences illumination conditions. To address this, we propose a Geometry-Guided Low-Light Enhancement Refine Framework (GG-LLERF) designed to assist low-light enhancement models in learning improved features for LLE by integrating geometric priors into the feature representation space. In this paper, we employ depth priors as the geometric representation. Our approach focuses on the integration of depth priors into various LLE frameworks using a unified methodology. This methodology comprises two key novel modules. First, a depth-aware feature extraction module is designed to inject depth priors into the image representation. Then, Hierarchical Depth-Guided Feature Fusion Module (HDGFFM) is formulated with a cross-domain attention mechanism, which combines depth-aware features with the original image features within the LLE model. We conducted extensive experiments on public low-light image and video enhancement benchmarks. The results illustrate that our designed framework significantly enhances existing LLE methods.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</title>
<link>https://arxiv.org/abs/2406.02462</link>
<guid>https://arxiv.org/abs/2406.02462</guid>
<content:encoded><![CDATA[
arXiv:2406.02462v3 Announce Type: replace 
Abstract: Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems. First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized Gaussian Splatting Editing with Contextual Awareness</title>
<link>https://arxiv.org/abs/2408.00083</link>
<guid>https://arxiv.org/abs/2408.00083</guid>
<content:encoded><![CDATA[
arXiv:2408.00083v2 Announce Type: replace 
Abstract: Recent text-guided generation of individual 3D object has achieved great success using diffusion priors. However, these methods are not suitable for object insertion and replacement tasks as they do not consider the background, leading to illumination mismatches within the environment. To bridge the gap, we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian Splatting (3DGS) representation. Our key observation is that inpainting by the state-of-the-art conditional 2D diffusion model is consistent with background in lighting. To leverage the prior knowledge from the well-trained diffusion models for 3D object generation, our approach employs a coarse-to-fine objection optimization pipeline with inpainted views. In the first coarse step, we achieve image-to-3D lifting given an ideal inpainted view. The process employs 3D-aware diffusion prior from a view-conditioned diffusion model, which preserves illumination present in the conditioning image. To acquire an ideal inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a single view that best represents the scene illumination in target region. In the second Texture Enhancement step, we introduce a novel Depth-guided Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and texture details with the inpainting diffusion prior, beyond the scope of the 3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only provides fine-grained texture enhancement, but also urges optimization to respect scene lighting. Our approach efficiently achieves local editing with global illumination consistency without explicitly modeling light transport. We demonstrate robustness of our method by evaluating editing in real scenes containing explicit highlight and shadows, and compare against the state-of-the-art text-to-3D editing methods.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Dataset for Video-Based Neurodivergent Classification Leveraging Extra-Stimulatory Behavior</title>
<link>https://arxiv.org/abs/2409.04598</link>
<guid>https://arxiv.org/abs/2409.04598</guid>
<content:encoded><![CDATA[
arXiv:2409.04598v2 Announce Type: replace 
Abstract: Facial expressions and actions differ among different individuals at varying degrees of intensity given responses to external stimuli, particularly among those that are neurodivergent. Such behaviors affect people in terms of overall health, communication, and sensory processing. Deep learning can be responsibly leveraged to improve productivity in addressing this task, and help medical professionals to accurately understand such behaviors. In this work, we introduce the Video ASD dataset-a dataset that contains video frame convolutional and attention map feature data-to foster further progress in the task of ASD classification. Unlike many recent studies in ASD classification with MRI data, which require expensive specialized equipment, our method utilizes a powerful but relatively affordable GPU, a standard computer setup, and a video camera for inference. Results show that our model effectively generalizes and understands key differences in the distinct movements of the children. Additionally, we test foundation models on this data to showcase how movement noise affects performance and the need for more data and more complex labels.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text Alignment</title>
<link>https://arxiv.org/abs/2409.14336</link>
<guid>https://arxiv.org/abs/2409.14336</guid>
<content:encoded><![CDATA[
arXiv:2409.14336v2 Announce Type: replace 
Abstract: Zero-shot action recognition, which addresses the issue of scalability and generalization in action recognition and allows the models to adapt to new and unseen actions dynamically, is an important research topic in computer vision communities. The key to zero-shot action recognition lies in aligning visual features with semantic vectors representing action categories. Most existing methods either directly project visual features onto the semantic space of text category or learn a shared embedding space between the two modalities. However, a direct projection cannot accurately align the two modalities, and learning robust and discriminative embedding space between visual and text representations is often difficult. To address these issues, we introduce Dual Visual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition. The DVTA consists of two alignment modules--Direct Alignment (DA) and Augmented Alignment (AA)--along with a designed Semantic Description Enhancement (SDE). The DA module maps the skeleton features to the semantic space through a specially designed visual projector, followed by the SDE, which is based on cross-attention to enhance the connection between skeleton and text, thereby reducing the gap between modalities. The AA module further strengthens the learning of the embedding space by utilizing deep metric learning to learn the similarity between skeleton and text. Our approach achieves state-of-the-art performances on several popular zero-shot skeleton-based action recognition benchmarks. The code is available at: https://github.com/jidongkuang/DVTA.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The unrealized potential of agroforestry for an emissions-intensive agricultural commodity</title>
<link>https://arxiv.org/abs/2410.20882</link>
<guid>https://arxiv.org/abs/2410.20882</guid>
<content:encoded><![CDATA[
arXiv:2410.20882v2 Announce Type: replace 
Abstract: Reconciling agricultural production with climate-change mitigation is a formidable sustainability problem. Retaining trees in agricultural systems is one proposed solution, but the magnitude of the current and future-potential benefit that trees contribute to climate-change mitigation remains uncertain. Here, we help to resolve these issues across a West African region that produces ~60% of the world's cocoa, a crop contributing one of the highest carbon footprints of all foods. Using machine learning, we mapped shade-tree cover and carbon stocks across the region and found that existing average cover is low (~13%) and poorly aligned with climate threats. Yet, increasing shade-tree cover to a minimum of 30% could sequester an additional 307 million tonnes of CO2e, enough to offset ~167% of contemporary cocoa-related emissions in Ghana and C\^ote d'Ivoire--without reducing production. Our approach is transferable to other shade-grown crops and aligns with emerging carbon market and sustainability reporting frameworks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LBONet: Supervised Spectral Descriptors for Shape Analysis</title>
<link>https://arxiv.org/abs/2411.08272</link>
<guid>https://arxiv.org/abs/2411.08272</guid>
<content:encoded><![CDATA[
arXiv:2411.08272v3 Announce Type: replace 
Abstract: The Laplace-Beltrami operator has established itself in the field of non-rigid shape analysis due to its many useful properties such as being invariant under isometric transformation, having a countable eigensystem forming an orthornormal basis, and fully characterizing geodesic distances of the manifold. However, this invariancy only applies under isometric deformations, which leads to a performance breakdown in many real-world applications. In recent years emphasis has been placed upon extracting optimal features using deep learning methods,however spectral signatures play a crucial role and still add value. In this paper we take a step back, revisiting the LBO and proposing a supervised way to learn several operators on a manifold. Depending on the task, by applying these functions, we can train the LBO eigenbasis to be more task-specific. The optimization of the LBO leads to enormous improvements to established descriptors such as the heat kernel signature in various tasks such as retrieval, classification, segmentation, and correspondence, proving the adaption of the LBO eigenbasis to both global and highly local learning settings.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Density Control for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2411.10133</link>
<guid>https://arxiv.org/abs/2411.10133</guid>
<content:encoded><![CDATA[
arXiv:2411.10133v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated outstanding performance in novel view synthesis, achieving a balance between rendering quality and real-time performance. 3DGS employs Adaptive Density Control (ADC) to increase the number of Gaussians. However, the clone and split operations within ADC are not sufficiently efficient, impacting optimization speed and detail recovery. Additionally, overfitted Gaussians that affect rendering quality may exist, and the original ADC is unable to remove them. To address these issues, we propose two key innovations: (1) Long-Axis Split, which precisely controls the position, shape, and opacity of child Gaussians to minimize the difference before and after splitting. (2) Recovery-Aware Pruning, which leverages differences in recovery speed after resetting opacity to prune overfitted Gaussians, thereby improving generalization performance. Experimental results show that our method significantly enhances rendering quality. Due to resubmission reasons, this version has been abandoned. The improved version is available at https://xiaobin2001.github.io/improved-gs-web .
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature Extraction and Interaction with Low-Resolution Images</title>
<link>https://arxiv.org/abs/2412.02197</link>
<guid>https://arxiv.org/abs/2412.02197</guid>
<content:encoded><![CDATA[
arXiv:2412.02197v3 Announce Type: replace 
Abstract: In real-world applications of image recognition tasks, such as human pose estimation, cameras often capture objects, like human bodies, at low resolutions. This scenario poses a challenge in extracting and leveraging multi-scale features, which is often essential for precise inference. To address this challenge, we propose a new attention mechanism, named cascaded multi-scale attention (CMSA), tailored for use in CNN-ViT hybrid architectures, to handle low-resolution inputs effectively. The design of CMSA enables the extraction and seamless integration of features across various scales without necessitating the downsampling of the input image or feature maps. This is achieved through a novel combination of grouped multi-head self-attention mechanisms with window-based local attention and cascaded fusion of multi-scale features over different scales. This architecture allows for the effective handling of features across different scales, enhancing the model's ability to perform tasks such as human pose estimation, head pose estimation, and more with low-resolution images. Our experimental results show that the proposed method outperforms existing state-of-the-art methods in these areas with fewer parameters, showcasing its potential for broad application in real-world scenarios where capturing high-resolution images is not feasible. Code is available at https://github.com/xyongLu/CMSA.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation</title>
<link>https://arxiv.org/abs/2412.11183</link>
<guid>https://arxiv.org/abs/2412.11183</guid>
<content:encoded><![CDATA[
arXiv:2412.11183v2 Announce Type: replace 
Abstract: Recent diffusion models have demonstrated remarkable performance in both 3D scene generation and perception tasks. Nevertheless, existing methods typically separate these two processes, acting as a data augmenter to generate synthetic data for downstream perception tasks. In this work, we propose OccScene, a novel mutual learning paradigm that integrates fine-grained 3D perception and high-quality generation in a unified framework, achieving a cross-task win-win effect. OccScene generates new and consistent 3D realistic scenes only depending on text prompts, guided with semantic occupancy in a joint-training diffusion framework. To align the occupancy with the diffusion latent, a Mamba-based Dual Alignment module is introduced to incorporate fine-grained semantics and geometry as perception priors. Within OccScene, the perception module can be effectively improved with customized and diverse generated scenes, while the perception priors in return enhance the generation performance for mutual benefits. Extensive experiments show that OccScene achieves realistic 3D scene generation in broad indoor and outdoor scenarios, while concurrently boosting the perception models to achieve substantial performance improvements in the 3D perception task of semantic occupancy prediction.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Knowledge-Preserving Decomposition with Adaptive Layer Selection for Few-Shot Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2501.05017</link>
<guid>https://arxiv.org/abs/2501.05017</guid>
<content:encoded><![CDATA[
arXiv:2501.05017v3 Announce Type: replace 
Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces a critical challenge: balancing the retention of prior knowledge with the acquisition of new classes. Existing methods either freeze the backbone to prevent catastrophic forgetting, sacrificing plasticity, or add new modules, incurring high costs. These approaches treat pretrained models as black boxes, overlooking two key opportunities to exploit their internal capacity: reusing redundant representational space within layers and selectively adapting layers based on their sensitivity to forgetting. We propose CKPD-FSCIL, a unified framework that unlocks the underutilized capacity of pretrained weights, achieving a superior stability-plasticity balance with zero inference overhead. Our design integrates two continuously adapting mechanisms: At the weight level, a Continuous Knowledge-Preserving Decomposition mechanism uses feature covariance to split each weight matrix into a frozen subspace that safeguards prior knowledge and a learnable, redundant subspace for new tasks. At the layer level, a Continuous Adaptive Layer Selection mechanism leverages an Adapter Sensitivity Ratio to automatically select layers with the highest redundant capacity and lowest forgetting risk for adaptation. By targeting only safe, high-potential subspaces and layers, CKPD-FSCIL enables efficient adaptation. After each session, the learned adapters are merged back into the original weights, ensuring zero additional parameters or FLOPs during inference. Extensive experiments on multiple FSCIL benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in both adaptability and knowledge retention. The code is available at https://github.com/xiaojieli0903/CKPD-FSCIL.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review of Demographic Fairness in Face Recognition</title>
<link>https://arxiv.org/abs/2502.02309</link>
<guid>https://arxiv.org/abs/2502.02309</guid>
<content:encoded><![CDATA[
arXiv:2502.02309v3 Announce Type: replace 
Abstract: Demographic fairness in face recognition (FR) has emerged as a critical area of research, given its impact on fairness, equity, and reliability across diverse applications. As FR technologies are increasingly deployed globally, disparities in performance across demographic groups -- such as race, ethnicity, and gender -- have garnered significant attention. These biases not only compromise the credibility of FR systems but also raise ethical concerns, especially when these technologies are employed in sensitive domains. This review consolidates extensive research efforts providing a comprehensive overview of the multifaceted aspects of demographic fairness in FR.
  We systematically examine the primary causes, datasets, assessment metrics, and mitigation approaches associated with demographic disparities in FR. By categorizing key contributions in these areas, this work provides a structured approach to understanding and addressing the complexity of this issue. Finally, we highlight current advancements and identify emerging challenges that need further investigation. This article aims to provide researchers with a unified perspective on the state-of-the-art while emphasizing the critical need for equitable and trustworthy FR systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion</title>
<link>https://arxiv.org/abs/2502.06860</link>
<guid>https://arxiv.org/abs/2502.06860</guid>
<content:encoded><![CDATA[
arXiv:2502.06860v3 Announce Type: replace 
Abstract: The ability to automatically complete a partial sketch that depicts a complex scene, e.g., "a woman chatting with a man in the park", is very useful. However, existing sketch generation methods create sketches from scratch; they do not complete a partial sketch in the style of the original. To address this challenge, we introduce AutoSketch, a styleaware vector sketch completion method that accommodates diverse sketch styles. Our key observation is that the style descriptions of a sketch in natural language preserve the style during automatic sketch completion. Thus, we use a pretrained vision-language model (VLM) to describe the styles of the partial sketches in natural language and replicate these styles using newly generated strokes. We initially optimize the strokes to match an input prompt augmented by style descriptions extracted from the VLM. Such descriptions allow the method to establish a diffusion prior in close alignment with that of the partial sketch. Next, we utilize the VLM to generate an executable style adjustment code that adjusts the strokes to conform to the desired style. We compare our method with existing methods across various sketch styles and prompts, performed extensive ablation studies and qualitative and quantitative evaluations, and demonstrate that AutoSketch can support various sketch scenarios.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LBM: Latent Bridge Matching for Fast Image-to-Image Translation</title>
<link>https://arxiv.org/abs/2503.07535</link>
<guid>https://arxiv.org/abs/2503.07535</guid>
<content:encoded><![CDATA[
arXiv:2503.07535v2 Announce Type: replace 
Abstract: In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation. We provide an implementation at https://github.com/gojasper/LBM.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-3D Generation using Jensen-Shannon Score Distillation</title>
<link>https://arxiv.org/abs/2503.10660</link>
<guid>https://arxiv.org/abs/2503.10660</guid>
<content:encoded><![CDATA[
arXiv:2503.10660v3 Announce Type: replace 
Abstract: Score distillation sampling is an effective technique to generate 3D models from text prompts, utilizing pre-trained large-scale text-to-image diffusion models as guidance. However, the produced 3D assets tend to be over-saturating, over-smoothing, with limited diversity. These issues are results from a reverse Kullback-Leibler (KL) divergence objective, which makes the optimization unstable and results in mode-seeking behavior. In this paper, we derive a bounded score distillation objective based on Jensen-Shannon divergence (JSD), which stabilizes the optimization process and produces high-quality 3D generation. JSD can match well generated and target distribution, therefore mitigating mode seeking. We provide a practical implementation of JSD by utilizing the theory of generative adversarial networks to define an approximate objective function for the generator, assuming the discriminator is well trained. By assuming the discriminator following a log-odds classifier, we propose a minority sampling algorithm to estimate the gradients of our proposed objective, providing a practical implementation for JSD. We conduct both theoretical and empirical studies to validate our method. Experimental results on T3Bench demonstrate that our method can produce high-quality and diversified 3D assets.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2503.11652</link>
<guid>https://arxiv.org/abs/2503.11652</guid>
<content:encoded><![CDATA[
arXiv:2503.11652v2 Announce Type: replace 
Abstract: Egocentric 3D human pose estimation has been actively studied using cameras installed in front of a head-mounted device (HMD). While frontal placement is the optimal and the only option for some tasks, such as hand tracking, it remains unclear if the same holds for full-body tracking due to self-occlusion and limited field-of-view coverage. Notably, even the state-of-the-art methods often fail to estimate accurate 3D poses in many scenarios, such as when HMD users tilt their heads upward -- a common motion in human activities. A key limitation of existing HMD designs is their neglect of the back of the body, despite its potential to provide crucial 3D reconstruction cues. Hence, this paper investigates the usefulness of rear cameras for full-body tracking. We also show that simply adding rear views to the frontal inputs is not optimal for existing methods due to their dependence on individual 2D joint detectors without effective multi-view integration. To address this issue, we propose a new transformer-based method that refines 2D joint heatmap estimation with multi-view information and heatmap uncertainty, thereby improving 3D pose tracking. Also, we introduce two new large-scale datasets, Ego4View-Syn and Ego4View-RW, for a rear-view evaluation. Our experiments show that the new camera configurations with back views provide superior support for 3D pose tracking compared to only frontal placements. The proposed method achieves significant improvement over the current state of the art (>10% on MPJPE). The source code, trained models, and datasets are available on our project page at https://4dqv.mpi-inf.mpg.de/EgoRear/.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaIC: State Space Models for High-Performance Learned Image Compression</title>
<link>https://arxiv.org/abs/2503.12461</link>
<guid>https://arxiv.org/abs/2503.12461</guid>
<content:encoded><![CDATA[
arXiv:2503.12461v3 Announce Type: replace 
Abstract: A high-performance image compression algorithm is crucial for real-time information transmission across numerous fields. Despite rapid progress in image compression, computational inefficiency and poor redundancy modeling still pose significant bottlenecks, limiting practical applications. Inspired by the effectiveness of state space models (SSMs) in capturing long-range dependencies, we leverage SSMs to address computational inefficiency in existing methods and improve image compression from multiple perspectives. In this paper, we integrate the advantages of SSMs for better efficiency-performance trade-off and propose an enhanced image compression approach through refined context modeling, which we term MambaIC. Specifically, we explore context modeling to adaptively refine the representation of hidden states. Additionally, we introduce window-based local attention into channel-spatial entropy modeling to reduce potential spatial redundancy during compression, thereby increasing efficiency. Comprehensive qualitative and quantitative results validate the effectiveness and efficiency of our approach, particularly for high-resolution image compression. Code is released at https://github.com/AuroraZengfh/MambaIC.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Multi-Order Graph Regularized NMF with Dual Sparsity for Hyperspectral Unmixing</title>
<link>https://arxiv.org/abs/2503.19258</link>
<guid>https://arxiv.org/abs/2503.19258</guid>
<content:encoded><![CDATA[
arXiv:2503.19258v2 Announce Type: replace 
Abstract: Hyperspectral unmixing (HU) is a critical yet challenging task in remote sensing. However, existing nonnegative matrix factorization (NMF) methods with graph learning mostly focus on first-order or second-order nearest neighbor relationships and usually require manual parameter tuning, which fails to characterize intrinsic data structures. To address the above issues, we propose a novel adaptive multi-order graph regularized NMF method (MOGNMF) with three key features. First, multi-order graph regularization is introduced into the NMF framework to exploit global and local information comprehensively. Second, these parameters associated with the multi-order graph are learned adaptively through a data-driven approach. Third, dual sparsity is embedded to obtain better robustness, i.e., $\ell_{1/2}$-norm on the abundance matrix and $\ell_{2,1}$-norm on the noise matrix. To solve the proposed model, we develop an alternating minimization algorithm whose subproblems have explicit solutions, thus ensuring effectiveness. Experiments on simulated and real hyperspectral data indicate that the proposed method delivers better unmixing results.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception</title>
<link>https://arxiv.org/abs/2504.16616</link>
<guid>https://arxiv.org/abs/2504.16616</guid>
<content:encoded><![CDATA[
arXiv:2504.16616v3 Announce Type: replace 
Abstract: Event cameras, with microsecond temporal resolution and high dynamic range (HDR) characteristics, emit high-speed event stream for perception tasks. Despite the recent advancement in GNN-based perception methods, they are prone to use straightforward pairwise connectivity mechanisms in the pure Euclidean space where they struggle to capture long-range dependencies and fail to effectively characterize the inherent hierarchical structures of non-uniformly distributed event stream. To this end, in this paper we propose a novel approach named EHGCN, which is a pioneer to perceive event stream in both Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an adaptive sampling strategy to dynamically regulate sampling rates, retaining discriminative events while attenuating chaotic noise. Then we present a Markov Vector Field (MVF)-driven motion-aware hyperedge generation method based on motion state transition probabilities, thereby eliminating cross-target spurious associations and providing critically topological priors while capturing long-range dependencies between events. Finally, we propose a Euclidean-Hyperbolic GCN to fuse the information locally aggregated and globally hierarchically modeled in Euclidean and hyperbolic spaces, respectively, to achieve hybrid event perception. Experimental results on event perception tasks such as object detection and recognition validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</title>
<link>https://arxiv.org/abs/2504.17371</link>
<guid>https://arxiv.org/abs/2504.17371</guid>
<content:encoded><![CDATA[
arXiv:2504.17371v3 Announce Type: replace 
Abstract: Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention</title>
<link>https://arxiv.org/abs/2505.17097</link>
<guid>https://arxiv.org/abs/2505.17097</guid>
<content:encoded><![CDATA[
arXiv:2505.17097v2 Announce Type: replace 
Abstract: Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications. However, ICL remains unstable, even with well-matched in-context demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the provided context. While existing efforts focus on prompt engineering or post-hoc logit calibration, we instead investigate the underlying attention dynamics to overcome LVLMs' inherent limitations. We identify two critical deficits in their self-attention that impair effective ICL. To bridge the gap, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play and training-free method that dynamically modulates LVLM's attention logits based on the input in-context sequence. CAMA employs a two-stage attention modulation to address both identified deficits, enhancing the focus on semantically significant tokens, particularly visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, demonstrating great effectiveness and generalization. It can also activate the desired effects of prompt engineering methods and remains robust under diverse sequence configurations. Thus, CAMA paves the way for deeper explorations of attention dynamics to advance multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR</title>
<link>https://arxiv.org/abs/2505.17423</link>
<guid>https://arxiv.org/abs/2505.17423</guid>
<content:encoded><![CDATA[
arXiv:2505.17423v2 Announce Type: replace 
Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still require human supervision. For example, tasks like traffic officers reviewing hour-long dashcam footage or researchers screening conference videos can benefit from concise summaries that reduce cognitive load and save time. Yet current vision-language models (VLMs) often produce verbose, redundant outputs that hinder task performance. Existing video caption evaluation depends on costly human annotations and overlooks the summaries' utility in downstream tasks. We address these gaps with Video-to-text Information Bottleneck Evaluation (VIBE), an annotation-free method that scores VLM outputs using two metrics: grounding (how well the summary aligns with visual content) and utility (how informative it is for the task). VIBE selects from randomly sampled VLM outputs by ranking them according to the two scores to support effective human decision-making. Human studies on LearningPaper24, SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE consistently improve performance-boosting task accuracy by up to 61.23% and reducing response time by 75.77% compared to naive VLM summaries or raw video.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement</title>
<link>https://arxiv.org/abs/2507.07908</link>
<guid>https://arxiv.org/abs/2507.07908</guid>
<content:encoded><![CDATA[
arXiv:2507.07908v2 Announce Type: replace 
Abstract: Remote physiological measurement (RPM) has emerged as a promising non-invasive method for monitoring physiological signals using the non-contact device. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based RPM models in unseen deployment environments, considerations in aspects such as privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for RPM tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of BVP signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration (\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.01225</link>
<guid>https://arxiv.org/abs/2508.01225</guid>
<content:encoded><![CDATA[
arXiv:2508.01225v2 Announce Type: replace 
Abstract: In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance. Project Page available at: https://zhaihaotian.github.io/MCP-ICCV25/
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPSv3: Towards Wide-Spectrum Human Preference Score</title>
<link>https://arxiv.org/abs/2508.03789</link>
<guid>https://arxiv.org/abs/2508.03789</guid>
<content:encoded><![CDATA[
arXiv:2508.03789v2 Announce Type: replace 
Abstract: Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Training for Handwritten Mathematical Expression Recognition</title>
<link>https://arxiv.org/abs/2508.09220</link>
<guid>https://arxiv.org/abs/2508.09220</guid>
<content:encoded><![CDATA[
arXiv:2508.09220v2 Announce Type: replace 
Abstract: Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression \textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily due to the arduous and costly process of manual annotation. To bridge this gap, we propose a novel method integrating limited handwritten formulas with large-scale LaTeX-rendered formulas by developing a scalable data engine to generate complex and consistent LaTeX sequences. With this engine, we built the largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80 million high-quality training instances. Then we propose \texttt{TexTeller}, the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a relatively small HME dataset. The expansive training dataset and our refined pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA) performance across nearly all benchmarks. To advance the field, we will openly release our complete model, entire dataset, and full codebase, enabling further research building upon our contributions.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blink-to-code: real-time Morse code communication via eye blink detection and classification</title>
<link>https://arxiv.org/abs/2508.09344</link>
<guid>https://arxiv.org/abs/2508.09344</guid>
<content:encoded><![CDATA[
arXiv:2508.09344v2 Announce Type: replace 
Abstract: This study proposes a real-time system that translates voluntary eye blinks into Morse code, enabling communication for individuals with severe motor impairments. Using a standard webcam and computer vision, the system detects and classifies blinks as short (dot) or long (dash), then decodes them into alphanumeric characters. Experiments with five participants show 62% decoding accuracy and 18-20 seconds response times, demonstrating a viable, low-cost assistive communication method.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</title>
<link>https://arxiv.org/abs/2508.09977</link>
<guid>https://arxiv.org/abs/2508.09977</guid>
<content:encoded><![CDATA[
arXiv:2508.09977v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection</title>
<link>https://arxiv.org/abs/2402.17018</link>
<guid>https://arxiv.org/abs/2402.17018</guid>
<content:encoded><![CDATA[
arXiv:2402.17018v2 Announce Type: replace-cross 
Abstract: We experimented with front-end enhanced neural models where a differentiable and fully convolutional model with a skip connection is added before a frozen backbone classifier. By training such composite models using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks-including APGD and FAB-T attacks from the AutoAttack package-which we attribute to gradient masking. Although gradient masking is not new, the degree we observe is striking for fully differentiable models without obvious gradient-shattering-e.g., JPEG compression-or gradient-diminishing components.
  The training recipe to produce such models is also remarkably stable and reproducible: We applied it to three datasets (CIFAR10, CIFAR100, and ImageNet) and several modern architectures (including vision Transformers) without a single failure case. While black-box attacks such as the SQUARE attack and zero-order PGD can partially overcome gradient masking, these attacks are easily defeated by simple randomized ensembles. We estimate that these ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet (while retaining almost all clean accuracy of the original classifiers) despite having near-zero accuracy under adaptive attacks.
  Adversarially training the backbone further amplifies this front-end "robustness". On CIFAR10, the respective randomized ensemble achieved 90.8$\pm 2.5\%$ (99\% CI) accuracy under the full AutoAttack while having only 18.2$\pm 3.6\%$ accuracy under the adaptive attack ($\varepsilon=8/255$, $L^\infty$ norm). We conclude the paper with a discussion of whether randomized ensembling can serve as a practical defense.
  Code and instructions to reproduce key results are available. https://github.com/searchivarius/curious_case_of_gradient_masking
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment of Diffusion Models: Fundamentals, Challenges, and Future</title>
<link>https://arxiv.org/abs/2409.07253</link>
<guid>https://arxiv.org/abs/2409.07253</guid>
<content:encoded><![CDATA[
arXiv:2409.07253v3 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions and generate results with undesired properties or even harmful content. Inspired by the success and popularity of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of text-to-image diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction</title>
<link>https://arxiv.org/abs/2505.05054</link>
<guid>https://arxiv.org/abs/2505.05054</guid>
<content:encoded><![CDATA[
arXiv:2505.05054v2 Announce Type: replace-cross 
Abstract: The computational imaging technique of Fourier Ptychographic Microscopy (FPM) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. However, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. Therefore, in this paper, we investigate the idea of classifying the image content in the FPM measurements directly without performing a reconstruction step first. We show that Convolutional Neural Networks (CNN) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. Furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Deep Learning for Geometry Problem Solving</title>
<link>https://arxiv.org/abs/2507.11936</link>
<guid>https://arxiv.org/abs/2507.11936</guid>
<content:encoded><![CDATA[
arXiv:2507.11936v5 Announce Type: replace-cross 
Abstract: Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving U-Net Confidence on TEM Image Data with L2-Regularization, Transfer Learning, and Deep Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.16779</link>
<guid>https://arxiv.org/abs/2507.16779</guid>
<content:encoded><![CDATA[
arXiv:2507.16779v2 Announce Type: replace-cross 
Abstract: With ever-increasing data volumes, it is essential to develop automated approaches for identifying nanoscale defects in transmission electron microscopy (TEM) images. However, compared to features in conventional photographs, nanoscale defects in TEM images exhibit far greater variation due to the complex contrast mechanisms and intricate defect structures. These challenges often result in much less labeled data and higher rates of annotation errors, posing significant obstacles to improving machine learning model performance for TEM image analysis. To address these limitations, we examined transfer learning by leveraging large, pre-trained models used for natural images.
  We demonstrated that by using the pre-trained encoder and L2-regularization, semantically complex features are ignored in favor of simpler, more reliable cues, substantially improving the model performance. However, this improvement cannot be captured by conventional evaluation metrics such as F1-score, which can be skewed by human annotation errors treated as ground truth. Instead, we introduced novel evaluation metrics that are independent of the annotation accuracy. Using grain boundary detection in UO2 TEM images as a case study, we found that our approach led to a 57% increase in defect detection rate, which is a robust and holistic measure of model performance on the TEM dataset used in this work. Finally, we showed that model self-confidence is only achieved through transfer learning and fine-tuning of very deep layers.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScrewSplat: An End-to-End Method for Articulated Object Recognition</title>
<link>https://arxiv.org/abs/2508.02146</link>
<guid>https://arxiv.org/abs/2508.02146</guid>
<content:encoded><![CDATA[
arXiv:2508.02146v2 Announce Type: replace-cross 
Abstract: Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model. See the project website at: https://screwsplat.github.io.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of 3D Counterfactual Brain MRI Generation</title>
<link>https://arxiv.org/abs/2508.02880</link>
<guid>https://arxiv.org/abs/2508.02880</guid>
<content:encoded><![CDATA[
arXiv:2508.02880v2 Announce Type: replace-cross 
Abstract: Counterfactual generation offers a principled framework for simulating hypothetical changes in medical imaging, with potential applications in understanding disease mechanisms and generating physiologically plausible data. However, generating realistic structural 3D brain MRIs that respect anatomical and causal constraints remains challenging due to data scarcity, structural complexity, and the lack of standardized evaluation protocols. In this work, we convert six generative models into 3D counterfactual approaches by incorporating an anatomy-guided framework based on a causal graph, in which regional brain volumes serve as direct conditioning inputs. Each model is evaluated with respect to composition, reversibility, realism, effectiveness and minimality on T1-weighted brain MRIs (T1w MRIs) from the Alzheimer's Disease Neuroimaging Initiative (ADNI). In addition, we test the generalizability of each model with respect to T1w MRIs of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Our results indicate that anatomically grounded conditioning successfully modifies the targeted anatomical regions; however, it exhibits limitations in preserving non-targeted structures. Beyond laying the groundwork for more interpretable and clinically relevant generative modeling of brain MRIs, this benchmark highlights the need for novel architectures that more accurately capture anatomical interdependencies.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Predictive Value of Preoperative MRI for Erectile Dysfunction Following Radical Prostatectomy</title>
<link>https://arxiv.org/abs/2508.03461</link>
<guid>https://arxiv.org/abs/2508.03461</guid>
<content:encoded><![CDATA[
arXiv:2508.03461v2 Announce Type: replace-cross 
Abstract: Accurate preoperative prediction of erectile dysfunction (ED) is important for counseling patients undergoing radical prostatectomy. While clinical features are established predictors, the added value of preoperative MRI remains underexplored. We investigate whether MRI provides additional predictive value for ED at 12 months post-surgery, evaluating four modeling strategies: (1) a clinical-only baseline, representing current state-of-the-art; (2) classical models using handcrafted anatomical features derived from MRI; (3) deep learning models trained directly on MRI slices; and (4) multimodal fusion of imaging and clinical inputs. Imaging-based models (maximum AUC 0.569) slightly outperformed handcrafted anatomical approaches (AUC 0.554) but fell short of the clinical baseline (AUC 0.663). Fusion models offered marginal gains (AUC 0.586) but did not exceed clinical-only performance. SHAP analysis confirmed that clinical features contributed most to predictive performance. Saliency maps from the best-performing imaging model suggested a predominant focus on anatomically plausible regions, such as the prostate and neurovascular bundles. While MRI-based models did not improve predictive performance over clinical features, our findings suggest that they try to capture patterns in relevant anatomical structures and may complement clinical predictors in future multimodal approaches.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedDino: A foundation model for red blood cell analysis</title>
<link>https://arxiv.org/abs/2508.08180</link>
<guid>https://arxiv.org/abs/2508.08180</guid>
<content:encoded><![CDATA[
arXiv:2508.08180v2 Announce Type: replace-cross 
Abstract: Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at https://github.com/Snarci/RedDino, and the pretrained models can be downloaded from our Hugging Face collection at https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heatmap Regression without Soft-Argmax for Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2508.14929</link>
<guid>https://arxiv.org/abs/2508.14929</guid>
<content:encoded><![CDATA[
<div> heatmap regression, facial landmark detection, soft-argmax, structured prediction framework, state-of-the-art performance

Summary:<br />
The article discusses facial landmark detection in computer vision and the use of heatmap regression-based methods for accurate results. It challenges the conventional approach of using Soft-argmax for end-to-end training on deep nets by proposing an alternative training objective based on the structured prediction framework. By revisiting this choice, the proposed method achieves state-of-the-art performance on three facial landmark benchmarks  WFLW, COFW, and 300W. The new approach enables faster convergence during training, 2.2 times faster specifically, while maintaining better or competitive accuracy. The code for the method is available on GitHub for reference and further exploration. The study showcases the potential of alternative training methods to enhance facial landmark detection performance and efficiency in computer vision applications. 

Summary: <br /> The article discusses facial landmark detection in computer vision and the use of heatmap regression-based methods for accurate results. It challenges the conventional approach of using Soft-argmax for end-to-end training on deep nets by proposing an alternative training objective based on the structured prediction framework. By revisiting this choice, the proposed method achieves state-of-the-art performance on three facial landmark benchmarks  WFLW, COFW, and 300W. The new approach enables faster convergence during training, 2.2 times faster specifically, while maintaining better or competitive accuracy. The code for the method is available on GitHub for reference and further exploration. The study showcases the potential of alternative training methods to enhance facial landmark detection performance and efficiency in computer vision applications.  <div>
arXiv:2508.14929v1 Announce Type: new 
Abstract: Facial landmark detection is an important task in computer vision with numerous applications, such as head pose estimation, expression analysis, face swapping, etc. Heatmap regression-based methods have been widely used to achieve state-of-the-art results in this task. These methods involve computing the argmax over the heatmaps to predict a landmark. Since argmax is not differentiable, these methods use a differentiable approximation, Soft-argmax, to enable end-to-end training on deep-nets. In this work, we revisit this long-standing choice of using Soft-argmax and demonstrate that it is not the only way to achieve strong performance. Instead, we propose an alternative training objective based on the classic structured prediction framework. Empirically, our method achieves state-of-the-art performance on three facial landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during training while maintaining better/competitive accuracy. Our code is available here: https://github.com/ca-joe-yang/regression-without-softarg.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Graph Neural Network for Image Classification</title>
<link>https://arxiv.org/abs/2508.14958</link>
<guid>https://arxiv.org/abs/2508.14958</guid>
<content:encoded><![CDATA[
<div> Keywords: Image classification, Graph Convolutional Networks, Voronoi diagrams, Delaunay triangulations, Computer vision <br />
Summary: <br />
- The study introduces a novel approach that integrates Graph Convolutional Networks (GCNs) with Voronoi diagrams for improved image classification.
- Images are represented as graphs, with pixels or regions as vertices, refined using Delaunay triangulations to optimize their representation.
- The proposed model shows significant improvements in preprocessing efficiency and classification accuracy across various benchmark datasets, outperforming existing methods.
- Particularly effective in complex scenes and fine-grained categories, the model surpasses state-of-the-art approaches.
- Experimental results, validated through cross-validation, demonstrate the effectiveness of combining GCNs with Voronoi diagrams in advancing image classification and expanding graph-based learning paradigms in computer vision and unstructured data analysis. <br /> <div>
arXiv:2508.14958v1 Announce Type: new 
Abstract: The rapid progress in image classification has been largely driven by the adoption of Graph Convolutional Networks (GCNs), which offer a robust framework for handling complex data structures. This study introduces a novel approach that integrates GCNs with Voronoi diagrams to enhance image classification by leveraging their ability to effectively model relational data. Unlike conventional convolutional neural networks (CNNs), our method represents images as graphs, where pixels or regions function as vertices. These graphs are then refined using corresponding Delaunay triangulations, optimizing their representation. The proposed model achieves significant improvements in both preprocessing efficiency and classification accuracy across various benchmark datasets, surpassing state-of-the-art approaches, particularly in challenging scenarios involving intricate scenes and fine-grained categories. Experimental results, validated through cross-validation, underscore the effectiveness of combining GCNs with Voronoi diagrams for advancing image classification. This research not only presents a novel perspective on image classification but also expands the potential applications of graph-based learning paradigms in computer vision and unstructured data analysis.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation</title>
<link>https://arxiv.org/abs/2508.14965</link>
<guid>https://arxiv.org/abs/2508.14965</guid>
<content:encoded><![CDATA[
<div> query-based framework, transformer detector, 9-DoF pose estimation, RGB-only, state of the art<br />
Summary:<br />
The article introduces YOPO, a single-stage, query-based framework for accurately recovering the 9-DoF pose of unseen instances from RGB images. YOPO unifies object detection and pose estimation at the category level without additional data, surpassing RGB-only methods and closing the gap to RGB-D systems. It consists of a transformer detector augmented with a lightweight pose head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian matching cost. Trained end-to-end with only RGB images and category-level pose labels, YOPO achieves state-of-the-art performance on three benchmarks, including an IoU of 79.6% on the REAL275 dataset and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric. The approach addresses the core challenge of accurate pose estimation for robotics and automation, providing a simpler and more efficient solution. Visit the project repository for code, models, and additional qualitative results. <br />Summary: <div>
arXiv:2508.14965v1 Announce Type: new 
Abstract: Accurately recovering the full 9-DoF pose of unseen instances within specific categories from a single RGB image remains a core challenge for robotics and automation. Most existing solutions still rely on pseudo-depth, CAD models, or multi-stage cascades that separate 2D detection from pose estimation. Motivated by the need for a simpler, RGB-only alternative that learns directly at the category level, we revisit a longstanding question: Can object detection and 9-DoF pose estimation be unified with high performance, without any additional data? We show that they can with our method, YOPO, a single-stage, query-based framework that treats category-level 9-DoF estimation as a natural extension of 2D detection. YOPO augments a transformer detector with a lightweight pose head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian matching cost. The model is trained end-to-end only with RGB images and category-level pose labels. Despite its minimalist design, YOPO sets a new state of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6% $\rm{IoU}_{50}$ and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric, surpassing prior RGB-only methods and closing much of the gap to RGB-D systems. The code, models, and additional qualitative results can be found on our project.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection</title>
<link>https://arxiv.org/abs/2508.14980</link>
<guid>https://arxiv.org/abs/2508.14980</guid>
<content:encoded><![CDATA[
<div> Framework, Face Recognition, Spoofing Attacks, Training Approach, Liveness cues<br />
<br />
Summary: 
The article introduces the Paired-Sampling Contrastive Framework, a unified training approach for face recognition systems to detect both physical presentation attacks and digital forgeries. This framework utilizes matched pairs of genuine and attack selfies to learn liveness cues that are independent of the attack modality. By integrating both types of attacks into a single model, the framework reduces system complexity, inference latency, and vulnerability to combined attack vectors. The method outperforms existing solutions in the detection of spoofing attempts, achieving an average classification error rate of 2.10 percent on the Face Anti-Spoofing Challenge benchmark. Additionally, the framework is lightweight with low computational requirements (4.46 GFLOPs) and can be trained in under an hour, making it suitable for practical deployment in real-world scenarios. The code and pretrained models are publicly available for further research and experimentation. <br /> <div>
arXiv:2508.14980v1 Announce Type: new 
Abstract: Modern face recognition systems remain vulnerable to spoofing attempts, including both physical presentation attacks and digital forgeries. Traditionally, these two attack vectors have been handled by separate models, each targeting its own artifacts and modalities. However, maintaining distinct detectors increases system complexity and inference latency and leaves systems exposed to combined attack vectors. We propose the Paired-Sampling Contrastive Framework, a unified training approach that leverages automatically matched pairs of genuine and attack selfies to learn modality-agnostic liveness cues. Evaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital Attack Detection benchmark, our method achieves an average classification error rate (ACER) of 2.10 percent, outperforming prior solutions. The framework is lightweight (4.46 GFLOPs) and trains in under one hour, making it practical for real-world deployment. Code and pretrained models are available at https://github.com/xPONYx/iccv2025_deepfake_challenge.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAIGen: Training-Free Adversarial Image Generation via Diffusion Models</title>
<link>https://arxiv.org/abs/2508.15020</link>
<guid>https://arxiv.org/abs/2508.15020</guid>
<content:encoded><![CDATA[
<div> Keyword: adversarial attacks, diffusion models, image generation, black-box method, selective RGB channel<br />
Summary:<br />
The paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. It utilizes diffusion models to produce high-quality adversarial examples in just 3-20 sampling steps. By injecting perturbations during the mixing step interval, the method achieves comparable attack effectiveness without processing all timesteps. A selective RGB channel strategy is implemented to preserve image structure while maximizing misclassification in target models. Visual quality is maintained with a PSNR above 30 dB across various datasets. On ImageNet, TAIGen achieves high success rates against different source models, generating adversarial examples 10x faster than existing diffusion-based attacks. It also exhibits the lowest robust accuracy, indicating its significant impact as defense mechanisms are less successful in purifying TAIGen-generated images.<br /> 
Summary: <div>
arXiv:2508.15020v1 Announce Type: new 
Abstract: Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement</title>
<link>https://arxiv.org/abs/2508.15027</link>
<guid>https://arxiv.org/abs/2508.15027</guid>
<content:encoded><![CDATA[
<div> Keywords: Concealed visual perception, reversible unfolding network, generative refinement, diffusion model, bi-level optimization.

Summary: 
The article introduces a novel method called RUN++ for concealed visual perception (CVP) that leverages a reversible unfolding network with generative refinement. The approach encompasses both the mask and RGB domains, addressing uncertainty in a principled manner. The network consists of three modules: CORE for object region extraction, CARE for context-aware region enhancement, and FINE for finetuning via noise-based enhancement. FINE utilizes a targeted Bernoulli diffusion model to refine uncertain regions, improving segmentation accuracy without high computational costs. This integration of reversible modeling and diffusion allows RUN++ to focus on ambiguous areas, reducing false positives and negatives. Additionally, the article proposes a bi-level optimization framework for robust CVP systems that are resilient to real-world degradations. The innovative approach of RUN++ showcases its efficacy in addressing uncertainty in CVP tasks with a focus on accuracy and efficiency.
<br /><br />Summary: <div>
arXiv:2508.15027v1 Announce Type: new 
Abstract: Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging</title>
<link>https://arxiv.org/abs/2508.15057</link>
<guid>https://arxiv.org/abs/2508.15057</guid>
<content:encoded><![CDATA[
<div> methane, livestock, emissions, segmentation, dietary  
Summary:  
GasTwinFormer is introduced as a hybrid vision transformer designed for real-time methane emission segmentation and dietary classification in optical gas imaging. The architecture incorporates a Mix Twin encoder with global and local attention mechanisms and a lightweight LR-ASPP decoder for multi-scale feature aggregation. A comprehensive beef cattle methane emission dataset is used for training and evaluation, achieving 74.47% mIoU and 83.63% mF1 for segmentation with high efficiency. The model has only 3.348M parameters, 3.428G FLOPs, and a fast inference speed of 114.9 FPS. Perfect dietary classification accuracy of 100% is achieved, showcasing the effectiveness of leveraging diet-emission correlations. Extensive ablation studies validate each architectural component, demonstrating GasTwinFormer as a practical solution for real-time livestock emission monitoring. Visit the project page at gastwinformer.github.io.  
<br /><br />Summary: <div>
arXiv:2508.15057v1 Announce Type: new 
Abstract: Livestock methane emissions represent 32% of human-caused methane production, making automated monitoring critical for climate mitigation strategies. We introduce GasTwinFormer, a hybrid vision transformer for real-time methane emission segmentation and dietary classification in optical gas imaging through a novel Mix Twin encoder alternating between spatially-reduced global attention and locally-grouped attention mechanisms. Our architecture incorporates a lightweight LR-ASPP decoder for multi-scale feature aggregation and enables simultaneous methane segmentation and dietary classification in a unified framework. We contribute the first comprehensive beef cattle methane emission dataset using OGI, containing 11,694 annotated frames across three dietary treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation while maintaining exceptional efficiency with only 3.348M parameters, 3.428G FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect dietary classification accuracy (100%), demonstrating the effectiveness of leveraging diet-emission correlations. Extensive ablation studies validate each architectural component, establishing GasTwinFormer as a practical solution for real-time livestock emission monitoring. Please see our project page at gastwinformer.github.io.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CurveFlow: Curvature-Guided Flow Matching for Image Generation</title>
<link>https://arxiv.org/abs/2508.15093</link>
<guid>https://arxiv.org/abs/2508.15093</guid>
<content:encoded><![CDATA[
<div> Keywords: rectified flow models, non-linear trajectories, semantic alignment, image generation, curvature guidance<br />
Summary: <br />
CurveFlow introduces a new flow matching framework that incorporates curvature guidance to learn smooth, non-linear trajectories in text-to-image generation. The method includes a robust curvature regularization technique to penalize abrupt changes in trajectory dynamics. Extensive experiments on MS COCO datasets show that CurveFlow outperforms standard rectified flow models and other non-linear baselines, such as Rectified Diffusion, in semantic consistency metrics like BLEU, METEOR, ROUGE, and CLAIR. The results indicate that the curvature-aware modeling enhances the model's ability to follow complex instructions accurately while maintaining high image quality. The code for CurveFlow is publicly available on GitHub at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow. <div>
arXiv:2508.15093v1 Announce Type: new 
Abstract: Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment</title>
<link>https://arxiv.org/abs/2508.15130</link>
<guid>https://arxiv.org/abs/2508.15130</guid>
<content:encoded><![CDATA[
<div> Keywords: no-reference image quality assessment, self-supervised framework, hierarchical ranking, contrastive learning, generalization performance

Summary:
HiRQA is a self-supervised framework for image quality assessment that combines hierarchical ranking and contrastive learning to predict quality scores using only the input image, without reliance on pristine references or subjective labels. It introduces a higher-order ranking loss to supervise quality predictions through relational ordering, an embedding distance loss to enforce consistency between feature distances and perceptual differences, and a training-time contrastive alignment loss guided by structured textual prompts. Despite being trained on synthetic distortions, HiRQA demonstrates effective generalization to authentic degradations such as lens flare, haze, motion blur, and low-light conditions. A lightweight variant, HiRQA-S, offers real-time deployment with an inference time of only 3.5 ms per image. Extensive experiments confirm HiRQA's state-of-the-art performance, strong generalization ability, and scalability. 

<br /><br />Summary: <div>
arXiv:2508.15130v1 Announce Type: new 
Abstract: Despite significant progress in no-reference image quality assessment (NR-IQA), dataset biases and reliance on subjective labels continue to hinder their generalization performance. We propose HiRQA, Hierarchical Ranking and Quality Alignment), a self-supervised, opinion-unaware framework that offers a hierarchical, quality-aware embedding through a combination of ranking and contrastive learning. Unlike prior approaches that depend on pristine references or auxiliary modalities at inference time, HiRQA predicts quality scores using only the input image. We introduce a novel higher-order ranking loss that supervises quality predictions through relational ordering across distortion pairs, along with an embedding distance loss that enforces consistency between feature distances and perceptual differences. A training-time contrastive alignment loss, guided by structured textual prompts, further enhances the learned representation. Trained only on synthetic distortions, HiRQA generalizes effectively to authentic degradations, as demonstrated through evaluation on various distortions such as lens flare, haze, motion blur, and low-light conditions. For real-time deployment, we introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of only 3.5 ms per image. Extensive experiments across synthetic and authentic benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong generalization ability, and scalability.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments</title>
<link>https://arxiv.org/abs/2508.15158</link>
<guid>https://arxiv.org/abs/2508.15158</guid>
<content:encoded><![CDATA[
<div> 3D reconstruction, multi-view, edge computing, reliability, portfolio theory

Summary:
The paper presents a novel edge resource management strategy inspired by portfolio theory for reliable multi-view 3D reconstruction in dynamic environments. The study focuses on the challenges of maintaining reconstruction quality in edge environments with spatiotemporally correlated disruptions. By using a genetic algorithm to solve the optimization problem, the proposed strategy ensures reconstruction quality satisfaction even under adverse conditions. The research demonstrates the benefits of the camera selection strategy in guaranteeing reliable 3D reconstruction compared to traditional baseline methods. The approach is particularly useful for critical applications such as emergency response and public safety, where near-real-time latency requirements and ad-hoc compute resource needs are common. <div>
arXiv:2508.15158v1 Announce Type: new 
Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use cases that require rapid situational-awareness, such as emergency response, tactical scenarios, and public safety. In many cases, their near-real-time latency requirements and ad-hoc needs for compute resources necessitate adoption of `Just-in-time' edge environments where the system is set up on the fly to support the applications during the mission lifetime. However, reliability issues can arise from the inherent dynamism and operational adversities of such edge environments, resulting in spatiotemporally correlated disruptions that impact the camera operations, which can lead to sustained degradation of reconstruction quality. In this paper, we propose a novel portfolio theory inspired edge resource management strategy for reliable multi-view 3D reconstruction against possible system disruptions. Our proposed methodology can guarantee reconstruction quality satisfaction even when the cameras are prone to spatiotemporally correlated disruptions. The portfolio theoretic optimization problem is solved using a genetic algorithm that converges quickly for realistic system settings. Using publicly available and customized 3D datasets, we demonstrate the proposed camera selection strategy's benefits in guaranteeing reliable 3D reconstruction against traditional baseline strategies, under spatiotemporal disruptions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis</title>
<link>https://arxiv.org/abs/2508.15168</link>
<guid>https://arxiv.org/abs/2508.15168</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic Retinopathy, Deep Learning, Explainable AI, Vision-Language Models, Medical Image Analysis

Summary:
XDR-LVLM is a novel framework that uses Vision-Language Large Models for precise diagnosis of Diabetic Retinopathy (DR) with transparent explanations. It combines a specialized Medical Vision Encoder and an LVLM Core to analyze fundus images and generate detailed diagnostic reports. The model achieves state-of-the-art performance on the DDR dataset, with high accuracy in disease diagnosis and concept detection. Additionally, XDR-LVLM provides explanations linking observed features to the diagnosis, enhancing clinical utility. Human evaluations confirm the accuracy and fluency of the generated explanations, demonstrating the model's ability to bridge the gap between automated diagnosis and clinical needs. Overall, XDR-LVLM offers interpretable insights into DR diagnosis, enabling clinicians to make informed decisions based on robust analysis. 

<br /><br />Summary: <div>
arXiv:2508.15168v1 Announce Type: new 
Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating early and accurate diagnosis. While deep learning models have shown promise in DR detection, their black-box nature often hinders clinical adoption due to a lack of transparency and interpretability. To address this, we propose XDR-LVLM (eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis coupled with natural language-based explanations. XDR-LVLM integrates a specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt Engineering and Multi-stage Fine-tuning to deeply understand pathological features within fundus images and generate comprehensive diagnostic reports. These reports explicitly include DR severity grading, identification of key pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and detailed explanations linking observed features to the diagnosis. Extensive experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and an F1 Score of 79.92% for disease diagnosis, and superior results for concept detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the high fluency, accuracy, and clinical utility of the generated explanations, showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and clinical needs by providing robust and interpretable insights.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion</title>
<link>https://arxiv.org/abs/2508.15169</link>
<guid>https://arxiv.org/abs/2508.15169</guid>
<content:encoded><![CDATA[
<div> control conditions, street-level perspective views, scene generation, image diffusion models, geometric alignment

Summary:<br />
The paper introduces MeSS, a method for generating realistic outdoor scenes using city mesh models as a geometric prior. The approach enhances image diffusion models by improving cross-view consistency, resulting in high-quality, style-consistent scenes. The pipeline includes three stages: generating sparse views, propagating denser intermediate views, and eliminating visual inconsistencies. A 3D Gaussian Splatting scene is reconstructed concurrently with generation. The method outperforms existing approaches in geometric alignment and generation quality. The synthesized scenes can be rendered in various styles through relighting and style transfer techniques. <div>
arXiv:2508.15169v1 Announce Type: new 
Abstract: Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis</title>
<link>https://arxiv.org/abs/2508.15189</link>
<guid>https://arxiv.org/abs/2508.15189</guid>
<content:encoded><![CDATA[
<div> Dataset, Surgical Wounds, Deep Learning, Diagnosis, Multimodal Learning<br />
Summary:<br />
The article introduces SurgWound, an open-source dataset containing 697 annotated surgical wound images. A benchmark for surgical wound diagnosis is also presented, comprising visual question answering and report generation tasks. The WoundQwen framework is proposed for surgical wound diagnosis, utilizing a three-stage learning approach. The framework involves the use of multiple language models for accurate prediction of wound characteristics, assessment of infection risk, and personalized patient instructions based on image analysis. This innovative approach aims to improve surgical wound care by providing detailed diagnostics and guiding timely interventions, ultimately leading to better patient outcomes.<br /><br />Summary: <div>
arXiv:2508.15189v1 Announce Type: new 
Abstract: Surgical site infection (SSI) is one of the most common and costly healthcare-associated infections and and surgical wound care remains a significant clinical challenge in preventing SSIs and improving patient outcomes. While recent studies have explored the use of deep learning for preliminary surgical wound screening, progress has been hindered by concerns over data privacy and the high costs associated with expert annotation. Currently, no publicly available dataset or benchmark encompasses various types of surgical wounds, resulting in the absence of an open-source Surgical-Wound screening tool. To address this gap: (1) we present SurgWound, the first open-source dataset featuring a diverse array of surgical wound types. It contains 697 surgical wound images annotated by 3 professional surgeons with eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce the first benchmark for surgical wound diagnosis, which includes visual question answering (VQA) and report generation tasks to comprehensively evaluate model performance. (3) Furthermore, we propose a three-stage learning framework, WoundQwen, for surgical wound diagnosis. In the first stage, we employ five independent MLLMs to accurately predict specific surgical wound characteristics. In the second stage, these predictions serve as additional knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess infection risk and guide subsequent interventions. In the third stage, we train a MLLM that integrates the diagnostic results from the previous two stages to produce a comprehensive report. This three-stage framework can analyze detailed surgical wound characteristics and provide subsequent instructions to patients based on surgical images, paving the way for personalized wound care, timely intervention, and improved patient outcomes.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.15207</link>
<guid>https://arxiv.org/abs/2508.15207</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, autonomous driving, behavior modelling, adversarial agent, failure scenarios

Summary:
In the realm of reinforcement learning, where agents are trained to exhibit optimal behavior in specific environments, the accurate modeling of rule-based surrounding agents is crucial for safety-critical applications like autonomous driving. Current methods rely on behavior modeling strategies and IDM models to capture the actions of surrounding agents. This study introduces a novel approach that utilizes a learning-based method to derive adversarial behavior for rule-based agents, thereby inducing failure scenarios for evaluation. By pitting the adversarial agent against traditional rule-based agents, a discernible decrease in cumulative reward is observed. This work highlights the significance of considering adversarial behavior within the training framework for reinforcing the robustness of autonomous systems in challenging environments. 

<br /><br />Summary: <div>
arXiv:2508.15207v1 Announce Type: new 
Abstract: Existing approaches in reinforcement learning train an agent to learn desired optimal behavior in an environment with rule based surrounding agents. In safety critical applications such as autonomous driving it is crucial that the rule based agents are modelled properly. Several behavior modelling strategies and IDM models are used currently to model the surrounding agents. We present a learning based method to derive the adversarial behavior for the rule based agents to cause failure scenarios. We evaluate our adversarial agent against all the rule based agents and show the decrease in cumulative reward.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology</title>
<link>https://arxiv.org/abs/2508.15208</link>
<guid>https://arxiv.org/abs/2508.15208</guid>
<content:encoded><![CDATA[
<div> Instance-level segmentation, renal pathology, morphological quantification, DyMorph-B2I, binary masks<br />
<br />
Summary: <br />
The study presents DyMorph-B2I, a dynamic segmentation pipeline for accurate morphological quantification of renal pathology functional units. Traditional post-processing techniques struggle with the diverse morphologies and complex connectivity in renal tissue. DyMorph-B2I integrates watershed, skeletonization, and morphological operations with adaptive refinement and hyperparameter tuning for each functional unit. Through parameter optimization, it effectively separates adherent and heterogeneous structures in binary masks, outperforming individual methods and na\"ive combinations. This enhances instance separation and enables more precise morphometric analysis in renal pathology workflows. The pipeline is publicly accessible at https://github.com/ddrrnn123/DyMorph-B2I. <div>
arXiv:2508.15208v1 Announce Type: new 
Abstract: Accurate morphological quantification of renal pathology functional units relies on instance-level segmentation, yet most existing datasets and automated methods provide only binary (semantic) masks, limiting the precision of downstream analyses. Although classical post-processing techniques such as watershed, morphological operations, and skeletonization, are often used to separate semantic masks into instances, their individual effectiveness is constrained by the diverse morphologies and complex connectivity found in renal tissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided binary-to-instance segmentation pipeline tailored for renal pathology. Our approach integrates watershed, skeletonization, and morphological operations within a unified framework, complemented by adaptive geometric refinement and customizable hyperparameter tuning for each class of functional unit. Through systematic parameter optimization, DyMorph-B2I robustly separates adherent and heterogeneous structures present in binary masks. Experimental results demonstrate that our method outperforms individual classical approaches and na\"ive combinations, enabling superior instance separation and facilitating more accurate morphometric analysis in renal pathology workflows. The pipeline is publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</title>
<link>https://arxiv.org/abs/2508.15216</link>
<guid>https://arxiv.org/abs/2508.15216</guid>
<content:encoded><![CDATA[
<div> Keywords: accident prediction, Advanced Driver Assistance Systems, dash-cam videos, spatio-temporal features, recurrent network

Summary:
Accident prediction is crucial for enhancing road safety and minimizing injuries and property damage. Advanced Driver Assistance Systems (ADAS) play a vital role in anticipating accidents before they occur. This study focuses on using dash-cam videos as a cost-effective solution for accident prediction. The proposed STAGNet model incorporates improved spatio-temporal features and employs a recurrent network to enhance the accuracy of accident prediction from dash-cam videos. Experimental results on three public datasets demonstrate that the STAGNet model outperforms existing methods in terms of average precision and mean time-to-collision values. The model shows superior performance both in cross-validation within a dataset and in training and testing on different datasets. The findings highlight the potential of utilizing dash-cam videos and advanced neural networks for effective accident prediction in road safety applications.<br /><br />Summary: <div>
arXiv:2508.15216v1 Announce Type: new 
Abstract: Accident prediction and timely warnings play a key role in improving road safety by reducing the risk of injury to road users and minimizing property damage. Advanced Driver Assistance Systems (ADAS) are designed to support human drivers and are especially useful when they can anticipate potential accidents before they happen. While many existing systems depend on a range of sensors such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents a more challenging but a more cost-effective and easily deployable solution. In this work, we incorporate better spatio-temporal features and aggregate them through a recurrent network to improve upon state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets show that our proposed STAGNet model achieves higher average precision and mean time-to-collision values than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Multi-Modal Coding for High-Quality 3D Generation</title>
<link>https://arxiv.org/abs/2508.15228</link>
<guid>https://arxiv.org/abs/2508.15228</guid>
<content:encoded><![CDATA[
<div> Generative model, Multi-modal, 3D modeling, TriMM, RGB, RGBD, point clouds  
Summary:  
TriMM is a new 3D-native generative model that leverages multi-modal data (RGB, RGBD, point cloud) for enhanced 3D asset modeling. It introduces collaborative multi-modal coding to combine modality-specific features while maintaining their unique strengths. Auxilary 2D and 3D supervision improve robustness and performance. The model employs a triplane latent diffusion model for high-quality 3D asset generation, enhancing both texture and geometric detail. TriMM achieves competitive performance with models trained on large-scale datasets using minimal training data. Experiments on various datasets demonstrate its effectiveness, and the model can also incorporate other multi-modal datasets for 3D generation.  
<br /><br />Summary: <div>
arXiv:2508.15228v1 Announce Type: new 
Abstract: 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Center-Oriented Prototype Contrastive Clustering</title>
<link>https://arxiv.org/abs/2508.15231</link>
<guid>https://arxiv.org/abs/2508.15231</guid>
<content:encoded><![CDATA[
<div> probabilistic clustering center-oriented prototype contrastive learning dual consistency learning prototype drift movement-invariant features

Summary:
The paper introduces a novel center-oriented prototype contrastive clustering framework to address the conflict problem in contrastive learning for clustering tasks. The framework comprises a soft prototype contrastive module and a dual consistency learning module. The soft prototype contrastive module calculates prototypes based on the probability of sample belonging to the cluster center, reducing inter-class conflicts and prototype drift. The dual consistency learning module aligns transformations of samples and their neighborhoods, ensuring transformation-invariant semantic information and compact intra-cluster distribution. Experimental results on five datasets demonstrate the effectiveness of the proposed method compared to the state-of-the-art. Code for the approach is available on GitHub for further exploration and integration. <br /><br />Summary: <div>
arXiv:2508.15231v1 Announce Type: new 
Abstract: Contrastive learning is widely used in clustering tasks due to its discriminative representation. However, the conflict problem between classes is difficult to solve effectively. Existing methods try to solve this problem through prototype contrast, but there is a deviation between the calculation of hard prototypes and the true cluster center. To address this problem, we propose a center-oriented prototype contrastive clustering framework, which consists of a soft prototype contrastive module and a dual consistency learning module. In short, the soft prototype contrastive module uses the probability that the sample belongs to the cluster center as a weight to calculate the prototype of each category, while avoiding inter-class conflicts and reducing prototype drift. The dual consistency learning module aligns different transformations of the same sample and the neighborhoods of different samples respectively, ensuring that the features have transformation-invariant semantic information and compact intra-cluster distribution, while providing reliable guarantees for the calculation of prototypes. Extensive experiments on five datasets show that the proposed method is effective compared to the SOTA. Our code is published on https://github.com/LouisDong95/CPCC.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</title>
<link>https://arxiv.org/abs/2508.15232</link>
<guid>https://arxiv.org/abs/2508.15232</guid>
<content:encoded><![CDATA[
<div> Keywords: Aerial Vision-and-Language Navigation, Dual-Altitude UAV Collaborative VLN, HaL-13k dataset, multi-grained perspectives, AeroDuo framework

Summary:
Aerial Vision-and-Language Navigation (VLN) task aims at enabling UAVs to navigate outdoor environments using natural language instructions and visual cues. Due to the complexity of UAV trajectories, achieving reliable performance is challenging. To address this, a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN) is introduced, where two UAVs operate at different altitudes for broad environmental reasoning and precise navigation. The HaL-13k dataset containing collaborative high-low UAV demonstration trajectories is created to support training and evaluation. A dual-UAV collaborative VLN framework, AeroDuo, is proposed with one UAV integrating a language model for target reasoning and the other using a policy for navigation. The UAVs work collaboratively with minimal coordination exchange. This approach leverages the mobility of UAVs to provide multi-grained perspectives while maintaining manageable learning spaces, enhancing performance in VLN tasks. 

<br /><br />Summary: <div>
arXiv:2508.15232v1 Announce Type: new 
Abstract: Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural language instructions and visual cues. However, due to the extended trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN performance is challenging and often requires human intervention or overly detailed instructions. To harness the advantages of UAVs' high mobility, which could provide multi-grained perspectives, while maintaining a manageable motion space for learning, we introduce a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct altitudes: a high-altitude UAV responsible for broad environmental reasoning, and a low-altitude UAV tasked with precise navigation. To support the training and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising 13,838 collaborative high-low UAV demonstration trajectories, each paired with target-oriented language instructions. This dataset includes both unseen maps and an unseen object validation set to systematically evaluate the model's generalization capabilities across novel environments and unfamiliar targets. To consolidate their complementary strengths, we propose a dual-UAV collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a multimodal large language model (Pilot-LLM) for target reasoning, while the low-altitude UAV employs a lightweight multi-stage policy for navigation and target grounding. The two UAVs work collaboratively and only exchange minimal coordinate information to ensure efficiency.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretrained Diffusion Models Are Inherently Skipped-Step Samplers</title>
<link>https://arxiv.org/abs/2508.15233</link>
<guid>https://arxiv.org/abs/2508.15233</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, skipped-step sampling, generation tasks, DDIM, accelerated sampling

Summary: 
This paper introduces a new approach called skipped-step sampling to accelerate the generation process in diffusion models. The traditional step-by-step generation process in diffusion models can be time-consuming, and existing methods like DDIM have attempted to reduce the number of sampling steps. The authors argue that the original diffusion process can achieve efficiency without non-Markovian processes. By introducing skipped-step sampling, multiple intermediate denoising steps can be bypassed, resulting in accelerated sampling in a Markovian way. The research shows that this mechanism is derived from the same training objective as the standard diffusion model, suggesting that it is an intrinsic property of pretrained diffusion models. The proposed method, which integrates skipped-step sampling with DDIM, achieves high-quality generation with significantly fewer sampling steps. Extensive experiments on popular pretrained diffusion models validate the effectiveness of the approach. <br /><br />Summary: <div>
arXiv:2508.15233v1 Announce Type: new 
Abstract: Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent</title>
<link>https://arxiv.org/abs/2508.15243</link>
<guid>https://arxiv.org/abs/2508.15243</guid>
<content:encoded><![CDATA[
<div> Keywords: Comp-X, image compression, large language model agent, interactive, coding framework<br />
<br />
Summary: <br />
Comp-X is a new image compression paradigm that utilizes a large language model agent to intelligently interact with users. The paradigm addresses limitations of traditional image codecs by incorporating a multi-functional coding framework that can adapt to different coding modes based on user requirements. An interactive coding agent is employed to teach the language model agent how to understand coding requests and select appropriate modes. A dedicated benchmark, IIC-bench, is introduced for evaluation. Experimental results show that Comp-X can efficiently understand coding requests and maintain impressive compression performance. This approach opens up possibilities for artificial general intelligence in image compression. <div>
arXiv:2508.15243v1 Announce Type: new 
Abstract: We present Comp-X, the first intelligently interactive image compression paradigm empowered by the impressive reasoning capability of large language model (LLM) agent. Notably, commonly used image codecs usually suffer from limited coding modes and rely on manual mode selection by engineers, making them unfriendly for unprofessional users. To overcome this, we advance the evolution of image coding paradigm by introducing three key innovations: (i) multi-functional coding framework, which unifies different coding modes of various objective/requirements, including human-machine perception, variable coding, and spatial bit allocation, into one framework. (ii) interactive coding agent, where we propose an augmented in-context learning method with coding expert feedback to teach the LLM agent how to understand the coding request, mode selection, and the use of the coding tools. (iii) IIC-bench, the first dedicated benchmark comprising diverse user requests and the corresponding annotations from coding experts, which is systematically designed for intelligently interactive image compression evaluation. Extensive experimental results demonstrate that our proposed Comp-X can understand the coding requests efficiently and achieve impressive textual interaction capability. Meanwhile, it can maintain comparable compression performance even with a single coding framework, providing a promising avenue for artificial general intelligence (AGI) in image compression.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images</title>
<link>https://arxiv.org/abs/2508.15256</link>
<guid>https://arxiv.org/abs/2508.15256</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, computational pathology, vision-language model, interpretability, lymph node datasets

Summary:
Ano-NAViLa is a novel anomaly detection model designed for computational pathology, specifically focusing on identifying rare anomalies in pathology images where data may be limited. The model integrates both normal and abnormal pathology knowledge to enhance accuracy and robustness in detecting anomalies. By leveraging a vision-language model and a trainable MLP, Ano-NAViLa provides interpretable results through image-text associations. The model outperforms existing methods in anomaly detection and localization on lymph node datasets from different organs, showcasing its effectiveness in pathology applications.<br /><br />Summary: <div>
arXiv:2508.15256v1 Announce Type: new 
Abstract: Anomaly detection in computational pathology aims to identify rare and scarce anomalies where disease-related data are often limited or missing. Existing anomaly detection methods, primarily designed for industrial settings, face limitations in pathology due to computational constraints, diverse tissue structures, and lack of interpretability. To address these challenges, we propose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented Vision-Language model for Anomaly detection in pathology images. Ano-NAViLa is built on a pre-trained vision-language model with a lightweight trainable MLP. By incorporating both normal and abnormal pathology knowledge, Ano-NAViLa enhances accuracy and robustness to variability in pathology images and provides interpretability through image-text associations. Evaluated on two lymph node datasets from different organs, Ano-NAViLa achieves the state-of-the-art performance in anomaly detection and localization, outperforming competing models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</title>
<link>https://arxiv.org/abs/2508.15272</link>
<guid>https://arxiv.org/abs/2508.15272</guid>
<content:encoded><![CDATA[
<div> Transformer decoder, topology reasoning, autonomous driving, lane detection, Redundancy Assignment <br />
<br />
Summary: 
The paper introduces RATopo, a strategy for improved lane topology reasoning in autonomous driving systems. Unlike existing methods that rely on limited one-to-one assignments for topology supervision, RATopo enables quantity-rich and diverse geometry topology supervision. By rearranging the Transformer decoder and incorporating multiple parallel cross-attention blocks with independent parameters, RATopo allows for redundant lane predictions and effective one-to-many assignments. This approach enhances the diversity of detected lanes and improves both lane-lane and lane-traffic topology performance. Experimental results on OpenLane-V2 show that RATopo is model-agnostic and can seamlessly integrate into existing topology reasoning frameworks, offering consistent enhancements in autonomous driving systems. <br /> <div>
arXiv:2508.15272v1 Announce Type: new 
Abstract: Lane topology reasoning plays a critical role in autonomous driving by modeling the connections among lanes and the topological relationships between lanes and traffic elements. Most existing methods adopt a first-detect-then-reason paradigm, where topological relationships are supervised based on the one-to-one assignment results obtained during the detection stage. This supervision strategy results in suboptimal topology reasoning performance due to the limited range of valid supervision. In this paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology reasoning that enables quantity-rich and geometry-diverse topology supervision. Specifically, we restructure the Transformer decoder by swapping the cross-attention and self-attention layers. This allows redundant lane predictions to be retained before suppression, enabling effective one-to-many assignment. We also instantiate multiple parallel cross-attention blocks with independent parameters, which further enhances the diversity of detected lanes. Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is model-agnostic and can be seamlessly integrated into existing topology reasoning frameworks, consistently improving both lane-lane and lane-traffic topology performance.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding</title>
<link>https://arxiv.org/abs/2508.15297</link>
<guid>https://arxiv.org/abs/2508.15297</guid>
<content:encoded><![CDATA[
<div> Keywords: design patent analysis, CLIP models, multimodal patent retrieval, AI-driven patent analysis, patent classification

Summary:
Design patent analysis traditionally relies on image data, which can lack comprehensive visual context and semantic information. The DesignCLIP framework leverages CLIP models to improve design patent applications using a large-scale dataset of U.S. design patents. DesignCLIP incorporates class-aware classification, contrastive learning, detailed image captions, and multi-views image learning to enhance patent analysis. The framework outperforms baseline and state-of-the-art models in patent classification and retrieval tasks. Multimodal patent retrieval offered by DesignCLIP has the potential to boost creativity and innovation in design by providing diverse sources of inspiration. The study demonstrates the effectiveness of multimodal approaches to advance patent analysis and underscores the promise of AI-driven solutions in the field. The codebase for DesignCLIP is publicly available for further research and development.

Summary: <br /><br />Design patent analysis traditionally relies on image data, which can lack comprehensive visual context and semantic information. The DesignCLIP framework leverages CLIP models to improve design patent applications using a large-scale dataset of U.S. design patents. DesignCLIP incorporates class-aware classification, contrastive learning, detailed image captions, and multi-views image learning to enhance patent analysis. The framework outperforms baseline and state-of-the-art models in patent classification and retrieval tasks. Multimodal patent retrieval offered by DesignCLIP has the potential to boost creativity and innovation in design by providing diverse sources of inspiration. The study demonstrates the effectiveness of multimodal approaches to advance patent analysis and underscores the promise of AI-driven solutions in the field. The codebase for DesignCLIP is publicly available for further research and development. <div>
arXiv:2508.15297v1 Announce Type: new 
Abstract: In the field of design patent analysis, traditional tasks such as patent classification and patent image retrieval heavily depend on the image data. However, patent images -- typically consisting of sketches with abstract and structural elements of an invention -- often fall short in conveying comprehensive visual context and semantic information. This inadequacy can lead to ambiguities in evaluation during prior art searches. Recent advancements in vision-language models, such as CLIP, offer promising opportunities for more reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP models to develop a unified framework DesignCLIP for design patent applications with a large-scale dataset of U.S. design patents. To address the unique characteristics of patent data, DesignCLIP incorporates class-aware classification and contrastive learning, utilizing generated detailed captions for patent images and multi-views image learning. We validate the effectiveness of DesignCLIP across various downstream tasks, including patent classification and patent retrieval. Additionally, we explore multimodal patent retrieval, which provides the potential to enhance creativity and innovation in design by offering more diverse sources of inspiration. Our experiments show that DesignCLIP consistently outperforms baseline and SOTA models in the patent domain on all tasks. Our findings underscore the promise of multimodal approaches in advancing patent analysis. The codebase is available here: https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
<link>https://arxiv.org/abs/2508.15298</link>
<guid>https://arxiv.org/abs/2508.15298</guid>
<content:encoded><![CDATA[
<div> Keywords: Congenital heart defect, Ultrasound videos, Machine learning, Temporal modeling, Uncertainty quantification

Summary:
Temporal Prompt Alignment (TPA) is introduced as a method for fetal congenital heart defect (CHD) classification in ultrasound videos. It addresses the challenges of image noise and probe positioning variability by leveraging temporal information and prompt-aware contrastive learning. TPA extracts features from video frames, captures heart motion with a temporal extractor, and aligns representations with class-specific text prompts using a contrastive loss. A Conditional Variational Autoencoder Style Modulation (CVAESM) module enhances calibration for clinical reliability by quantifying uncertainty in the classification. The performance of TPA was validated on both a private dataset for CHD detection and a public dataset for systolic dysfunction, achieving state-of-the-art macro F1 scores for CHD diagnosis and reducing expected calibration error and adaptive ECE. Overall, TPA offers a comprehensive framework for accurate and reliable classification of fetal CHD in ultrasound videos. 

<br /><br />Summary: <div>
arXiv:2508.15298v1 Announce Type: new 
Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT</title>
<link>https://arxiv.org/abs/2508.15299</link>
<guid>https://arxiv.org/abs/2508.15299</guid>
<content:encoded><![CDATA[
<div> LiDAR, player tracking, sports, multitmodal dataset, real-time tracking<br />
<br />
Summary: 
The paper introduces BasketLiDAR, a multimodal dataset for real-time 3D player tracking in basketball. Traditional systems struggle with the complexities of the sport, but using LiDAR point clouds and multi-view camera footage, the authors achieve improved accuracy and reduced computational cost. The dataset includes 4,445 frames and 3,105 player IDs, synchronized across three LiDAR sensors and multi-view cameras. The proposed approach leverages LiDAR's high-precision 3D spatial information, enabling real-time operation and superior tracking performance, particularly in occlusion scenarios. The method includes a real-time tracking pipeline using LiDAR alone and a multimodal pipeline that fuses LiDAR and camera data. This combination enhances tactical analysis, performance evaluation, and spectator experience in basketball games. The dataset is available for further research and development purposes. <div>
arXiv:2508.15299v1 Announce Type: new 
Abstract: Real-time 3D trajectory player tracking in sports plays a crucial role in tactical analysis, performance evaluation, and enhancing spectator experience. Traditional systems rely on multi-camera setups, but are constrained by the inherently two-dimensional nature of video data and the need for complex 3D reconstruction processing, making real-time analysis challenging. Basketball, in particular, represents one of the most difficult scenarios in the MOT field, as ten players move rapidly and complexly within a confined court space, with frequent occlusions caused by intense physical contact.
  To address these challenges, this paper constructs BasketLiDAR, the first multimodal dataset in the sports MOT field that combines LiDAR point clouds with synchronized multi-view camera footage in a professional basketball environment, and proposes a novel MOT framework that simultaneously achieves improved tracking accuracy and reduced computational cost. The BasketLiDAR dataset contains a total of 4,445 frames and 3,105 player IDs, with fully synchronized IDs between three LiDAR sensors and three multi-view cameras. We recorded 5-on-5 and 3-on-3 game data from actual professional basketball players, providing complete 3D positional information and ID annotations for each player. Based on this dataset, we developed a novel MOT algorithm that leverages LiDAR's high-precision 3D spatial information. The proposed method consists of a real-time tracking pipeline using LiDAR alone and a multimodal tracking pipeline that fuses LiDAR and camera data. Experimental results demonstrate that our approach achieves real-time operation, which was difficult with conventional camera-only methods, while achieving superior tracking performance even under occlusion conditions. The dataset is available upon request at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2508.15313</link>
<guid>https://arxiv.org/abs/2508.15313</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, SAM-based segmentation, training-free paradigm, Camouflaged object detection, benchmark datasets <br />
Summary: <br />
The article introduces a novel approach called RAG-SEG for camouflaged object detection, addressing the challenges of similarity between objects and backgrounds. It decouples the task into two stages: Retrieval-Augmented Generation (RAG) for prompt generation and SAM-based segmentation (SEG) for refinement without the need for extensive training. RAG-SEG utilizes unsupervised clustering to create a compact retrieval database for efficient feature retrieval. During inference, retrieved features guide mask generation using SAM2, achieving competitive performance without traditional training requirements. Extensive experiments on benchmark datasets demonstrate RAG-SEG's effectiveness, with results comparable to or surpassing state-of-the-art methods. The method's computational efficiency allows for practical implementation even on personal laptops. Further analysis in the Appendix discusses limitations, potential extensions for salient object detection, and suggestions for improvement. <br /> <div>
arXiv:2508.15313v1 Announce Type: new 
Abstract: Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoEraser: Concept Erasure in Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.15314</link>
<guid>https://arxiv.org/abs/2508.15314</guid>
<content:encoded><![CDATA[
<div> privacy, copyright, safety, T2V diffusion models, VideoEraser

Summary:<br />
The article introduces VideoEraser, a training-free framework aimed at preventing text-to-video (T2V) diffusion models from generating harmful or misleading content. The framework consists of two stages: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). VideoEraser can seamlessly integrate with T2V models to suppress undesirable concepts such as object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser outperforms prior methods in terms of efficacy, integrity, fidelity, robustness, and generalizability. It reduces undesirable content generation by 46% on average across the four tasks compared to baselines. This framework addresses concerns related to privacy, copyright, and safety in T2V models, providing a solution to control the production and distribution of potentially harmful content. <div>
arXiv:2508.15314v1 Announce Type: new 
Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling</title>
<link>https://arxiv.org/abs/2508.15336</link>
<guid>https://arxiv.org/abs/2508.15336</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, autonomous vehicles, deep learning, pose detection, road crossing intent prediction<br />
Summary: <br />
- The study focused on predicting the intent of road crossing by pedestrians using deep learning models. 
- Three different sequence modeling techniques were analyzed, with 1D CNN proving to be the fastest model for prediction.
- GRU outperformed LSTM in predicting road crossing intents.
- The study integrated pose detection outputs with sequence modeling for an end-to-end deep learning framework.
- Experimental setup involved video analysis to predict poses and sequence modeling for temporal predictions. <div>
arXiv:2508.15336v1 Announce Type: new 
Abstract: The world is constantly moving towards AI based systems and autonomous vehicles are now reality in different parts of the world. These vehicles require sensors and cameras to detect objects and maneuver according to that. It becomes important to for such vehicles to also predict from a distant if a person is about to cross a road or not. The current study focused on predicting the intent of crossing the road by pedestrians in an experimental setup. The study involved working with deep learning models to predict poses and sequence modelling for temporal predictions. The study analysed three different sequence modelling to understand the prediction behaviour and it was found out that GRU was better in predicting the intent compared to LSTM model but 1D CNN was the best model in terms of speed. The study involved video analysis, and the output of pose detection model was integrated later on to sequence modelling techniques for an end-to-end deep learning framework for predicting road crossing intents.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features</title>
<link>https://arxiv.org/abs/2508.15353</link>
<guid>https://arxiv.org/abs/2508.15353</guid>
<content:encoded><![CDATA[
<div> Transformer-based, multimodal fusion, object detection, autonomous driving, radar-camera models
Summary: 
Transformer-based model RCDINO enhances object detection by fusing visual backbone features with semantically rich representations from the pretrained DINOv2 foundation model. This fusion enriches visual representations, improving detection performance while maintaining compatibility with the baseline architecture. Experiments on the nuScenes dataset show that RCDINO achieves state-of-the-art performance among radar-camera models, with 56.4 NDS and 48.1 mAP. The implementation of RCDINO is available on GitHub for further exploration and use. <br /><br />Summary: <div>
arXiv:2508.15353v1 Announce Type: new 
Abstract: Three-dimensional object detection is essential for autonomous driving and robotics, relying on effective fusion of multimodal data from cameras and radar. This work proposes RCDINO, a multimodal transformer-based model that enhances visual backbone features by fusing them with semantically rich representations from the pretrained DINOv2 foundation model. This approach enriches visual representations and improves the model's detection performance while preserving compatibility with the baseline architecture. Experiments on the nuScenes dataset demonstrate that RCDINO achieves state-of-the-art performance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our implementation is available at https://github.com/OlgaMatykina/RCDINO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on How Video-LLMs Answer Video Questions</title>
<link>https://arxiv.org/abs/2508.15360</link>
<guid>https://arxiv.org/abs/2508.15360</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, attention knockouts, interpretability, efficiency, video question answering

Summary:<br /><br />
1. The study uses attention knockouts to analyze Video Large Language Models, revealing that video information extraction primarily occurs in early layers, with lower layers focusing on perceptual encoding and higher layers on abstract reasoning.
2. In a fine-grained setting, certain intermediate layers play a crucial role in video question answering, while most other layers contribute minimally.
3. Spatial-temporal modeling in Video-LLMs relies more on language-guided retrieval than on intra- and inter-frame self-attention among video tokens.
4. Insights from the study can be used to reduce attention computation in Video-LLMs, enhancing efficiency.
5. This research is the first to systematically uncover how Video-LLMs process and understand video content, offering insights into interpretability and efficiency for future studies. 

Summary: <div>
arXiv:2508.15360v1 Announce Type: new 
Abstract: Taking advantage of large-scale data and pretrained language models, Video Large Language Models (Video-LLMs) have shown strong capabilities in answering video questions. However, most existing efforts focus on improving performance, with limited attention to understanding their internal mechanisms. This paper aims to bridge this gap through a systematic empirical study. To interpret existing VideoLLMs, we adopt attention knockouts as our primary analytical tool and design three variants: Video Temporal Knockout, Video Spatial Knockout, and Language-to-Video Knockout. Then, we apply these three knockouts on different numbers of layers (window of layers). By carefully controlling the window of layers and types of knockouts, we provide two settings: a global setting and a fine-grained setting. Our study reveals three key findings: (1) Global setting indicates Video information extraction primarily occurs in early layers, forming a clear two-stage process -- lower layers focus on perceptual encoding, while higher layers handle abstract reasoning; (2) In the fine-grained setting, certain intermediate layers exert an outsized impact on video question answering, acting as critical outliers, whereas most other layers contribute minimally; (3) In both settings, we observe that spatial-temporal modeling relies more on language-guided retrieval than on intra- and inter-frame self-attention among video tokens, despite the latter's high computational cost. Finally, we demonstrate that these insights can be leveraged to reduce attention computation in Video-LLMs. To our knowledge, this is the first work to systematically uncover how Video-LLMs internally process and understand video content, offering interpretability and efficiency perspectives for future research.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer learning optimization based on evolutionary selective fine tuning</title>
<link>https://arxiv.org/abs/2508.15367</link>
<guid>https://arxiv.org/abs/2508.15367</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, transfer learning, fine-tuning, evolutionary algorithm, image classification <br />
Summary:<br />
1. Deep learning has made significant progress in image analysis, but the computational demands of large models are challenging.<br />
2. Transfer learning is a strategy to adapt pre-trained models to new tasks, with traditional fine-tuning potentially leading to overfitting and high computational costs.<br />
3. BioTune introduces an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to improve transfer learning efficiency, reducing computational complexity.<br />
4. BioTune uses an evolutionary algorithm to identify specific layers for fine-tuning based on the target task, leading to competitive or enhanced accuracy and efficiency compared to existing methods like AutoRGN and LoRA.<br />
5. By focusing the fine-tuning on relevant layers, BioTune decreases the number of trainable parameters, potentially reducing computational costs and enabling more efficient transfer learning across diverse datasets.<br /> <div>
arXiv:2508.15367v1 Announce Type: new 
Abstract: Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Conditioned 3D Gaussian Splat Quantization</title>
<link>https://arxiv.org/abs/2508.15372</link>
<guid>https://arxiv.org/abs/2508.15372</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, compression, archival, adaptability, scene changes

Summary: 
The study introduces Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) to improve compression efficiency and adaptability in 3D scene rendering. It addresses limitations of current compression methods by efficiently quantizing scenes to the kilobyte range and accommodating post-archival scene changes. This is achieved by leveraging inter-Gaussian and inter-attribute correlations and using shared codebooks across scenes. The encoding, quantization, and decoding processes are trained jointly, enabling effective conditional decoding based on captured images. ICGS-Quantizer outperforms existing methods in both compression efficiency and adaptability to scene changes. The approach offers a practical solution for compressing and managing large-scale scenes or extensive scene collections while preserving visual fidelity. The code, model, and data are made publicly available on GitHub for further research and development. 

Summary: <div>
arXiv:2508.15372v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians</title>
<link>https://arxiv.org/abs/2508.15376</link>
<guid>https://arxiv.org/abs/2508.15376</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene reconstruction, Gaussian splatting, driving scenarios, neural Gaussian representations, dynamic-static decoupling <br />
<br />
Summary: 
The article introduces DriveSplat, a novel method for high-quality reconstruction of driving scenarios using neural Gaussian representations with dynamic-static decoupling. By partitioning the scene into near, middle, and far regions, the method enhances close-range detail representation, particularly suited for linear motion patterns of driving viewpoints. Deformable neural Gaussians model non-rigid dynamic actors, with parameters adjusted by a learnable deformation network. The framework is supervised by depth and normal priors from pre-trained models to improve geometric accuracy. DriveSplat has been evaluated on Waymo and KITTI datasets, showing superior performance in novel-view synthesis for driving scenarios. <div>
arXiv:2508.15376v1 Announce Type: new 
Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIO: Refining Mutual Information and Causal Chain to Enhance Machine Abstract Reasoning Ability</title>
<link>https://arxiv.org/abs/2508.15387</link>
<guid>https://arxiv.org/abs/2508.15387</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, abstract reasoning, Raven's Progressive Matrices, causal chain modeling, mutual information <br />
<br />
Summary: Despite the effective performance of current deep learning models, abstract reasoning remains a challenge. This paper focuses on solving Raven's Progressive Matrices (RPM) problems to enhance machine intelligence in this area. It analyzes the complete causal chain in RPM tasks and designs a network architecture based on this analysis. However, the baseline model DIO falls short in acquiring human reasoning logic due to limitations in the optimization objective. Three improvement methods are then proposed to overcome these limitations. The tightness of the lower bound affects mutual information maximization, which does not capture the causal relationship between subjects and objects. These improvements aim to address these limitations and improve the abstract reasoning abilities of machine intelligence. <div>
arXiv:2508.15387v1 Announce Type: new 
Abstract: Despite the outstanding performance of current deep learning models across various domains, their fundamental bottleneck in abstract reasoning remains unresolved. To address this challenge, the academic community has introduced Raven's Progressive Matrices (RPM) problems as an authoritative benchmark for evaluating the abstract reasoning capabilities of deep learning algorithms, with a focus on core intelligence dimensions such as abstract reasoning, pattern recognition, and complex problem-solving. Therefore, this paper centers on solving RPM problems, aiming to contribute to enhancing the abstract reasoning abilities of machine intelligence. Firstly, this paper adopts a ``causal chain modeling'' perspective to systematically analyze the complete causal chain in RPM tasks: image $\rightarrow$ abstract attributes $\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency $\rightarrow$ correct answer. Based on this analysis, the network architecture of the baseline model DIO is designed. However, experiments reveal that the optimization objective formulated for DIO, namely maximizing the variational lower bound of mutual information between the context and the correct option, fails to enable the model to genuinely acquire the predefined human reasoning logic. This is attributed to two main reasons: the tightness of the lower bound significantly impacts the effectiveness of mutual information maximization, and mutual information, as a statistical measure, does not capture the causal relationship between subjects and objects. To overcome these limitations, this paper progressively proposes three improvement methods:
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiking Variational Graph Representation Inference for Video Summarization</title>
<link>https://arxiv.org/abs/2508.15389</link>
<guid>https://arxiv.org/abs/2508.15389</guid>
<content:encoded><![CDATA[
<div> Keywords: video summarization, Spiking Variational Graph Network, keyframe extraction, dynamic aggregation graph reasoner, variational inference reconstruction module

Summary:
The article introduces a novel approach called the Spiking Variational Graph Network (SpiVG) for efficient video summarization. SpiVG addresses the limitations of existing methods by utilizing Spiking Neural Networks for keyframe extraction, a Dynamic Aggregation Graph Reasoner for adaptable reasoning across video frames, and a Variational Inference Reconstruction Module to handle uncertainty and noise during feature fusion. The SpiVG network outperforms existing methods on various datasets, demonstrating its effectiveness in capturing global temporal dependencies and maintaining semantic coherence in video content. The codes and pre-trained models for SpiVG are publicly available on GitHub, allowing for further exploration and implementation in video summarization tasks. <br /><br />Summary: <div>
arXiv:2508.15389v1 Announce Type: new 
Abstract: With the rise of short video content, efficient video summarization techniques for extracting key information have become crucial. However, existing methods struggle to capture the global temporal dependencies and maintain the semantic coherence of video content. Additionally, these methods are also influenced by noise during multi-channel feature fusion. We propose a Spiking Variational Graph (SpiVG) Network, which enhances information density and reduces computational complexity. First, we design a keyframe extractor based on Spiking Neural Networks (SNN), leveraging the event-driven computation mechanism of SNNs to learn keyframe features autonomously. To enable fine-grained and adaptable reasoning across video frames, we introduce a Dynamic Aggregation Graph Reasoner, which decouples contextual object consistency from semantic perspective coherence. We present a Variational Inference Reconstruction Module to address uncertainty and noise arising during multi-channel feature fusion. In this module, we employ Evidence Lower Bound Optimization (ELBO) to capture the latent structure of multi-channel feature distributions, using posterior distribution regularization to reduce overfitting. Experimental results show that SpiVG surpasses existing methods across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes and pre-trained models are available at https://github.com/liwrui/SpiVG.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations</title>
<link>https://arxiv.org/abs/2508.15404</link>
<guid>https://arxiv.org/abs/2508.15404</guid>
<content:encoded><![CDATA[
<div> hyperparameter tuning, Masked Autoencoders (MAEs), spatial correlations, linear MAE, non-linear MAE

Summary: 
This study explores the relationship between hyperparameters and performance in Masked Autoencoders (MAEs) for vision models. The focus is on how MAEs learn spatial correlations in input images. Analytical derivation of linear MAE features reveals that masking ratio and patch size influence the capture of short- and long-range spatial correlations. Non-linear MAEs adapt to dataset spatial correlations beyond second-order statistics. The findings suggest that hyperparameters can be selected to affect the types of spatial features learned by MAEs, offering insights for practical implementation. <div>
arXiv:2508.15404v1 Announce Type: new 
Abstract: Masked Autoencoders (MAEs) have emerged as a powerful pretraining technique for vision foundation models. Despite their effectiveness, they require extensive hyperparameter tuning (masking ratio, patch size, encoder/decoder layers) when applied to novel datasets. While prior theoretical works have analyzed MAEs in terms of their attention patterns and hierarchical latent variable models, the connection between MAE hyperparameters and performance on downstream tasks is relatively unexplored. This work investigates how MAEs learn spatial correlations in the input image. We analytically derive the features learned by a linear MAE and show that masking ratio and patch size can be used to select for features that capture short- and long-range spatial correlations. We extend this analysis to non-linear MAEs to show that MAE representations adapt to spatial correlations in the dataset, beyond second-order statistics. Finally, we discuss some insights on how to select MAE hyper-parameters in practice.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Temporal Information Propagation for Moving Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2508.15415</link>
<guid>https://arxiv.org/abs/2508.15415</guid>
<content:encoded><![CDATA[
<div> Keywords: Moving infrared small target detection, Bidirectional temporal information propagation, Local Temporal Motion Fusion, Global Temporal Motion Fusion, Spatio-Temporal Fusion<br />
Summary:
The article introduces a novel method, BIRD, for moving infrared small target detection. Traditional methods use sliding window approaches, which do not optimize the entire video clip and overlook global temporal information, leading to suboptimal results. BIRD utilizes bidirectional temporal information propagation, combining local and global temporal information in a recursive manner. The proposed method consists of Local Temporal Motion Fusion and Global Temporal Motion Fusion modules to model spatio-temporal dependencies and aggregate propagation features. Additionally, the video clip is jointly optimized with a Spatio-Temporal Fusion loss. Experimental results demonstrate that BIRD achieves state-of-the-art performance with fast inference speed, surpassing existing approaches in moving infrared small target detection. <br /><br />Summary: <div>
arXiv:2508.15415v1 Announce Type: new 
Abstract: Moving infrared small target detection is broadly adopted in infrared search and track systems, and has attracted considerable research focus in recent years. The existing learning-based multi-frame methods mainly aggregate the information of adjacent frames in a sliding window fashion to assist the detection of the current frame. However, the sliding-window-based methods do not consider joint optimization of the entire video clip and ignore the global temporal information outside the sliding window, resulting in redundant computation and sub-optimal performance. In this paper, we propose a Bidirectional temporal information propagation method for moving InfraRed small target Detection, dubbed BIRD. The bidirectional propagation strategy simultaneously utilizes local temporal information of adjacent frames and global temporal information of past and future frames in a recursive fashion. Specifically, in the forward and backward propagation branches, we first design a Local Temporal Motion Fusion (LTMF) module to model local spatio-temporal dependency between a target frame and its two adjacent frames. Then, a Global Temporal Motion Fusion (GTMF) module is developed to further aggregate the global propagation feature with the local fusion feature. Finally, the bidirectional aggregated features are fused and input into the detection head for detection. In addition, the entire video clip is jointly optimized by the traditional detection loss and the additional Spatio-Temporal Fusion (STF) loss. Extensive experiments demonstrate that the proposed BIRD method not only achieves the state-of-the-art performance but also shows a fast inference speed.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles</title>
<link>https://arxiv.org/abs/2508.15431</link>
<guid>https://arxiv.org/abs/2508.15431</guid>
<content:encoded><![CDATA[
<div> object recognition, machine learning, car damage inspection, YOLOv8, deep learning 

Summary:
Machine learning-based techniques using the YOLOv8 object recognition framework provide a solution for automatically detecting tiny surface flaws, like microscopic dents, on car exteriors. Traditional manual car damage inspection methods are time-consuming and often unreliable in detecting small imperfections. A custom dataset with annotated car surface photos was created for training the YOLOv8m model and its variants, YOLOv8m-t4 and YOLOv8m-t42, with real-time data augmentation techniques. Experimental results showed high detection accuracy and low inference latency, making the model suitable for real-time applications such as automated insurance evaluations. YOLOv8m-t42 outperformed YOLOv8m-t4 in identifying microscopic surface defects, with higher precision, recall, and F1-score. Although YOLOv8m-t42 exhibits slower convergence, it offers greater accuracy and consistency for practical dent detection applications. The model's performance was validated using evaluation parameters such as mAP, precision, recall, and F1-score. <br /><br /> Summary: <div>
arXiv:2508.15431v1 Announce Type: new 
Abstract: Conventional car damage inspection techniques are labor-intensive, manual, and frequently overlook tiny surface imperfections like microscopic dents. Machine learning provides an innovative solution to the increasing demand for quicker and more precise inspection methods. The paper uses the YOLOv8 object recognition framework to provide a deep learning-based solution for automatically detecting microscopic surface flaws, notably tiny dents, on car exteriors. Traditional automotive damage inspection procedures are manual, time-consuming, and frequently unreliable at detecting tiny flaws. To solve this, a bespoke dataset containing annotated photos of car surfaces under various lighting circumstances, angles, and textures was created. To improve robustness, the YOLOv8m model and its customized variants, YOLOv8m-t4 and YOLOv8m-t42, were trained employing real-time data augmentation approaches. Experimental results show that the technique has excellent detection accuracy and low inference latency, making it suited for real-time applications such as automated insurance evaluations and automobile inspections. Evaluation parameters such as mean Average Precision (mAP), precision, recall, and F1-score verified the model's efficacy. With a precision of 0.86, recall of 0.84, and F1-score of 0.85, the YOLOv8m-t42 model outperformed the YOLOv8m-t4 model (precision: 0.81, recall: 0.79, F1-score: 0.80) in identifying microscopic surface defects. With a little reduced mAP@0.5:0.95 of 0.20, the mAP@0.5 for YOLOv8m-t42 stabilized at 0.60. Furthermore, YOLOv8m-t42's PR curve area was 0.88, suggesting more consistent performance than YOLOv8m-t4 (0.82). YOLOv8m-t42 has greater accuracy and is more appropriate for practical dent detection applications, even though its convergence is slower.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Moments in Time using Video Queries</title>
<link>https://arxiv.org/abs/2508.15439</link>
<guid>https://arxiv.org/abs/2508.15439</guid>
<content:encoded><![CDATA[
<div> Transformer, Video-to-video moment retrieval, Semantic alignment, Dual-stage sequence alignment, Self-supervised pre-training 

Summary: 
MATR is a transformer-based model for Video-to-video moment retrieval, focusing on semantic alignment and complex dependencies between query and target videos. It utilizes dual-stage sequence alignment to encode correlations and dependencies, enabling precise moment localization. The model employs foreground/background classification and boundary prediction heads to accurately identify moments in the target video. Additionally, a self-supervised pre-training technique helps provide task-specific initialization for MATR, improving performance significantly on ActivityNet-VRL and SportsMoments datasets. Experimental results show notable increases in Recall@1 and mIoU compared to state-of-the-art methods, demonstrating MATR's effectiveness in accurately localizing unseen events or moments in target videos using a query video. 

<br /><br />Summary: <div>
arXiv:2508.15439v1 Announce Type: new 
Abstract: Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen events or moments in a target video using a query video. This task poses several challenges, such as the need for semantic frame-level alignment and modeling complex dependencies between query and target videos. To tackle this challenging problem, we introduce MATR (Moment Alignment TRansformer), a transformer-based model designed to capture semantic context as well as the temporal details necessary for precise moment localization. MATR conditions target video representations on query video features using dual-stage sequence alignment that encodes the required correlations and dependencies. These representations are then used to guide foreground/background classification and boundary prediction heads, enabling the model to accurately identify moments in the target video that semantically match with the query video. Additionally, to provide a strong task-specific initialization for MATR, we propose a self-supervised pre-training technique that involves training the model to localize random clips within videos. Extensive experiments demonstrate that MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU on an absolute scale compared to state-of-the-art methods on the popular ActivityNet-VRL dataset. Additionally, on our newly proposed dataset, SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an absolute scale over strong baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework</title>
<link>https://arxiv.org/abs/2508.15457</link>
<guid>https://arxiv.org/abs/2508.15457</guid>
<content:encoded><![CDATA[
<div> Sparse-view reconstruction, novel view synthesis, camera pose estimation, dense stereo module, view interpolation.

Summary:
A new method is proposed for 3D Gaussian Splatting (3DGS) that tackles the challenge of reconstructing 3D scenes from extremely sparse-view inputs without relying on Structure-from-Motion (SfM). The approach combines a dense stereo module for pose estimation and point cloud reconstruction, along with a view interpolation module that generates consistent content for improved supervision during training. Additional regularization techniques are introduced to enhance geometrical structures and rendered content quality. Experimental results show a significant performance improvement over existing methods, with a 2.75dB increase in PSNR under extremely sparse-view conditions using only 2 training views. The synthesized images demonstrate minimal distortion and rich high-frequency details, showcasing superior visual quality compared to current techniques. <br /><br />Summary: <div>
arXiv:2508.15457v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion</title>
<link>https://arxiv.org/abs/2508.15476</link>
<guid>https://arxiv.org/abs/2508.15476</guid>
<content:encoded><![CDATA[
<div> Keyword: Medical image segmentation, Lightweight model, Attention mechanism, Channel redundancy, LGMSNet <br />
Summary: <br />
Medical image segmentation is crucial for disease diagnosis and treatment planning in resource-constrained clinical settings. Existing lightweight models often sacrifice performance for efficiency, lacking global contextual perception capabilities due to the absence of attention mechanisms. The channel redundancy issue under the same convolutional kernels in medical imaging also poses a challenge for effective feature extraction. To address these issues, LGMSNet, a novel lightweight framework, combines local and global dual multiscale approaches to achieve high performance with minimal computational overhead. LGMSNet utilizes heterogeneous intra-layer kernels to extract local high-frequency information and mitigate channel redundancy, while incorporating sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across multiple datasets demonstrate LGMSNet's superiority over existing methods, particularly excelling in zero-shot generalization tests on unseen datasets, showcasing its potential for deployment in resource-limited medical settings. <div>
arXiv:2508.15476v1 Announce Type: new 
Abstract: Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration</title>
<link>https://arxiv.org/abs/2508.15500</link>
<guid>https://arxiv.org/abs/2508.15500</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, clothed human avatars, multi-view, SMPL-X body model, normal map integration

Summary:
MExECON is a new pipeline for 3D reconstruction of clothed human avatars from sparse multi-view RGB images. It expands on the ECON single-view method by incorporating multiple viewpoints to enhance geometry and body pose estimation. The Joint Multi-view Body Optimization (JMBO) algorithm is at the heart of the pipeline, fitting a single SMPL-X body model across all input views to ensure multi-view consistency. This optimized body model acts as a low-frequency precursor for surface reconstruction, with normal map integration adding geometric details. MExECON combines normal maps from front and back views to capture intricate surface features like clothing folds and hairstyles accurately. These enhancements are achieved without the need for network retraining. Experimental results demonstrate that MExECON consistently outperforms the single-view baseline and is competitive with contemporary few-shot 3D reconstruction techniques.<br /><br />Summary: MExECON introduces a pipeline for 3D reconstruction of clothed human avatars using multi-view images. By extending the ECON method to incorporate multiple viewpoints, it enhances geometry and pose estimation. The JMBO algorithm facilitates joint optimization of a single SMPL-X body model across all input views to maintain consistency. The optimized body model serves as a prior for surface reconstruction, with normal maps adding detailed geometry. MExECON integrates normal maps from front and back views to capture fine surface details. The pipeline achieves notable improvements over single-view approaches and performs well compared to few-shot reconstruction methods. <div>
arXiv:2508.15500v1 Announce Type: new 
Abstract: This work presents MExECON, a novel pipeline for 3D reconstruction of clothed human avatars from sparse multi-view RGB images. Building on the single-view method ECON, MExECON extends its capabilities to leverage multiple viewpoints, improving geometry and body pose estimation. At the core of the pipeline is the proposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a single SMPL-X body model jointly across all input views, enforcing multi-view consistency. The optimized body model serves as a low-frequency prior that guides the subsequent surface reconstruction, where geometric details are added via normal map integration. MExECON integrates normal maps from both front and back views to accurately capture fine-grained surface details such as clothing folds and hairstyles. All multi-view gains are achieved without requiring any network re-training. Experimental results show that MExECON consistently improves fidelity over the single-view baseline and achieves competitive performance compared to modern few-shot 3D reconstruction methods.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion</title>
<link>https://arxiv.org/abs/2508.15505</link>
<guid>https://arxiv.org/abs/2508.15505</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Image Fusion, AdaSFFuse, Adaptive Approximate Wavelet Transform, Spatial-Frequency Mamba Blocks, Frequency Decoupling

Summary:
AdaSFFuse is a novel framework for Multimodal Image Fusion (MMIF) that addresses challenges such as modality misalignment, detail destruction, and task-specific limitations. It introduces the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, enabling fine-grained extraction and alignment of distinct frequency characteristics. The Spatial-Frequency Mamba Blocks facilitate efficient fusion in spatial and frequency domains. By dynamically adjusting through learnable mappings, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments across four MMIF tasks demonstrate AdaSFFuse's superior fusion performance, offering a balance between efficiency and effectiveness. The code will be made publicly available, ensuring accessibility for further research and development. <br /><br />Summary: <div>
arXiv:2508.15505v1 Announce Type: new 
Abstract: Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF) -- demonstrate AdaSFFuse's superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors</title>
<link>https://arxiv.org/abs/2508.15529</link>
<guid>https://arxiv.org/abs/2508.15529</guid>
<content:encoded><![CDATA[
<div> Keywords: driving logs, autonomous vehicles, trajectory extrapolation, generative priors, geometric consistency 

Summary: <br /><br />ExtraGS is a new framework proposed for trajectory extrapolation in simulated driving scenes for autonomous vehicles. It integrates geometric and generative priors to improve the realism and geometric consistency of extrapolated views. The core of ExtraGS is the Road Surface Gaussian representation, combining Gaussian-Signed Distance Functions and Far Field Gaussians to efficiently handle distant objects. A self-supervised uncertainty estimation framework based on spherical harmonics allows for selective integration of generative priors only where artifacts occur. Extensive experiments demonstrate that ExtraGS outperforms existing methods in enhancing realism and preserving fidelity along the original trajectory. By combining geometric and generative priors, ExtraGS offers a more holistic approach to trajectory extrapolation for autonomous driving vehicles. <div>
arXiv:2508.15529v1 Announce Type: new 
Abstract: Synthesizing extrapolated views from recorded driving logs is critical for simulating driving scenes for autonomous driving vehicles, yet it remains a challenging task. Recent methods leverage generative priors as pseudo ground truth, but often lead to poor geometric consistency and over-smoothed renderings. To address these limitations, we propose ExtraGS, a holistic framework for trajectory extrapolation that integrates both geometric and generative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG) representation based on a hybrid Gaussian-Signed Distance Function (SDF) design, and Far Field Gaussians (FFG) that use learnable scaling factors to efficiently handle distant objects. Furthermore, we develop a self-supervised uncertainty estimation framework based on spherical harmonics that enables selective integration of generative priors only where extrapolation artifacts occur. Extensive experiments on multiple datasets, diverse multi-camera setups, and various generative priors demonstrate that ExtraGS significantly enhances the realism and geometric consistency of extrapolated views, while preserving high fidelity along the original trajectory.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</title>
<link>https://arxiv.org/abs/2508.15535</link>
<guid>https://arxiv.org/abs/2508.15535</guid>
<content:encoded><![CDATA[
<div> Keywords: GroupSketch, vector sketch animation, multi-object interactions, Motion Initialization, Motion Refinement

Summary:
GroupSketch introduces a novel method for vector sketch animation, addressing challenges in handling multi-object interactions and complex motions. The method comprises a two-stage pipeline: Motion Initialization and Motion Refinement. In the first stage, the input sketch is divided into semantic groups, defining key frames for coarse animation generation. The second stage utilizes a Group-based Displacement Network (GDN) to refine the animation by predicting group-specific displacement fields. Specialized modules like Context-conditioned Feature Enhancement (CCFE) improve temporal consistency. Extensive experiments show that GroupSketch outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, expanding practical applications in sketch animation.<br /><br />Summary: GroupSketch introduces a two-stage pipeline for vector sketch animation, effectively handling multi-object interactions and complex motions. The method utilizes Motion Initialization and Motion Refinement stages, with a Group-based Displacement Network (GDN) for animation refinement. Specialized modules enhance temporal consistency, demonstrating superior performance compared to existing methods for generating high-quality, multi-object animations. <div>
arXiv:2508.15535v1 Announce Type: new 
Abstract: We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems</title>
<link>https://arxiv.org/abs/2508.15537</link>
<guid>https://arxiv.org/abs/2508.15537</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, road structure segmentation, differential attention, dual-stream fusion, multi-scale dilation <br />
Summary:
D3FNet is a new network designed for segmenting narrow roads in high-resolution remote sensing imagery. It addresses challenges such as limited width, fragmented topology, and occlusions. The network incorporates a Differential Attention Dilation Extraction (DADE) module to enhance road features while suppressing background noise. It also uses a Dual-stream Decoding Fusion Mechanism (DDFM) to balance spatial precision and semantic context by integrating original and attention-modulated features. Additionally, a multi-scale dilation strategy is employed to improve prediction accuracy. D3FNet outperforms existing models by specifically targeting fine-grained, occluded, and low-contrast road segments. Experimental results on DeepGlobe and CHN6-CUG benchmarks demonstrate superior performance in terms of IoU and recall. Ablation studies confirm the effectiveness of attention-guided encoding and dual-path decoding in achieving accurate road segmentation. <br /><br />Summary: D3FNet is a novel network that excels at extracting narrow roads in high-resolution remote sensing images. It introduces innovative modules and strategies to enhance road feature extraction, integration, and prediction accuracy. Results show its superiority in segmenting challenging road regions, making it a robust solution for complex remote perception scenarios. <div>
arXiv:2508.15537v1 Announce Type: new 
Abstract: Extracting narrow roads from high-resolution remote sensing imagery remains a significant challenge due to their limited width, fragmented topology, and frequent occlusions. To address these issues, we propose D3FNet, a Dilated Dual-Stream Differential Attention Fusion Network designed for fine-grained road structure segmentation in remote perception systems. Built upon the encoder-decoder backbone of D-LinkNet, D3FNet introduces three key innovations:(1) a Differential Attention Dilation Extraction (DADE) module that enhances subtle road features while suppressing background noise at the bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates original and attention-modulated features to balance spatial precision with semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9) that mitigates gridding artifacts and improves continuity in narrow road prediction. Unlike conventional models that overfit to generic road widths, D3FNet specifically targets fine-grained, occluded, and low-contrast road segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show that D3FNet achieves superior IoU and recall on challenging road regions, outperforming state-of-the-art baselines. Ablation studies further verify the complementary synergy of attention-guided encoding and dual-path decoding. These results confirm D3FNet as a robust solution for fine-grained narrow road extraction in complex remote and cooperative perception scenarios.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment</title>
<link>https://arxiv.org/abs/2508.15568</link>
<guid>https://arxiv.org/abs/2508.15568</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time adaptation, zero-shot robustness, Gaussian probabilistic inference, class-conditional likelihoods, scalability

Summary:
ADAPT is a novel method for test-time adaptation that improves zero-shot robustness by modeling class-conditional likelihoods using updated class means and a shared covariance matrix. This approach reframes the task as Gaussian probabilistic inference, enabling closed-form, training-free inference without the need for backpropagation or iterative optimization. To address potential bias, ADAPT introduces regularization guided by CLIP priors and historical knowledge. Importantly, this method requires no source data, gradient updates, or full access to target data, supporting online and transductive settings. Extensive experiments demonstrate ADAPT's state-of-the-art performance across diverse benchmarks, showcasing superior scalability and robustness under various distribution shifts. <div>
arXiv:2508.15568v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) enhances the zero-shot robustness under distribution shifts by leveraging unlabeled test data during inference. Despite notable advances, several challenges still limit its broader applicability. First, most methods rely on backpropagation or iterative optimization, which limits scalability and hinders real-time deployment. Second, they lack explicit modeling of class-conditional feature distributions. This modeling is crucial for producing reliable decision boundaries and calibrated predictions, but it remains underexplored due to the lack of both source data and supervision at test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and backPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian probabilistic inference task by modeling class-conditional likelihoods using gradually updated class means and a shared covariance matrix. This enables closed-form, training-free inference. To correct potential likelihood bias, we introduce lightweight regularization guided by CLIP priors and a historical knowledge bank. ADAPT requires no source data, no gradient updates, and no full access to target data, supporting both online and transductive settings. Extensive experiments across diverse benchmarks demonstrate that our method achieves state-of-the-art performance under a wide range of distribution shifts with superior scalability and robustness.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency First: A Two-Stage Approach for Improving Image INR</title>
<link>https://arxiv.org/abs/2508.15582</link>
<guid>https://arxiv.org/abs/2508.15582</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit Neural Representations, spectral bias, high-frequency details, two-stage training, neighbor-aware soft mask

Summary:<br />
Implicit Neural Representations (INRs) represent images as continuous functions over spatial coordinates, offering an alternative to pixel-based formats. However, neural networks tend to favor low-frequency components, leading to difficulty in capturing high-frequency details such as sharp edges and textures. To address this, a two-stage training strategy is proposed, where a neighbor-aware soft mask assigns higher weights to pixels with strong local variations in the first stage, focusing on fine details before transitioning to full-image training. Experimental results demonstrate improved reconstruction quality and complement existing INR methods. This approach introduces frequency-aware importance to pixels in image INR, providing a promising solution to mitigate the spectral bias problem.<br />Summary: <div>
arXiv:2508.15582v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) have emerged as a powerful alternative to traditional pixel-based formats by modeling images as continuous functions over spatial coordinates. A key challenge, however, lies in the spectral bias of neural networks, which tend to favor low-frequency components while struggling to capture high-frequency (HF) details such as sharp edges and fine textures. While prior approaches have addressed this limitation through architectural modifications or specialized activation functions, we propose an orthogonal direction by directly guiding the training process. Specifically, we introduce a two-stage training strategy where a neighbor-aware soft mask adaptively assigns higher weights to pixels with strong local variations, encouraging early focus on fine details. The model then transitions to full-image training. Experimental results show that our approach consistently improves reconstruction quality and complements existing INR methods. As a pioneering attempt to assign frequency-aware importance to pixels in image INR, our work offers a new avenue for mitigating the spectral bias problem.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis</title>
<link>https://arxiv.org/abs/2508.15613</link>
<guid>https://arxiv.org/abs/2508.15613</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud registration, outlier rates, truncated least squares, convex relaxation, Branch and Bound <br />
Summary: 
The paper introduces a new approach for robust point cloud registration, particularly focusing on dealing with high outlier rates. By utilizing a truncated least squares formulation, the proposed method can handle up to 95% outliers. The key innovation lies in a novel linear time convex relaxation and a contractor method to accelerate the Branch and Bound algorithm. This approach achieves provably global optimal results in a significantly reduced time frame compared to existing methods. While currently limited to solving rotation-only TLS problems, the solver showcases impressive speed improvements, outperforming state-of-the-art SDP solvers. Formal proofs for global optimality are provided, along with empirical evidence demonstrating efficacy even in cases with local minima near the global minimum.<br /><br /> <div>
arXiv:2508.15613v1 Announce Type: new 
Abstract: Recent results showed that point cloud registration with given correspondences can be made robust to outlier rates of up to 95\% using the truncated least squares (TLS) formulation. However, solving this combinatorial optimization problem to global optimality is challenging. Provably globally optimal approaches using semidefinite programming (SDP) relaxations take hundreds of seconds for 100 points. In this paper, we propose a novel linear time convex relaxation as well as a contractor method to speed up Branch and Bound (BnB). Our solver can register two 3D point clouds with 100 points to provable global optimality in less than half a second when the axis of rotation is provided. Although it currently cannot solve the full 6DoF problem, it is two orders of magnitude faster than the state-of-the-art SDP solver STRIDE when solving the rotation-only TLS problem. In addition to providing a formal proof for global optimality, we present empirical evidence of global optimality using adversarial instances with local minimas close to the global minimum.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models</title>
<link>https://arxiv.org/abs/2508.15629</link>
<guid>https://arxiv.org/abs/2508.15629</guid>
<content:encoded><![CDATA[
arXiv:2508.15629v1 Announce Type: new 
Abstract: Wildlife and human activities are key components of landscape systems. Understanding their spatial distribution is essential for evaluating human wildlife interactions and informing effective conservation planning. Multiperspective monitoring of wildlife and human activities by combining camera traps and drone imagery. Capturing the spatial patterns of their distributions, which allows the identification of the overlap of their activity zones and the assessment of the degree of human wildlife conflict. The study was conducted in Chitwan National Park (CNP), Nepal, and adjacent regions. Images collected by visible and nearinfrared camera traps and thermal infrared drones from February to July 2022 were processed to create training and testing datasets, which were used to build deep learning models to automatic identify wildlife and human activities. Drone collected thermal imagery was used for detecting targets to provide a multiple monitoring perspective. Spatial pattern analysis was performed to identify animal and resident activity hotspots and delineation potential human wildlife conflict zones. Among the deep learning models tested, YOLOv11s achieved the highest performance with a precision of 96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most effective for detecting objects in camera trap imagery. Drone based thermal imagery, analyzed with an enhanced Faster RCNN model, added a complementary aerial viewpoint for camera trap detections. Spatial pattern analysis identified clear hotspots for both wildlife and human activities and their overlapping patterns within certain areas in the CNP and buffer zones indicating potential conflict. This study reveals human wildlife conflicts within the conserved landscape. Integrating multiperspective monitoring with automated object detection enhances wildlife surveillance and landscape management.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.15641</link>
<guid>https://arxiv.org/abs/2508.15641</guid>
<content:encoded><![CDATA[
arXiv:2508.15641v1 Announce Type: new 
Abstract: Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</title>
<link>https://arxiv.org/abs/2508.15646</link>
<guid>https://arxiv.org/abs/2508.15646</guid>
<content:encoded><![CDATA[
arXiv:2508.15646v1 Announce Type: new 
Abstract: Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance</title>
<link>https://arxiv.org/abs/2508.15650</link>
<guid>https://arxiv.org/abs/2508.15650</guid>
<content:encoded><![CDATA[
arXiv:2508.15650v1 Announce Type: new 
Abstract: Deep neural networks for 3D point clouds have been demonstrated to be vulnerable to adversarial examples. Previous 3D adversarial attack methods often exploit certain information about the target models, such as model parameters or outputs, to generate adversarial point clouds. However, in realistic scenarios, it is challenging to obtain any information about the target models under conditions of absolute security. Therefore, we focus on transfer-based attacks, where generating adversarial point clouds does not require any information about the target models. Based on our observation that the critical features used for point cloud classification are consistent across different DNN architectures, we propose CFG, a novel transfer-based black-box attack method that improves the transferability of adversarial point clouds via the proposed Critical Feature Guidance. Specifically, our method regularizes the search of adversarial point clouds by computing the importance of the extracted features, prioritizing the corruption of critical features that are likely to be adopted by diverse architectures. Further, we explicitly constrain the maximum deviation extent of the generated adversarial point clouds in the loss function to ensure their imperceptibility. Extensive experiments conducted on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the proposed CFG outperforms the state-of-the-art attack methods by a large margin.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction</title>
<link>https://arxiv.org/abs/2508.15653</link>
<guid>https://arxiv.org/abs/2508.15653</guid>
<content:encoded><![CDATA[
arXiv:2508.15653v1 Announce Type: new 
Abstract: Online HD map construction is a fundamental task in autonomous driving systems, aiming to acquire semantic information of map elements around the ego vehicle based on real-time sensor inputs. Recently, several approaches have achieved promising results by incorporating offline priors such as SD maps and HD maps or by fusing multi-modal data. However, these methods depend on stale offline maps and multi-modal sensor suites, resulting in avoidable computational overhead at inference. To address these limitations, we employ a knowledge distillation strategy to transfer knowledge from multimodal models with prior knowledge to an efficient, low-cost, and vision-centric student model. Specifically, we propose MapKD, a novel multi-level cross-modal knowledge distillation framework with an innovative Teacher-Coach-Student (TCS) paradigm. This framework consists of: (1) a camera-LiDAR fusion model with SD/HD map priors serving as the teacher; (2) a vision-centric coach model with prior knowledge and simulated LiDAR to bridge the cross-modal knowledge transfer gap; and (3) a lightweight vision-based student model. Additionally, we introduce two targeted knowledge distillation strategies: Token-Guided 2D Patch Distillation (TGPD) for bird's eye view feature alignment and Masked Semantic Response Distillation (MSRD) for semantic learning guidance. Extensive experiments on the challenging nuScenes dataset demonstrate that MapKD improves the student model by +6.68 mIoU and +10.94 mAP while simultaneously accelerating inference speed. The code is available at:https://github.com/2004yan/MapKD2026.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps</title>
<link>https://arxiv.org/abs/2508.15672</link>
<guid>https://arxiv.org/abs/2508.15672</guid>
<content:encoded><![CDATA[
arXiv:2508.15672v1 Announce Type: new 
Abstract: Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions</title>
<link>https://arxiv.org/abs/2508.15688</link>
<guid>https://arxiv.org/abs/2508.15688</guid>
<content:encoded><![CDATA[
arXiv:2508.15688v1 Announce Type: new 
Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive capability in visual tasks, but their fine-tuning often suffers from bias in class-imbalanced scene. Recent works have introduced large language models (LLMs) to enhance VLM fine-tuning with supplementing semantic information. However, they often overlook inherent class imbalance in VLMs' pre-training, which may lead to bias accumulation in downstream tasks. To address this problem, this paper proposes a Multi-dimensional Dynamic Prompt Routing (MDPR) framework. MDPR constructs a comprehensive knowledge base for classes, spanning five visual-semantic dimensions. During fine-tuning, the dynamic routing mechanism aligns global visual classes, retrieves optimal prompts, and balances fine-grained semantics, yielding stable predictions through logits fusion. Extensive experiments on long-tailed benchmarks, including CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves comparable results with current SOTA methods. Ablation studies further confirm the effectiveness of our semantic library for tail classes, and show that our dynamic routing incurs minimal computational overhead, making MDPR a flexible and efficient enhancement for VLM fine-tuning under data imbalance.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title>
<link>https://arxiv.org/abs/2508.15717</link>
<guid>https://arxiv.org/abs/2508.15717</guid>
<content:encoded><![CDATA[
arXiv:2508.15717v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception</title>
<link>https://arxiv.org/abs/2508.15720</link>
<guid>https://arxiv.org/abs/2508.15720</guid>
<content:encoded><![CDATA[
arXiv:2508.15720v1 Announce Type: new 
Abstract: Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</title>
<link>https://arxiv.org/abs/2508.15751</link>
<guid>https://arxiv.org/abs/2508.15751</guid>
<content:encoded><![CDATA[
arXiv:2508.15751v1 Announce Type: new 
Abstract: Purpose: Recent developments in computational pathology have been driven by advances in Vision Foundation Models, particularly the Segment Anything Model (SAM). This model facilitates nuclei segmentation through two primary methods: prompt-based zero-shot segmentation and the use of cell-specific SAM models for direct segmentation. These approaches enable effective segmentation across a range of nuclei and cells. However, general vision foundation models often face challenges with fine-grained semantic segmentation, such as identifying specific nuclei subtypes or particular cells. Approach: In this paper, we propose the molecular-empowered All-in-SAM Model to advance computational pathology by leveraging the capabilities of vision foundation models. This model incorporates a full-stack approach, focusing on: (1) annotation-engaging lay annotators through molecular-empowered learning to reduce the need for detailed pixel-level annotations, (2) learning-adapting the SAM model to emphasize specific semantics, which utilizes its strong generalizability with SAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating Molecular-Oriented Corrective Learning (MOCL). Results: Experimental results from both in-house and public datasets show that the All-in-SAM model significantly improves cell classification performance, even when faced with varying annotation quality. Conclusions: Our approach not only reduces the workload for annotators but also extends the accessibility of precise biomedical image analysis to resource-limited settings, thereby advancing medical diagnostics and automating pathology image analysis.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waver: Wave Your Way to Lifelike Video Generation</title>
<link>https://arxiv.org/abs/2508.15761</link>
<guid>https://arxiv.org/abs/2508.15761</guid>
<content:encoded><![CDATA[
arXiv:2508.15761v1 Announce Type: new 
Abstract: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling</title>
<link>https://arxiv.org/abs/2508.15767</link>
<guid>https://arxiv.org/abs/2508.15767</guid>
<content:encoded><![CDATA[
arXiv:2508.15767v1 Announce Type: new 
Abstract: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
arXiv:2508.15769v1 Announce Type: new 
Abstract: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Autoregressive Modeling for Instruction-Guided Image Editing</title>
<link>https://arxiv.org/abs/2508.15772</link>
<guid>https://arxiv.org/abs/2508.15772</guid>
<content:encoded><![CDATA[
arXiv:2508.15772v1 Announce Type: new 
Abstract: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Group Inference for Diverse and High-Quality Generation</title>
<link>https://arxiv.org/abs/2508.15773</link>
<guid>https://arxiv.org/abs/2508.15773</guid>
<content:encoded><![CDATA[
arXiv:2508.15773v1 Announce Type: new 
Abstract: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</title>
<link>https://arxiv.org/abs/2508.15774</link>
<guid>https://arxiv.org/abs/2508.15774</guid>
<content:encoded><![CDATA[
arXiv:2508.15774v1 Announce Type: new 
Abstract: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis</title>
<link>https://arxiv.org/abs/2508.14917</link>
<guid>https://arxiv.org/abs/2508.14917</guid>
<content:encoded><![CDATA[
arXiv:2508.14917v1 Announce Type: cross 
Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Structural Phenotypes with Functional Data for Early Prediction of Primary Angle Closure Glaucoma Progression</title>
<link>https://arxiv.org/abs/2508.14922</link>
<guid>https://arxiv.org/abs/2508.14922</guid>
<content:encoded><![CDATA[
arXiv:2508.14922v1 Announce Type: cross 
Abstract: Purpose: To classify eyes as slow or fast glaucoma progressors in patients with primary angle closure glaucoma (PACG) using an integrated approach combining optic nerve head (ONH) structural features and sector-based visual field (VF) functional parameters. Methods: PACG patients with >5 reliable VF tests over >5 years were included. Progression was assessed in Zeiss Forum, with baseline VF within six months of OCT. Fast progression was VFI decline <-2.0% per year; slow progression >-2.0% per year. OCT volumes were AI-segmented to extract 31 ONH parameters. The Glaucoma Hemifield Test defined five regions per hemifield, aligned with RNFL distribution. Mean sensitivity per region was combined with structural parameters to train ML classifiers. Multiple models were tested, and SHAP identified key predictors. Main outcome measures: Classification of slow versus fast progressors using combined structural and functional data. Results: We analyzed 451 eyes from 299 patients. Mean VFI progression was -0.92% per year; 369 eyes progressed slowly and 82 rapidly. The Random Forest model combining structural and functional features achieved the best performance (AUC = 0.87, 2000 Monte Carlo iterations). SHAP identified six key predictors: inferior MRW, inferior and inferior-temporal RNFL thickness, nasal-temporal LC curvature, superior nasal VF sensitivity, and inferior RNFL and GCL+IPL thickness. Models using only structural or functional features performed worse with AUC of 0.82 and 0.78, respectively. Conclusions: Combining ONH structural and VF functional parameters significantly improves classification of progression risk in PACG. Inferior ONH features, MRW and RNFL thickness, were the most predictive, highlighting the critical role of ONH morphology in monitoring disease progression.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot</title>
<link>https://arxiv.org/abs/2508.14994</link>
<guid>https://arxiv.org/abs/2508.14994</guid>
<content:encoded><![CDATA[
arXiv:2508.14994v1 Announce Type: cross 
Abstract: In hazardous and remote environments, robotic systems perform critical tasks demanding improved safety and efficiency. Among these, quadruped robots with manipulator arms offer mobility and versatility for complex operations. However, teleoperating quadruped robots is challenging due to the lack of integrated obstacle detection and intuitive control methods for the robotic arm, increasing collision risks in confined or dynamically changing workspaces. Teleoperation via joysticks or pads can be non-intuitive and demands a high level of expertise due to its complexity, culminating in a high cognitive load on the operator. To address this challenge, a teleoperation approach that directly maps human arm movements to the robotic manipulator offers a simpler and more accessible solution. This work proposes an intuitive remote control by leveraging a vision-based pose estimation pipeline that utilizes an external camera with a machine learning-based model to detect the operator's wrist position. The system maps these wrist movements into robotic arm commands to control the robot's arm in real-time. A trajectory planner ensures safe teleoperation by detecting and preventing collisions with both obstacles and the robotic arm itself. The system was validated on the real robot, demonstrating robust performance in real-time control. This teleoperation approach provides a cost-effective solution for industrial applications where safety, precision, and ease of use are paramount, ensuring reliable and intuitive robotic control in high-risk environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>