<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12547</link>
<guid>https://arxiv.org/abs/2511.12547</guid>
<content:encoded><![CDATA[
<div> Keywords: generative diffusion models, fine-grained augmentation, classifier guidance, temporal dynamics, confidence modulation<br /><br />Summary:<br /><br />Generative diffusion models have shown potential for data augmentation but face challenges in fine-grained visual categorization (FGVC) due to the need for capturing subtle, category-specific features. Standard methods like text-based Classifier-Free Guidance (CFG) often fail to provide the level of specificity required, sometimes generating misleading images that harm classifier performance. To overcome this, the authors propose Hierarchically Guided Fine-grained Augmentation (HiGFA), a novel approach that leverages the temporal sampling stages of diffusion models. HiGFA applies strong text and transformed contour guidance with fixed strength during early-to-mid sampling to set up the overall scene, style, and structure. In the final sampling stages, it shifts to specialized fine-grained classifier guidance while dynamically adjusting the strengths of all guidance signals based on their confidence predictions. This hierarchical and confidence-driven strategy balances the global formation of structure with detailed refinement, resulting in synthetic images that are both diverse and faithful to category-specific features. Experimental results on multiple FGVC datasets demonstrate HiGFA’s effectiveness, showing improved synthetic image quality and enhanced fine-grained classification performance. <div>
arXiv:2511.12547v2 Announce Type: replace 
Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</title>
<link>https://arxiv.org/abs/2511.12919</link>
<guid>https://arxiv.org/abs/2511.12919</guid>
<content:encoded><![CDATA[
<div> Keywords: 6D pose estimation, one-reference view, autoregressive model, coordinate map tokenization, transformer decoder  

<br /><br />Summary:  
This article addresses the challenge of 6D pose estimation for novel objects without access to their full 3D models, a key task in robotics and augmented reality. It proposes CoordAR, an innovative autoregressive framework designed to estimate 6D object poses using only a single reference view, alleviating the need for comprehensive 3D data. CoordAR represents 3D-3D correspondences between the query and reference images as discrete tokens generated in a probabilistic, autoregressive manner, improving global consistency compared to traditional coordinate regression methods. The approach introduces a novel coordinate map tokenization scheme that discretizes the 3D space for probabilistic prediction, enhancing accuracy. Moreover, it employs a modality-decoupled encoding strategy that processes RGB appearance and coordinate information separately to better capture distinct features. CoordAR’s core architecture features an autoregressive transformer decoder conditioned on both position-aligned query features and the sequence of previously generated tokens, enabling effective correspondence regression. Experimental results demonstrate that CoordAR substantially outperforms existing state-of-the-art methods across multiple benchmarks, showing remarkable robustness to common challenges such as symmetry, occlusion, and real-world complexities. This work represents a significant advancement in one-reference view 6D pose estimation with improved consistency and uncertainty handling. <div>
arXiv:2511.12919v2 Announce Type: replace 
Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer</title>
<link>https://arxiv.org/abs/2511.14111</link>
<guid>https://arxiv.org/abs/2511.14111</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, Cascaded-ViT, Cascaded-Chunk Feed Forward Network, Energy Efficiency, Accuracy-Per-FLOP (APF)  <br /><br />Summary:<br /><br />Vision Transformers (ViTs) have shown strong performance in computer vision but are often limited by high computational, memory, and energy requirements, restricting their use in resource-constrained environments. This paper introduces Cascaded-ViT (CViT), a lightweight and compute-efficient vision transformer architecture that incorporates a novel feedforward design termed Cascaded-Chunk Feed Forward Network (CCFFN). CCFFN enhances parameter and FLOP efficiency by splitting input features, achieving this without compromising accuracy. Experimental results on ImageNet-1K demonstrate that the CViT-XL model attains a Top-1 accuracy of 75.5%, while reducing FLOPs by 15% and energy consumption by 3.3% compared to EfficientViT-M5. Across varied model sizes, CViT consistently maintains the lowest energy consumption, making it well-suited for deployment on battery-limited devices such as mobile phones and drones. Additionally, a newly proposed metric, Accuracy-Per-FLOP (APF), which assesses compute efficiency relative to accuracy, ranks CViT models among the top performers. Notably, CViT-L surpasses EfficientViT-M2 in accuracy by 2.2% while maintaining comparable APF scores, underscoring its balanced efficiency and performance. <div>
arXiv:2511.14111v2 Announce Type: replace 
Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.14169</link>
<guid>https://arxiv.org/abs/2511.14169</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, token compression, object-level token merging, human vision system, computational efficiency<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) effectively integrate text and image understanding by converting images into sequences of patch-level tokens aligned with their architectures. <br />2. However, patch-level tokenization causes a quadratic increase in the number of image tokens, which results in significant computational and memory demands. <br />3. This tokenization approach also misaligns with the human vision cognition system, leading to hallucinations and unnecessary computational redundancy. <br />4. To mitigate these issues, the authors propose an object-level token merging strategy termed Adaptive Token compression, which aligns more closely with human vision processes. <br />5. Experimental results across multiple benchmarks demonstrate that their method uses only about 10% of the tokens compared to vanilla models while maintaining nearly 96% of the original performance. <br />6. Furthermore, comprehensive comparisons with relevant works confirm that this approach effectively balances token compression and model accuracy. <br />7. The authors plan to release their code publicly to facilitate further research and application. <div>
arXiv:2511.14169v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</title>
<link>https://arxiv.org/abs/2511.14208</link>
<guid>https://arxiv.org/abs/2511.14208</guid>
<content:encoded><![CDATA[
<div> Video inverse problems, diffusion models, video reconstruction, amortized inference, real-time processing  

<br /><br />Summary:  
1. The paper addresses video inverse problems important for applications like streaming, telepresence, and AR/VR, emphasizing the need for high perceptual quality alongside low latency.  
2. Existing diffusion-based methods provide strong reconstruction quality but either suffer from temporal artifacts due to image diffusion models with temporal regularizers or are too slow for real-time use when using native video diffusion models.  
3. The authors propose InstantViR, an amortized inference framework that uses a pre-trained video diffusion prior and distills a bidirectional video diffusion teacher model into a causal autoregressive student model, which restores degraded videos in a single forward pass.  
4. This distillation is prior-driven, needing only the teacher model and known degradation operators, and does not require paired clean/noisy video training data.  
5. To further improve speed, the paper introduces a LeanVAE backbone replacing the original VAE via a teacher-space regularized distillation, enabling fast latent-space processing.  
6. Experiments across tasks like streaming random inpainting, Gaussian deblurring, and super-resolution demonstrate that InstantViR matches or exceeds diffusion-based baselines’ quality while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100x speedups.  
7. The results establish that diffusion-based video reconstruction can be practical for real-time, interactive, and streaming use cases, integrating high-quality video restoration into modern vision applications. <div>
arXiv:2511.14208v2 Announce Type: replace 
Abstract: Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2D Gaussians Spatial Transport for Point-supervised Density Regression</title>
<link>https://arxiv.org/abs/2511.14477</link>
<guid>https://arxiv.org/abs/2511.14477</guid>
<content:encoded><![CDATA[
<div> Gaussian Spatial Transport, Gaussian splatting, transport plan, pixel-annotation correspondence, computer vision tasks<br /><br />Summary:<br /><br />This paper introduces Gaussian Spatial Transport (GST), a novel framework that uses Gaussian splatting to map probability measures from image coordinate space to annotation maps. The authors propose a Gaussian splatting-based method to estimate pixel-annotation correspondence accurately. Leveraging this correspondence, they compute a transport plan grounded in Bayesian probability, offering a theoretically sound and practical approach. To integrate this transport plan into common neural network training workflows, the paper derives a specialized loss function that quantifies discrepancy after transport, allowing end-to-end optimization. The method's efficacy is validated through extensive experiments on representative computer vision tasks such as crowd counting and landmark detection, demonstrating improved results. Compared to traditional optimal transport methods, GST significantly improves training efficiency by eliminating expensive iterative computations of the transport plan during training. This enhancement makes GST more scalable and practical for real-world applications. The authors also provide open-source code to encourage further research and application of their framework. Overall, GST presents a novel, efficient, and effective approach for aligning image probabilities to annotation maps, with promising impact on tasks requiring precise spatial correspondence in computer vision. <div>
arXiv:2511.14477v2 Announce Type: replace 
Abstract: This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap</title>
<link>https://arxiv.org/abs/2511.14620</link>
<guid>https://arxiv.org/abs/2511.14620</guid>
<content:encoded><![CDATA[
<div> Keywords: fall prediction, biomechanical data, spatio-temporal graph convolutional network, simulation-reality gap, personalization  

<br /><br />Summary:  
Falls are a major cause of injury and decreased independence among older adults, prompting the need for effective fall prediction systems. Vision-based fall prediction offers a non-invasive approach to anticipate falls moments before impact, but the limited availability of real-world fall data poses development challenges. This study introduces the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model integrating pose and biomechanical information via a cross-attention fusion mechanism, which outperforms the baseline ST-GCN by 5.32% and 2.91% F1-score on simulated stunt-actor and MUVIM datasets. The model’s spatio-temporal attention mechanism provides interpretability by highlighting critical joints and temporal phases involved in falls. Despite achieving an 89.0% F1-score with full supervision on simulated data, the model’s zero-shot generalization to unseen subjects falls sharply to 35.9%, indicating a significant simulation-to-reality gap likely caused by biases such as "intent-to-fall" cues. This gap widens for older adults with conditions like diabetes or frailty due to their unique movement patterns. To overcome these limitations, the study proposes personalization strategies and advocates for privacy-preserving data pipelines to facilitate real-world validation. Ultimately, bridging the gap between simulated and real-world data remains critical to developing reliable fall prediction systems for vulnerable elderly populations. <div>
arXiv:2511.14620v2 Announce Type: replace 
Abstract: Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as 'intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2409.13930</link>
<guid>https://arxiv.org/abs/2409.13930</guid>
<content:encoded><![CDATA[
<div> Limited Angle Computed Tomography, Residual Null-Space Diffusion, Stochastic Differential Equations, Range-Null Space Decomposition, Image Reconstruction<br /><br />Summary:<br /><br />This paper addresses the challenge of Limited Angle Computed Tomography (LACT), where incomplete scanning angles cause distortions and artifacts in reconstructed images. To overcome this, the authors propose a novel approach called Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which incorporate mean-reverting stochastic differential equations to model the diffusion process more effectively. The RN-SDEs are designed to serve as a learned prior that enhances image reconstruction quality despite severe degradation. The study demonstrates the generalizability of RN-SDEs through application on two distinct LACT datasets, ChromSTEM and C4KC-KiTS, validating its wide applicability. A key innovation includes the use of Range-Null Space Decomposition (RNSD) based rectification, which enforces data consistency and improves reconstruction accuracy. Extensive experiments reveal that the proposed method outperforms existing state-of-the-art techniques across a broad range of LACT tasks. Furthermore, the paper provides a rigorous quantitative comparison highlighting the computational complexity and runtime efficiency of RN-SDEs, establishing their advantage not only in reconstruction quality but also in practical usability. Overall, this work offers a powerful and efficient solution for recovering high-quality images in challenging limited angle CT scenarios. <div>
arXiv:2409.13930v3 Announce Type: replace-cross 
Abstract: Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, resulting in distortion or artifacts in the reconstructed images. This is referred to as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose the use of Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, we conducted experiments with two different LACT datasets, ChromSTEM and C4KC-KiTS. Through extensive experiments, we demonstrate that by leveraging learned MR-SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, we can recover high-quality images from severely degraded ones and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of RN-SDE with other networks, in terms of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks</title>
<link>https://arxiv.org/abs/2511.17576</link>
<guid>https://arxiv.org/abs/2511.17576</guid>
<content:encoded><![CDATA[
<div> Body Fat Percentage, AI Models, ResNet, Anthropometric Data, Low-Cost Estimation<br /><br />Summary:<br /><br />1. The study addresses the challenge of tracking body fat percentage, which is important for effective weight management, noting the limitations of expensive gold-standard methods like DEXA scans. <br />2. It proposes artificial intelligence (AI) models as accessible, low-cost alternatives utilizing frontal body images and basic anthropometric measurements. <br />3. Researchers compiled a unique dataset of 535 samples, combining 253 cases with anthropometric data (weight, height, neck, ankle, wrist) and 282 images scraped from Reddit posts where users self-reported their body fat percentages, some claiming DEXA-derived values. <br />4. Two model approaches were developed: (1) image-based models built on the ResNet architecture, and (2) regression models relying solely on anthropometric measurements. <br />5. The best-performing image-based model yielded a Root Mean Square Error (RMSE) of 4.44% and a coefficient of determination (R^2) of 0.807, indicating good predictive performance. <br />6. The study also outlines a multimodal fusion framework for combining image and measurement data, which could be employed when paired datasets are available in the future. <br />7. Overall, results indicate AI-assisted methods can provide practical, low-cost estimations of body fat, with potential applications for consumer health and fitness technologies. <div>
arXiv:2511.17576v1 Announce Type: new 
Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding</title>
<link>https://arxiv.org/abs/2511.17596</link>
<guid>https://arxiv.org/abs/2511.17596</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Autoencoder, metadata extraction, broadcast media, LUMA dataset, reconstruction loss<br /><br />Summary:<br /><br />1. The article addresses the challenge in broadcast and media organizations of automating content indexing, tagging, and metadata generation using AI systems that are typically unimodal and thus limited in understanding complex cross-modal relationships.<br /><br />2. To overcome this, the authors propose a Multimodal Autoencoder (MMAE) designed to learn unified representations across text, audio, and visual modalities, enabling end-to-end automation of metadata extraction and semantic clustering.<br /><br />3. The MMAE is trained on the newly introduced LUMA dataset, which contains fully aligned multimodal triplets that reflect real-world media content, allowing for realistic training conditions.<br /><br />4. The model works by minimizing joint reconstruction losses across different modalities, which helps it discover modality-invariant semantic structures without the need for large paired or contrastive datasets.<br /><br />5. Experimental results show significant improvements in clustering and alignment metrics such as Silhouette score, Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI) compared to linear baselines. These findings suggest that reconstruction-based multimodal embeddings can be foundational for scalable metadata generation and cross-modal retrieval in broadcast archives, enhancing automation, searchability, and efficiency in content management workflows. <div>
arXiv:2511.17596v1 Announce Type: new 
Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction</title>
<link>https://arxiv.org/abs/2511.17597</link>
<guid>https://arxiv.org/abs/2511.17597</guid>
<content:encoded><![CDATA[
<div> Wildfire, Dataset, Time-series Forecasting, Multimodal, British Columbia<br /><br />Summary:<br /><br />1. The paper addresses the challenge of wildfire risk prediction, which is complicated by the interplay of fuel conditions, meteorology, topography, and human activities.<br />2. It highlights the scarcity of comprehensive publicly available benchmark datasets that combine long-term temporal data, extensive spatial coverage, and multiple data modalities.<br />3. To fill this gap, the authors introduce a new dataset spanning 25 years at daily resolution, covering 240 million hectares in British Columbia and surrounding areas.<br />4. The dataset includes 38 covariates representing various wildfire drivers such as active fire detections, weather variables, fuel and terrain characteristics, and human-related factors.<br />5. Using this dataset, the study evaluates different time-series forecasting models including CNN-based, linear, Transformer-based, and Mamba-based architectures.<br />6. The authors also explore the impact of position embedding techniques and analyze the relative importance of different factors driving wildfire risk.<br />7. The dataset and code are publicly accessible at the provided GitHub repository, promoting reproducibility and further research in wildfire prediction. <div>
arXiv:2511.17597v1 Announce Type: new 
Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Structured Data Extraction from Perspectively Distorted Documents</title>
<link>https://arxiv.org/abs/2511.17607</link>
<guid>https://arxiv.org/abs/2511.17607</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Character Recognition, Multi-modal Large Language Models, Perspective Distortion, Gemini-1.5-pro, Structure-Recognition Accuracy<br /><br />Summary:<br /><br />1. The paper addresses Optical Character Recognition (OCR) for data extraction in documents, which is important for applications like digitizing medical records and recognizing road signs. <br /><br />2. Multi-modal Large Language Models (LLMs), specifically Gemini-1.5-pro, have shown strong performance in OCR tasks, but their accuracy can be affected by image distortions. <br /><br />3. While previous work focused on the impact of in-plane rotations on OCR accuracy, this study investigates the effects of perspective distortions commonly found in real-world document images, which have higher complexity and degrees of freedom.<br /><br />4. The authors model typical perspective distortions as isosceles-trapezoidal transformations, reducing the complexity from eight parameters to two key parameters: rotation angle and distortion ratio.<br /><br />5. Experiments using synthetically generated documents varying these parameters evaluate both character-recognition accuracy (traditional OCR accuracy) and structure-recognition accuracy (correctness of reading order). <br /><br />6. Results show that while character-recognition accuracy degrades moderately with distortion, structure-recognition accuracy suffers significantly.<br /><br />7. Importantly, a simple rotational correction can improve structure-recognition accuracy, highlighting a practical preprocessing step to enhance OCR performance with multi-modal LLMs on distorted documents.<br /><br />8. These findings provide useful insights for deploying LLM-based OCR in real-world scenarios with perspective distortions. <div>
arXiv:2511.17607v1 Announce Type: new 
Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF</title>
<link>https://arxiv.org/abs/2511.17609</link>
<guid>https://arxiv.org/abs/2511.17609</guid>
<content:encoded><![CDATA[
<div> Keywords: Unscented Kalman Filter, multi-camera tracking, 3D ground truth, pose keypoints, homography projection

<br /><br />Summary:  
This paper presents a novel approach for accurate 3D ground truth estimation crucial for applications like autonomous navigation, surveillance, and robotics. The method uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint annotations from multiple calibrated cameras into precise 3D positions. By leveraging human-annotated 2D data, the proposed multi-camera single-object tracking algorithm projects 2D coordinates into robust 3D world coordinates using homography-based projection combined with UKF fusion. The approach processes multi-view input to estimate both object locations and shapes. It effectively handles common challenges such as occlusions, enabling reliable tracking across views. Evaluation on well-known datasets including CMC, Wildtrack, and Panoptic demonstrates its high accuracy in 3D localization, outperforming existing methods that provide only ground-plane positioning. Importantly, this method estimates the full 3D shape of objects, not just their location on the ground plane. Additionally, it offers a scalable and fully automatic solution suitable for multi-camera systems relying solely on 2D image annotations, removing the need for manual 3D labeling or specialized sensors. <div>
arXiv:2511.17609v1 Announce Type: new 
Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression</title>
<link>https://arxiv.org/abs/2511.17612</link>
<guid>https://arxiv.org/abs/2511.17612</guid>
<content:encoded><![CDATA[
<div> low-light enhancement, traffic images, unsupervised learning, image decomposition, deep learning<br /><br />Summary: Enhancing low-light traffic images is essential for improving perception in autonomous driving, intelligent transportation, and urban surveillance. Due to challenges like low illumination, noise, motion blur, and glare from headlights and street lamps, nighttime traffic images often have poor visibility, which hinders object detection and scene understanding. The authors propose a fully unsupervised multi-stage deep learning framework specifically designed for low-light traffic image enhancement. The framework works by decomposing input images into illumination and reflectance components and progressively refining them through three specialized modules: Illumination Adaptation to correct brightness globally and locally; Reflectance Restoration employing spatial-channel attention to suppress noise and recover structural details; and Over-Exposure Compensation to reconstruct saturated areas and balance scene luminance. The network is trained without paired ground-truth images, relying on self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses. Experimental results on general and traffic-specific datasets show the method outperforms state-of-the-art techniques quantitatively (via PSNR, SSIM, LPIPS, NIQE) and qualitatively. The framework successfully enhances visibility and preserves image structure, thus improving the reliability of downstream perception tasks in real-world low-light traffic environments. <div>
arXiv:2511.17612v1 Announce Type: new 
Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17614</link>
<guid>https://arxiv.org/abs/2511.17614</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, data augmentation, HSMix, superpixels, brightness mixing<br /><br />Summary:<br /><br />1. Medical image segmentation is often hindered by data scarcity due to the high cost of annotation and the rarity of some diseases, which leads to overfitting.<br />2. Traditional strategies like self-supervised and semi-supervised learning help mitigate this but involve complexities such as hand-crafted pretexts or precise pseudo-labeling.<br />3. Data augmentation offers a simpler alternative to address data scarcity and has improved performance in image recognition; however, local image editing augmentation techniques for segmentation remain underexplored.<br />4. The authors propose HSMix, a novel data augmentation method involving hard and soft mixing of medical images using homogeneous regions (superpixels) from two source images, combined with brightness adjustments based on pixel-wise saliency coefficients.<br />5. Corresponding ground-truth segmentation masks undergo the same mixing to ensure label consistency.<br />6. HSMix leverages contour and saliency information to preserve local semantic integrity while enhancing augmentation diversity.<br />7. It is a plug-and-play, model-agnostic method applicable across various medical imaging modalities.<br />8. Extensive experiments show its effectiveness across diverse medical segmentation tasks.<br />9. The source code is publicly available at the provided GitHub repository for reproducibility and further research. <div>
arXiv:2511.17614v1 Announce Type: new 
Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis</title>
<link>https://arxiv.org/abs/2511.17615</link>
<guid>https://arxiv.org/abs/2511.17615</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image, Multi-Concept Personalization, Adaptive Blending, Appearance Attention, Concept Leakage<br /><br />Summary:<br /><br />This paper addresses the challenge of integrating multiple personalized concepts into a single image using text-to-image (T2I) synthesis, a task that existing methods struggle with especially in complex multi-object scenes. The authors identify that current approaches often cause unintended alterations both in personalized and non-personalized regions, resulting in the loss of prompt structure and semantic inconsistencies. To overcome these issues, they propose PnP-MIX, a plug-and-play multi-concept adaptive blending method that requires no additional tuning. PnP-MIX leverages guided appearance attention to faithfully represent each personalized concept’s appearance. Additionally, the approach includes a mask-guided noise mixing strategy designed to protect non-personalized regions (e.g., background or unrelated objects) while accurately integrating personalized objects. To further enhance compositional fidelity and reduce concept leakage—where features of personalized concepts improperly spread to other regions—the authors introduce background dilution++, a technique that improves localization of features within intended areas. Extensive experiments demonstrate that PnP-MIX consistently outperforms existing methods in both single- and multi-concept personalization settings, proving its robustness and superior performance without needing model retraining or tuning. <div>
arXiv:2511.17615v1 Announce Type: new 
Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach</title>
<link>https://arxiv.org/abs/2511.17618</link>
<guid>https://arxiv.org/abs/2511.17618</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Question Answering, Foundational Question Generation, Spatio-temporal Dynamics, Embedding Integration, Visual-Question Alignment

<br /><br />Summary:  
Conventional Video Question Answering (VQA) methods focus primarily on learning from existing question-answer pairs that are mostly event-centric, which limits the model's ability to understand the comprehensive context of a video scene. Due to the scarcity of annotations related to fundamental information such as object categories, spatial arrangements, and descriptive visual attributes, these models struggle to form a holistic understanding of the environment, thereby restricting their reasoning and generalization capabilities. To address this, the paper introduces FIQ (Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach), a novel framework that generates foundational Q&amp;A pairs directly from descriptive information extracted from videos. This approach enriches training data with core scene-level attributes, enabling models to develop deeper foundational comprehension and improve reasoning across tasks. Additionally, the authors propose a VQ-CAlign module designed to align question embeddings with corresponding visual features, maintaining essential contextual cues and enhancing the model’s adaptability for downstream VQA tasks. Experimental evaluations on the SUTD-TrafficQA dataset demonstrate that FIQ outperforms current baseline methods, achieving state-of-the-art performance in video question answering by providing improved generalizability and reasoning capabilities. <div>
arXiv:2511.17618v1 Announce Type: new 
Abstract: Conventional VQA approaches primarily rely on question-answer (Q&amp;A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&amp;A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds</title>
<link>https://arxiv.org/abs/2511.17619</link>
<guid>https://arxiv.org/abs/2511.17619</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, 3D object detection, corner-aligned regression, bird’s-eye-view, weak supervision  

<br /><br />Summary:  
This paper addresses a key challenge in LiDAR-based 3D object detection, where traditional center-aligned regression is unstable because object centers often lie in sparse or empty regions on the bird’s-eye-view (BEV). This instability arises due to the front-surface bias of LiDAR point clouds, which results in noisy and inaccurate bounding box predictions. To solve this, the authors propose a corner-aligned regression method that shifts the prediction target from the unstable centers to geometrically informative corners, which are located in denser, more observable regions of the BEV. The approach leverages geometric constraints linking corners and 2D image boxes, enabling partial recovery of 3D bounding box parameters from corner annotations. This facilitates a weakly supervised learning paradigm that does not require complete 3D labels. They design a simple, corner-aware detection head that can be integrated into existing detectors. Experiments on the KITTI dataset demonstrate that this method improves performance by 3.5% AP compared to center-based baselines and achieves 83% of the accuracy of fully supervised models using only BEV corner clicks, highlighting the effectiveness and practical benefit of the corner-aligned regression strategy. <div>
arXiv:2511.17619v1 Announce Type: new 
Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?</title>
<link>https://arxiv.org/abs/2511.17633</link>
<guid>https://arxiv.org/abs/2511.17633</guid>
<content:encoded><![CDATA[
<div> Keywords: Binary Neural Networks, low-bit precision, depth-wise convolutions, model compression, MobileNet V1<br /><br />Summary:<br /><br />This paper addresses the challenges of extreme quantization in Binary Neural Networks (BNNs), particularly focusing on lightweight architectures with depth-wise convolutions, which traditionally suffer from limited representational capacity and unstable training. To overcome these issues, the authors introduce a novel 1.58-bit convolution technique that improves expressiveness beyond standard binary constraints. Additionally, they propose a pre-Batch Normalization (pre-BN) residual connection designed to stabilize optimization processes by enhancing the Hessian condition number. These two innovations collectively enable, for the first time, successful binarization of depth-wise convolutions in BNNs. The method demonstrates remarkable efficiency, achieving only 33 million operations (OPs) on the ImageNet dataset when using MobileNet V1 as the base architecture. This establishes a new state-of-the-art performance within BNNs, surpassing prior approaches with comparable computational budgets. Beyond ImageNet, the approach consistently delivers superior accuracy on multiple benchmarks, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements reaching up to 9.3 percentage points. Overall, the work significantly advances low-bit precision neural network design by balancing efficiency, accuracy, and training stability. <div>
arXiv:2511.17633v1 Announce Type: new 
Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection</title>
<link>https://arxiv.org/abs/2511.17634</link>
<guid>https://arxiv.org/abs/2511.17634</guid>
<content:encoded><![CDATA[
<div> Keywords: Score-based diffusion, Fokker-Planck, Krylov projection, Sparse solvers, Image generation<br /><br />Summary: This paper introduces a novel framework designed to accelerate score-based diffusion models by reformulating the stable diffusion process within the Fokker-Planck equation framework. This approach transforms image generation into solving large linear systems, which typically demands significant computational resources when processing many images. To address this, the authors propose a cross-matrix Krylov projection method that leverages mathematical similarities among matrices by constructing a shared subspace from "seed" matrices to efficiently solve for subsequent "target" matrices. Experimental results demonstrate that this technique achieves a substantial runtime reduction ranging from 15.8% to 43.7% compared to conventional sparse linear solvers. Furthermore, when benchmarked against DDPM baselines in denoising tasks, the method attains speedups of up to 115×. Importantly, the framework maintains high-quality image generation under fixed computational budgets, unlike DDPM methods which fail to produce recognizable images under the same constraints. Overall, this work presents a practical and efficient solution for high-quality image synthesis in environments with limited computational resources. <div>
arXiv:2511.17634v1 Announce Type: new 
Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification</title>
<link>https://arxiv.org/abs/2511.17635</link>
<guid>https://arxiv.org/abs/2511.17635</guid>
<content:encoded><![CDATA[
<div> Pediatric pancreatitis, machine learning, meta-imputation, Gaussian mixture models, MRI radiomics<br /><br />Summary:<br /><br />1. Pediatric pancreatitis, encompassing both acute and chronic forms, is a severe inflammatory disease that is difficult to diagnose clinically. <br />2. Diagnosing pediatric pancreatitis using machine learning techniques is challenging due to limited data samples and complex multimodal MRI imaging. <br />3. The study proposes Upstream Probabilistic Meta-Imputation (UPMI), a lightweight data augmentation method applied in a low-dimensional meta-feature space, rather than directly on image data. <br />4. Logistic regression models specific to MRI modalities (T1-weighted and T2-weighted images) generate probabilities that form a 7-dimensional meta-feature vector representing patient data. <br />5. Class-conditional Gaussian mixture models are estimated within each fold of cross-validation to generate synthetic meta-features, which supplement real meta-features for training a Random Forest meta-classifier. <br />6. The method was tested on 67 pediatric patients with paired T1W/T2W MRI scans and demonstrated a mean AUC of 0.908 ± 0.072, outperforming the baseline model trained on real data alone (mean AUC 0.864 ± 0.061), thus providing around 5% relative improvement in diagnostic accuracy. <div>
arXiv:2511.17635v1 Announce Type: new 
Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.17636</link>
<guid>https://arxiv.org/abs/2511.17636</guid>
<content:encoded><![CDATA[
<div> Keywords: Out-of-Distribution detection, activation rectification, typical set refinement, skewness-based refinement, energy score  

<br /><br />Summary: Out-of-Distribution (OOD) detection is essential for deploying machine learning models reliably in open-world environments where inputs can be unexpected or anomalous. Activation-based methods improve OOD detection by suppressing anomalous activations and increasing the separation between in-distribution (ID) and OOD samples. However, existing activation rectification approaches tend to ignore individual channel characteristics and distributional skewness, leading to inaccurate estimation of the typical set and improper inclusion of anomalous activations. To overcome this, the paper proposes a typical set refinement technique that incorporates both discriminability and activity to create a channel-aware typical set. Additionally, a skewness-based refinement is introduced to correct distributional bias in the typical set estimation. The refined activations are then used to compute an energy score for OOD detection. Experiments conducted on ImageNet-1K and CIFAR-100 datasets show that the proposed method achieves state-of-the-art performance. Moreover, it generalizes well across different backbone architectures and OOD scoring functions, indicating robustness and broad applicability of the approach. <div>
arXiv:2511.17636v1 Announce Type: new 
Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios</title>
<link>https://arxiv.org/abs/2511.17649</link>
<guid>https://arxiv.org/abs/2511.17649</guid>
<content:encoded><![CDATA[
<div> Autonomous intelligence, Tangible Control Interfaces, Embodied Benchmark, Multi-modal Reasoning, SWITCH Benchmark<br /><br />Summary:<br /><br />The paper introduces SWITCH (Semantic World Interface Tasks for Control and Handling), a new embodied, task-driven benchmark designed to evaluate autonomous agents' abilities to interact effectively with real-world tangible control interfaces (TCIs) like light switches, appliance panels, and embedded GUIs. These TCIs require commonsense reasoning, physics understanding, causal prediction, and outcome verification due to delayed or indirect effects. Current benchmarks inadequately test crucial aspects like grounding, partial observability from video input, and post-hoc verification in practical, situated environments — gaps SWITCH aims to address. SWITCH-Basic, the first iteration, assesses five complementary capabilities: task-aware visual question answering (VQA), semantic UI grounding, action generation, state-transition prediction, and result verification, all from egocentric RGB video input across a diverse set of 98 real devices spanning 351 tasks. Evaluation of commercial and open large multimodal models (LMMMs) shows inconsistent performance, often with a tendency to rely on textual cues rather than fully leveraging visual or video evidence, highlighting current models' limitations in these contexts. The benchmark is released openly along with data, code, and held-out splits to support reproducible evaluation and facilitate community-driven improvements and future iterations, aiming to foster advancements in embodied intelligence and autonomous control. Resources are available at the provided GitHub link. <div>
arXiv:2511.17649v1 Announce Type: new 
Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment</title>
<link>https://arxiv.org/abs/2511.17655</link>
<guid>https://arxiv.org/abs/2511.17655</guid>
<content:encoded><![CDATA[
<div> Keywords: brain tumor classification, deep learning, MRI, compact CNN, explainability<br /><br />Summary:<br /><br />This study presents a comprehensive deep learning system for automated brain tumor classification using MRI images, benchmarking six architectures including five ImageNet-pretrained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom compact CNN with 1.31 million parameters. The system standardizes preprocessing, training protocols, and evaluation metrics, employing AdamW optimizer, CosineAnnealingLR scheduler, and early stopping with patience = 7, ensuring consistent performance comparison across models. Interpretability is enhanced using Grad-CAM and GradientShap explanations to identify anatomically relevant attention areas, addressing black-box concerns. The compact CNN achieves a remarkable 96.49% accuracy, being 100 times smaller than Inception-ResNet V2 and enabling real-time inference (~375 ms) on edge devices, making it feasible for deployment in low-resource settings. Evaluation extends beyond accuracy, incorporating intersection over union, Hausdorff distance, precision-recall curves, and confusion matrices, providing a robust assessment framework. Inception-ResNet V2 attains state-of-the-art results with 99.53% accuracy and above 99.50% in precision, recall, and F1-score, surpassing recent benchmarks. This end-to-end solution balances accuracy, interpretability, and deployability, facilitating trustworthy AI applications in both advanced and under-resourced healthcare systems, with the potential for clinical screening and triage use. <div>
arXiv:2511.17655v1 Announce Type: new 
Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation</title>
<link>https://arxiv.org/abs/2511.17668</link>
<guid>https://arxiv.org/abs/2511.17668</guid>
<content:encoded><![CDATA[
<div> Medical vision-language segmentation, continual learning, parameter-efficient fine-tuning, catastrophic forgetting, Fisher-memory coordination  

<br /><br />Summary:  
This article addresses the problem of catastrophic forgetting in medical vision-language segmentation models when adapting to new anatomical structures, which currently necessitates full retraining and hinders clinical use. It highlights the lack of targeted continual learning methods designed specifically for medical vision-language tasks. To overcome this, the authors propose MedPEFT-CL, a parameter-efficient continual learning framework built on a dual-phase CLIPSeg architecture. The first phase, adaptive learning, uses semantic similarity-based adapter allocation and prompt similarity analysis to efficiently fine-tune parameters for new medical tasks. The second phase, knowledge consolidation, utilizes bi-directional Fisher-memory coordination to prevent forgetting of previously learned tasks. This approach creates a reinforcing cycle where consolidation guides replay priorities and the introduction of new tasks generates challenging samples that improve retention strategies. Key contributions include (1) a semantic-driven adapter allocation mechanism enabling efficient learning of new tasks, (2) a bi-modal LoRA adaptation that reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination to mitigate catastrophic forgetting. Extensive experiments on diverse medical datasets demonstrate that MedPEFT-CL effectively preserves past knowledge, mitigates forgetting, and achieves strong performance retention with minimal parameter overhead, making it suitable for continual learning scenarios in medical vision-language applications. <div>
arXiv:2511.17668v1 Announce Type: new 
Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Person Recognition in Aerial Surveillance: A Decade Survey</title>
<link>https://arxiv.org/abs/2511.17674</link>
<guid>https://arxiv.org/abs/2511.17674</guid>
<content:encoded><![CDATA[
<div> Keywords: aerial surveillance, human-centric tasks, drones, computer vision, machine learning<br /><br />Summary:<br /><br />This paper presents a comprehensive review of over 150 research papers from the past decade focusing on human-centric aerial surveillance tasks. It highlights the emerging use of airborne platforms such as drones and UAVs, which provide advantages including large-scale coverage, mobility, ease of deployment, and covert observation capabilities. The core focus is on detecting, identifying, and re-identifying humans from aerial perspectives, a domain distinct from traditional ground-based surveillance. The paper identifies and discusses unique challenges associated with aerial human-centric tasks, such as viewpoint variations, occlusions, and resolution constraints. It compiles and analyzes publicly available aerial datasets relevant to these tasks, serving as a valuable resource for researchers. Moreover, it delves into contemporary computer vision and machine learning approaches, evaluating how current methods address aerial-specific challenges and suggesting techniques for their improvement. Finally, the review concludes by outlining existing research gaps and open questions, offering guidance for future studies to advance the field of aerial human surveillance effectively. <div>
arXiv:2511.17674v1 Announce Type: new 
Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2511.17681</link>
<guid>https://arxiv.org/abs/2511.17681</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Multi-Object Tracking, multi-modal large language models, motion modality, Vision-Motion-Reference Alignment, Motion-Guided Prediction Head<br /><br />Summary:<br /><br />Referring Multi-Object Tracking (RMOT) builds upon traditional multi-object tracking by incorporating natural language references for multi-modal fusion tracking, but current benchmarks mainly focus on static features like appearance and initial positions, neglecting dynamic object motion changes such as velocity and direction shifts. This static regulation results in temporal inconsistencies between static language references and dynamic visual data, limiting tracking performance. To overcome these challenges, the authors propose the Vision-Motion-Reference aligned RMOT framework (VMRMOT), which integrates a motion modality extracted from object dynamics, enhancing alignment across vision, motion, and language through multi-modal large language models (MLLMs). They introduce motion-aware descriptions based on dynamic behaviors and leverage MLLMs’ temporal reasoning capabilities to extract motion features as the motion modality. The Vision-Motion-Reference Alignment (VMRA) module is designed to hierarchically align visual queries with motion and language references, improving cross-modal consistency, while the Motion-Guided Prediction Head (MGPH) utilizes the motion modality to boost prediction performance. This approach marks the first use of MLLMs for vision-reference alignment in RMOT. Extensive experiments on several RMOT benchmarks show that VMRMOT surpasses existing state-of-the-art methods, demonstrating the effectiveness of incorporating motion information and multimodal alignment in tracking tasks. <div>
arXiv:2511.17681v1 Announce Type: new 
Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Counting Mechanisms in Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17699</link>
<guid>https://arxiv.org/abs/2511.17699</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, counting tasks, mechanistic interpretability, numerical representation, vision-language models<br /><br />Summary:<br /><br />This paper investigates how large language models (LLMs) and large vision-language models (LVLMs) represent and process numerical information during counting tasks. Controlled experiments using repeated textual and visual items facilitate analysis of model behavior by means of causal mediation and activation patching. The authors introduce CountScope, a specialized tool for mechanistic interpretability focused on numerical content. Results demonstrate that individual tokens or visual features embed latent positional count information which can be extracted and generalized across different contexts. Layerwise analysis reveals a structured emergence of numerical representations: lower layers mainly encode smaller counts while higher layers represent larger numbers. The study identifies an internal counter mechanism that updates incrementally with each item, localized primarily in the final token or visual region, and transferable between contexts. In LVLMs, numerical information is found not only in textual embeddings but also within visual embeddings, with counts shifting between background and foreground regions depending on spatial arrangement. Models also exploit structural cues such as textual separators, which act as shortcuts for tracking counts and affect numerical prediction accuracy. Overall, counting is shown to be a structured, progressive, and layerwise computational process in both LLMs and LVLMs, influenced by the architectural properties of the vision encoder. <div>
arXiv:2511.17699v1 Announce Type: new 
Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions</title>
<link>https://arxiv.org/abs/2511.17722</link>
<guid>https://arxiv.org/abs/2511.17722</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, counting performance, attention allocation, synthetic benchmark, attention interventions  

<br /><br />Summary:  
The article addresses how Vision Language Models (VLMs) demonstrate inherent biases when responding to queries about visual properties of images, particularly in tasks requiring specific focus such as counting. The authors create a synthetic benchmark dataset alongside an evaluation framework designed to systematically assess how changes in image and prompt properties affect counting performance. They explore variations in input parameters including the number of objects, object color, background color, texture, and prompt specificity to understand fluctuations in attention allocation within open-source VLMs. The study further introduces attention-based interventions aimed at modulating the model’s focus on visual tokens across different layers. Evaluations of these interventions reveal that although counting remains a difficult task for VLMs—especially under conditions of high visual or linguistic complexity—certain attention modulation techniques can achieve modest improvements. Overall, the research contributes a comprehensive methodology to investigate VLM behavior in counting tasks, emphasizing the interplay between model attention mechanisms and input characteristics, and highlights potential paths toward enhancing counting accuracy in future VLM developments. <div>
arXiv:2511.17722v1 Announce Type: new 
Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography</title>
<link>https://arxiv.org/abs/2511.17724</link>
<guid>https://arxiv.org/abs/2511.17724</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiovascular diseases, X-ray Coronary Angiography, vessel segmentation, domain generalization, channel regularization

<br /><br />Summary: Cardiovascular diseases remain the top cause of mortality worldwide, with X-ray Coronary Angiography (XCA) serving as the clinical gold standard for real-time cardiac interventions. Accurate segmentation of coronary vessels from XCA images is crucial for quantitative assessments such as stenosis severity measurement, which supports clinical decision-making. However, domain shifts caused by diverse imaging protocols and patient demographics complicate the development of generalized vessel segmentation models. This problem is intensified by the scarcity of annotated datasets, necessitating the use of Single-source Domain Generalization (SDG) techniques. Existing SDG methods predominantly rely on augmentation, which may not effectively prevent overfitting to artificial or synthetic domains. To address these challenges, the authors propose "AngioDG," a novel approach incorporating a channel regularization strategy that enhances domain generalization. AngioDG interprets contributions of early feature channels to task-specific metrics and dynamically reweights these channels, amplifying domain-invariant features while suppressing domain-specific ones. Evaluation on six distinct XCA datasets demonstrates that AngioDG achieves superior out-of-distribution segmentation performance compared to existing methods, without compromising in-domain test accuracy, thus proving its effectiveness and robustness for practical applications in coronary vessel segmentation. <div>
arXiv:2511.17724v1 Announce Type: new 
Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation</title>
<link>https://arxiv.org/abs/2511.17727</link>
<guid>https://arxiv.org/abs/2511.17727</guid>
<content:encoded><![CDATA[
<div> Stroke rehabilitation, vision-language models, motion identification, dose quantification, impairment prediction<br /><br />Summary:<br /><br />1. The study explores the application of vision-language models (VLMs) in stroke rehabilitation, focusing on two main challenges: automatic quantification of rehabilitation dose and impairment from videos.<br /><br />2. The problems are framed as motion-identification tasks that can be addressed by VLMs without requiring task-specific training or finetuning.<br /><br />3. The evaluation was conducted on a cohort consisting of 29 healthy controls and 51 stroke survivors to assess the effectiveness of current VLMs.<br /><br />4. Results indicate that existing VLMs do not have sufficient fine-grained motion understanding to precisely quantify rehabilitation dose or reliably predict impairment scores.<br /><br />5. Despite limitations, optimized prompting and post-processing enable VLMs to classify high-level activities from a few video frames, detect motion and grasp with moderate accuracy, and estimate dose counts within 25% of the ground truth for mildly impaired and healthy individuals.<br /><br />6. The findings emphasize both the current shortcomings and the future potential of using VLMs for data-driven stroke rehabilitation and broader clinical video analysis applications. <div>
arXiv:2511.17727v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.17731</link>
<guid>https://arxiv.org/abs/2511.17731</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought, multimodal reasoning, VisReason, visual datasets, large-scale annotation  

<br /><br />Summary:  
This paper addresses the gap in Chain-of-Thought (CoT) prompting for multimodal large language models (MLLMs), highlighting the lack of extensive datasets that enable rich, spatially grounded visual reasoning. It introduces VisReason, a large-scale dataset with 489,000 annotated examples across four diverse domains, featuring multi-round, human-like rationales that support interpretable and compositional visual reasoning processes. Additionally, the authors present VisReason-Pro, a refined subset of 165,000 examples created using an expert-level GPT annotator, which includes detailed reasoning traces and 3D spatial grounding facilitated by depth-aware annotations. The datasets are used to fine-tune the Qwen2.5-VL model, resulting in significant gains in stepwise visual reasoning accuracy, interpretability, and generalization across benchmarks. Experimental results demonstrate that training on VisReason improves the systematic and generalizable reasoning capabilities of MLLMs. The paper positions VisReason as a foundational resource to advance human-like visual reasoning and stimulate progress towards next-generation multimodal intelligence systems. <div>
arXiv:2511.17731v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2511.17735</link>
<guid>https://arxiv.org/abs/2511.17735</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, foundation models, feature discovery, ecological imagery, scientific archives<br /><br />Summary: This paper addresses the challenge of discovering unknown patterns in vast scientific archives by leveraging foundation models trained on large-scale datasets. Unlike typical methods that focus on extracting features for pre-specified targets and are limited to confirming known patterns, the authors explore the potential of sparse autoencoders (SAEs) to enable open-ended feature discovery. They conduct controlled rediscovery studies to evaluate the alignment of SAE-learned features with semantic concepts on standard segmentation benchmarks, comparing their performance with other label-free methods using concept-alignment metrics. A key scientific case study applies this approach to ecological imagery, demonstrating the ability of SAEs to identify fine-grained anatomical structures without relying on segmentation or part labels, thus validating the method against ground-truth data. Despite focusing primarily on vision tasks within ecology, the methodology is domain-agnostic and applicable to other scientific model domains such as protein structures, genomics, and climate data. The results suggest that sparse decomposition through SAEs provides a practical and scalable tool for probing the internal representations of foundation models, facilitating a transition from merely confirming known information to enabling genuine scientific discovery across disciplines. <div>
arXiv:2511.17735v1 Announce Type: new 
Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2511.17747</link>
<guid>https://arxiv.org/abs/2511.17747</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, identity masking, adversarial perturbations, privacy preservation, face verification<br /><br />Summary:<br /><br />The article introduces AEGIS, a novel privacy-preserving framework designed specifically for 3D Gaussian Splatting-based photorealistic facial avatars. With the surge of 3D avatars in biometric authentication systems, concerns about online identity theft have intensified, particularly due to the lack of robust methods for identity masking beyond 2D images. AEGIS uniquely addresses the challenge by applying adversarial perturbations directly to the Gaussian color coefficients of the avatar, guided by a pre-trained face verification network. This approach ensures consistent identity concealment across multiple viewpoints without needing to retrain the avatar’s geometry or structure. The framework successfully achieves full de-identification, demonstrated by the reduction of face retrieval and verification accuracy to 0%. Despite these modifications, AEGIS maintains high perceptual quality, as measured by SSIM and PSNR scores (0.9555 and 35.52 dB respectively). Importantly, it preserves essential facial attributes like age, race, gender, and emotion, balancing privacy protection with minimal visual distortion. This work fills a critical gap in 3D avatar privacy and provides a practical solution for safeguarding identity in dynamic, multi-view biometric contexts. <div>
arXiv:2511.17747v1 Announce Type: new 
Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration</title>
<link>https://arxiv.org/abs/2511.17750</link>
<guid>https://arxiv.org/abs/2511.17750</guid>
<content:encoded><![CDATA[
<div> Keywords: image correspondence, feature matching, 3D geometry, vision foundation models, SPIDER<br /><br />Summary:<br /><br />Reliable image correspondences are essential for vision-based spatial perception, enabling tasks such as 3D structure reconstruction and camera pose estimation. However, matching features across diverse domains like aerial, indoor, and outdoor scenes remains difficult due to significant variations in appearance, scale, and viewpoint. Traditionally, feature matching has focused on 2D-to-2D correspondences, but recent advances in 3D foundation models have introduced spatially coherent matching based on two-view geometry. Despite their strengths, these 3D models tend to focus on dominant planar regions such as walls or ground surfaces, often missing finer geometric details, especially when the viewpoint changes drastically. To understand these limitations, the authors conducted linear probe experiments evaluating various vision foundation models' performance in image matching. Based on these findings, they propose SPIDER, a universal image matching framework that uses a shared backbone for feature extraction combined with two specialized network heads designed to estimate both 2D and 3D correspondences ranging from coarse to fine scales. Finally, the authors present a new evaluation benchmark targeting unconstrained scenarios with large baselines to better assess image matching performance. SPIDER significantly outperforms state-of-the-art methods, showcasing its versatility and strength as a universal image matching approach. <div>
arXiv:2511.17750v1 Announce Type: new 
Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation</title>
<link>https://arxiv.org/abs/2511.17755</link>
<guid>https://arxiv.org/abs/2511.17755</guid>
<content:encoded><![CDATA[
<div> Reasoning segmentation, semi-supervised learning, multimodal LLM, pseudo-label filtering, contrastive alignment<br /><br />Summary:  
Reasoning segmentation aims to generate pixel-accurate masks for targets described by complex and often implicit instructions, necessitating context-dependent reasoning across scenes. Despite recent advances from multimodal language models in instruction-following segmentation, these models struggle with generalization due to limited and costly labeled data capturing rich linguistic context. To address this, the authors propose CORA, a semi-supervised framework that leverages both limited labeled images and a large set of unlabeled data for training. CORA introduces three core innovations: conditional visual instructions that encode spatial and contextual object relationships, a noisy pseudo-label filtering technique based on consistency of outputs from Multimodal Large Language Models (LLMs) across semantically equivalent queries, and a token-level contrastive alignment mechanism to improve feature consistency between labeled and pseudo-labeled samples. Collectively, these components enable CORA to perform robust reasoning segmentation under minimal supervision. Evaluation on benchmark datasets shows strong results, with CORA outperforming existing baselines by 2.3% on Cityscapes using only 100 labeled images, and improving performance by 2.4% on PanNuke with just 180 labeled images. This demonstrates CORA’s effectiveness in scenarios with constrained annotation availability across diverse domains such as urban scenes and histopathology. <div>
arXiv:2511.17755v1 Announce Type: new 
Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers</title>
<link>https://arxiv.org/abs/2511.17757</link>
<guid>https://arxiv.org/abs/2511.17757</guid>
<content:encoded><![CDATA[
<div> Hyperspectral Unmixing, Transformer, Variational Autoencoder, Dirichlet Prior, Endmember Variability<br /><br />Summary:  
The paper introduces the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T), a novel model for hyperspectral unmixing aimed at separating mixed spectral signatures into pure material components.  The approach harnesses the global context modeling strength of transformer architectures combined with a Dirichlet prior in the latent space, which enforces physically meaningful constraints like sum-to-one and non-negativity for the abundance estimates. Unlike traditional methods relying on fixed ground truth spectra, LDVAE-T treats materials as bundled endmembers, allowing the model to capture intrinsic spectral variability. Specifically, the decoder outputs a mean spectrum and segmentwise covariance for each endmember in each patch, modeling correlated spectral variability. The transformer encoder then produces Dirichlet-distributed abundances to mix these learned bundles, preserving both interpretability and flexibility. Experimentation on three benchmark datasets—Samson, Jasper Ridge, and HYDICE Urban—demonstrates that LDVAE-T consistently outperforms current state-of-the-art models in abundance estimation and endmember extraction. Performance improvements are quantitatively measured using root mean squared error and spectral angle distance, showing the method’s enhanced accuracy and robustness in hyperspectral image analysis. <div>
arXiv:2511.17757v1 Announce Type: new 
Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deepfake Geography: Detecting AI-Generated Satellite Images</title>
<link>https://arxiv.org/abs/2511.17766</link>
<guid>https://arxiv.org/abs/2511.17766</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, satellite imagery, Vision Transformers, deepfake detection, model interpretability  

<br /><br />Summary:  
The rapid progress of generative models such as StyleGAN2 and Stable Diffusion threatens the authenticity of satellite imagery, which is crucial for scientific and security applications. Unlike facial deepfake detection, satellite imagery presents unique challenges including terrain inconsistencies and structural artifacts that require specialized approaches. This study compares Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for identifying AI-generated satellite images. A large, curated dataset of over 130,000 RGB images from DM-AER and FSI datasets was used for evaluation. Results show that ViTs outperform CNNs significantly, achieving 95.11% accuracy versus 87.02%, due to their superior capability to capture long-range dependencies and global semantic information. To enhance transparency, interpretability techniques like Grad-CAM (for CNNs) and Chefer's attention attribution (for ViTs) were applied, uncovering differing model behaviors and supporting the trustworthiness of decisions. The study finds ViTs excel at detecting structural inconsistencies and repetitive texture patterns typical of synthetic satellite images. Future research will expand these findings to multispectral and SAR data, and incorporate frequency-domain analysis to improve detection robustness and protect the integrity of satellite imagery in critical use cases. <div>
arXiv:2511.17766v1 Announce Type: new 
Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</title>
<link>https://arxiv.org/abs/2511.17792</link>
<guid>https://arxiv.org/abs/2511.17792</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, robot path planning, Target-Bench, semantic targets, trajectory accuracy<br /><br />Summary: The paper introduces Target-Bench, the first benchmark designed specifically to evaluate world models on mapless robot path planning toward semantic targets in real-world environments. Target-Bench includes 450 video sequences collected by robots across 45 semantic categories, accompanied by SLAM-based ground truth trajectories to support quantitative analysis. The authors develop an evaluation pipeline that recovers camera motion from generated videos and assesses planning performance using five complementary metrics focused on target-reaching capability, trajectory accuracy, and directional consistency. State-of-the-art world models such as Sora 2, Veo 3.1, and the Wan series are evaluated, with the best off-the-shelf model (Wan2.2-Flash) achieving an overall score of only 0.299, highlighting significant current limitations for robotic planning tasks. They further demonstrate that fine-tuning an open-source 5-billion-parameter world model on only 325 scenarios from Target-Bench improves the overall score to 0.345, marking over a 400% increase relative to the base model (0.066) and a 15% improvement over the best off-the-shelf model. The authors commit to releasing the code and dataset to foster further research and improvements in world model-based robot path planning. <div>
arXiv:2511.17792v1 Announce Type: new 
Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Guided Alignment in Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17793</link>
<guid>https://arxiv.org/abs/2511.17793</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, attention patterns, object hallucination, cross-attention layers, Segment Anything Model<br /><br />Summary: This paper investigates the challenges faced by Large Vision-Language Models (VLMs), focusing on the multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs). The authors identify that concatenation-based architectures often struggle to differentiate semantically matching image-text pairs from non-matching ones, which contributes significantly to object hallucination in these models. To overcome this, they propose a novel framework called Attention-Guided Efficient Vision-Language Models (AGE-VLM). AGE-VLM introduces interleaved cross-attention layers designed to embed vision capabilities directly into small pretrained language models, thereby improving visual grounding. The approach leverages spatial knowledge distilled from the Segment Anything Model (SAM) to guide the attention mechanism, ensuring the model "looks" at the correct image regions. This targeted attention reduces hallucination substantially. The framework is evaluated on various vision-centric benchmarks, where it demonstrates performance that is either superior or comparable to existing efficient VLM methods. The study offers key insights into attention behavior in vision-language fusion, paving the way for future research aimed at enhancing visual and linguistic understanding in multimodal AI systems. <div>
arXiv:2511.17793v1 Announce Type: new 
Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pillar-0: A New Frontier for Radiology Foundation Models</title>
<link>https://arxiv.org/abs/2511.17803</link>
<guid>https://arxiv.org/abs/2511.17803</guid>
<content:encoded><![CDATA[
<div> Radiology, Foundation Model, CT, MRI, AUROC  

<br /><br />Summary:  
This paper introduces Pillar-0, a radiology foundation model pretrained on a large dataset comprising 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a major academic center. The study also presents RATE, a scalable framework that leverages large language models (LLMs) to extract structured labels for 366 radiologic findings with near-perfect accuracy. Pillar-0 processes volumetric CT and MRI data in a way that preserves high-fidelity 3D and grayscale contrast information, addressing limitations of prior models that treated scans as low-quality 2D slices. Testing on extensive internal datasets shows Pillar-0 achieves mean AUROCs between 82.9 and 90.1 across different body regions, outperforming leading existing models such as MedGemma, MedImageInsight, Lingshu, and Merlin by 7.8 to 15.8 AUROC points and leading in 87.2% of tasks. External validation on the Stanford Abdominal CT dataset confirms its superiority over top baselines. Beyond classification, Pillar-0 extends to lung cancer risk prediction, surpassing the previous state-of-the-art Sybil by 3.0 C-index points on NLST and generalizing well to other cohorts. Additionally, it demonstrates sample efficiency in brain hemorrhage detection, achieving over 95 AUROC using significantly less data. Together, Pillar-0 and RATE offer an open, clinically rigorous foundation for high-performance radiology AI, enabling advanced applications previously limited by computational and data constraints. <div>
arXiv:2511.17803v1 Announce Type: new 
Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking</title>
<link>https://arxiv.org/abs/2511.17805</link>
<guid>https://arxiv.org/abs/2511.17805</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural activities, self-supervised learning, Plackett-Luce model, video representation, temporal order<br /><br />Summary:<br />1. The paper addresses the challenge that current self-supervised learning (SSL) methods fail to capture the procedural nature inherent in activities such as cooking and surgical operations, which follow a specific temporal order.<br />2. It demonstrates with a motivating experiment that models pretrained on forward and time-reversed video sequences produce nearly identical features, indicating a lack of awareness of workflow progression.<br />3. To overcome this, the authors propose PL-Stitch, a novel SSL framework that leverages the temporal order of video frames as an explicit supervisory signal.<br />4. PL-Stitch incorporates two probabilistic objectives based on the Plackett-Luce (PL) model: a primary objective which trains the model to sort frames chronologically, and a secondary spatio-temporal jigsaw loss that captures detailed cross-frame object relationships.<br />5. Extensive evaluation on five benchmarks related to surgical phase recognition and cooking action segmentation shows that PL-Stitch significantly outperforms existing methods, yielding improvements such as +11.4 percentage points in k-NN accuracy on Cholec80 and +5.7 points in linear probing accuracy on Breakfast, thus proving its effectiveness in procedural video representation learning. <div>
arXiv:2511.17805v1 Announce Type: new 
Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion</title>
<link>https://arxiv.org/abs/2511.17806</link>
<guid>https://arxiv.org/abs/2511.17806</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view radar, 3D bounding box diffusion, indoor perception, cross-view feature association, object detection  

<br /><br />Summary:  
1. The paper addresses multi-view indoor radar perception, emphasizing its advantages in cost-effectiveness and privacy preservation compared to other sensing modalities.  
2. Existing methods often depend on implicit cross-view radar feature associations, such as proposal pairing or query-to-feature cross-attention, which can cause ambiguous feature matching and reduce detection performance in complex indoor environments.  
3. To overcome these issues, the authors propose REXO, a novel approach that extends the 2D bounding box diffusion process from DiffusionDet into 3D radar space, enabling explicit cross-view radar feature association.  
4. REXO uses noisy 3D bounding boxes to guide a cross-view radar-conditioned denoising process, improving the clarity and accuracy of feature matching across views.  
5. By incorporating prior knowledge that the detected person is in contact with the ground, REXO reduces diffusion parameters, enhancing computational efficiency without compromising performance.  
6. The method is validated on two publicly available indoor radar datasets, HIBER and MMVR, where it outperforms state-of-the-art approaches by significant margins (+4.22 AP on HIBER and +11.02 AP on MMVR), demonstrating its effectiveness for multi-view indoor radar object detection tasks. <div>
arXiv:2511.17806v1 Announce Type: new 
Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Weighted Non-IID Sampling for Flow Matching Models</title>
<link>https://arxiv.org/abs/2511.17812</link>
<guid>https://arxiv.org/abs/2511.17812</guid>
<content:encoded><![CDATA[
<div> Keywords: flow-matching models, importance-weighted sampling, non-IID sampling, score-based regularization, residual velocity field<br /><br />Summary:  
The paper addresses the challenge of estimating expectations of functions from flow-matching model outputs under limited sampling budgets. Traditional independent sampling methods result in high-variance estimates, particularly when rare but influential outcomes dominate. The authors propose an innovative importance-weighted non-IID sampling framework that draws multiple correlated samples to better explore diverse, prominent areas of the flow's distribution while preserving unbiasedness through estimated importance weights. A key contribution is the introduction of a score-based regularization mechanism that leverages the gradient of the log probability (score function) to promote sample diversity specifically within high-density regions, preventing samples from drifting off the data manifold. Additionally, the work pioneers an approach for computing importance weights of non-IID samples by learning a residual velocity field, ensuring the generated samples' marginal distribution matches the target distribution. Empirical results demonstrate that this method produces both diverse and high-quality samples as well as precise estimates of importance weights and expectations. Overall, the approach significantly improves the reliable characterization and estimation of outputs from flow-matching models. The authors also commit to releasing their code publicly on GitHub, enabling further research and verification. <div>
arXiv:2511.17812v1 Announce Type: new 
Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QAL: A Loss for Recall Precision Balance in 3D Reconstruction</title>
<link>https://arxiv.org/abs/2511.17824</link>
<guid>https://arxiv.org/abs/2511.17824</guid>
<content:encoded><![CDATA[
<div> Volumetric learning, 3D vision, Quality-Aware Loss, Chamfer Distance, robotic manipulation<br /><br />Summary:<br /><br />This paper addresses the limitations of existing training objectives for volumetric learning in 3D vision tasks such as completion, reconstruction, and mesh generation, which predominantly rely on Chamfer Distance (CD) or Earth Mover’s Distance (EMD). These traditional metrics do not effectively balance recall and precision, often missing thin structures and under-represented regions. To overcome this, the authors propose a novel Quality-Aware Loss (QAL), which serves as a drop-in replacement for CD/EMD. QAL uniquely combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into adjustable components. Experimental results demonstrate that QAL consistently improves coverage, achieving average gains of +4.3 points over CD and +2.8 points over other leading alternatives. These improvements, while modest in percentage, significantly enhance the recovery of fine details that previous methods overlook. Extensive ablation studies confirm the stable performance of QAL across various hyperparameters and output resolutions. Further, retraining experiments on datasets such as PCN and ShapeNet validate its generalization across architectures and data. Importantly, completions trained with QAL yield higher grasp scores in the GraspNet evaluation, indicating direct benefits for robotic manipulation reliability. Overall, QAL presents a principled, interpretable, and practical training objective for enhanced 3D vision and safety-critical robotics applications. <div>
arXiv:2511.17824v1 Announce Type: new 
Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations</title>
<link>https://arxiv.org/abs/2511.17828</link>
<guid>https://arxiv.org/abs/2511.17828</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation models, breast imaging, BiomedCLIP, BI-RADS classification, multi-modality training<br /><br />Summary:<br /><br />This study explores the application of foundation models, specifically BiomedCLIP, in the domain of breast imaging, which has been less investigated compared to other medical imaging tasks. The researchers adapted BiomedCLIP to automate BI-RADS breast density classification by utilizing multi-modality mammographic data including synthesized 2D images, digital mammography (DM), and digital breast tomosynthesis (DBT). A comprehensive dataset of 96,995 mammographic images was employed, comparing single-modality training (synthesized 2D only) against multi-modality training approaches. Both approaches achieved comparable accuracy (0.73 for single-modality and 0.74 for multi-modality), but the multi-modality model demonstrated greater versatility across imaging types and maintained consistently high AUC values above 0.84 across different BI-RADS categories. Additionally, external validation on the RSNA and EMBED datasets confirmed strong generalization capabilities with AUC scores ranging from 0.80 to 0.93. GradCAM visualizations illustrated clinically relevant and consistent model attention, enhancing interpretability and robustness. Overall, this research highlights the promise of foundation models like BiomedCLIP for breast imaging tasks, suggesting potential future applications in diagnostics and broader clinical utility. <div>
arXiv:2511.17828v1 Announce Type: new 
Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Show Me: Unifying Instructional Image and Video Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.17839</link>
<guid>https://arxiv.org/abs/2511.17839</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, image manipulation, video prediction, spatial-temporal consistency, instruction-guided generation<br /><br />Summary:<br /><br />The paper addresses the challenge of generating visual instructions in context by unifying two traditionally separate tasks: text-guided image manipulation and video prediction. Prior methods either focus on static spatial changes or temporal action sequences but rarely integrate both, leading to shortcomings such as ignoring the progression of actions or the final goals. To overcome this, the authors propose ShowMe, a framework that selectively activates spatial and temporal components within video diffusion models to perform both tasks seamlessly. They introduce structure and motion consistency rewards to ensure structural fidelity and temporal coherence in generated outputs. The framework leverages the spatial knowledge gained from video pretraining to enhance the realism and contextual consistency of non-rigid image edits. Concurrently, the instruction-guided manipulation capability strengthens the model's goal-oriented reasoning for video prediction. Experimental results across multiple benchmarks demonstrate that ShowMe outperforms specialized expert models in both instructional image and video generation tasks. This highlights the effectiveness of video diffusion models as unified transformers capable of modeling action-object states over space and time, enabling more interactive and context-aware visual instruction generation. <div>
arXiv:2511.17839v1 Announce Type: new 
Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception</title>
<link>https://arxiv.org/abs/2511.17843</link>
<guid>https://arxiv.org/abs/2511.17843</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent cooperative perception, bandwidth efficiency, semantic feature encoding, redundancy elimination, communication policy  

<br /><br />Summary:  
This paper addresses the challenge of limited communication bandwidth in multi-agent cooperative perception (CP) systems, such as those used in autonomous driving, where occlusion and limited sensing range hinder single-agent performance. The authors critique existing methods that focus on compression or heuristic message selection without considering the semantic relevance or redundancy across agents’ sensory data. To maximize the utility of every transmitted bit, they propose a joint semantic feature encoding and transmission framework named JigsawComm. JigsawComm is end-to-end trainable and uses a regularized encoder to extract semantically relevant, sparse features from each agent. It incorporates a lightweight Feature Utility Estimator that predicts each agent's feature contribution to the final perception task, producing meta utility maps exchanged among agents. These maps enable the computation of an optimal transmission policy that selects features with the highest utility scores per location, effectively eliminating redundant transmissions. This policy scales with constant communication cost \(\mathcal{O}(1)\) regardless of the number of agents. Evaluations on the OPV2V and DAIR-V2X benchmarks demonstrate that JigsawComm can reduce data volume by more than 500 times compared to prior methods while maintaining or improving perception accuracy, highlighting its communication efficiency and semantic awareness in cooperative perception tasks. <div>
arXiv:2511.17843v1 Announce Type: new 
Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2511.17844</link>
<guid>https://arxiv.org/abs/2511.17844</guid>
<content:encoded><![CDATA[
<div> Fine-tuning, text-to-video diffusion models, generative controls, synthetic data, data efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of fine-tuning large-scale text-to-video diffusion models to incorporate new generative controls, such as physical camera parameters like shutter speed and aperture. Traditional methods require vast, high-quality datasets which are difficult and expensive to obtain. The authors propose a novel, data-efficient fine-tuning strategy that leverages sparse, low-quality synthetic data instead of photorealistic real-world data. Their experiments reveal that models fine-tuned on such simple synthetic datasets not only learn the desired controls effectively but also outperform models fine-tuned on high-fidelity "real" data. To explain this unexpected outcome, the paper provides a comprehensive framework that both intuitively and quantitatively justifies why training on synthetic data leads to superior generative control performance. Overall, the work highlights a promising direction for improving text-to-video model customization while significantly lowering the data requirements and costs associated with fine-tuning these complex generative systems. <div>
arXiv:2511.17844v1 Announce Type: new 
Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use</title>
<link>https://arxiv.org/abs/2511.17881</link>
<guid>https://arxiv.org/abs/2511.17881</guid>
<content:encoded><![CDATA[
<div> Document Visual Question Answering, spatial graph reasoning, memory-augmented inference, multi-modal framework, interpretable reasoning<br /><br />Summary: Document Visual Question Answering (DocVQA) tasks require models to understand and integrate textual semantics, spatial layouts, and visual features, yet current approaches face challenges in explicit spatial reasoning, handling high-resolution documents efficiently, performing multi-hop reasoning, and maintaining interpretability. The proposed MGA-VQA framework addresses these limitations by combining token-level encoding with spatial graph reasoning, allowing the model to capture intricate spatial relationships within documents. It further incorporates a memory-augmented inference mechanism to improve multi-step reasoning capabilities, enhancing the model’s ability to process complex queries. To manage computational efficiency, MGA-VQA employs question-guided compression techniques that reduce processing overhead without sacrificing critical information. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways, enabling transparent reasoning processes, along with structured memory access which makes intermediate reasoning steps easier to understand. Comprehensive evaluations on six benchmark datasets—FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO—demonstrate that MGA-VQA achieves superior accuracy and efficiency, consistently improving both answer prediction and spatial localization tasks. The results highlight the framework’s strength in balancing performance with interpretability and computational efficiency in DocVQA scenarios. <div>
arXiv:2511.17881v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArticFlow: Generative Simulation of Articulated Mechanisms</title>
<link>https://arxiv.org/abs/2511.17883</link>
<guid>https://arxiv.org/abs/2511.17883</guid>
<content:encoded><![CDATA[
<div> Articulated 3D generation, flow matching, action-conditioned kinematics, latent flow, point cloud synthesis<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating articulated 3D shapes, which involve complex, action-dependent deformations and suffer from limited datasets compared to static 3D shape generation.<br />2. The authors propose ArticFlow, a novel two-stage flow matching framework that learns a controllable velocity field transforming noise to target point sets with explicit action control.<br />3. ArticFlow consists of two coupled components: a latent flow that maps noise into a shape-prior code and a point flow that moves points conditioned on both the action and the shape prior, enabling flexible generation across various articulated categories.<br />4. Experiments on the MuJoCo Menagerie dataset demonstrate that ArticFlow functions effectively as both a generative model and a neural simulator, capable of predicting action-conditioned kinematics and synthesizing novel morphologies by interpolating in the latent space.<br />5. Compared to specialized object simulators and action-conditioned static point-cloud generators, ArticFlow shows superior kinematic accuracy and improved shape quality, highlighting action-conditioned flow matching as a promising approach for controllable, high-quality articulated mechanism generation. <div>
arXiv:2511.17883v1 Announce Type: new 
Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning</title>
<link>https://arxiv.org/abs/2511.17885</link>
<guid>https://arxiv.org/abs/2511.17885</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, visual token pruning, mixture-of-experts, FastMMoE, inference acceleration<br /><br />Summary:<br /><br />1. Multimodal large language models (MLLMs) face challenges with high-resolution visual inputs, which produce lengthy sequences of visual tokens, causing increased computational and memory demands as well as longer inference latency. <br /><br />2. Reducing redundant visual tokens is essential to alleviate these burdens and enable MLLM deployment in environments with limited resources or strict latency requirements.<br /><br />3. Existing visual token pruning methods primarily depend on attention-based redundancy detection and are designed with dense architectures in mind, limiting their effectiveness on mixture-of-experts (MoE) models.<br /><br />4. The proposed Fast Multimodal Mixture-of-Experts (FastMMoE) is a training-free acceleration framework tailored for MoE-based MLLMs, which strategically reduces expert activation and prunes visual tokens based on similarities in their routing probability distributions.<br /><br />5. Experimental results on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 show that FastMMoE can reduce FLOPs by up to 55.0% while preserving approximately 95.5% of the original model performance, outperforming state-of-the-art dense-model pruning methods like FastV and SparseVLM across various retention rates. <div>
arXiv:2511.17885v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA</title>
<link>https://arxiv.org/abs/2511.17886</link>
<guid>https://arxiv.org/abs/2511.17886</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, Knowledge distillation, CLIP-style models, Multimodal tasks, Performance scaling<br /><br />Summary:<br /><br />1. Vision-language models (VLMs) have demonstrated significant success in numerous multimodal applications but require extensive computational resources, limiting their ease of deployment.  
2. Knowledge distillation (KD) is a proven technique for creating lightweight models that maintain strong performance, validated in both natural language processing and computer vision.  
3. Despite its effectiveness in those fields, KD has seen limited application in VLMs, especially for CLIP-style models, and mostly in small teacher models and narrowly scoped tasks such as classification or retrieval.  
4. This work conducts the first comprehensive study of KD applied across a spectrum of CLIP-style teacher models ranging from standard baselines to large state-of-the-art architectures.  
5. Contrary to established trends in NLP and vision domains, the study reveals that stronger teacher models do not consistently produce better student models in VLM distillation. In fact, many current distillation methods do not scale well, resulting in decreased performance on complex downstream multimodal tasks like visual question answering.  
6. These findings challenge existing assumptions in knowledge distillation for multimodal models and highlight the need for new approaches designed to enhance parameter efficiency without compromising task performance. <div>
arXiv:2511.17886v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization</title>
<link>https://arxiv.org/abs/2511.17888</link>
<guid>https://arxiv.org/abs/2511.17888</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, overfitting, negative attention, DreamBooth, inference-time control<br /><br />Summary:  
This paper addresses the overfitting issue present in large-scale text-to-image personalization, particularly when training on a small number of images. Traditional solutions like DreamBooth use a class-specific prior-preservation loss to combat overfitting, which increases training computational cost and limits user control during inference. To overcome these problems, the authors propose Mask-Integrated Negative Attention Diffusion (MINDiff), a novel approach that introduces negative attention to suppress the subject’s influence in irrelevant masked regions by modifying the cross-attention mechanism during inference. This enables more precise semantic control and improves alignment between the text prompt and generated image by reducing subject dominance outside relevant areas. Users can adjust a scale parameter λ at inference time to balance the trade-off between subject fidelity and text alignment, providing enhanced flexibility. Importantly, MINDiff operates entirely during inference without changing the model architecture or requiring re-training, allowing easy application to existing DreamBooth models. Experimental results demonstrate that MINDiff more effectively mitigates overfitting compared to the prior-preservation loss method. The authors also provide their implementation publicly, facilitating adoption and further exploration of their technique. <div>
arXiv:2511.17888v1 Announce Type: new 
Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Audio-Visual Dataset Distillation</title>
<link>https://arxiv.org/abs/2511.17890</link>
<guid>https://arxiv.org/abs/2511.17890</guid>
<content:encoded><![CDATA[
<div> Audio-Visual Dataset Distillation, Cross-Modal Alignment, Pretraining, Decoupled Representations, Dataset Compression<br /><br />Summary:<br /><br />1. The paper addresses Audio-Visual Dataset Distillation, which aims to compress large-scale audio-visual datasets into smaller subsets while maintaining the original performance. <br />2. Existing Distribution Matching (DM) methods fail to effectively capture intrinsic cross-modal alignment, leading to challenges in the distillation process.<br />3. Previous attempts to introduce cross-modal matching suffer from two main issues: inconsistent modality mapping spaces caused by independently and randomly initialized encoders, and deterioration of modality-specific information due to direct modality interactions.<br />4. To overcome these challenges, the authors propose DAVDD, a pretraining-based decoupled distillation framework that uses a diverse pretrained feature bank for stable modality features and a lightweight decoupler bank to disentangle common and private representations.<br />5. DAVDD introduces Common Intermodal Matching combined with a Sample-Distribution Joint Alignment strategy to align shared representations both at the sample and global distribution levels, while completely isolating private representations to preserve modality-specific cues.<br />6. Extensive experiments on multiple benchmarks demonstrate that DAVDD achieves state-of-the-art results under various IPC (Images Per Class) settings, proving the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation.<br />7. The authors plan to release the code to facilitate further research in this area. <div>
arXiv:2511.17890v1 Announce Type: new 
Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation</title>
<link>https://arxiv.org/abs/2511.17904</link>
<guid>https://arxiv.org/abs/2511.17904</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D scene representation, multimodal semantics, voxelized anchor structure, feature-aware significance evaluation<br /><br />Summary:<br /><br />This paper introduces CUS-GS, a Compact Unified Structured Gaussian Splatting representation aimed at bridging the gap between semantics-oriented and structure-oriented 3D scene representations. The authors design a voxelized anchor structure that forms a spatial scaffold to capture explicit 3D geometry while simultaneously extracting multimodal semantic features from foundation models including CLIP, DINOv2, and SEEM. A key contribution is the multimodal latent feature allocation mechanism that unifies appearance, geometry, and semantic information across heterogeneous feature spaces, enabling a consistent multimodal representation. Additionally, the method incorporates a feature-aware significance evaluation strategy to dynamically guide the growth and pruning of anchors, effectively eliminating redundant or invalid anchors while preserving semantic integrity. Extensive experimental validation demonstrates that CUS-GS attains competitive performance against state-of-the-art approaches but with significantly fewer parameters—6 million compared to around 35 million for comparable models—indicating superior model efficiency. This highlights an excellent balance between accuracy and compactness in 3D scene representation, making CUS-GS a compelling framework for effective and efficient multimodal 3D modeling. <div>
arXiv:2511.17904v1 Announce Type: new 
Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation</title>
<link>https://arxiv.org/abs/2511.17914</link>
<guid>https://arxiv.org/abs/2511.17914</guid>
<content:encoded><![CDATA[
<div> Dataset Distillation, Long-tailed Distribution, Soft Labels, Adaptive Soft-label Alignment, Imbalance-aware Generalization Bound<br /><br />Summary: Dataset distillation is a technique that compresses large datasets into smaller, highly informative synthetic datasets to reduce storage and training costs. However, existing methods primarily focus on balanced datasets and face challenges when applied to real-world long-tailed distributions where some classes have significantly fewer samples. This work highlights the importance of soft labels in addressing these challenges in long-tailed dataset distillation and analyzes the causes of performance degradation. The authors derive an imbalance-aware generalization bound for models trained on distilled datasets, providing theoretical insight into the issue. They identify two main sources of bias in soft labels—one arising from the distillation model itself and the other from the synthetic distilled images—by systematically perturbing data imbalance levels. To mitigate these biases, the paper proposes ADSA, an Adaptive Soft-label Alignment module designed to calibrate and correct the entangled biases. ADSA is lightweight and easily integrated into existing distillation frameworks. Experiments on the ImageNet-1k-LT dataset demonstrate that ADSA significantly improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4% with extreme data compression settings. The method proves robust across various label budgets and distillation methods, offering a generalizable solution for long-tailed dataset distillation. The code is publicly available for reproducibility and further research. <div>
arXiv:2511.17914v1 Announce Type: new 
Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization</title>
<link>https://arxiv.org/abs/2511.17918</link>
<guid>https://arxiv.org/abs/2511.17918</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, few-shot generalization, Frequency-Adaptive Sharpness Regularization, loss landscape sharpness<br /><br />Summary:<br /><br />This paper addresses the problem of 3D Gaussian Splatting (3DGS) overfitting to sparse observations in few-shot novel viewpoint scenarios, which limits its generalization capabilities. Viewing novel view synthesis as a generalization challenge, the authors propose Frequency-Adaptive Sharpness Regularization (FASR) to improve 3DGS training by guiding it toward solutions that generalize better to unseen viewpoints. While Sharpness-Aware Minimization (SAM) is known to improve generalization in classification by smoothing the loss landscape, directly applying it to 3DGS is ineffective because it over-regularizes, leading to loss of high-frequency details. To overcome this, FASR adapts regularization strength and neighborhood radius based on local image frequency, balancing sharpness reduction and detail preservation. This frequency-adaptive approach prevents artifacts such as floaters in novel views and retains fine details that standard SAM tends to oversmooth. Extensive experiments on diverse datasets and configurations show that FASR consistently enhances the performance of multiple 3DGS baseline methods. The authors also commit to releasing their code publicly to support reproducibility and further research. <div>
arXiv:2511.17918v1 Announce Type: new 
Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17927</link>
<guid>https://arxiv.org/abs/2511.17927</guid>
<content:encoded><![CDATA[
arXiv:2511.17927v1 Announce Type: new 
Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection</title>
<link>https://arxiv.org/abs/2511.17929</link>
<guid>https://arxiv.org/abs/2511.17929</guid>
<content:encoded><![CDATA[
arXiv:2511.17929v1 Announce Type: new 
Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2511.17930</link>
<guid>https://arxiv.org/abs/2511.17930</guid>
<content:encoded><![CDATA[
arXiv:2511.17930v1 Announce Type: new 
Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion</title>
<link>https://arxiv.org/abs/2511.17932</link>
<guid>https://arxiv.org/abs/2511.17932</guid>
<content:encoded><![CDATA[
arXiv:2511.17932v1 Announce Type: new 
Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction</title>
<link>https://arxiv.org/abs/2511.17941</link>
<guid>https://arxiv.org/abs/2511.17941</guid>
<content:encoded><![CDATA[
arXiv:2511.17941v1 Announce Type: new 
Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</title>
<link>https://arxiv.org/abs/2511.17943</link>
<guid>https://arxiv.org/abs/2511.17943</guid>
<content:encoded><![CDATA[
arXiv:2511.17943v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Temporal Sampling for Efficient MLLM Video Understanding</title>
<link>https://arxiv.org/abs/2511.17945</link>
<guid>https://arxiv.org/abs/2511.17945</guid>
<content:encoded><![CDATA[
arXiv:2511.17945v1 Announce Type: new 
Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m \alpha_i^2L^2)$, where $\sum_{i=1}^m \alpha_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-speaker Attention Alignment for Multimodal Social Interaction</title>
<link>https://arxiv.org/abs/2511.17952</link>
<guid>https://arxiv.org/abs/2511.17952</guid>
<content:encoded><![CDATA[
arXiv:2511.17952v1 Announce Type: new 
Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17958</link>
<guid>https://arxiv.org/abs/2511.17958</guid>
<content:encoded><![CDATA[
arXiv:2511.17958v1 Announce Type: new 
Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment</title>
<link>https://arxiv.org/abs/2511.17962</link>
<guid>https://arxiv.org/abs/2511.17962</guid>
<content:encoded><![CDATA[
arXiv:2511.17962v1 Announce Type: new 
Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.17964</link>
<guid>https://arxiv.org/abs/2511.17964</guid>
<content:encoded><![CDATA[
arXiv:2511.17964v1 Announce Type: new 
Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification</title>
<link>https://arxiv.org/abs/2511.17965</link>
<guid>https://arxiv.org/abs/2511.17965</guid>
<content:encoded><![CDATA[
arXiv:2511.17965v1 Announce Type: new 
Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking</title>
<link>https://arxiv.org/abs/2511.17967</link>
<guid>https://arxiv.org/abs/2511.17967</guid>
<content:encoded><![CDATA[
arXiv:2511.17967v1 Announce Type: new 
Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2511.17973</link>
<guid>https://arxiv.org/abs/2511.17973</guid>
<content:encoded><![CDATA[
arXiv:2511.17973v1 Announce Type: new 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17979</link>
<guid>https://arxiv.org/abs/2511.17979</guid>
<content:encoded><![CDATA[
arXiv:2511.17979v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plan-X: Instruct Video Generation via Semantic Planning</title>
<link>https://arxiv.org/abs/2511.17986</link>
<guid>https://arxiv.org/abs/2511.17986</guid>
<content:encoded><![CDATA[
arXiv:2511.17986v1 Announce Type: new 
Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17988</link>
<guid>https://arxiv.org/abs/2511.17988</guid>
<content:encoded><![CDATA[
arXiv:2511.17988v1 Announce Type: new 
Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining</title>
<link>https://arxiv.org/abs/2511.17993</link>
<guid>https://arxiv.org/abs/2511.17993</guid>
<content:encoded><![CDATA[
arXiv:2511.17993v1 Announce Type: new 
Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale</title>
<link>https://arxiv.org/abs/2511.18005</link>
<guid>https://arxiv.org/abs/2511.18005</guid>
<content:encoded><![CDATA[
arXiv:2511.18005v1 Announce Type: new 
Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging</title>
<link>https://arxiv.org/abs/2511.18007</link>
<guid>https://arxiv.org/abs/2511.18007</guid>
<content:encoded><![CDATA[
arXiv:2511.18007v1 Announce Type: new 
Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios</title>
<link>https://arxiv.org/abs/2511.18011</link>
<guid>https://arxiv.org/abs/2511.18011</guid>
<content:encoded><![CDATA[
arXiv:2511.18011v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection</title>
<link>https://arxiv.org/abs/2511.18012</link>
<guid>https://arxiv.org/abs/2511.18012</guid>
<content:encoded><![CDATA[
arXiv:2511.18012v1 Announce Type: new 
Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Retinal Ganglion Cells with Neural Differential Equations</title>
<link>https://arxiv.org/abs/2511.18014</link>
<guid>https://arxiv.org/abs/2511.18014</guid>
<content:encoded><![CDATA[
arXiv:2511.18014v1 Announce Type: new 
Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaX: Image Super-Resolution with State Predictive Control</title>
<link>https://arxiv.org/abs/2511.18028</link>
<guid>https://arxiv.org/abs/2511.18028</guid>
<content:encoded><![CDATA[
arXiv:2511.18028v1 Announce Type: new 
Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation</title>
<link>https://arxiv.org/abs/2511.18037</link>
<guid>https://arxiv.org/abs/2511.18037</guid>
<content:encoded><![CDATA[
arXiv:2511.18037v1 Announce Type: new 
Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</title>
<link>https://arxiv.org/abs/2511.18050</link>
<guid>https://arxiv.org/abs/2511.18050</guid>
<content:encoded><![CDATA[
arXiv:2511.18050v1 Announce Type: new 
Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment</title>
<link>https://arxiv.org/abs/2511.18055</link>
<guid>https://arxiv.org/abs/2511.18055</guid>
<content:encoded><![CDATA[
arXiv:2511.18055v1 Announce Type: new 
Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semi-Supervised Active Learning for Remote Sensing</title>
<link>https://arxiv.org/abs/2511.18058</link>
<guid>https://arxiv.org/abs/2511.18058</guid>
<content:encoded><![CDATA[
arXiv:2511.18058v1 Announce Type: new 
Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)</title>
<link>https://arxiv.org/abs/2511.18063</link>
<guid>https://arxiv.org/abs/2511.18063</guid>
<content:encoded><![CDATA[
arXiv:2511.18063v1 Announce Type: new 
Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&amp;E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.18075</link>
<guid>https://arxiv.org/abs/2511.18075</guid>
<content:encoded><![CDATA[
arXiv:2511.18075v1 Announce Type: new 
Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.18082</link>
<guid>https://arxiv.org/abs/2511.18082</guid>
<content:encoded><![CDATA[
arXiv:2511.18082v1 Announce Type: new 
Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More: An Explainable AI Framework for Lightweight Malaria Classification</title>
<link>https://arxiv.org/abs/2511.18083</link>
<guid>https://arxiv.org/abs/2511.18083</guid>
<content:encoded><![CDATA[
arXiv:2511.18083v1 Announce Type: new 
Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective</title>
<link>https://arxiv.org/abs/2511.18089</link>
<guid>https://arxiv.org/abs/2511.18089</guid>
<content:encoded><![CDATA[
arXiv:2511.18089v1 Announce Type: new 
Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Versatile Recompression-Aware Perceptual Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.18090</link>
<guid>https://arxiv.org/abs/2511.18090</guid>
<content:encoded><![CDATA[
arXiv:2511.18090v1 Announce Type: new 
Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spotlight: Identifying and Localizing Video Generation Errors Using VLMs</title>
<link>https://arxiv.org/abs/2511.18102</link>
<guid>https://arxiv.org/abs/2511.18102</guid>
<content:encoded><![CDATA[
arXiv:2511.18102v1 Announce Type: new 
Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning</title>
<link>https://arxiv.org/abs/2511.18104</link>
<guid>https://arxiv.org/abs/2511.18104</guid>
<content:encoded><![CDATA[
arXiv:2511.18104v1 Announce Type: new 
Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens</title>
<link>https://arxiv.org/abs/2511.18105</link>
<guid>https://arxiv.org/abs/2511.18105</guid>
<content:encoded><![CDATA[
arXiv:2511.18105v1 Announce Type: new 
Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training</title>
<link>https://arxiv.org/abs/2511.18115</link>
<guid>https://arxiv.org/abs/2511.18115</guid>
<content:encoded><![CDATA[
arXiv:2511.18115v1 Announce Type: new 
Abstract: We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures</title>
<link>https://arxiv.org/abs/2511.18116</link>
<guid>https://arxiv.org/abs/2511.18116</guid>
<content:encoded><![CDATA[
arXiv:2511.18116v1 Announce Type: new 
Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning</title>
<link>https://arxiv.org/abs/2511.18120</link>
<guid>https://arxiv.org/abs/2511.18120</guid>
<content:encoded><![CDATA[
arXiv:2511.18120v1 Announce Type: new 
Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging</title>
<link>https://arxiv.org/abs/2511.18121</link>
<guid>https://arxiv.org/abs/2511.18121</guid>
<content:encoded><![CDATA[
arXiv:2511.18121v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18123</link>
<guid>https://arxiv.org/abs/2511.18123</guid>
<content:encoded><![CDATA[
arXiv:2511.18123v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation</title>
<link>https://arxiv.org/abs/2511.18127</link>
<guid>https://arxiv.org/abs/2511.18127</guid>
<content:encoded><![CDATA[
arXiv:2511.18127v1 Announce Type: new 
Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video4Edit: Viewing Image Editing as a Degenerate Temporal Process</title>
<link>https://arxiv.org/abs/2511.18131</link>
<guid>https://arxiv.org/abs/2511.18131</guid>
<content:encoded><![CDATA[
arXiv:2511.18131v1 Announce Type: new 
Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation</title>
<link>https://arxiv.org/abs/2511.18136</link>
<guid>https://arxiv.org/abs/2511.18136</guid>
<content:encoded><![CDATA[
arXiv:2511.18136v1 Announce Type: new 
Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compact neural networks for astronomy with optimal transport bias correction</title>
<link>https://arxiv.org/abs/2511.18139</link>
<guid>https://arxiv.org/abs/2511.18139</guid>
<content:encoded><![CDATA[
arXiv:2511.18139v1 Announce Type: new 
Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors</title>
<link>https://arxiv.org/abs/2511.18152</link>
<guid>https://arxiv.org/abs/2511.18152</guid>
<content:encoded><![CDATA[
arXiv:2511.18152v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design</title>
<link>https://arxiv.org/abs/2511.18163</link>
<guid>https://arxiv.org/abs/2511.18163</guid>
<content:encoded><![CDATA[
arXiv:2511.18163v1 Announce Type: new 
Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nested Unfolding Network for Real-World Concealed Object Segmentation</title>
<link>https://arxiv.org/abs/2511.18164</link>
<guid>https://arxiv.org/abs/2511.18164</guid>
<content:encoded><![CDATA[
arXiv:2511.18164v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses</title>
<link>https://arxiv.org/abs/2511.18173</link>
<guid>https://arxiv.org/abs/2511.18173</guid>
<content:encoded><![CDATA[
arXiv:2511.18173v1 Announce Type: new 
Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera</title>
<link>https://arxiv.org/abs/2511.18174</link>
<guid>https://arxiv.org/abs/2511.18174</guid>
<content:encoded><![CDATA[
arXiv:2511.18174v1 Announce Type: new 
Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching</title>
<link>https://arxiv.org/abs/2511.18185</link>
<guid>https://arxiv.org/abs/2511.18185</guid>
<content:encoded><![CDATA[
arXiv:2511.18185v1 Announce Type: new 
Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization</title>
<link>https://arxiv.org/abs/2511.18192</link>
<guid>https://arxiv.org/abs/2511.18192</guid>
<content:encoded><![CDATA[
arXiv:2511.18192v1 Announce Type: new 
Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity</title>
<link>https://arxiv.org/abs/2511.18200</link>
<guid>https://arxiv.org/abs/2511.18200</guid>
<content:encoded><![CDATA[
arXiv:2511.18200v1 Announce Type: new 
Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading</title>
<link>https://arxiv.org/abs/2511.18204</link>
<guid>https://arxiv.org/abs/2511.18204</guid>
<content:encoded><![CDATA[
arXiv:2511.18204v1 Announce Type: new 
Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI</title>
<link>https://arxiv.org/abs/2511.18208</link>
<guid>https://arxiv.org/abs/2511.18208</guid>
<content:encoded><![CDATA[
arXiv:2511.18208v1 Announce Type: new 
Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using MLIR Transform to Design Sliced Convolution Algorithm</title>
<link>https://arxiv.org/abs/2511.18222</link>
<guid>https://arxiv.org/abs/2511.18222</guid>
<content:encoded><![CDATA[
arXiv:2511.18222v1 Announce Type: new 
Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel qMRI Reconstruction from 4x Accelerated Acquisitions</title>
<link>https://arxiv.org/abs/2511.18232</link>
<guid>https://arxiv.org/abs/2511.18232</guid>
<content:encoded><![CDATA[
arXiv:2511.18232v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning</title>
<link>https://arxiv.org/abs/2511.18242</link>
<guid>https://arxiv.org/abs/2511.18242</guid>
<content:encoded><![CDATA[
arXiv:2511.18242v1 Announce Type: new 
Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</title>
<link>https://arxiv.org/abs/2511.18254</link>
<guid>https://arxiv.org/abs/2511.18254</guid>
<content:encoded><![CDATA[
arXiv:2511.18254v1 Announce Type: new 
Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization</title>
<link>https://arxiv.org/abs/2511.18255</link>
<guid>https://arxiv.org/abs/2511.18255</guid>
<content:encoded><![CDATA[
arXiv:2511.18255v1 Announce Type: new 
Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.18262</link>
<guid>https://arxiv.org/abs/2511.18262</guid>
<content:encoded><![CDATA[
arXiv:2511.18262v1 Announce Type: new 
Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors</title>
<link>https://arxiv.org/abs/2511.18264</link>
<guid>https://arxiv.org/abs/2511.18264</guid>
<content:encoded><![CDATA[
arXiv:2511.18264v1 Announce Type: new 
Abstract: Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models</title>
<link>https://arxiv.org/abs/2511.18271</link>
<guid>https://arxiv.org/abs/2511.18271</guid>
<content:encoded><![CDATA[
arXiv:2511.18271v1 Announce Type: new 
Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation</title>
<link>https://arxiv.org/abs/2511.18272</link>
<guid>https://arxiv.org/abs/2511.18272</guid>
<content:encoded><![CDATA[
arXiv:2511.18272v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point-to-Point: Sparse Motion Guidance for Controllable Video Editing</title>
<link>https://arxiv.org/abs/2511.18277</link>
<guid>https://arxiv.org/abs/2511.18277</guid>
<content:encoded><![CDATA[
arXiv:2511.18277v1 Announce Type: new 
Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation</title>
<link>https://arxiv.org/abs/2511.18281</link>
<guid>https://arxiv.org/abs/2511.18281</guid>
<content:encoded><![CDATA[
arXiv:2511.18281v1 Announce Type: new 
Abstract: Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System</title>
<link>https://arxiv.org/abs/2511.18286</link>
<guid>https://arxiv.org/abs/2511.18286</guid>
<content:encoded><![CDATA[
arXiv:2511.18286v1 Announce Type: new 
Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes</title>
<link>https://arxiv.org/abs/2511.18290</link>
<guid>https://arxiv.org/abs/2511.18290</guid>
<content:encoded><![CDATA[
arXiv:2511.18290v1 Announce Type: new 
Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition</title>
<link>https://arxiv.org/abs/2511.18305</link>
<guid>https://arxiv.org/abs/2511.18305</guid>
<content:encoded><![CDATA[
arXiv:2511.18305v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScriptViT: Vision Transformer-Based Personalized Handwriting Generation</title>
<link>https://arxiv.org/abs/2511.18307</link>
<guid>https://arxiv.org/abs/2511.18307</guid>
<content:encoded><![CDATA[
arXiv:2511.18307v1 Announce Type: new 
Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification</title>
<link>https://arxiv.org/abs/2511.18316</link>
<guid>https://arxiv.org/abs/2511.18316</guid>
<content:encoded><![CDATA[
arXiv:2511.18316v1 Announce Type: new 
Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement</title>
<link>https://arxiv.org/abs/2511.18317</link>
<guid>https://arxiv.org/abs/2511.18317</guid>
<content:encoded><![CDATA[
arXiv:2511.18317v1 Announce Type: new 
Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification</title>
<link>https://arxiv.org/abs/2511.18326</link>
<guid>https://arxiv.org/abs/2511.18326</guid>
<content:encoded><![CDATA[
arXiv:2511.18326v1 Announce Type: new 
Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters</title>
<link>https://arxiv.org/abs/2511.18329</link>
<guid>https://arxiv.org/abs/2511.18329</guid>
<content:encoded><![CDATA[
arXiv:2511.18329v1 Announce Type: new 
Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsistCompose: Unified Multimodal Layout Control for Image Composition</title>
<link>https://arxiv.org/abs/2511.18333</link>
<guid>https://arxiv.org/abs/2511.18333</guid>
<content:encoded><![CDATA[
arXiv:2511.18333v1 Announce Type: new 
Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2511.18344</link>
<guid>https://arxiv.org/abs/2511.18344</guid>
<content:encoded><![CDATA[
arXiv:2511.18344v1 Announce Type: new 
Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement</title>
<link>https://arxiv.org/abs/2511.18346</link>
<guid>https://arxiv.org/abs/2511.18346</guid>
<content:encoded><![CDATA[
arXiv:2511.18346v1 Announce Type: new 
Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference</title>
<link>https://arxiv.org/abs/2511.18352</link>
<guid>https://arxiv.org/abs/2511.18352</guid>
<content:encoded><![CDATA[
arXiv:2511.18352v1 Announce Type: new 
Abstract: Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRANSPORTER: Transferring Visual Semantics from VLM Manifolds</title>
<link>https://arxiv.org/abs/2511.18359</link>
<guid>https://arxiv.org/abs/2511.18359</guid>
<content:encoded><![CDATA[
arXiv:2511.18359v1 Announce Type: new 
Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alias-free 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.18367</link>
<guid>https://arxiv.org/abs/2511.18367</guid>
<content:encoded><![CDATA[
arXiv:2511.18367v1 Announce Type: new 
Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer</title>
<link>https://arxiv.org/abs/2511.18370</link>
<guid>https://arxiv.org/abs/2511.18370</guid>
<content:encoded><![CDATA[
arXiv:2511.18370v1 Announce Type: new 
Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18373</link>
<guid>https://arxiv.org/abs/2511.18373</guid>
<content:encoded><![CDATA[
arXiv:2511.18373v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Curriculum Reinforces Compositional Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.18378</link>
<guid>https://arxiv.org/abs/2511.18378</guid>
<content:encoded><![CDATA[
arXiv:2511.18378v1 Announce Type: new 
Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models</title>
<link>https://arxiv.org/abs/2511.18380</link>
<guid>https://arxiv.org/abs/2511.18380</guid>
<content:encoded><![CDATA[
arXiv:2511.18380v1 Announce Type: new 
Abstract: Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access</title>
<link>https://arxiv.org/abs/2511.18382</link>
<guid>https://arxiv.org/abs/2511.18382</guid>
<content:encoded><![CDATA[
arXiv:2511.18382v1 Announce Type: new 
Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection</title>
<link>https://arxiv.org/abs/2511.18385</link>
<guid>https://arxiv.org/abs/2511.18385</guid>
<content:encoded><![CDATA[
arXiv:2511.18385v1 Announce Type: new 
Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: , , . Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.18386</link>
<guid>https://arxiv.org/abs/2511.18386</guid>
<content:encoded><![CDATA[
arXiv:2511.18386v1 Announce Type: new 
Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Weak-to-Strong Generalization for CLIP-based Classification</title>
<link>https://arxiv.org/abs/2511.18396</link>
<guid>https://arxiv.org/abs/2511.18396</guid>
<content:encoded><![CDATA[
arXiv:2511.18396v1 Announce Type: new 
Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering</title>
<link>https://arxiv.org/abs/2511.18399</link>
<guid>https://arxiv.org/abs/2511.18399</guid>
<content:encoded><![CDATA[
arXiv:2511.18399v1 Announce Type: new 
Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation</title>
<link>https://arxiv.org/abs/2511.18416</link>
<guid>https://arxiv.org/abs/2511.18416</guid>
<content:encoded><![CDATA[
arXiv:2511.18416v1 Announce Type: new 
Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI</title>
<link>https://arxiv.org/abs/2511.18422</link>
<guid>https://arxiv.org/abs/2511.18422</guid>
<content:encoded><![CDATA[
arXiv:2511.18422v1 Announce Type: new 
Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images</title>
<link>https://arxiv.org/abs/2511.18424</link>
<guid>https://arxiv.org/abs/2511.18424</guid>
<content:encoded><![CDATA[
arXiv:2511.18424v1 Announce Type: new 
Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection</title>
<link>https://arxiv.org/abs/2511.18425</link>
<guid>https://arxiv.org/abs/2511.18425</guid>
<content:encoded><![CDATA[
arXiv:2511.18425v1 Announce Type: new 
Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation</title>
<link>https://arxiv.org/abs/2511.18434</link>
<guid>https://arxiv.org/abs/2511.18434</guid>
<content:encoded><![CDATA[
arXiv:2511.18434v1 Announce Type: new 
Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection</title>
<link>https://arxiv.org/abs/2511.18436</link>
<guid>https://arxiv.org/abs/2511.18436</guid>
<content:encoded><![CDATA[
arXiv:2511.18436v1 Announce Type: new 
Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2511.18437</link>
<guid>https://arxiv.org/abs/2511.18437</guid>
<content:encoded><![CDATA[
arXiv:2511.18437v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCoGS: Real-time ReColoring for Gaussian Splatting scenes</title>
<link>https://arxiv.org/abs/2511.18441</link>
<guid>https://arxiv.org/abs/2511.18441</guid>
<content:encoded><![CDATA[
arXiv:2511.18441v1 Announce Type: new 
Abstract: Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SineProject: Machine Unlearning for Stable Vision Language Alignment</title>
<link>https://arxiv.org/abs/2511.18444</link>
<guid>https://arxiv.org/abs/2511.18444</guid>
<content:encoded><![CDATA[
arXiv:2511.18444v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs</title>
<link>https://arxiv.org/abs/2511.18448</link>
<guid>https://arxiv.org/abs/2511.18448</guid>
<content:encoded><![CDATA[
arXiv:2511.18448v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering</title>
<link>https://arxiv.org/abs/2511.18452</link>
<guid>https://arxiv.org/abs/2511.18452</guid>
<content:encoded><![CDATA[
arXiv:2511.18452v1 Announce Type: new 
Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading</title>
<link>https://arxiv.org/abs/2511.18454</link>
<guid>https://arxiv.org/abs/2511.18454</guid>
<content:encoded><![CDATA[
arXiv:2511.18454v1 Announce Type: new 
Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding</title>
<link>https://arxiv.org/abs/2511.18463</link>
<guid>https://arxiv.org/abs/2511.18463</guid>
<content:encoded><![CDATA[
arXiv:2511.18463v1 Announce Type: new 
Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span</title>
<link>https://arxiv.org/abs/2511.18470</link>
<guid>https://arxiv.org/abs/2511.18470</guid>
<content:encoded><![CDATA[
arXiv:2511.18470v1 Announce Type: new 
Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale</title>
<link>https://arxiv.org/abs/2511.18471</link>
<guid>https://arxiv.org/abs/2511.18471</guid>
<content:encoded><![CDATA[
arXiv:2511.18471v1 Announce Type: new 
Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements</title>
<link>https://arxiv.org/abs/2511.18473</link>
<guid>https://arxiv.org/abs/2511.18473</guid>
<content:encoded><![CDATA[
arXiv:2511.18473v1 Announce Type: new 
Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression</title>
<link>https://arxiv.org/abs/2511.18504</link>
<guid>https://arxiv.org/abs/2511.18504</guid>
<content:encoded><![CDATA[
arXiv:2511.18504v1 Announce Type: new 
Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives</title>
<link>https://arxiv.org/abs/2511.18507</link>
<guid>https://arxiv.org/abs/2511.18507</guid>
<content:encoded><![CDATA[
arXiv:2511.18507v1 Announce Type: new 
Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging</title>
<link>https://arxiv.org/abs/2511.18513</link>
<guid>https://arxiv.org/abs/2511.18513</guid>
<content:encoded><![CDATA[
arXiv:2511.18513v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging</title>
<link>https://arxiv.org/abs/2511.18514</link>
<guid>https://arxiv.org/abs/2511.18514</guid>
<content:encoded><![CDATA[
arXiv:2511.18514v1 Announce Type: new 
Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion</title>
<link>https://arxiv.org/abs/2511.18516</link>
<guid>https://arxiv.org/abs/2511.18516</guid>
<content:encoded><![CDATA[
arXiv:2511.18516v1 Announce Type: new 
Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation</title>
<link>https://arxiv.org/abs/2511.18533</link>
<guid>https://arxiv.org/abs/2511.18533</guid>
<content:encoded><![CDATA[
arXiv:2511.18533v1 Announce Type: new 
Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction</title>
<link>https://arxiv.org/abs/2511.18534</link>
<guid>https://arxiv.org/abs/2511.18534</guid>
<content:encoded><![CDATA[
arXiv:2511.18534v1 Announce Type: new 
Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Deraining with Video Diffusion Models</title>
<link>https://arxiv.org/abs/2511.18537</link>
<guid>https://arxiv.org/abs/2511.18537</guid>
<content:encoded><![CDATA[
arXiv:2511.18537v1 Announce Type: new 
Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction</title>
<link>https://arxiv.org/abs/2511.18559</link>
<guid>https://arxiv.org/abs/2511.18559</guid>
<content:encoded><![CDATA[
arXiv:2511.18559v1 Announce Type: new 
Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation</title>
<link>https://arxiv.org/abs/2511.18570</link>
<guid>https://arxiv.org/abs/2511.18570</guid>
<content:encoded><![CDATA[
arXiv:2511.18570v1 Announce Type: new 
Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation</title>
<link>https://arxiv.org/abs/2511.18591</link>
<guid>https://arxiv.org/abs/2511.18591</guid>
<content:encoded><![CDATA[
arXiv:2511.18591v1 Announce Type: new 
Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI</title>
<link>https://arxiv.org/abs/2511.18595</link>
<guid>https://arxiv.org/abs/2511.18595</guid>
<content:encoded><![CDATA[
arXiv:2511.18595v1 Announce Type: new 
Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeAR: Coupled Neural Asset-Renderer Stack</title>
<link>https://arxiv.org/abs/2511.18600</link>
<guid>https://arxiv.org/abs/2511.18600</guid>
<content:encoded><![CDATA[
arXiv:2511.18600v1 Announce Type: new 
Abstract: Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data</title>
<link>https://arxiv.org/abs/2511.18601</link>
<guid>https://arxiv.org/abs/2511.18601</guid>
<content:encoded><![CDATA[
arXiv:2511.18601v1 Announce Type: new 
Abstract: In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Localization Enforced Deep Anomaly Detection Using Fundus Images</title>
<link>https://arxiv.org/abs/2511.18627</link>
<guid>https://arxiv.org/abs/2511.18627</guid>
<content:encoded><![CDATA[
arXiv:2511.18627v1 Announce Type: new 
Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Health system learning achieves generalist neuroimaging models</title>
<link>https://arxiv.org/abs/2511.18640</link>
<guid>https://arxiv.org/abs/2511.18640</guid>
<content:encoded><![CDATA[
arXiv:2511.18640v1 Announce Type: new 
Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis</title>
<link>https://arxiv.org/abs/2511.18654</link>
<guid>https://arxiv.org/abs/2511.18654</guid>
<content:encoded><![CDATA[
arXiv:2511.18654v1 Announce Type: new 
Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Physical Adversarial Patches Using Dynamically Optimized Clusters</title>
<link>https://arxiv.org/abs/2511.18656</link>
<guid>https://arxiv.org/abs/2511.18656</guid>
<content:encoded><![CDATA[
arXiv:2511.18656v1 Announce Type: new 
Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Augmentation Strategies for Robust Lane Marking Detection</title>
<link>https://arxiv.org/abs/2511.18668</link>
<guid>https://arxiv.org/abs/2511.18668</guid>
<content:encoded><![CDATA[
arXiv:2511.18668v1 Announce Type: new 
Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement</title>
<link>https://arxiv.org/abs/2511.18672</link>
<guid>https://arxiv.org/abs/2511.18672</guid>
<content:encoded><![CDATA[
arXiv:2511.18672v1 Announce Type: new 
Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers</title>
<link>https://arxiv.org/abs/2511.18673</link>
<guid>https://arxiv.org/abs/2511.18673</guid>
<content:encoded><![CDATA[
arXiv:2511.18673v1 Announce Type: new 
Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis</title>
<link>https://arxiv.org/abs/2511.18676</link>
<guid>https://arxiv.org/abs/2511.18676</guid>
<content:encoded><![CDATA[
arXiv:2511.18676v1 Announce Type: new 
Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.18677</link>
<guid>https://arxiv.org/abs/2511.18677</guid>
<content:encoded><![CDATA[
arXiv:2511.18677v1 Announce Type: new 
Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Geometry Image-Based Representations with Optimal Transport (OT)</title>
<link>https://arxiv.org/abs/2511.18679</link>
<guid>https://arxiv.org/abs/2511.18679</guid>
<content:encoded><![CDATA[
arXiv:2511.18679v1 Announce Type: new 
Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework</title>
<link>https://arxiv.org/abs/2511.18682</link>
<guid>https://arxiv.org/abs/2511.18682</guid>
<content:encoded><![CDATA[
arXiv:2511.18682v1 Announce Type: new 
Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation</title>
<link>https://arxiv.org/abs/2511.18684</link>
<guid>https://arxiv.org/abs/2511.18684</guid>
<content:encoded><![CDATA[
arXiv:2511.18684v1 Announce Type: new 
Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents</title>
<link>https://arxiv.org/abs/2511.18685</link>
<guid>https://arxiv.org/abs/2511.18685</guid>
<content:encoded><![CDATA[
arXiv:2511.18685v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification</title>
<link>https://arxiv.org/abs/2511.18691</link>
<guid>https://arxiv.org/abs/2511.18691</guid>
<content:encoded><![CDATA[
arXiv:2511.18691v1 Announce Type: new 
Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Surround-View Fisheye Camera 3D Object Detection</title>
<link>https://arxiv.org/abs/2511.18695</link>
<guid>https://arxiv.org/abs/2511.18695</guid>
<content:encoded><![CDATA[
arXiv:2511.18695v1 Announce Type: new 
Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dendritic Convolution for Noise Image Recognition</title>
<link>https://arxiv.org/abs/2511.18699</link>
<guid>https://arxiv.org/abs/2511.18699</guid>
<content:encoded><![CDATA[
arXiv:2511.18699v1 Announce Type: new 
Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction</title>
<link>https://arxiv.org/abs/2511.18701</link>
<guid>https://arxiv.org/abs/2511.18701</guid>
<content:encoded><![CDATA[
arXiv:2511.18701v1 Announce Type: new 
Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoD: A Diffusion Foundation Model for Image Compression</title>
<link>https://arxiv.org/abs/2511.18706</link>
<guid>https://arxiv.org/abs/2511.18706</guid>
<content:encoded><![CDATA[
arXiv:2511.18706v1 Announce Type: new 
Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation</title>
<link>https://arxiv.org/abs/2511.18711</link>
<guid>https://arxiv.org/abs/2511.18711</guid>
<content:encoded><![CDATA[
arXiv:2511.18711v1 Announce Type: new 
Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.18713</link>
<guid>https://arxiv.org/abs/2511.18713</guid>
<content:encoded><![CDATA[
arXiv:2511.18713v1 Announce Type: new 
Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing What Matters: Visual Preference Policy Optimization for Visual Generation</title>
<link>https://arxiv.org/abs/2511.18719</link>
<guid>https://arxiv.org/abs/2511.18719</guid>
<content:encoded><![CDATA[
arXiv:2511.18719v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.18729</link>
<guid>https://arxiv.org/abs/2511.18729</guid>
<content:encoded><![CDATA[
arXiv:2511.18729v1 Announce Type: new 
Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</title>
<link>https://arxiv.org/abs/2511.18734</link>
<guid>https://arxiv.org/abs/2511.18734</guid>
<content:encoded><![CDATA[
arXiv:2511.18734v1 Announce Type: new 
Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Ahead: Foresight Intelligence in MLLMs and World Models</title>
<link>https://arxiv.org/abs/2511.18735</link>
<guid>https://arxiv.org/abs/2511.18735</guid>
<content:encoded><![CDATA[
arXiv:2511.18735v1 Announce Type: new 
Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion</title>
<link>https://arxiv.org/abs/2511.18742</link>
<guid>https://arxiv.org/abs/2511.18742</guid>
<content:encoded><![CDATA[
arXiv:2511.18742v1 Announce Type: new 
Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any4D: Open-Prompt 4D Generation from Natural Language and Images</title>
<link>https://arxiv.org/abs/2511.18746</link>
<guid>https://arxiv.org/abs/2511.18746</guid>
<content:encoded><![CDATA[
arXiv:2511.18746v1 Announce Type: new 
Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.18757</link>
<guid>https://arxiv.org/abs/2511.18757</guid>
<content:encoded><![CDATA[
arXiv:2511.18757v1 Announce Type: new 
Abstract: We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from "what is seen" to "where to see", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement</title>
<link>https://arxiv.org/abs/2511.18763</link>
<guid>https://arxiv.org/abs/2511.18763</guid>
<content:encoded><![CDATA[
arXiv:2511.18763v1 Announce Type: new 
Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NI-Tex: Non-isometric Image-based Garment Texture Generation</title>
<link>https://arxiv.org/abs/2511.18765</link>
<guid>https://arxiv.org/abs/2511.18765</guid>
<content:encoded><![CDATA[
arXiv:2511.18765v1 Announce Type: new 
Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment</title>
<link>https://arxiv.org/abs/2511.18766</link>
<guid>https://arxiv.org/abs/2511.18766</guid>
<content:encoded><![CDATA[
arXiv:2511.18766v1 Announce Type: new 
Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Garment Conditioning in Diffusion-based Virtual Try-On</title>
<link>https://arxiv.org/abs/2511.18775</link>
<guid>https://arxiv.org/abs/2511.18775</guid>
<content:encoded><![CDATA[
arXiv:2511.18775v1 Announce Type: new 
Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection</title>
<link>https://arxiv.org/abs/2511.18780</link>
<guid>https://arxiv.org/abs/2511.18780</guid>
<content:encoded><![CDATA[
arXiv:2511.18780v1 Announce Type: new 
Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data</title>
<link>https://arxiv.org/abs/2511.18781</link>
<guid>https://arxiv.org/abs/2511.18781</guid>
<content:encoded><![CDATA[
arXiv:2511.18781v1 Announce Type: new 
Abstract: Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution</title>
<link>https://arxiv.org/abs/2511.18786</link>
<guid>https://arxiv.org/abs/2511.18786</guid>
<content:encoded><![CDATA[
arXiv:2511.18786v1 Announce Type: new 
Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Task Transfer in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18787</link>
<guid>https://arxiv.org/abs/2511.18787</guid>
<content:encoded><![CDATA[
arXiv:2511.18787v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoDETR: Stereo-based Transformer for 3D Object Detection</title>
<link>https://arxiv.org/abs/2511.18788</link>
<guid>https://arxiv.org/abs/2511.18788</guid>
<content:encoded><![CDATA[
arXiv:2511.18788v1 Announce Type: new 
Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing</title>
<link>https://arxiv.org/abs/2511.18792</link>
<guid>https://arxiv.org/abs/2511.18792</guid>
<content:encoded><![CDATA[
arXiv:2511.18792v1 Announce Type: new 
Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion</title>
<link>https://arxiv.org/abs/2511.18801</link>
<guid>https://arxiv.org/abs/2511.18801</guid>
<content:encoded><![CDATA[
arXiv:2511.18801v1 Announce Type: new 
Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</title>
<link>https://arxiv.org/abs/2511.18806</link>
<guid>https://arxiv.org/abs/2511.18806</guid>
<content:encoded><![CDATA[
arXiv:2511.18806v1 Announce Type: new 
Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache</title>
<link>https://arxiv.org/abs/2511.18811</link>
<guid>https://arxiv.org/abs/2511.18811</guid>
<content:encoded><![CDATA[
arXiv:2511.18811v1 Announce Type: new 
Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video</title>
<link>https://arxiv.org/abs/2511.18814</link>
<guid>https://arxiv.org/abs/2511.18814</guid>
<content:encoded><![CDATA[
arXiv:2511.18814v1 Announce Type: new 
Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.18816</link>
<guid>https://arxiv.org/abs/2511.18816</guid>
<content:encoded><![CDATA[
arXiv:2511.18816v1 Announce Type: new 
Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring</title>
<link>https://arxiv.org/abs/2511.18817</link>
<guid>https://arxiv.org/abs/2511.18817</guid>
<content:encoded><![CDATA[
arXiv:2511.18817v1 Announce Type: new 
Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiP: Taming Diffusion Models in Pixel Space</title>
<link>https://arxiv.org/abs/2511.18822</link>
<guid>https://arxiv.org/abs/2511.18822</guid>
<content:encoded><![CDATA[
arXiv:2511.18822v1 Announce Type: new 
Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.18823</link>
<guid>https://arxiv.org/abs/2511.18823</guid>
<content:encoded><![CDATA[
arXiv:2511.18823v1 Announce Type: new 
Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the alignment between infants' visual and linguistic experience using multimodal language models</title>
<link>https://arxiv.org/abs/2511.18824</link>
<guid>https://arxiv.org/abs/2511.18824</guid>
<content:encoded><![CDATA[
arXiv:2511.18824v1 Announce Type: new 
Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-Save: Towards Scoring and Attribution for Generated Video Evaluation</title>
<link>https://arxiv.org/abs/2511.18825</link>
<guid>https://arxiv.org/abs/2511.18825</guid>
<content:encoded><![CDATA[
arXiv:2511.18825v1 Announce Type: new 
Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification</title>
<link>https://arxiv.org/abs/2511.18826</link>
<guid>https://arxiv.org/abs/2511.18826</guid>
<content:encoded><![CDATA[
arXiv:2511.18826v1 Announce Type: new 
Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection</title>
<link>https://arxiv.org/abs/2511.18827</link>
<guid>https://arxiv.org/abs/2511.18827</guid>
<content:encoded><![CDATA[
arXiv:2511.18827v1 Announce Type: new 
Abstract: Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction</title>
<link>https://arxiv.org/abs/2511.18831</link>
<guid>https://arxiv.org/abs/2511.18831</guid>
<content:encoded><![CDATA[
arXiv:2511.18831v1 Announce Type: new 
Abstract: The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories</title>
<link>https://arxiv.org/abs/2511.18834</link>
<guid>https://arxiv.org/abs/2511.18834</guid>
<content:encoded><![CDATA[
arXiv:2511.18834v1 Announce Type: new 
Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FVAR: Visual Autoregressive Modeling via Next Focus Prediction</title>
<link>https://arxiv.org/abs/2511.18838</link>
<guid>https://arxiv.org/abs/2511.18838</guid>
<content:encoded><![CDATA[
arXiv:2511.18838v1 Announce Type: new 
Abstract: Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moir\'e patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.18839</link>
<guid>https://arxiv.org/abs/2511.18839</guid>
<content:encoded><![CDATA[
arXiv:2511.18839v1 Announce Type: new 
Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration</title>
<link>https://arxiv.org/abs/2511.18847</link>
<guid>https://arxiv.org/abs/2511.18847</guid>
<content:encoded><![CDATA[
arXiv:2511.18847v1 Announce Type: new 
Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization</title>
<link>https://arxiv.org/abs/2511.18851</link>
<guid>https://arxiv.org/abs/2511.18851</guid>
<content:encoded><![CDATA[
arXiv:2511.18851v1 Announce Type: new 
Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos</title>
<link>https://arxiv.org/abs/2511.18856</link>
<guid>https://arxiv.org/abs/2511.18856</guid>
<content:encoded><![CDATA[
arXiv:2511.18856v1 Announce Type: new 
Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling</title>
<link>https://arxiv.org/abs/2511.18858</link>
<guid>https://arxiv.org/abs/2511.18858</guid>
<content:encoded><![CDATA[
arXiv:2511.18858v1 Announce Type: new 
Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection</title>
<link>https://arxiv.org/abs/2511.18865</link>
<guid>https://arxiv.org/abs/2511.18865</guid>
<content:encoded><![CDATA[
arXiv:2511.18865v1 Announce Type: new 
Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HunyuanVideo 1.5 Technical Report</title>
<link>https://arxiv.org/abs/2511.18870</link>
<guid>https://arxiv.org/abs/2511.18870</guid>
<content:encoded><![CDATA[
arXiv:2511.18870v1 Announce Type: new 
Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction</title>
<link>https://arxiv.org/abs/2511.18873</link>
<guid>https://arxiv.org/abs/2511.18873</guid>
<content:encoded><![CDATA[
arXiv:2511.18873v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference</title>
<link>https://arxiv.org/abs/2511.18875</link>
<guid>https://arxiv.org/abs/2511.18875</guid>
<content:encoded><![CDATA[
arXiv:2511.18875v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Facade Segmentation for Solar Photovoltaic Suitability</title>
<link>https://arxiv.org/abs/2511.18882</link>
<guid>https://arxiv.org/abs/2511.18882</guid>
<content:encoded><![CDATA[
arXiv:2511.18882v1 Announce Type: new 
Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicWorld: Interactive Geometry-driven Video World Exploration</title>
<link>https://arxiv.org/abs/2511.18886</link>
<guid>https://arxiv.org/abs/2511.18886</guid>
<content:encoded><![CDATA[
arXiv:2511.18886v1 Announce Type: new 
Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model</title>
<link>https://arxiv.org/abs/2511.18888</link>
<guid>https://arxiv.org/abs/2511.18888</guid>
<content:encoded><![CDATA[
arXiv:2511.18888v1 Announce Type: new 
Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting</title>
<link>https://arxiv.org/abs/2511.18894</link>
<guid>https://arxiv.org/abs/2511.18894</guid>
<content:encoded><![CDATA[
arXiv:2511.18894v1 Announce Type: new 
Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation</title>
<link>https://arxiv.org/abs/2511.18919</link>
<guid>https://arxiv.org/abs/2511.18919</guid>
<content:encoded><![CDATA[
arXiv:2511.18919v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models</title>
<link>https://arxiv.org/abs/2511.18920</link>
<guid>https://arxiv.org/abs/2511.18920</guid>
<content:encoded><![CDATA[
arXiv:2511.18920v1 Announce Type: new 
Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18921</link>
<guid>https://arxiv.org/abs/2511.18921</guid>
<content:encoded><![CDATA[
arXiv:2511.18921v1 Announce Type: new 
Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control</title>
<link>https://arxiv.org/abs/2511.18922</link>
<guid>https://arxiv.org/abs/2511.18922</guid>
<content:encoded><![CDATA[
arXiv:2511.18922v1 Announce Type: new 
Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttenDence: Maximizing Attention Confidence for Test Time Adaptation</title>
<link>https://arxiv.org/abs/2511.18925</link>
<guid>https://arxiv.org/abs/2511.18925</guid>
<content:encoded><![CDATA[
arXiv:2511.18925v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FineXtrol: Controllable Motion Generation via Fine-Grained Text</title>
<link>https://arxiv.org/abs/2511.18927</link>
<guid>https://arxiv.org/abs/2511.18927</guid>
<content:encoded><![CDATA[
arXiv:2511.18927v1 Announce Type: new 
Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search</title>
<link>https://arxiv.org/abs/2511.18929</link>
<guid>https://arxiv.org/abs/2511.18929</guid>
<content:encoded><![CDATA[
arXiv:2511.18929v1 Announce Type: new 
Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeCoR - Velocity Contrastive Regularization for Flow Matching</title>
<link>https://arxiv.org/abs/2511.18942</link>
<guid>https://arxiv.org/abs/2511.18942</guid>
<content:encoded><![CDATA[
arXiv:2511.18942v1 Announce Type: new 
Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both "where to go" and "where not to go." To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.
  On ImageNet-1K 256$\times$256, VeCoR yields 22\% and 35\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining</title>
<link>https://arxiv.org/abs/2511.18946</link>
<guid>https://arxiv.org/abs/2511.18946</guid>
<content:encoded><![CDATA[
arXiv:2511.18946v1 Announce Type: new 
Abstract: In addition to evaluating tumor morphology using H&amp;E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&amp;E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eevee: Towards Close-up High-resolution Video-based Virtual Try-on</title>
<link>https://arxiv.org/abs/2511.18957</link>
<guid>https://arxiv.org/abs/2511.18957</guid>
<content:encoded><![CDATA[
arXiv:2511.18957v1 Announce Type: new 
Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery</title>
<link>https://arxiv.org/abs/2511.18968</link>
<guid>https://arxiv.org/abs/2511.18968</guid>
<content:encoded><![CDATA[
arXiv:2511.18968v1 Announce Type: new 
Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs</title>
<link>https://arxiv.org/abs/2511.18976</link>
<guid>https://arxiv.org/abs/2511.18976</guid>
<content:encoded><![CDATA[
arXiv:2511.18976v1 Announce Type: new 
Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models</title>
<link>https://arxiv.org/abs/2511.18978</link>
<guid>https://arxiv.org/abs/2511.18978</guid>
<content:encoded><![CDATA[
arXiv:2511.18978v1 Announce Type: new 
Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection</title>
<link>https://arxiv.org/abs/2511.18983</link>
<guid>https://arxiv.org/abs/2511.18983</guid>
<content:encoded><![CDATA[
arXiv:2511.18983v1 Announce Type: new 
Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2511.18989</link>
<guid>https://arxiv.org/abs/2511.18989</guid>
<content:encoded><![CDATA[
arXiv:2511.18989v1 Announce Type: new 
Abstract: Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>View-Consistent Diffusion Representations for 3D-Consistent Video Generation</title>
<link>https://arxiv.org/abs/2511.18991</link>
<guid>https://arxiv.org/abs/2511.18991</guid>
<content:encoded><![CDATA[
arXiv:2511.18991v1 Announce Type: new 
Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization</title>
<link>https://arxiv.org/abs/2511.18993</link>
<guid>https://arxiv.org/abs/2511.18993</guid>
<content:encoded><![CDATA[
arXiv:2511.18993v1 Announce Type: new 
Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation</title>
<link>https://arxiv.org/abs/2511.19004</link>
<guid>https://arxiv.org/abs/2511.19004</guid>
<content:encoded><![CDATA[
arXiv:2511.19004v1 Announce Type: new 
Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting</title>
<link>https://arxiv.org/abs/2511.19021</link>
<guid>https://arxiv.org/abs/2511.19021</guid>
<content:encoded><![CDATA[
arXiv:2511.19021v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, {\alpha} and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling</title>
<link>https://arxiv.org/abs/2511.19024</link>
<guid>https://arxiv.org/abs/2511.19024</guid>
<content:encoded><![CDATA[
arXiv:2511.19024v1 Announce Type: new 
Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \underline{l}ayer\underline{i}nteraction and MoE-based \underline{f}eature d\underline{e}coupling, termed \textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\texttt{Life-IQA}}.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric</title>
<link>https://arxiv.org/abs/2511.19032</link>
<guid>https://arxiv.org/abs/2511.19032</guid>
<content:encoded><![CDATA[
arXiv:2511.19032v1 Announce Type: new 
Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay</title>
<link>https://arxiv.org/abs/2511.19033</link>
<guid>https://arxiv.org/abs/2511.19033</guid>
<content:encoded><![CDATA[
arXiv:2511.19033v1 Announce Type: new 
Abstract: Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones</title>
<link>https://arxiv.org/abs/2511.19035</link>
<guid>https://arxiv.org/abs/2511.19035</guid>
<content:encoded><![CDATA[
arXiv:2511.19035v1 Announce Type: new 
Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedSAM3: Delving into Segment Anything with Medical Concepts</title>
<link>https://arxiv.org/abs/2511.19046</link>
<guid>https://arxiv.org/abs/2511.19046</guid>
<content:encoded><![CDATA[
arXiv:2511.19046v1 Announce Type: new 
Abstract: Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation</title>
<link>https://arxiv.org/abs/2511.19049</link>
<guid>https://arxiv.org/abs/2511.19049</guid>
<content:encoded><![CDATA[
arXiv:2511.19049v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space</title>
<link>https://arxiv.org/abs/2511.19057</link>
<guid>https://arxiv.org/abs/2511.19057</guid>
<content:encoded><![CDATA[
arXiv:2511.19057v1 Announce Type: new 
Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation</title>
<link>https://arxiv.org/abs/2511.19062</link>
<guid>https://arxiv.org/abs/2511.19062</guid>
<content:encoded><![CDATA[
arXiv:2511.19062v1 Announce Type: new 
Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding, Accelerating, and Improving MeanFlow Training</title>
<link>https://arxiv.org/abs/2511.19065</link>
<guid>https://arxiv.org/abs/2511.19065</guid>
<content:encoded><![CDATA[
arXiv:2511.19065v1 Announce Type: new 
Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling</title>
<link>https://arxiv.org/abs/2511.19067</link>
<guid>https://arxiv.org/abs/2511.19067</guid>
<content:encoded><![CDATA[
arXiv:2511.19067v1 Announce Type: new 
Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.19071</link>
<guid>https://arxiv.org/abs/2511.19071</guid>
<content:encoded><![CDATA[
arXiv:2511.19071v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-based 3D Human Pose Estimation using WiFi Signals</title>
<link>https://arxiv.org/abs/2511.19105</link>
<guid>https://arxiv.org/abs/2511.19105</guid>
<content:encoded><![CDATA[
arXiv:2511.19105v1 Announce Type: new 
Abstract: WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HABIT: Human Action Benchmark for Interactive Traffic in CARLA</title>
<link>https://arxiv.org/abs/2511.19109</link>
<guid>https://arxiv.org/abs/2511.19109</guid>
<content:encoded><![CDATA[
arXiv:2511.19109v1 Announce Type: new 
Abstract: Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</title>
<link>https://arxiv.org/abs/2511.19111</link>
<guid>https://arxiv.org/abs/2511.19111</guid>
<content:encoded><![CDATA[
arXiv:2511.19111v1 Announce Type: new 
Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion</title>
<link>https://arxiv.org/abs/2511.19117</link>
<guid>https://arxiv.org/abs/2511.19117</guid>
<content:encoded><![CDATA[
arXiv:2511.19117v1 Announce Type: new 
Abstract: The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images</title>
<link>https://arxiv.org/abs/2511.19119</link>
<guid>https://arxiv.org/abs/2511.19119</guid>
<content:encoded><![CDATA[
arXiv:2511.19119v1 Announce Type: new 
Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP</title>
<link>https://arxiv.org/abs/2511.19126</link>
<guid>https://arxiv.org/abs/2511.19126</guid>
<content:encoded><![CDATA[
arXiv:2511.19126v1 Announce Type: new 
Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery</title>
<link>https://arxiv.org/abs/2511.19134</link>
<guid>https://arxiv.org/abs/2511.19134</guid>
<content:encoded><![CDATA[
arXiv:2511.19134v1 Announce Type: new 
Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation</title>
<link>https://arxiv.org/abs/2511.19137</link>
<guid>https://arxiv.org/abs/2511.19137</guid>
<content:encoded><![CDATA[
arXiv:2511.19137v1 Announce Type: new 
Abstract: Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2511.19145</link>
<guid>https://arxiv.org/abs/2511.19145</guid>
<content:encoded><![CDATA[
arXiv:2511.19145v1 Announce Type: new 
Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation</title>
<link>https://arxiv.org/abs/2511.19147</link>
<guid>https://arxiv.org/abs/2511.19147</guid>
<content:encoded><![CDATA[
arXiv:2511.19147v1 Announce Type: new 
Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</title>
<link>https://arxiv.org/abs/2511.19149</link>
<guid>https://arxiv.org/abs/2511.19149</guid>
<content:encoded><![CDATA[
arXiv:2511.19149v1 Announce Type: new 
Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Preference Optimization for Image Restoration</title>
<link>https://arxiv.org/abs/2511.19169</link>
<guid>https://arxiv.org/abs/2511.19169</guid>
<content:encoded><![CDATA[
arXiv:2511.19169v1 Announce Type: new 
Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes</title>
<link>https://arxiv.org/abs/2511.19172</link>
<guid>https://arxiv.org/abs/2511.19172</guid>
<content:encoded><![CDATA[
arXiv:2511.19172v1 Announce Type: new 
Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification</title>
<link>https://arxiv.org/abs/2511.19180</link>
<guid>https://arxiv.org/abs/2511.19180</guid>
<content:encoded><![CDATA[
arXiv:2511.19180v1 Announce Type: new 
Abstract: One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation</title>
<link>https://arxiv.org/abs/2511.19183</link>
<guid>https://arxiv.org/abs/2511.19183</guid>
<content:encoded><![CDATA[
arXiv:2511.19183v1 Announce Type: new 
Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection</title>
<link>https://arxiv.org/abs/2511.19187</link>
<guid>https://arxiv.org/abs/2511.19187</guid>
<content:encoded><![CDATA[
arXiv:2511.19187v1 Announce Type: new 
Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2511.19198</link>
<guid>https://arxiv.org/abs/2511.19198</guid>
<content:encoded><![CDATA[
arXiv:2511.19198v1 Announce Type: new 
Abstract: Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLASH: A Benchmark for Cross-Modal Contradiction Detection</title>
<link>https://arxiv.org/abs/2511.19199</link>
<guid>https://arxiv.org/abs/2511.19199</guid>
<content:encoded><![CDATA[
arXiv:2511.19199v1 Announce Type: new 
Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?</title>
<link>https://arxiv.org/abs/2511.19200</link>
<guid>https://arxiv.org/abs/2511.19200</guid>
<content:encoded><![CDATA[
arXiv:2511.19200v1 Announce Type: new 
Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.19202</link>
<guid>https://arxiv.org/abs/2511.19202</guid>
<content:encoded><![CDATA[
arXiv:2511.19202v1 Announce Type: new 
Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</title>
<link>https://arxiv.org/abs/2511.19217</link>
<guid>https://arxiv.org/abs/2511.19217</guid>
<content:encoded><![CDATA[
arXiv:2511.19217v1 Announce Type: new 
Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.19220</link>
<guid>https://arxiv.org/abs/2511.19220</guid>
<content:encoded><![CDATA[
arXiv:2511.19220v1 Announce Type: new 
Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.19221</link>
<guid>https://arxiv.org/abs/2511.19221</guid>
<content:encoded><![CDATA[
arXiv:2511.19221v1 Announce Type: new 
Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Plug-and-play Memory for Guiding Video Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19229</link>
<guid>https://arxiv.org/abs/2511.19229</guid>
<content:encoded><![CDATA[
arXiv:2511.19229v1 Announce Type: new 
Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes</title>
<link>https://arxiv.org/abs/2511.19235</link>
<guid>https://arxiv.org/abs/2511.19235</guid>
<content:encoded><![CDATA[
arXiv:2511.19235v1 Announce Type: new 
Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation</title>
<link>https://arxiv.org/abs/2511.19254</link>
<guid>https://arxiv.org/abs/2511.19254</guid>
<content:encoded><![CDATA[
arXiv:2511.19254v1 Announce Type: new 
Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.19261</link>
<guid>https://arxiv.org/abs/2511.19261</guid>
<content:encoded><![CDATA[
arXiv:2511.19261v1 Announce Type: new 
Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment</title>
<link>https://arxiv.org/abs/2511.19268</link>
<guid>https://arxiv.org/abs/2511.19268</guid>
<content:encoded><![CDATA[
arXiv:2511.19268v1 Announce Type: new 
Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection</title>
<link>https://arxiv.org/abs/2511.19274</link>
<guid>https://arxiv.org/abs/2511.19274</guid>
<content:encoded><![CDATA[
arXiv:2511.19274v1 Announce Type: new 
Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReMatch: Boosting Representation through Matching for Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2511.19278</link>
<guid>https://arxiv.org/abs/2511.19278</guid>
<content:encoded><![CDATA[
arXiv:2511.19278v1 Announce Type: new 
Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.19294</link>
<guid>https://arxiv.org/abs/2511.19294</guid>
<content:encoded><![CDATA[
arXiv:2511.19294v1 Announce Type: new 
Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection</title>
<link>https://arxiv.org/abs/2511.19301</link>
<guid>https://arxiv.org/abs/2511.19301</guid>
<content:encoded><![CDATA[
arXiv:2511.19301v1 Announce Type: new 
Abstract: Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.
  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2511.19306</link>
<guid>https://arxiv.org/abs/2511.19306</guid>
<content:encoded><![CDATA[
arXiv:2511.19306v1 Announce Type: new 
Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach</title>
<link>https://arxiv.org/abs/2511.19316</link>
<guid>https://arxiv.org/abs/2511.19316</guid>
<content:encoded><![CDATA[
arXiv:2511.19316v1 Announce Type: new 
Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis</title>
<link>https://arxiv.org/abs/2511.19319</link>
<guid>https://arxiv.org/abs/2511.19319</guid>
<content:encoded><![CDATA[
arXiv:2511.19319v1 Announce Type: new 
Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation</title>
<link>https://arxiv.org/abs/2511.19320</link>
<guid>https://arxiv.org/abs/2511.19320</guid>
<content:encoded><![CDATA[
arXiv:2511.19320v1 Announce Type: new 
Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation</title>
<link>https://arxiv.org/abs/2511.19326</link>
<guid>https://arxiv.org/abs/2511.19326</guid>
<content:encoded><![CDATA[
arXiv:2511.19326v1 Announce Type: new 
Abstract: Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse</title>
<link>https://arxiv.org/abs/2511.19339</link>
<guid>https://arxiv.org/abs/2511.19339</guid>
<content:encoded><![CDATA[
arXiv:2511.19339v1 Announce Type: new 
Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning</title>
<link>https://arxiv.org/abs/2511.19343</link>
<guid>https://arxiv.org/abs/2511.19343</guid>
<content:encoded><![CDATA[
arXiv:2511.19343v1 Announce Type: new 
Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting</title>
<link>https://arxiv.org/abs/2511.19351</link>
<guid>https://arxiv.org/abs/2511.19351</guid>
<content:encoded><![CDATA[
arXiv:2511.19351v1 Announce Type: new 
Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Growing with the Generator: Self-paced GRPO for Video Generation</title>
<link>https://arxiv.org/abs/2511.19356</link>
<guid>https://arxiv.org/abs/2511.19356</guid>
<content:encoded><![CDATA[
arXiv:2511.19356v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation</title>
<link>https://arxiv.org/abs/2511.19365</link>
<guid>https://arxiv.org/abs/2511.19365</guid>
<content:encoded><![CDATA[
arXiv:2511.19365v1 Announce Type: new 
Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification</title>
<link>https://arxiv.org/abs/2511.19367</link>
<guid>https://arxiv.org/abs/2511.19367</guid>
<content:encoded><![CDATA[
arXiv:2511.19367v1 Announce Type: new 
Abstract: Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval</title>
<link>https://arxiv.org/abs/2511.19380</link>
<guid>https://arxiv.org/abs/2511.19380</guid>
<content:encoded><![CDATA[
arXiv:2511.19380v1 Announce Type: new 
Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation</title>
<link>https://arxiv.org/abs/2511.19394</link>
<guid>https://arxiv.org/abs/2511.19394</guid>
<content:encoded><![CDATA[
arXiv:2511.19394v1 Announce Type: new 
Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Video Instructions: Visual Signals as Generative Control</title>
<link>https://arxiv.org/abs/2511.19401</link>
<guid>https://arxiv.org/abs/2511.19401</guid>
<content:encoded><![CDATA[
arXiv:2511.19401v1 Announce Type: new 
Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</title>
<link>https://arxiv.org/abs/2511.19418</link>
<guid>https://arxiv.org/abs/2511.19418</guid>
<content:encoded><![CDATA[
arXiv:2511.19418v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.19425</link>
<guid>https://arxiv.org/abs/2511.19425</guid>
<content:encoded><![CDATA[
arXiv:2511.19425v1 Announce Type: new 
Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction</title>
<link>https://arxiv.org/abs/2511.19426</link>
<guid>https://arxiv.org/abs/2511.19426</guid>
<content:encoded><![CDATA[
arXiv:2511.19426v1 Announce Type: new 
Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</title>
<link>https://arxiv.org/abs/2511.19430</link>
<guid>https://arxiv.org/abs/2511.19430</guid>
<content:encoded><![CDATA[
arXiv:2511.19430v1 Announce Type: new 
Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cloud4D</title>
<link>https://arxiv.org/abs/2511.19431</link>
<guid>https://arxiv.org/abs/2511.19431</guid>
<content:encoded><![CDATA[
arXiv:2511.19431v1 Announce Type: new 
Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</title>
<link>https://arxiv.org/abs/2511.19434</link>
<guid>https://arxiv.org/abs/2511.19434</guid>
<content:encoded><![CDATA[
arXiv:2511.19434v1 Announce Type: new 
Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Image-to-Video Models Good Zero-Shot Image Editors?</title>
<link>https://arxiv.org/abs/2511.19435</link>
<guid>https://arxiv.org/abs/2511.19435</guid>
<content:encoded><![CDATA[
arXiv:2511.19435v1 Announce Type: new 
Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</title>
<link>https://arxiv.org/abs/2511.19436</link>
<guid>https://arxiv.org/abs/2511.19436</guid>
<content:encoded><![CDATA[
arXiv:2511.19436v1 Announce Type: new 
Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context</title>
<link>https://arxiv.org/abs/2511.19437</link>
<guid>https://arxiv.org/abs/2511.19437</guid>
<content:encoded><![CDATA[
arXiv:2511.19437v1 Announce Type: new 
Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOD: Boundary-based Out-Of-Distribution Data Generation</title>
<link>https://arxiv.org/abs/2508.00350</link>
<guid>https://arxiv.org/abs/2508.00350</guid>
<content:encoded><![CDATA[
arXiv:2508.00350v1 Announce Type: cross 
Abstract: Harnessing the power of diffusion models to synthesize auxiliary training data based on latent space features has proven effective in enhancing out-of-distribution (OOD) detection performance. However, extracting effective features outside the in-distribution (ID) boundary in latent space remains challenging due to the difficulty of identifying decision boundaries between classes. This paper proposes a novel framework called Boundary-based Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD features and generates human-compatible outlier images using diffusion models. BOOD first learns a text-conditioned latent feature space from the ID dataset, selects ID features closest to the decision boundary, and perturbs them to cross the decision boundary to form OOD features. These synthetic OOD features are then decoded into images in pixel space by a diffusion model. Compared to previous works, BOOD provides a more training efficient strategy for synthesizing informative OOD features, facilitating clearer distinctions between ID and OOD data. Extensive experimental results on common benchmarks demonstrate that BOOD surpasses the state-of-the-art method significantly, achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27% improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saving Foundation Flow-Matching Priors for Inverse Problems</title>
<link>https://arxiv.org/abs/2511.16520</link>
<guid>https://arxiv.org/abs/2511.16520</guid>
<content:encoded><![CDATA[
arXiv:2511.16520v1 Announce Type: cross 
Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices</title>
<link>https://arxiv.org/abs/2511.17508</link>
<guid>https://arxiv.org/abs/2511.17508</guid>
<content:encoded><![CDATA[
arXiv:2511.17508v1 Announce Type: cross 
Abstract: Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder</title>
<link>https://arxiv.org/abs/2511.17547</link>
<guid>https://arxiv.org/abs/2511.17547</guid>
<content:encoded><![CDATA[
arXiv:2511.17547v1 Announce Type: cross 
Abstract: Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks</title>
<link>https://arxiv.org/abs/2511.17564</link>
<guid>https://arxiv.org/abs/2511.17564</guid>
<content:encoded><![CDATA[
arXiv:2511.17564v1 Announce Type: cross 
Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-adaptive Weight Quantization for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2511.17567</link>
<guid>https://arxiv.org/abs/2511.17567</guid>
<content:encoded><![CDATA[
arXiv:2511.17567v1 Announce Type: cross 
Abstract: Weight quantization in spiking neural networks (SNNs) could further reduce energy consumption. However, quantizing weights without sacrificing accuracy remains challenging. In this study, inspired by astrocyte-mediated synaptic modulation in the biological nervous systems, we propose Temporal-adaptive Weight Quantization (TaWQ), which incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension. Extensive experiments on static (e.g., ImageNet) and neuromorphic (e.g., CIFAR10-DVS) datasets demonstrate that our TaWQ maintains high energy efficiency (4.12M, 0.63mJ) while incurring a negligible quantization loss of only 0.22% on ImageNet.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoCogNav: Cognition-aware Human Egocentric Navigation</title>
<link>https://arxiv.org/abs/2511.17581</link>
<guid>https://arxiv.org/abs/2511.17581</guid>
<content:encoded><![CDATA[
arXiv:2511.17581v1 Announce Type: cross 
Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Straight Flows: Variational Flow Matching for Efficient Generation</title>
<link>https://arxiv.org/abs/2511.17583</link>
<guid>https://arxiv.org/abs/2511.17583</guid>
<content:encoded><![CDATA[
arXiv:2511.17583v1 Announce Type: cross 
Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.17585</link>
<guid>https://arxiv.org/abs/2511.17585</guid>
<content:encoded><![CDATA[
arXiv:2511.17585v1 Announce Type: cross 
Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?</title>
<link>https://arxiv.org/abs/2511.17643</link>
<guid>https://arxiv.org/abs/2511.17643</guid>
<content:encoded><![CDATA[
arXiv:2511.17643v1 Announce Type: cross 
Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots</title>
<link>https://arxiv.org/abs/2511.17652</link>
<guid>https://arxiv.org/abs/2511.17652</guid>
<content:encoded><![CDATA[
arXiv:2511.17652v1 Announce Type: cross 
Abstract: Advances in AI have introduced several strong models in computational pathology to usher it into the era of multi-modal diagnosis, analysis, and interpretation. However, the current pathology-specific visual language models still lack capacities in making diagnosis with rigorous reasoning paths as well as handling divergent tasks, and thus challenges of building AI Copilots for real scenarios still exist. Here we introduce TeamPath, an AI system powered by reinforcement learning and router-enhanced solutions based on large-scale histopathology multimodal datasets, to work as a virtual assistant for expert-level disease diagnosis, patch-level information summarization, and cross-modality generation to integrate transcriptomic information for the clinical usage. We also collaborate with pathologists from Yale School of Medicine to demonstrate that TeamPath can assist them in working more efficiently by identifying and correcting expert conclusions and reasoning paths. Overall, TeamPath can flexibly choose the best settings according to the needs, and serve as an innovative and reliable system for information communication across different modalities and experts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CubeletWorld: A New Abstraction for Scalable 3D Modeling</title>
<link>https://arxiv.org/abs/2511.17664</link>
<guid>https://arxiv.org/abs/2511.17664</guid>
<content:encoded><![CDATA[
arXiv:2511.17664v1 Announce Type: cross 
Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams</title>
<link>https://arxiv.org/abs/2511.17693</link>
<guid>https://arxiv.org/abs/2511.17693</guid>
<content:encoded><![CDATA[
arXiv:2511.17693v1 Announce Type: cross 
Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography</title>
<link>https://arxiv.org/abs/2511.17744</link>
<guid>https://arxiv.org/abs/2511.17744</guid>
<content:encoded><![CDATA[
arXiv:2511.17744v1 Announce Type: cross 
Abstract: Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots</title>
<link>https://arxiv.org/abs/2511.17889</link>
<guid>https://arxiv.org/abs/2511.17889</guid>
<content:encoded><![CDATA[
arXiv:2511.17889v1 Announce Type: cross 
Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior</title>
<link>https://arxiv.org/abs/2511.17895</link>
<guid>https://arxiv.org/abs/2511.17895</guid>
<content:encoded><![CDATA[
arXiv:2511.17895v1 Announce Type: cross 
Abstract: Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animated Territorial Data Extractor (ATDE): A Computer-Vision Method for Extracting Territorial Data from Animated Historical Maps</title>
<link>https://arxiv.org/abs/2511.17920</link>
<guid>https://arxiv.org/abs/2511.17920</guid>
<content:encoded><![CDATA[
arXiv:2511.17920v1 Announce Type: cross 
Abstract: We present Animated Territorial Data Extractor (ATDE), a computer vision tool that extracts quantitative territorial data from animated historical map videos. ATDE employs HSV-based color segmentation, RGB channel filtering, and Direct-Neighbor Filtering to identify and count pixels representing territorial control. Combined with preprocessing for temporal alignment and cross-video scaling, the pipeline converts animated videos into structured time-series data. We demonstrate the tool on ten Chinese dynasties (200 BCE - 1912 CE), producing year-by-year pixel counts that align with expected historical patterns. While not a substitute for authoritative historical datasets, ATDE is well-suited for educational demonstrations, preliminary data exploration, and comparative analysis of territorial dynamics. The tool requires no pre-existing shapefiles and can be applied to any animated map video given seed colors and basic configuration. Code and examples are available on GitHub.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game</title>
<link>https://arxiv.org/abs/2511.17925</link>
<guid>https://arxiv.org/abs/2511.17925</guid>
<content:encoded><![CDATA[
arXiv:2511.17925v1 Announce Type: cross 
Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data</title>
<link>https://arxiv.org/abs/2511.18066</link>
<guid>https://arxiv.org/abs/2511.18066</guid>
<content:encoded><![CDATA[
arXiv:2511.18066v1 Announce Type: cross 
Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.18140</link>
<guid>https://arxiv.org/abs/2511.18140</guid>
<content:encoded><![CDATA[
arXiv:2511.18140v1 Announce Type: cross 
Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</title>
<link>https://arxiv.org/abs/2511.18151</link>
<guid>https://arxiv.org/abs/2511.18151</guid>
<content:encoded><![CDATA[
arXiv:2511.18151v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images</title>
<link>https://arxiv.org/abs/2511.18197</link>
<guid>https://arxiv.org/abs/2511.18197</guid>
<content:encoded><![CDATA[
arXiv:2511.18197v1 Announce Type: cross 
Abstract: This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
<link>https://arxiv.org/abs/2511.18248</link>
<guid>https://arxiv.org/abs/2511.18248</guid>
<content:encoded><![CDATA[
arXiv:2511.18248v1 Announce Type: cross 
Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Tables to Signals: Revealing Spectral Adaptivity in TabPFN</title>
<link>https://arxiv.org/abs/2511.18278</link>
<guid>https://arxiv.org/abs/2511.18278</guid>
<content:encoded><![CDATA[
arXiv:2511.18278v1 Announce Type: cross 
Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis</title>
<link>https://arxiv.org/abs/2511.18287</link>
<guid>https://arxiv.org/abs/2511.18287</guid>
<content:encoded><![CDATA[
arXiv:2511.18287v1 Announce Type: cross 
Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video</title>
<link>https://arxiv.org/abs/2511.18322</link>
<guid>https://arxiv.org/abs/2511.18322</guid>
<content:encoded><![CDATA[
arXiv:2511.18322v1 Announce Type: cross 
Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection</title>
<link>https://arxiv.org/abs/2511.18336</link>
<guid>https://arxiv.org/abs/2511.18336</guid>
<content:encoded><![CDATA[
arXiv:2511.18336v1 Announce Type: cross 
Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing UAV Search under Occlusion using Next Best View Planning</title>
<link>https://arxiv.org/abs/2511.18353</link>
<guid>https://arxiv.org/abs/2511.18353</guid>
<content:encoded><![CDATA[
arXiv:2511.18353v1 Announce Type: cross 
Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.18415</link>
<guid>https://arxiv.org/abs/2511.18415</guid>
<content:encoded><![CDATA[
arXiv:2511.18415v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</title>
<link>https://arxiv.org/abs/2511.18417</link>
<guid>https://arxiv.org/abs/2511.18417</guid>
<content:encoded><![CDATA[
arXiv:2511.18417v1 Announce Type: cross 
Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels</title>
<link>https://arxiv.org/abs/2511.18457</link>
<guid>https://arxiv.org/abs/2511.18457</guid>
<content:encoded><![CDATA[
arXiv:2511.18457v1 Announce Type: cross 
Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.18468</link>
<guid>https://arxiv.org/abs/2511.18468</guid>
<content:encoded><![CDATA[
arXiv:2511.18468v1 Announce Type: cross 
Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation</title>
<link>https://arxiv.org/abs/2511.18493</link>
<guid>https://arxiv.org/abs/2511.18493</guid>
<content:encoded><![CDATA[
arXiv:2511.18493v1 Announce Type: cross 
Abstract: The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2511.18539</link>
<guid>https://arxiv.org/abs/2511.18539</guid>
<content:encoded><![CDATA[
arXiv:2511.18539v1 Announce Type: cross 
Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations</title>
<link>https://arxiv.org/abs/2511.18617</link>
<guid>https://arxiv.org/abs/2511.18617</guid>
<content:encoded><![CDATA[
arXiv:2511.18617v1 Announce Type: cross 
Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers</title>
<link>https://arxiv.org/abs/2511.18670</link>
<guid>https://arxiv.org/abs/2511.18670</guid>
<content:encoded><![CDATA[
arXiv:2511.18670v1 Announce Type: cross 
Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Rendering for High-Genus Surface Meshes from Multi-View Images</title>
<link>https://arxiv.org/abs/2511.18680</link>
<guid>https://arxiv.org/abs/2511.18680</guid>
<content:encoded><![CDATA[
arXiv:2511.18680v1 Announce Type: cross 
Abstract: We present a topology-informed inverse rendering approach for reconstructing high-genus surface meshes from multi-view images. Compared to 3D representations like voxels and point clouds, mesh-based representations are preferred as they enable the application of differential geometry theory and are optimized for modern graphics pipelines. However, existing inverse rendering methods often fail catastrophically on high-genus surfaces, leading to the loss of key topological features, and tend to oversmooth low-genus surfaces, resulting in the loss of surface details. This failure stems from their overreliance on Adam-based optimizers, which can lead to vanishing and exploding gradients. To overcome these challenges, we introduce an adaptive V-cycle remeshing scheme in conjunction with a re-parametrized Adam optimizer to enhance topological and geometric awareness. By periodically coarsening and refining the deforming mesh, our method informs mesh vertices of their current topology and geometry before optimization, mitigating gradient issues while preserving essential topological features. Additionally, we enforce topological consistency by constructing topological primitives with genus numbers that match those of ground truth using Gauss-Bonnet theorem. Experimental results demonstrate that our inverse rendering approach outperforms the current state-of-the-art method, achieving significant improvements in Chamfer Distance and Volume IoU, particularly for high-genus surfaces, while also enhancing surface details for low-genus surfaces.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking</title>
<link>https://arxiv.org/abs/2511.18692</link>
<guid>https://arxiv.org/abs/2511.18692</guid>
<content:encoded><![CDATA[
arXiv:2511.18692v1 Announce Type: cross 
Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Multi-Drone GNSS Tracking System for Marine Robots</title>
<link>https://arxiv.org/abs/2511.18694</link>
<guid>https://arxiv.org/abs/2511.18694</guid>
<content:encoded><![CDATA[
arXiv:2511.18694v1 Announce Type: cross 
Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Real-Time Anomaly Detection and Industrial Applications</title>
<link>https://arxiv.org/abs/2511.18698</link>
<guid>https://arxiv.org/abs/2511.18698</guid>
<content:encoded><![CDATA[
arXiv:2511.18698v1 Announce Type: cross 
Abstract: This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection</title>
<link>https://arxiv.org/abs/2511.18702</link>
<guid>https://arxiv.org/abs/2511.18702</guid>
<content:encoded><![CDATA[
arXiv:2511.18702v1 Announce Type: cross 
Abstract: General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction</title>
<link>https://arxiv.org/abs/2511.18716</link>
<guid>https://arxiv.org/abs/2511.18716</guid>
<content:encoded><![CDATA[
arXiv:2511.18716v1 Announce Type: cross 
Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation</title>
<link>https://arxiv.org/abs/2511.18724</link>
<guid>https://arxiv.org/abs/2511.18724</guid>
<content:encoded><![CDATA[
arXiv:2511.18724v1 Announce Type: cross 
Abstract: Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling Control for Imbalanced Calibration in Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2511.18773</link>
<guid>https://arxiv.org/abs/2511.18773</guid>
<content:encoded><![CDATA[
arXiv:2511.18773v1 Announce Type: cross 
Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoGS: Disentangling Invariants and Changes in Multi-Period Scenes</title>
<link>https://arxiv.org/abs/2511.18794</link>
<guid>https://arxiv.org/abs/2511.18794</guid>
<content:encoded><![CDATA[
arXiv:2511.18794v1 Announce Type: cross 
Abstract: Multi-period image collections are common in real-world applications. Cities are re-scanned for mapping, construction sites are revisited for progress tracking, and natural regions are monitored for environmental change. Such data form multi-period scenes, where geometry and appearance evolve. Reconstructing such scenes is an important yet underexplored problem. Existing pipelines rely on incompatible assumptions: static and in-the-wild methods enforce a single geometry, while dynamic ones assume smooth motion, both failing under long-term, discontinuous changes. To solve this problem, we introduce ChronoGS, a temporally modulated Gaussian representation that reconstructs all periods within a unified anchor scaffold. It's also designed to disentangle stable and evolving components, achieving temporally consistent reconstruction of multi-period scenes. To catalyze relevant research, we release ChronoScene dataset, a benchmark of real and synthetic multi-period scenes, capturing geometric and appearance variation. Experiments demonstrate that ChronoGS consistently outperforms baselines in reconstruction quality and temporal consistency. Our code and the ChronoScene dataset are publicly available at https://github.com/ZhongtaoWang/ChronoGS.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2511.18833</link>
<guid>https://arxiv.org/abs/2511.18833</guid>
<content:encoded><![CDATA[
arXiv:2511.18833v1 Announce Type: cross 
Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning</title>
<link>https://arxiv.org/abs/2511.18859</link>
<guid>https://arxiv.org/abs/2511.18859</guid>
<content:encoded><![CDATA[
arXiv:2511.18859v1 Announce Type: cross 
Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction</title>
<link>https://arxiv.org/abs/2511.18874</link>
<guid>https://arxiv.org/abs/2511.18874</guid>
<content:encoded><![CDATA[
arXiv:2511.18874v1 Announce Type: cross 
Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatMart: Material Reconstruction of 3D Objects via Diffusion</title>
<link>https://arxiv.org/abs/2511.18900</link>
<guid>https://arxiv.org/abs/2511.18900</guid>
<content:encoded><![CDATA[
arXiv:2511.18900v1 Announce Type: cross 
Abstract: Applying diffusion models to physically-based material estimation and generation has recently gained prominence. In this paper, we propose \ttt, a novel material reconstruction framework for 3D objects, offering the following advantages. First, \ttt\ adopts a two-stage reconstruction, starting with accurate material prediction from inputs and followed by prior-guided material generation for unobserved views, yielding high-fidelity results. Second, by utilizing progressive inference alongside the proposed view-material cross-attention (VMCA), \ttt\ enables reconstruction from an arbitrary number of input images, demonstrating strong scalability and flexibility. Finally, \ttt\ achieves both material prediction and generation capabilities through end-to-end optimization of a single diffusion model, without relying on additional pre-trained models, thereby exhibiting enhanced stability across various types of objects. Extensive experiments demonstrate that \ttt\ achieves superior performance in material reconstruction compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.18950</link>
<guid>https://arxiv.org/abs/2511.18950</guid>
<content:encoded><![CDATA[
arXiv:2511.18950v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</title>
<link>https://arxiv.org/abs/2511.18960</link>
<guid>https://arxiv.org/abs/2511.18960</guid>
<content:encoded><![CDATA[
arXiv:2511.18960v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach</title>
<link>https://arxiv.org/abs/2511.19080</link>
<guid>https://arxiv.org/abs/2511.19080</guid>
<content:encoded><![CDATA[
arXiv:2511.19080v1 Announce Type: cross 
Abstract: The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization</title>
<link>https://arxiv.org/abs/2511.19248</link>
<guid>https://arxiv.org/abs/2511.19248</guid>
<content:encoded><![CDATA[
arXiv:2511.19248v1 Announce Type: cross 
Abstract: Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments</title>
<link>https://arxiv.org/abs/2511.19396</link>
<guid>https://arxiv.org/abs/2511.19396</guid>
<content:encoded><![CDATA[
arXiv:2511.19396v1 Announce Type: cross 
Abstract: Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</title>
<link>https://arxiv.org/abs/2511.19413</link>
<guid>https://arxiv.org/abs/2511.19413</guid>
<content:encoded><![CDATA[
arXiv:2511.19413v1 Announce Type: cross 
Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Map Distillation Without Data</title>
<link>https://arxiv.org/abs/2511.19428</link>
<guid>https://arxiv.org/abs/2511.19428</guid>
<content:encoded><![CDATA[
arXiv:2511.19428v1 Announce Type: cross 
Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Horizons in Action Chunking</title>
<link>https://arxiv.org/abs/2511.19433</link>
<guid>https://arxiv.org/abs/2511.19433</guid>
<content:encoded><![CDATA[
arXiv:2511.19433v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $\pi_0$, $\pi_{0.5}$, and one-step regression policy $\pi_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $\pi_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Shape of Sight: A Homological Framework for Unifying Visual Perception</title>
<link>https://arxiv.org/abs/1802.04723</link>
<guid>https://arxiv.org/abs/1802.04723</guid>
<content:encoded><![CDATA[
arXiv:1802.04723v2 Announce Type: replace 
Abstract: Visual perception, the brain's construction of a stable world from sensory data, faces several long-standing, fundamental challenges. While often studied separately, these problems have resisted a single, unifying computational framework. In this perspective, we propose a homological framework for visual perception. We argue that the brain's latent representations are governed by their topological parity. This parity interpretation functionally separates homological structures into two distinct classes: 1) Even-dimensional homology ($H_{even}$) acts as static, integrative scaffolds. These structures bind context and content into ``wholes'' or ``what'', serving as the stable, resonant cavities for perceptual objects; 2) Odd-dimensional homology ($H_{odd}$) acts as dynamic, recurrent flows. These structures represent paths, transformations, and self-sustaining ``traces'' or ``where'' that navigate the perceptual landscape. This scaffold-and-flow model is supported by the ventral-dorsal pathway separation and provides a unified solution to three core problems in visual perception. Homological parity hypothesis recasts visual perception not as a linear computation, but as a dynamic interaction between stable, integrative structures and the recurrent, self-sustaining flows that run on them. This perspective offers a new mathematical foundation for linking neural dynamics to perception and cognition.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K-FACE: A Large-Scale KIST Face Database in Consideration with Unconstrained Environments</title>
<link>https://arxiv.org/abs/2103.02211</link>
<guid>https://arxiv.org/abs/2103.02211</guid>
<content:encoded><![CDATA[
arXiv:2103.02211v2 Announce Type: replace 
Abstract: In this paper, we introduce a new large-scale face database from KIST, denoted as K-FACE, and describe a novel capturing device specifically designed to obtain the data. The K-FACE database contains more than 1 million high-quality images of 1,000 subjects selected by considering the ratio of gender and age groups. It includes a variety of attributes, including 27 poses, 35 lighting conditions, three expressions, and occlusions by the combination of five types of accessories. As the K-FACE database is systematically constructed through a hemispherical capturing system with elaborate lighting control and multiple cameras, it is possible to accurately analyze the effects of factors that cause performance degradation, such as poses, lighting changes, and accessories. We consider not only the balance of external environmental factors, such as pose and lighting, but also the balance of personal characteristics such as gender and age group. The gender ratio is the same, while the age groups of subjects are uniformly distributed from the 20s to 50s for both genders. The K-FACE database can be extensively utilized in various vision tasks, such as face recognition, face frontalization, illumination normalization, face age estimation, and three-dimensional face model generation. We expect systematic diversity and uniformity of the K-FACE database to promote these research fields.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiview point cloud registration with anisotropic and space-varying localization noise</title>
<link>https://arxiv.org/abs/2201.00708</link>
<guid>https://arxiv.org/abs/2201.00708</guid>
<content:encoded><![CDATA[
arXiv:2201.00708v2 Announce Type: replace 
Abstract: In this paper, we address the problem of registering multiple point clouds corrupted with high anisotropic localization noise. Our approach follows the widely used framework of Gaussian mixture model (GMM) reconstruction with an expectation-maximization (EM) algorithm. Existing methods are based on an implicit assumption of space-invariant isotropic Gaussian noise. However, this assumption is violated in practice in applications such as single molecule localization microscopy (SMLM). To address this issue, we propose to introduce an explicit localization noise model that decouples shape modeling with the GMM from noise handling. We design a stochastic EM algorithm that considers noise-free data as a latent variable, with closed-form solutions at each EM step. The first advantage of our approach is to handle space-variant and anisotropic Gaussian noise with arbitrary covariances. The second advantage is to leverage the explicit noise model to impose prior knowledge about the noise that may be available from physical sensors. We show on various simulated data that our noise handling strategy improves significantly the robustness to high levels of anisotropic noise. We also demonstrate the performance of our method on real SMLM data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Graph Convolutional Recurrent Neural Network Model for Citywide Air Pollution Forecasting</title>
<link>https://arxiv.org/abs/2304.12630</link>
<guid>https://arxiv.org/abs/2304.12630</guid>
<content:encoded><![CDATA[
arXiv:2304.12630v2 Announce Type: replace 
Abstract: Citywide Air Pollution Forecasting tries to precisely predict the air quality multiple hours ahead for the entire city. This topic is challenged since air pollution varies in a spatiotemporal manner and depends on many complicated factors. Our previous research has solved the problem by considering the whole city as an image and leveraged a Convolutional Long Short-Term Memory (ConvLSTM) model to learn the spatiotemporal features. However, an image-based representation may not be ideal as air pollution and other impact factors have natural graph structures. In this research, we argue that a Graph Convolutional Network (GCN) can efficiently represent the spatial features of air quality readings in the whole city. Specially, we extend the ConvLSTM model to a Spatiotemporal Graph Convolutional Recurrent Neural Network (Spatiotemporal GCRNN) model by tightly integrating a GCN architecture into an RNN structure for efficient learning spatiotemporal characteristics of air quality values and their influential factors. Our extensive experiments prove the proposed model has a better performance compare to the state-of-the-art ConvLSTM model for air pollution predicting while the number of parameters is much smaller. Moreover, our approach is also superior to a hybrid GCN-based method in a real-world air pollution dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</title>
<link>https://arxiv.org/abs/2310.17218</link>
<guid>https://arxiv.org/abs/2310.17218</guid>
<content:encoded><![CDATA[
arXiv:2310.17218v2 Announce Type: replace 
Abstract: This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene Summarization: Clustering Scene Videos into Spatially Diverse Frames</title>
<link>https://arxiv.org/abs/2311.17940</link>
<guid>https://arxiv.org/abs/2311.17940</guid>
<content:encoded><![CDATA[
arXiv:2311.17940v3 Announce Type: replace 
Abstract: Humans are remarkably efficient at forming spatial understanding from just a few visual observations. When browsing real estate or navigating unfamiliar spaces, they intuitively select a small set of views that summarize the spatial layout. Inspired by this ability, we introduce scene summarization, the task of condensing long, continuous scene videos into a compact set of spatially diverse keyframes that facilitate global spatial reasoning. Unlike conventional video summarization-which focuses on user-edited, fragmented clips and often ignores spatial continuity-our goal is to mimic how humans abstract spatial layout from sparse views. We propose SceneSum, a two-stage self-supervised pipeline that first clusters video frames using visual place recognition to promote spatial diversity, then selects representative keyframes from each cluster under resource constraints. When camera trajectories are available, a lightweight supervised loss further refines clustering and selection. Experiments on real and simulated indoor datasets show that SceneSum produces more spatially informative summaries and outperforms existing video summarization baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Roadside Monocular 3D Detection Prompted by 2D Detection</title>
<link>https://arxiv.org/abs/2404.01064</link>
<guid>https://arxiv.org/abs/2404.01064</guid>
<content:encoded><![CDATA[
arXiv:2404.01064v4 Announce Type: replace 
Abstract: Roadside monocular 3D detection requires detecting objects of predefined classes in an RGB frame and predicting their 3D attributes, such as bird's-eye-view (BEV) locations. It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To address this task, we introduce Promptable 3D Detector (Pro3D), a novel detector design that leverages 2D detections as prompts. We build our Pro3D upon two key insights. First, compared to a typical 3D detector, a 2D detector is ``easier'' to train due to fewer loss terms and performs significantly better at localizing objects w.r.t 2D metrics. Second, once 2D detections precisely locate objects in the image, a 3D detector can focus on lifting these detections into 3D BEV, especially when fixed camera pose or scene geometry provide an informative prior. To encode and incorporate 2D detections, we explore three methods: (a) concatenating features from both 2D and 3D detectors, (b) attentively fusing 2D and 3D detector features, and (c) encoding properties of predicted 2D bounding boxes \{$x$, $y$, width, height, label\} and attentively fusing them with the 3D detector feature. Interestingly, the third method significantly outperforms the others, underscoring the effectiveness of 2D detections as prompts that offer precise object targets and allow the 3D detector to focus on lifting them into 3D. Pro3D is adaptable for use with a wide range of 2D and 3D detectors with minimal modifications. Comprehensive experiments demonstrate that our Pro3D significantly enhances existing methods, achieving state-of-the-art results on two contemporary benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QGait: Toward Accurate Quantization for Gait Recognition</title>
<link>https://arxiv.org/abs/2405.13859</link>
<guid>https://arxiv.org/abs/2405.13859</guid>
<content:encoded><![CDATA[
arXiv:2405.13859v2 Announce Type: replace 
Abstract: Existing deep learning methods have made significant progress in gait recognition. Quantization can facilitate the application of gait models as a model-agnostic general compression technique. Typically, appearance-based models binarize inputs into silhouette sequences. However, mainstream quantization methods prioritize minimizing task loss over quantization error, which is detrimental to gait recognition with binarized inputs. To address this, we propose a differentiable soft quantizer, which better simulates the gradient of the round function during backpropagation. This enables the network to learn from subtle input perturbations. However, our theoretical analysis and empirical studies reveal that directly applying the soft quantizer can hinder network convergence. We addressed this issue by adopting a two-stage training strategy, introducing a soft quantizer during the fine-tuning phase. However, in the first stage of training, we observed a significant change in the output distribution of different samples in the feature space compared to the full-precision network. It is this change that led to a loss in performance. Based on this, we propose an Inter-class Distance-guided Calibration (IDC) strategy to preserve the relative distance between the embeddings of samples with different labels. Extensive experiments validate the effectiveness of our approach, demonstrating state-of-the-art accuracy across various settings and datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchDeco: Training-Free Latent Composition for Precise Sketch Colourisation</title>
<link>https://arxiv.org/abs/2405.18716</link>
<guid>https://arxiv.org/abs/2405.18716</guid>
<content:encoded><![CDATA[
arXiv:2405.18716v2 Announce Type: replace 
Abstract: We introduce SketchDeco, a training-free approach to sketch colourisation that bridges the gap between professional design needs and intuitive, region-based control. Our method empowers artists to use simple masks and colour palettes for precise spatial and chromatic specification, avoiding both the tediousness of manual assignment and the ambiguity of text-based prompts. We reformulate this task as a novel, training-free composition problem. Our core technical contribution is a guided latent-space blending process: we first leverage diffusion inversion to precisely ``paint'' user-defined colours into specified regions, and then use a custom self-attention mechanism to harmoniously blend these local edits with a globally consistent base image. This ensures both local colour fidelity and global harmony without requiring any model fine-tuning. Our system produces high-quality results in 15--20 inference steps on consumer GPUs, making professional-quality, controllable colourisation accessible.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2406.18572</link>
<guid>https://arxiv.org/abs/2406.18572</guid>
<content:encoded><![CDATA[
arXiv:2406.18572v4 Announce Type: replace 
Abstract: This work tackles the problem of geo-localization with a new paradigm using a large vision-language model (LVLM) augmented with human inference knowledge. A primary challenge here is the scarcity of data for training the LVLM - existing street-view datasets often contain numerous low-quality images lacking visual clues, and lack any reasoning inference. To address the data-quality issue, we devise a CLIP-based network to quantify the degree of street-view images being locatable, leading to the creation of a new dataset comprising highly locatable street views. To enhance reasoning inference, we integrate external knowledge obtained from real geo-localization games, tapping into valuable human inference capabilities. The data are utilized to train GeoReasoner, which undergoes fine-tuning through dedicated reasoning and location-tuning stages. Qualitative and quantitative evaluations illustrate that GeoReasoner outperforms counterpart LVLMs by more than 25% at country-level and 38% at city-level geo-localization tasks, and surpasses StreetCLIP performance while requiring fewer training resources. The data and code are available at https://github.com/lingli1996/GeoReasoner.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PriorDrive: Enhancing Online HD Mapping with Unified Vector Priors</title>
<link>https://arxiv.org/abs/2409.05352</link>
<guid>https://arxiv.org/abs/2409.05352</guid>
<content:encoded><![CDATA[
arXiv:2409.05352v4 Announce Type: replace 
Abstract: High-Definition Maps (HD maps) are essential for the precise navigation and decision-making of autonomous vehicles, yet their creation and upkeep present significant cost and timeliness challenges. The online construction of HD maps using on-board sensors has emerged as a promising solution; however, these methods can be impeded by incomplete data due to occlusions and inclement weather, while their performance in distant regions remains unsatisfying. This paper proposes PriorDrive to address these limitations by directly harnessing the power of various vectorized prior maps, significantly enhancing the robustness and accuracy of online HD map construction. Our approach integrates a variety of prior maps uniformly, such as OpenStreetMap's Standard Definition Maps (SD maps), outdated HD maps from vendors, and locally constructed maps from historical vehicle data. To effectively integrate such prior information into online mapping models, we introduce a Hybrid Prior Representation (HPQuery) that standardizes the representation of diverse map elements. We further propose a Unified Vector Encoder (UVE), which employs fused prior embedding and a dual encoding mechanism to encode vector data. To improve the UVE's generalizability and performance, we propose a segment-level and point-level pre-training strategy that enables the UVE to learn the prior distribution of vector data. Through extensive testing on the nuScenes, Argoverse 2 and OpenLane-V2, we demonstrate that PriorDrive is highly compatible with various online mapping models and substantially improves map prediction capabilities. The integration of prior maps through PriorDrive offers a robust solution to the challenges of single-perception data, paving the way for more reliable autonomous vehicle navigation. Code is available at https://github.com/MIV-XJTU/PriorDrive.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title>
<link>https://arxiv.org/abs/2409.08840</link>
<guid>https://arxiv.org/abs/2409.08840</guid>
<content:encoded><![CDATA[
arXiv:2409.08840v4 Announce Type: replace 
Abstract: Collaborative perception (CP) leverages visual data from connected and autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV). Despite recent progress, current CP methods expand the ego vehicle's 360-degree perceptual range almost equally, which faces two key challenges. Firstly, in areas with uneven traffic distribution, focusing on directions with little traffic offers limited benefits. Secondly, under limited communication budgets, allocating excessive bandwidth to less critical directions lowers the perception accuracy in more vital areas. To address these issues, we propose Direct-CP, a proactive and direction-aware CP system aiming at improving CP in specific directions. Our key idea is to enable an ego vehicle to proactively signal its interested directions and readjust its attention to enhance local directional CP performance. To achieve this, we first propose an RSU-aided direction masking mechanism that assists an ego vehicle in identifying vital directions. Additionally, we design a direction-aware selective attention module to wisely aggregate pertinent features based on ego vehicle's directional priorities, communication budget, and the positional data of CAVs. Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture the divergence between directional CP outcomes and the ground truth, facilitating effective model training. Extensive experiments on the V2X-Sim 2.0 dataset demonstrate that our approach achieves 19.8\% higher local perception accuracy in interested directions and 2.5\% higher overall perception accuracy than the state-of-the-art methods in collaborative 3D object detection tasks. Codes are available at https://github.com/yihangtao/Directed-CP.git.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvement of Spiking Neural Network with Bit Planes and Color Models</title>
<link>https://arxiv.org/abs/2410.08229</link>
<guid>https://arxiv.org/abs/2410.08229</guid>
<content:encoded><![CDATA[
arXiv:2410.08229v5 Announce Type: replace 
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Complete Shapes: A Benchmark for Quantitative Evaluation of 3D Shape Surface Matching Algorithms</title>
<link>https://arxiv.org/abs/2411.03511</link>
<guid>https://arxiv.org/abs/2411.03511</guid>
<content:encoded><![CDATA[
arXiv:2411.03511v3 Announce Type: replace 
Abstract: Finding correspondences between 3D deformable shapes is an important and long-standing problem in geometry processing, computer vision, graphics, and beyond. While various shape matching datasets exist, they are mostly static or limited in size, restricting their adaptation to different problem settings, including both full and partial shape matching. In particular the existing partial shape matching datasets are small (fewer than 100 shapes) and thus unsuitable for data-hungry machine learning approaches. Moreover, the type of partiality present in existing datasets is often artificial and far from realistic. To address these limitations, we introduce a generic and flexible framework for the procedural generation of challenging full and partial shape matching datasets. Our framework allows the propagation of custom annotations across shapes, making it useful for various applications. By utilising our framework and manually creating cross-dataset correspondences between seven existing (complete geometry) shape matching datasets, we propose a new large benchmark BeCoS with a total of 2543 shapes. Based on this, we offer several challenging benchmark settings, covering both full and partial matching, for which we evaluate respective state-of-the-art methods as baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension</title>
<link>https://arxiv.org/abs/2411.13093</link>
<guid>https://arxiv.org/abs/2411.13093</guid>
<content:encoded><![CDATA[
arXiv:2411.13093v4 Announce Type: replace 
Abstract: Existing large video-language models (LVLMs) struggle to comprehend long videos correctly due to limited context. To address this problem, fine-tuning long-context LVLMs and employing GPT-based agents have emerged as promising solutions. However, fine-tuning LVLMs would require extensive high-quality data and substantial GPU resources, while GPT-based agents would rely on proprietary models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented Generation (Video-RAG), a training-free and cost-effective pipeline that employs visually-aligned auxiliary texts to help facilitate cross-modality alignment while providing additional information beyond the visual content. Specifically, we leverage open-source external tools to extract visually-aligned information from pure video data (e.g., audio, optical character, and object detection), and incorporate the extracted information into an existing LVLM as auxiliary texts, alongside video frames and queries, in a plug-and-play manner. Our Video-RAG offers several key advantages: (i) lightweight with low computing overhead due to single-turn retrieval; (ii) easy implementation and compatibility with any LVLM; and (iii) significant, consistent performance gains across long video understanding benchmarks, including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o when utilized with a 72B model.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Coreset Selection via Iterative Subspace Sampling</title>
<link>https://arxiv.org/abs/2411.15349</link>
<guid>https://arxiv.org/abs/2411.15349</guid>
<content:encoded><![CDATA[
arXiv:2411.15349v2 Announce Type: replace 
Abstract: Deep learning increasingly relies on massive data with substantial storage, annotation, and training costs. To reduce costs, coreset selection finds a representative subset of data to train models while ideally performing on par with the full data training. To maximize performance, current state-of-the-art coreset methods select data using dataset-specific ground truth labels and training. However, these methodological requirements prevent selection at scale on real-world, unlabeled data. To that end, this paper addresses the selection of coresets that achieve state-of-the-art performance but without using any labels or training on candidate data. Instead, our solution, Zero-Shot Coreset Selection via Iterative Subspace Sampling (ZCore), uses previously-trained foundation models to generate zero-shot, high-dimensional embedding spaces to interpret unlabeled data. ZCore then iteratively quantifies the relative value of all candidate data based on coverage and redundancy in numerous subspace distributions. Finally, ZCore selects a coreset sized for any data budget to train downstream models. We evaluate ZCore on four datasets and outperform several state-of-the-art label-based methods, especially at low data rates that provide the most substantial cost reduction. On ImageNet, ZCore selections for 10% training data achieve a downstream validation accuracy of 53.99%, which outperforms prior label-based methods and removes annotation and training costs for 1.15 million images. Our paper's code is publicly available at https://github.com/voxel51/zcore.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format</title>
<link>https://arxiv.org/abs/2411.17991</link>
<guid>https://arxiv.org/abs/2411.17991</guid>
<content:encoded><![CDATA[
arXiv:2411.17991v2 Announce Type: replace 
Abstract: Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and a query as input, after which the model generates a response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in a real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on a video-text duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When a text message ends, the video continues to play, akin to the alternative of two performers in a duet. We construct MMDuetIT, a video-text training dataset designed to adapt VideoLLMs to video-text duet interaction format. We also introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various time-sensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90\% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) with minimal training efforts, and also enable VideoLLMs to reply in a real-time manner as the video plays.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval</title>
<link>https://arxiv.org/abs/2412.01558</link>
<guid>https://arxiv.org/abs/2412.01558</guid>
<content:encoded><![CDATA[
arXiv:2412.01558v2 Announce Type: replace 
Abstract: Prevailing joint prediction transformers for Video Highlight Detection and Moment Retrieval (HD/MR) exhibit deficiencies in handling cross-task dynamics, achieving robust video-text alignment, and utilizing effective attention mechanisms, with the potential of Large Language/Vision-Language Models (LLMs/LVLMs) being largely untapped. This paper introduces VideoLights, a novel HD/MR framework addressing these limitations by incorporating: (i) Convolutional Projection and Feature Refinement modules with an alignment loss for enhanced video-text feature congruity; (ii) a Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware representations; (iii) a Uni-directional joint-task feedback mechanism for synergistic task improvement; (iv) hard positive/negative losses for adaptive learning; and (v) the leveraging of LVLMs (e.g., BLIP-2) for superior multimodal feature integration and intelligent pre-training with synthetic data. Comprehensive evaluations on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate that VideoLights significantly surpasses existing baselines, establishing new state-of-the-art performances. Codes and model checkpoints are available at https://github.com/dpaul06/VideoLights .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splats in Splats: Robust and Effective 3D Steganography towards Gaussian Splatting</title>
<link>https://arxiv.org/abs/2412.03121</link>
<guid>https://arxiv.org/abs/2412.03121</guid>
<content:encoded><![CDATA[
arXiv:2412.03121v2 Announce Type: replace 
Abstract: 3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe splats in splats, the first 3DGS steganography framework that embeds 3D content in 3DGS itself without modifying any attributes. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that our method significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3x faster rendering speed, while ensuring security, robustness, and user experience.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster and Better 3D Splatting via Group Training</title>
<link>https://arxiv.org/abs/2412.07608</link>
<guid>https://arxiv.org/abs/2412.07608</guid>
<content:encoded><![CDATA[
arXiv:2412.07608v3 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30\% faster convergence and improved rendering quality across diverse scenarios. Project Website: https://chengbo-wang.github.io/3DGS-with-Group-Training/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic Reconstruction</title>
<link>https://arxiv.org/abs/2501.01119</link>
<guid>https://arxiv.org/abs/2501.01119</guid>
<content:encoded><![CDATA[
arXiv:2501.01119v2 Announce Type: replace 
Abstract: Open-vocabulary panoptic reconstruction offers comprehensive scene understanding, enabling advances in embodied robotics and photorealistic simulation. In this paper, we propose PanopticRecon++, an end-to-end method that formulates panoptic reconstruction through a novel cross-attention perspective. This perspective models the relationship between 3D instances (as queries) and the scene's 3D embedding field (as keys) through their attention map. Unlike existing methods that separate the optimization of queries and keys or overlook spatial proximity, PanopticRecon++ introduces learnable 3D Gaussians as instance queries. This formulation injects 3D spatial priors to preserve proximity while maintaining end-to-end optimizability. Moreover, this query formulation facilitates the alignment of 2D open-vocabulary instance IDs across frames by leveraging optimal linear assignment with instance masks rendered from the queries. Additionally, we ensure semantic-instance segmentation consistency by fusing query-based instance segmentation probabilities with semantic probabilities in a novel panoptic head supervised by a panoptic loss. During training, the number of instance query tokens dynamically adapts to match the number of objects. PanopticRecon++ shows competitive performance in terms of 3D and 2D segmentation and reconstruction performance on both simulation and real-world datasets, and demonstrates a user case as a robot simulator. Our project website is at: https://yuxuan1206.github.io/panopticrecon_pp/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2501.03931</link>
<guid>https://arxiv.org/abs/2501.03931</guid>
<content:encoded><![CDATA[
arXiv:2501.03931v2 Announce Type: replace 
Abstract: We present MagicMirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that MagicMirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teacher Encoder-Student Decoder Denoising Guided Segmentation Network for Anomaly Detection</title>
<link>https://arxiv.org/abs/2501.12104</link>
<guid>https://arxiv.org/abs/2501.12104</guid>
<content:encoded><![CDATA[
arXiv:2501.12104v4 Announce Type: replace 
Abstract: Visual anomaly detection is a highly challenging task, often categorized as a one-class classification and segmentation problem. Recent studies have demonstrated that the student-teacher (S-T) framework effectively addresses this challenge. However, most S-T frameworks rely solely on pre-trained teacher networks to guide student networks in learning multi-scale similar features, overlooking the potential of the student networks to enhance learning through multi-scale feature fusion. In this study, we propose a novel model named PFADSeg, which integrates a pre-trained teacher network, a denoising student network with multi-scale feature fusion, and a guided anomaly segmentation network into a unified framework. By adopting a unique teacher-encoder and student-decoder denoising mode, the model improves the student network's ability to learn from teacher network features. Furthermore, an adaptive feature fusion mechanism is introduced to train a self-supervised segmentation network that synthesizes anomaly masks autonomously, significantly increasing detection performance. Rigorous evaluations on the widely-used MVTec AD dataset demonstrate that PFADSeg exhibits excellent performance, achieving an image-level AUC of 98.9%, a pixel-level mean precision of 76.4%, and an instance-level mean precision of 78.7%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes</title>
<link>https://arxiv.org/abs/2502.00392</link>
<guid>https://arxiv.org/abs/2502.00392</guid>
<content:encoded><![CDATA[
arXiv:2502.00392v3 Announce Type: replace 
Abstract: Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code are be publicly at https://github.com/sunzc-sunny/refdrone.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DICE: Distilling Classifier-Free Guidance into Text Embeddings</title>
<link>https://arxiv.org/abs/2502.03726</link>
<guid>https://arxiv.org/abs/2502.03726</guid>
<content:encoded><![CDATA[
arXiv:2502.03726v2 Announce Type: replace 
Abstract: Text-to-image diffusion models are capable of generating high-quality images, but suboptimal pre-trained text representations often result in these images failing to align closely with the given text prompts. Classifier-free guidance (CFG) is a popular and effective technique for improving text-image alignment in the generative process. However, CFG introduces significant computational overhead. In this paper, we present DIstilling CFG by sharpening text Embeddings (DICE) that replaces CFG in the sampling process with half the computational complexity while maintaining similar generation quality. DICE distills a CFG-based text-to-image diffusion model into a CFG-free version by refining text embeddings to replicate CFG-based directions. In this way, we avoid the computational drawbacks of CFG, enabling high-quality, well-aligned image generation at a fast sampling speed. Furthermore, examining the enhancement pattern, we identify the underlying mechanism of DICE that sharpens specific components of text embeddings to preserve semantic information while enhancing fine-grained details. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL, and PixArt-$\alpha$ demonstrate the effectiveness of our method. Code is available at https://github.com/zju-pi/dice.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable and Testable Vision Features via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2502.06755</link>
<guid>https://arxiv.org/abs/2502.06755</guid>
<content:encoded><![CDATA[
arXiv:2502.06755v2 Announce Type: replace 
Abstract: To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. While earlier work offers either rich semantics or direct control, few post-hoc tools supply both in a single, model-agnostic procedure. We use sparse autoencoders (SAEs) to bridge this gap; each sparse feature comes with real-image exemplars that reveal its meaning and a decoding vector that can be manipulated to probe its influence on downstream task behavior. By applying our method to widely-used pre-trained vision models, we reveal meaningful differences in the semantic abstractions learned by different pre-training objectives. We then show that a single SAE trained on frozen ViT activations supports patch-level causal edits across tasks (classification and segmentation) all without retraining the ViT or task heads. These qualitative, falsifiable demonstrations position SAEs as a practical bridge between concept discovery and causal probing of vision models. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/saev.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus Segmentation with Prototype Consistency Alignment and Conditional Self-Training</title>
<link>https://arxiv.org/abs/2502.07331</link>
<guid>https://arxiv.org/abs/2502.07331</guid>
<content:encoded><![CDATA[
arXiv:2502.07331v2 Announce Type: replace 
Abstract: Manual segmentation is labor-intensive, and automatic segmentation remains challenging due to the inherent variability in meniscal morphology, partial volume effects, and low contrast between the meniscus and surrounding tissues. To address these challenges, we propose ERANet, an innovative semi-supervised framework for meniscus segmentation that effectively leverages both labeled and unlabeled images through advanced augmentation and learning strategies. ERANet integrates three key components: edge replacement augmentation (ERA), prototype consistency alignment (PCA), and a conditional self-training (CST) strategy within a mean teacher architecture. ERA introduces anatomically relevant perturbations by simulating meniscal variations, ensuring that augmentations align with the structural context. PCA enhances segmentation performance by aligning intra-class features and promoting compact, discriminative feature representations, particularly in scenarios with limited labeled data. CST improves segmentation robustness by iteratively refining pseudo-labels and mitigating the impact of label noise during training. Together, these innovations establish ERANet as a robust and scalable solution for meniscus segmentation, effectively addressing key barriers to practical implementation. We validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and 3D Fast/Turbo Spin Echo (FSE/TSE) MRI sequences. The results demonstrate the superior performance of ERANet compared to state-of-the-art methods. The proposed framework achieves reliable and accurate segmentation of meniscus structures, even when trained on minimal labeled data. Extensive ablation studies further highlight the synergistic contributions of ERA, PCA, and CST, solidifying ERANet as a transformative solution for semi-supervised meniscus segmentation in medical imaging.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-1-to-3: One Single Sketch to 3D Detailed Face Reconstruction</title>
<link>https://arxiv.org/abs/2502.17852</link>
<guid>https://arxiv.org/abs/2502.17852</guid>
<content:encoded><![CDATA[
arXiv:2502.17852v3 Announce Type: replace 
Abstract: 3D face reconstruction from a single sketch is a critical yet underexplored task with significant practical applications. The primary challenges stem from the substantial modality gap between 2D sketches and 3D facial structures, including: (1) accurately extracting facial keypoints from 2D sketches; (2) preserving diverse facial expressions and fine-grained texture details; and (3) training a high-performing model with limited data. In this paper, we propose Sketch-1-to-3, a novel framework for realistic 3D face reconstruction from a single sketch, to address these challenges. Specifically, we first introduce the Geometric Contour and Texture Detail (GCTD) module, which enhances the extraction of geometric contours and texture details from facial sketches. Additionally, we design a deep learning architecture with a domain adaptation module and a tailored loss function to align sketches with the 3D facial space, enabling high-fidelity expression and texture reconstruction. To facilitate evaluation and further research, we construct SketchFaces, a real hand-drawn facial sketch dataset, and Syn-SketchFaces, a synthetic facial sketch dataset. Extensive experiments demonstrate that Sketch-1-to-3 achieves state-of-the-art performance in sketch-based 3D face reconstruction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised and Source-Free Ranking of Biomedical Segmentation Models</title>
<link>https://arxiv.org/abs/2503.00450</link>
<guid>https://arxiv.org/abs/2503.00450</guid>
<content:encoded><![CDATA[
arXiv:2503.00450v2 Announce Type: replace 
Abstract: Model transfer presents a solution to the challenges of segmentation in the biomedical community, where the immense cost of data annotation is a major bottleneck in the use of deep learning. At the same time, hundreds of models get trained on biomedical data, submitted to challenges, and posted in model zoos and repositories. A major hurdle to wider adoption of pre-trained models lies in the lack of methods for best model selection. While such methods have been proposed for classification models, semantic and instance segmentation model ranking remain largely unaddressed, especially in a practically important setting where no labels are available on the target dataset. Similarly, if unsupervised domain adaptation is used, practitioners are faced with the task of selecting the best adapted model without target domain labels. Building on previous work linking model generalisation and consistency under perturbation, we propose the first unsupervised and source-free transferability estimator for semantic and instance segmentation tasks. We evaluate on multiple segmentation problems across biomedical imaging, finding a strong correlation between the rankings based on our estimator and rankings based on target dataset performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Spots to Pixels: Dense Spatial Gene Expression Prediction from Histology Images</title>
<link>https://arxiv.org/abs/2503.01347</link>
<guid>https://arxiv.org/abs/2503.01347</guid>
<content:encoded><![CDATA[
arXiv:2503.01347v3 Announce Type: replace 
Abstract: Spatial transcriptomics (ST) measures gene expression at fine-grained spatial resolution, offering insights into tissue molecular landscapes. Previous methods for spatial gene expression prediction typically crop spots of interest from histopathology slide images, and train models to map each spot to a corresponding gene expression profile. However, these methods inherently lose the spatial resolution in gene expression: 1) each spot often contains multiple cells with distinct gene expression profiles; 2) spots are typically defined at fixed spatial resolutions, limiting the ability to predict gene expression at varying scales. To address these limitations, this paper presents PixNet, a dense prediction network capable of predicting spatially resolved gene expression across spots of varying sizes and scales directly from histopathology slide images. Different from previous methods that map individual spots to gene expression values, we generate a spatially dense continuous gene expression map from the histopathology slide image, and aggregate values within spots of interest to predict the gene expression. Our PixNet outperforms state-of-the-art methods on four common ST datasets in multiple spatial scales. The source code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monocular Person Localization under Camera Ego-motion</title>
<link>https://arxiv.org/abs/2503.02916</link>
<guid>https://arxiv.org/abs/2503.02916</guid>
<content:encoded><![CDATA[
arXiv:2503.02916v2 Announce Type: replace 
Abstract: Localizing a person from a moving monocular camera is critical for Human-Robot Interaction (HRI). To estimate the 3D human position from a 2D image, existing methods either depend on the geometric assumption of a fixed camera or use a position regression model trained on datasets containing little camera ego-motion. These methods are vulnerable to severe camera ego-motion, resulting in inaccurate person localization. We consider person localization as a part of a pose estimation problem. By representing a human with a four-point model, our method jointly estimates the 2D camera attitude and the person's 3D location through optimization. Evaluations on both public datasets and real robot experiments demonstrate our method outperforms baselines in person localization accuracy. Our method is further implemented into a person-following system and deployed on an agile quadruped robot.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</title>
<link>https://arxiv.org/abs/2503.10638</link>
<guid>https://arxiv.org/abs/2503.10638</guid>
<content:encoded><![CDATA[
arXiv:2503.10638v3 Announce Type: replace 
Abstract: Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. On 1D data, we find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. To validate this classifier-centric perspective on high-dimensional data, we assess whether a flow-matching postprocessing step that is designed to narrow the gap between a pre-trained diffusion model's learned distribution and the real data distribution, especially near decision boundaries, can improve the performance. Experiments on various datasets verify our classifier-centric understanding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in Text-to-Video Prediction</title>
<link>https://arxiv.org/abs/2503.12953</link>
<guid>https://arxiv.org/abs/2503.12953</guid>
<content:encoded><![CDATA[
arXiv:2503.12953v2 Announce Type: replace 
Abstract: Text-video prediction (TVP) is a downstream video generation task that requires a model to produce subsequent video frames given a series of initial video frames and text describing the required motion. In practice TVP methods focus on a particular category of videos depicting manipulations of objects carried out by human beings or robot arms. Previous methods adapt models pre-trained on text-to-image tasks, and thus tend to generate video that lacks the required continuity. A natural progression would be to leverage more recent pre-trained text-to-video (T2V) models. This approach is rendered more challenging by the fact that the most common fine-tuning technique, low-rank adaptation (LoRA), yields undesirable results. In this work, we propose an adaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA). Within the module, we devise a sub-module that produces frame-wise text embeddings from the input text, which acts as an additional text condition to aid generation. We use FCA to fine-tune the T2V model, which incorporates the initial frame(s) as an extra condition. We compare and discuss the more effective strategy for injecting such embeddings into the T2V model. We conduct extensive ablation studies on our design choices with quantitative and qualitative performance analysis. Our approach establishes a new state-of-the-art for the task of TVP. Our code is open-source at https://github.com/Cuberick-Orion/FCA .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance Learning for Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2503.16284</link>
<guid>https://arxiv.org/abs/2503.16284</guid>
<content:encoded><![CDATA[
arXiv:2503.16284v2 Announce Type: replace 
Abstract: Whole Slide Images (WSIs) are high-resolution digital scans widely used in medical diagnostics. WSI classification is typically approached using Multiple Instance Learning (MIL), where the slide is partitioned into tiles treated as interconnected instances. While attention-based MIL methods aim to identify the most informative tiles, they often fail to fully exploit the spatial relationships among them, potentially overlooking intricate tissue structures crucial for accurate diagnosis. To address this limitation, we propose Probabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL framework that integrates spatial context into the attention mechanism through learnable distance-decayed priors, formulated within a probabilistic interpretation of self-attention as a posterior distribution. This formulation enables a dynamic inference of spatial relationships during training, eliminating the need for predefined assumptions often imposed by previous approaches. Additionally, we suggest a spatial pruning strategy for the posterior, effectively reducing self-attention's quadratic complexity. To further enhance spatial modeling, we introduce a diversity loss that encourages variation among attention heads, ensuring each captures distinct spatial representations. Together, PSA-MIL enables a more data-driven and adaptive integration of spatial context, moving beyond predefined constraints. We achieve state-of-the-art performance across both contextual and non-contextual baselines, while significantly reducing computational costs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-REPA: Aligning Diffusion U-Nets to ViTs</title>
<link>https://arxiv.org/abs/2503.18414</link>
<guid>https://arxiv.org/abs/2503.18414</guid>
<content:encoded><![CDATA[
arXiv:2503.18414v2 Announce Type: replace 
Abstract: Representation Alignment (REPA) that aligns Diffusion Transformer (DiT) hidden-states with ViT visual encoders has proven highly effective in DiT training, demonstrating superior convergence properties, but it has not been validated on the canonical diffusion U-Net architecture that shows faster convergence compared to DiTs. However, adapting REPA to U-Net architectures presents unique challenges: (1) different block functionalities necessitate revised alignment strategies; (2) spatial-dimension inconsistencies emerge from U-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT hinder the effectiveness of tokenwise alignment. To encounter these challenges, we propose \textbf{U-REPA}, a representation alignment paradigm that bridges U-Net hidden states and ViT features as follows: Firstly, we propose via observation that due to skip connection, the middle stage of U-Net is the best alignment option. Secondly, we propose upsampling of U-Net features after passing them through MLPs. Thirdly, we observe difficulty when performing tokenwise similarity alignment, and further introduces a manifold loss that regularizes the relative similarity between samples. Experiments indicate that the resulting U-REPA could achieve excellent generation quality and greatly accelerates the convergence speed. With CFG guidance interval, U-REPA could reach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\times$ 256, and needs only half the total epochs to perform better than REPA under sd-vae-ft-ema. Codes: https://github.com/YuchuanTian/U-REPA
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergistic Bleeding Region and Point Detection in Laparoscopic Surgical Videos</title>
<link>https://arxiv.org/abs/2503.22174</link>
<guid>https://arxiv.org/abs/2503.22174</guid>
<content:encoded><![CDATA[
arXiv:2503.22174v3 Announce Type: replace 
Abstract: Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of the operative field to hinder the surgical process and increases the risk of postoperative complications. Intelligent detection of bleeding areas can quantify the blood loss to assist decision-making, while locating bleeding points helps surgeons quickly identify the source of bleeding and achieve hemostasis in time to improve surgical success rates. To fill the benchmark gap, we first construct a real-world laparoscopic surgical bleeding detection dataset, named SurgBlood, comprising 5,330 frames from 95 surgical video clips with bleeding region and point annotations. Accordingly, we develop a dual-task synergistic online detector called BlooDet, enabling simultaneous detection of bleeding regions and points in laparoscopic surgery. The baseline embraces a dual-branch bidirectional guid- ance design based on Segment Anything Model 2. The mask branch detects bleeding regions through adaptive edge and point prompt embeddings, while the point branch leverages mask memory to induce bleeding point memory modeling and captures point motion direction via inter-frame optical flow. By coupled bidirectional guidance, our framework explores spatial-temporal correlations while exploiting memory modeling to infer current bleeding status. Extensive experiments indicate that our method outperforms 13 counterparts in bleeding detection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeInv: Free Lunch for Improving DDIM Inversion</title>
<link>https://arxiv.org/abs/2503.23035</link>
<guid>https://arxiv.org/abs/2503.23035</guid>
<content:encoded><![CDATA[
arXiv:2503.23035v2 Announce Type: replace 
Abstract: Naive DDIM inversion process usually suffers from a trajectory deviation issue, i.e., the latent trajectory during reconstruction deviates from the one during inversion. To alleviate this issue, previous methods either learn to mitigate the deviation or design cumbersome compensation strategy to reduce the mismatch error, exhibiting substantial time and computation cost. In this work, we present a nearly free-lunch method (named FreeInv) to address the issue more effectively and efficiently. In FreeInv, we randomly transform the latent representation and keep the transformation the same between the corresponding inversion and reconstruction time-step. It is motivated from a statistical perspective that an ensemble of DDIM inversion processes for multiple trajectories yields a smaller trajectory mismatch error on expectation. Moreover, through theoretical analysis and empirical study, we show that FreeInv performs an efficient ensemble of multiple trajectories. FreeInv can be freely integrated into existing inversion-based image and video editing techniques. Especially for inverting video sequences, it brings more significant fidelity and efficiency improvements. Comprehensive quantitative and qualitative evaluation on PIE benchmark and DAVIS dataset shows that FreeInv remarkably outperforms conventional DDIM inversion, and is competitive among previous state-of-the-art inversion methods, with superior computation efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation</title>
<link>https://arxiv.org/abs/2503.23951</link>
<guid>https://arxiv.org/abs/2503.23951</guid>
<content:encoded><![CDATA[
arXiv:2503.23951v3 Announce Type: replace 
Abstract: Recent advancements in customized video generation have led to significant improvements in the simultaneous adaptation of appearance and motion. Typically, decoupling the appearance and motion training, prior methods often introduce concept interference, resulting in inaccurate rendering of appearance features or motion patterns. In addition, these methods often suffer from appearance contamination, in which background and foreground elements from reference videos distort the customized video. This paper aims to alleviate these issues by proposing JointTuner. The core motivation of our JointTuner is to enable joint optimization of both appearance and motion components, upon which two key innovations are developed, i.e., Gated Low-Rank Adaptation (GLoRA) and Appearance-independent Temporal Loss (AiT Loss). Specifically, GLoRA uses a context-aware activation layer, analogous to a gating regulator, to dynamically steer LoRA modules toward learning either appearance or motion while maintaining spatio-temporal consistency. Moreover, with the finding that channel-temporal shift noise suppresses appearance-related low-frequencies while enhancing motion-related high-frequencies, we designed the AiT Loss. This loss adds the same shift to the diffusion model's predicted noise during fine-tuning, forcing the model to prioritize learning motion patterns. JointTuner's architecture-agnostic design supports both UNet (e.g., ZeroScope) and Diffusion Transformer (e.g., CogVideoX) backbones, ensuring its customization capabilities scale with the evolution of foundational video models. Furthermore, we present a systematic evaluation framework for appearance-motion combined customization, covering 90 combinations evaluated along four critical dimensions: semantic alignment, motion dynamism, temporal consistency, and perceptual quality. Our project homepage is available online.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions</title>
<link>https://arxiv.org/abs/2504.01632</link>
<guid>https://arxiv.org/abs/2504.01632</guid>
<content:encoded><![CDATA[
arXiv:2504.01632v3 Announce Type: replace 
Abstract: The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remains underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvAD: Inversion-based Reconstruction-Free Anomaly Detection with Diffusion Models</title>
<link>https://arxiv.org/abs/2504.05662</link>
<guid>https://arxiv.org/abs/2504.05662</guid>
<content:encoded><![CDATA[
arXiv:2504.05662v3 Announce Type: replace 
Abstract: Despite the remarkable success, recent reconstruction-based anomaly detection (AD) methods via diffusion modeling still involve fine-grained noise-strength tuning and computationally expensive multi-step denoising, leading to a fundamental tension between fidelity and efficiency. In this paper, we propose InvAD, a novel inversion-based anomaly detection approach ("detection via noising in latent space") that circumvents explicit reconstruction. Importantly, we contend that the limitations in prior reconstruction-based methods originate from the prevailing "detection via denoising in RGB space" paradigm. To address this, we model AD under a reconstruction-free formulation, which directly infers the final latent variable corresponding to the input image via DDIM inversion, and then measures the deviation based on the known prior distribution for anomaly scoring. Specifically, in approximating the original probability flow ODE using the Euler method, we enforce only a few inversion steps to noise the clean image to pursue inference efficiency. As the added noise is adaptively derived with the learned diffusion model, the original features for the clean testing image can still be leveraged to yield high detection accuracy. We perform extensive experiments and detailed analyses across four widely used industrial and medical AD benchmarks under the unsupervised unified setting to demonstrate the effectiveness of our model, achieving state-of-the-art AD performance and approximately 2x inference-time speedup without diffusion distillation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model</title>
<link>https://arxiv.org/abs/2504.06144</link>
<guid>https://arxiv.org/abs/2504.06144</guid>
<content:encoded><![CDATA[
arXiv:2504.06144v2 Announce Type: replace 
Abstract: We present a training-free style-aligned image generation method that leverages a scale-wise autoregressive model. While large-scale text-to-image (T2I) models, particularly diffusion-based methods, have demonstrated impressive generation quality, they often suffer from style misalignment across generated image sets and slow inference speeds, limiting their practical usability. To address these issues, we propose three key components: initial feature replacement to ensure consistent background appearance, pivotal feature interpolation to align object placement, and dynamic style injection, which reinforces style consistency using a schedule function. Unlike previous methods requiring fine-tuning or additional training, our approach maintains fast inference while preserving individual content details. Extensive experiments show that our method achieves generation quality comparable to competing approaches, significantly improves style alignment, and delivers inference speeds over six times faster than the fastest model.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR</title>
<link>https://arxiv.org/abs/2504.19687</link>
<guid>https://arxiv.org/abs/2504.19687</guid>
<content:encoded><![CDATA[
arXiv:2504.19687v2 Announce Type: replace 
Abstract: Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it will potentially degrade image quality, even yields metal artifacts at the case of metallic implants. For simultaneous LDCT reconstruction and metal artifact reduction (LDMAR), existing deep learning-based efforts face two main limitations: i) the network design neglects multi-scale and within-scale information; ii) training a distinct model for each dose necessitates significant storage space for multiple doses. To fill these gaps, we propose a prompt guiding multi-scale adaptive sparse representation-driven network, abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet inspired from multi-scale sparsifying frames, and it can simultaneously employ within-scale characteristics and cross-scale complementarity owing to an elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively capture multiple contextual information to generate more faithful thresholds, achieved by fusing features from local, regional, and global levels. Furthermore, we elaborate a model interpretable dual domain LDMAR framework called PDuMSRNet, and train single model with a prompt guiding strategy for multiple dose levels. We build a prompt guiding module, whose input contains dose level, metal mask and input instance, to provide various guiding information, allowing a single model to accommodate various CT dose settings. Extensive experiments at various dose levels demonstrate that the proposed methods outperform the state-of-the-art LDMAR methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Efficient Video Generation via Dynamic Token Carving</title>
<link>https://arxiv.org/abs/2505.16864</link>
<guid>https://arxiv.org/abs/2505.16864</guid>
<content:encoded><![CDATA[
arXiv:2505.16864v2 Announce Type: replace 
Abstract: Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83$\times$ speedup with 0.01\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes</title>
<link>https://arxiv.org/abs/2505.17951</link>
<guid>https://arxiv.org/abs/2505.17951</guid>
<content:encoded><![CDATA[
arXiv:2505.17951v2 Announce Type: replace 
Abstract: We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor environments. SplatCo builds upon two novel components: (1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features that represent fine surface details. This fusion is achieved through a novel hierarchical compensation strategy, ensuring both global consistency and local detail preservation; and (2) a cross-view assisted training strategy that enhances multi-view consistency by synchronizing gradient updates across viewpoints, applying visibility-aware densification, and pruning overfitted or inaccurate Gaussians based on structural consistency. Through joint optimization of structural representation and multi-view coherence, SplatCo effectively reconstructs fine-grained geometric structures and complex textures in large-scale scenes. Comprehensive evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity, Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo consistently achieves higher reconstruction quality than state-of-the-art methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These results establish a new benchmark for high-fidelity rendering of large-scale unbounded scenes. Code and additional information are available at https://github.com/SCUT-BIP-Lab/SplatCo.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Endoscopic Surgical Image Restoration and Beyond</title>
<link>https://arxiv.org/abs/2505.19161</link>
<guid>https://arxiv.org/abs/2505.19161</guid>
<content:encoded><![CDATA[
arXiv:2505.19161v2 Announce Type: replace 
Abstract: In endoscopic surgery, a clear and high-quality visual field is critical for surgeons to make accurate intraoperative decisions. However, persistent visual degradation, including smoke generated by energy devices, lens fogging from thermal gradients, and lens contamination due to blood or tissue fluid splashes during surgical procedures, severely impairs visual clarity. These degenerations can seriously hinder surgical workflow and pose risks to patient safety. To systematically investigate and address various forms of surgical scene degradation, we introduce a real- world open-source surgical image restoration dataset covering endoscopic environments, called SurgClean, which involves multi-type image restoration tasks from two medical sites, i.e., desmoking, defogging, and desplashing. SurgClean comprises 3,113 images with diverse degradation types and corresponding paired reference labels. Based on SurgClean, we establish a standardized evaluation benchmark and provide performance for 22 representative generic task-specific image restoration approaches, including 12 generic and 10 task-specific image restoration approaches. Experimental results reveal substantial performance gaps relative to clinical requirements, highlighting a critical opportunity for algorithm advancements in intelligent surgical restoration. Furthermore, we explore the degradation discrepancies between surgical and natural scenes from structural perception and semantic under- standing perspectives, providing fundamental insights for domain-specific image restoration research. Our work aims to empower restoration algorithms and improve the efficiency of clinical procedures.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.19536</link>
<guid>https://arxiv.org/abs/2505.19536</guid>
<content:encoded><![CDATA[
arXiv:2505.19536v3 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show that FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x speed-up in the prefilling stage. Our code is available at https://github.com/TungChintao/FlowCut
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction</title>
<link>https://arxiv.org/abs/2505.20951</link>
<guid>https://arxiv.org/abs/2505.20951</guid>
<content:encoded><![CDATA[
arXiv:2505.20951v4 Announce Type: replace 
Abstract: Camera-based 3D semantic occupancy prediction offers an efficient and cost-effective solution for perceiving surrounding scenes in autonomous driving. However, existing works rely on explicit occupancy state inference, leading to numerous incorrect feature assignments, and insufficient samples restrict the learning of occupancy class inference. To address these challenges, we propose leveraging \textbf{D}epth awareness and \textbf{S}emantic aid to boost camera-based 3D semantic \textbf{Occ}upancy prediction (\textbf{DSOcc}). We jointly perform occupancy state and occupancy class inference, where soft occupancy confidence is calculated by non-learning method and multiplied with image features to make voxels aware of depth, enabling adaptive implicit occupancy state inference. Instead of enhancing feature learning, we directly utilize well-trained image semantic segmentation and fuse multiple frames with their occupancy probabilities to aid occupancy class inference, thereby enhancing robustness. Experimental results demonstrate that DSOcc achieves state-of-the-art performance on the SemanticKITTI dataset among camera-based methods and achieves competitive performance on the SSCBench-KITTI-360 and Occ3D-nuScenes datasets. Code will be released on github.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Upscale 3D Segmentations in Neuroimaging</title>
<link>https://arxiv.org/abs/2505.21697</link>
<guid>https://arxiv.org/abs/2505.21697</guid>
<content:encoded><![CDATA[
arXiv:2505.21697v2 Announce Type: replace 
Abstract: Obtaining high-resolution (HR) segmentations from coarse annotations is a pervasive challenge in computer vision. Applications include inferring pixel-level segmentations from token-level labels in vision transformers, upsampling coarse masks to full resolution, and transferring annotations from legacy low-resolution (LR) datasets to modern HR imagery. These challenges are especially acute in 3D neuroimaging, where manual labeling is costly and resolutions continually increase. We propose a scalable framework that generalizes across resolutions and domains by regressing signed distance maps, enabling smooth, boundary-aware supervision. Crucially, our model predicts one class at a time, which substantially reduces memory usage during training and inference (critical for large 3D volumes) and naturally supports generalization to unseen classes. Generalization is further improved through training on synthetic, domain-randomized data. We validate our approach on ultra-high-resolution (UHR) human brain MRI (~100 {\mu}m), where most existing methods operate at 1 mm resolution. Our framework effectively upsamples such standard-resolution segmentations to UHR detail. Results on synthetic and real data demonstrate superior scalability and generalization compared to conventional segmentation methods. Code is available at: https://github.com/HuXiaoling/Learn2Upscale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis in Chest X-Ray</title>
<link>https://arxiv.org/abs/2505.21698</link>
<guid>https://arxiv.org/abs/2505.21698</guid>
<content:encoded><![CDATA[
arXiv:2505.21698v2 Announce Type: replace 
Abstract: Recent vision-language foundation models deliver state-of-the-art results in natural image classification, but falter in medical images due to pronounced domain shifts. Training a medical foundation model also requires substantial resources, including extensive annotated data and high computational capacity. To bridge this gap with minimal overhead, we introduce MedBridge, a lightweight multimodal adaptation framework that flexibly re-purposes arbitrary pre-trained foundation VLMs for medical image diagnosis. MedBridge comprises three novel core components. First, a Focal Sampling module that subsamples and extracts high-resolution local regions to capture subtle pathological features, compensating for the limited input resolution of foundation VLMs. Second, a Query-Encoder model with a small set of learnable queries to align the feature maps of frozen VLMs with medical semantics, without requiring retraining of the backbone layers. Third, a Mixture of Experts mechanism, driven by learnable queries, harnesses the complementary strength of various VLMs to maximize diagnostic performance. We evaluate MedBridge on five chest radiograph benchmarks in three key adaptation tasks, demonstrating its superior performance in both cross-domain and in-domain adaptation settings under varying levels of training data availability. MedBridge achieved an improvement of 6-15% in AUC compared to state-of-the-art VLM adaptation methods in multi-label thoracic disease diagnosis, underscoring its effectiveness in leveraging diverse foundation models for accurate and data-efficient medical diagnosis. Our project and code are available at https://github.com/ai-med/MedBridge.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking</title>
<link>https://arxiv.org/abs/2505.24466</link>
<guid>https://arxiv.org/abs/2505.24466</guid>
<content:encoded><![CDATA[
arXiv:2505.24466v3 Announce Type: replace 
Abstract: Text-based person retrieval aims to identify a target individual from an image gallery using a natural language description. Existing methods primarily focus on appearance-driven cross-modal retrieval, yet face significant challenges due to the visual complexity of scenes and the inherent ambiguity of textual descriptions. The contextual information, such as landmarks and relational cues, provides complementary cues that can offer valuable complementary insights for retrieval, but remains underexploited in current approaches. Motivated by this limitation, we propose a novel paradigm: scene-aware text-based person retrieval, which explicitly integrates both individual appearance and global scene context to improve retrieval accuracy. To support this, we first introduce ScenePerson-13W, a large-scale benchmark dataset comprising over 100,000 real-world scenes with rich annotations encompassing both pedestrian attributes and scene context. Based on this dataset, we further present SA-Person, a two-stage retrieval framework. In the first stage, SA-Person performs discriminative appearance grounding by aligning textual descriptions with pedestrian-specific regions. In the second stage, it introduces SceneRanker, a training-free, scene-aware re-ranking module that refines retrieval results by jointly reasoning over pedestrian appearance and the global scene context. Extensive experiments on ScenePerson-13W and existing benchmarks demonstrate the effectiveness of our proposed SA-Person. Both the dataset and code will be publicly released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception</title>
<link>https://arxiv.org/abs/2506.01579</link>
<guid>https://arxiv.org/abs/2506.01579</guid>
<content:encoded><![CDATA[
arXiv:2506.01579v2 Announce Type: replace 
Abstract: Generating high-fidelity full-body human interactions with dynamic objects and static scenes remains a critical challenge in computer graphics and animation. Existing methods for human-object interaction often neglect scene context, leading to implausible penetrations, while human-scene interaction approaches struggle to coordinate fine-grained manipulations with long-range navigation. To address these limitations, we propose HOSIG, a novel framework for synthesizing full-body interactions through hierarchical scene perception. Our method decouples the task into three key components: 1) a scene-aware grasp pose generator that ensures collision-free whole-body postures with precise hand-object contact by integrating local geometry constraints, 2) a heuristic navigation algorithm that autonomously plans obstacle-avoiding paths in complex indoor environments via compressed 2D floor maps and dual-component spatial reasoning, and 3) a scene-guided motion diffusion model that generates trajectory-controlled, full-body motions with finger-level accuracy by incorporating spatial anchors and dual-space classifier-free guidance. Extensive experiments on the TRUMANS dataset demonstrate superior performance over state-of-the-art methods. Notably, our framework supports unlimited motion length through autoregressive generation and requires minimal manual intervention. This work bridges the critical gap between scene-aware navigation and dexterous object manipulation, advancing the frontier of embodied interaction synthesis. Codes will be available after publication. Project page: http://yw0208.github.io/hosig
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConMamba: Contrastive Vision Mamba for Plant Disease Detection</title>
<link>https://arxiv.org/abs/2506.03213</link>
<guid>https://arxiv.org/abs/2506.03213</guid>
<content:encoded><![CDATA[
arXiv:2506.03213v2 Announce Type: replace 
Abstract: Plant Disease Detection (PDD) is a key aspect of precision agriculture. However, existing deep learning methods often rely on extensively annotated datasets, which are time-consuming and costly to generate. Self-supervised Learning (SSL) offers a promising alternative by exploiting the abundance of unlabeled data. However, most existing SSL approaches suffer from high computational costs due to convolutional neural networks or transformer-based architectures. Additionally, they struggle to capture long-range dependencies in visual representation and rely on static loss functions that fail to align local and global features effectively. To address these challenges, we propose ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model (SSM) to capture long-range dependencies efficiently. Furthermore, we introduce a dual-level contrastive loss with dynamic weight adjustment to optimize local-global feature alignment. Experimental results on three benchmark datasets demonstrate that ConMamba significantly outperforms state-of-the-art methods across multiple evaluation metrics. This provides an efficient and robust solution for PDD.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning</title>
<link>https://arxiv.org/abs/2506.03596</link>
<guid>https://arxiv.org/abs/2506.03596</guid>
<content:encoded><![CDATA[
arXiv:2506.03596v2 Announce Type: replace 
Abstract: The field of controllable image generation has seen significant advancements, with various architectures improving generation layout consistency with control signals. However, contemporary methods still face challenges in bridging the semantic gap between input text prompts with sparse semantics and the target images, often over-relying on low-level control signals to infer regional details. To address this challenge, we propose ControlThinker, a novel framework that employs a "comprehend-then-generate" paradigm. Firstly, by incentivizing the visual reasoning capability of a MLLM, latent semantics from control images are mined to enrich text prompts. This enriched semantic understanding then seamlessly aids in image generation without the need for additional complex modifications. To further tackle the uncertainty arising from the ambiguity of control images, we encourage broader exploration of reasoning trajectories and select the optimal one using a metric-based output reward model (ORM). Extensive experimental results demonstrate that ControlThinker effectively mitigates the semantic gap between raw text prompts and target images, resulting in improved visual quality and semantic consistency across a wide range of benchmarks. The code and models are available at https://github.com/Maplebb/ControlThinker.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-SAM2: Accurate Quantization for Segment Anything Model 2</title>
<link>https://arxiv.org/abs/2506.09782</link>
<guid>https://arxiv.org/abs/2506.09782</guid>
<content:encoded><![CDATA[
arXiv:2506.09782v2 Announce Type: replace 
Abstract: The Segment Anything Model 2 (SAM2) is a powerful foundation model for promptable segmentation. However, its high computational and memory costs are a major barrier to deployment on resource-constrained devices. In this paper, we present Q-SAM2, an accurate low-bit quantization method that achieves high compression and high fidelity. To address performance degradation arising from challenging weight and activation distributions during quantization, Q-SAM2 introduces two novel contributions: Variance-Reduced Calibration (VRC), an initialization method that reduces weight statistical variance by minimizing the Frobenius norm over a small calibration batch; and Learnable Statistical Clipping (LSC), a Quantization-Aware Training (QAT) method that learns momentum-stabilized clipping factors to manage outliers in weights and activations. Comprehensive experiments demonstrate that Q-SAM2 achieves highly accurate inference with substantial efficiency gains, significantly surpassing state-of-the-art general QAT schemes, particularly in the ultra-low 2-bit regime. Specifically, Q-SAM2 achieves an accuracy gain of up to 9.7 ppt in J&amp;F on the video segmentation benchmark and 7.3 ppt in mIoU for instance segmentation over the best competing QAT model, all while achieving an 8x reduction in model size compared to the BF16 baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding</title>
<link>https://arxiv.org/abs/2506.10353</link>
<guid>https://arxiv.org/abs/2506.10353</guid>
<content:encoded><![CDATA[
arXiv:2506.10353v4 Announce Type: replace 
Abstract: Text-to-Motion generation has become a fundamental task in human-machine interaction, enabling the synthesis of realistic human motions from natural language descriptions. Although recent advances in large language models and reinforcement learning have contributed to high-quality motion generation, two major challenges remain. Existing approaches often fail to capture the temporal and causal complexities inherent in natural language, leading to oversimplified or incoherent motions. Additionally, RL-based methods are frequently overly complex, hindering their scalability and adaptability across various motion generation tasks. To address these challenges, we propose Motion-R1, a novel framework that combines decomposed Chain-of-Thought reasoning with reinforcement learning to enhance both the quality and interpretability of generated motions. Specifically, we introduce the Decomposed CoT Data Engine, which leverages an automated pipeline to synthesize high-quality reasoning data, allowing the model to better capture the temporal dependencies and causal relationships of human motion. We also propose RL Binding, a reinforcement learning strategy that incorporates multi-modal text-motion alignment into the RL reward function, guiding the model to produce motions that are both semantically accurate and motionally realistic. Extensive experiments across benchmark datasets demonstrate that Motion-R1 achieves state-of-the-art performance, with a 3.5% improvement in MM-Dist on HumanML3D and improvements in R-Precision and FID on KIT-ML and BABEL, surpassing existing methods across key metrics and highlighting its superior capability in handling complex motion generation tasks. Project page: https://motion-r1.github.io/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours</title>
<link>https://arxiv.org/abs/2506.13292</link>
<guid>https://arxiv.org/abs/2506.13292</guid>
<content:encoded><![CDATA[
arXiv:2506.13292v2 Announce Type: replace 
Abstract: Purpose: Accurate intraoperative X-ray/CT registration is essential for surgical navigation in orthopedic procedures. However, existing methods struggle with consistently achieving sub-millimeter accuracy, robustness under broad initial pose estimates or need manual key-point annotations. This work aims to address these challenges by proposing a novel multi-view X-ray/CT registration method for intraoperative bone registration. Methods: The proposed registration method consists of a multi-view, contour-based iterative closest point (ICP) optimization. Unlike previous methods, which attempt to match bone contours across the entire silhouette in both imaging modalities, we focus on matching specific subcategories of contours corresponding to bone substructures. This leads to reduced ambiguity in the ICP matches, resulting in a more robust and accurate registration solution. This approach requires only two X-ray images and operates fully automatically. Additionally, we contribute a dataset of 5 cadaveric specimens, including real X-ray images, X-ray image poses and the corresponding CT scans. Results: The proposed registration method is evaluated on real X-ray images using mean reprojection error (mRPD). The method consistently achieves sub-millimeter accuracy with a mRPD 0.67mm compared to 5.35mm by a commercial solution requiring manual intervention. Furthermore, the method offers improved practical applicability, being fully automatic. Conclusion: Our method offers a practical, accurate, and efficient solution for multi-view X-ray/CT registration in orthopedic surgeries, which can be easily combined with tracking systems. By improving registration accuracy and minimizing manual intervention, it enhances intraoperative navigation, contributing to more accurate and effective surgical outcomes in computer-assisted surgery (CAS).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2506.13589</link>
<guid>https://arxiv.org/abs/2506.13589</guid>
<content:encoded><![CDATA[
arXiv:2506.13589v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) perform well in video understanding but degrade on long videos due to fixed-length context and weak long-term dependency modeling. Retrieval-Augmented Generation (RAG) can expand knowledge dynamically, yet existing video RAG schemes adopt fixed retrieval paradigms that ignore query difficulty. This uniform design causes redundant computation and latency for simple queries, while coarse retrieval for complex, multi-hop reasoning can miss key information. Such single-step retrieval severely limits the trade-off between efficiency and cognitive depth. We propose AdaVideoRAG, an adaptive RAG framework for long-video understanding. A lightweight intent classifier dynamically selects suitable retrieval schemes according to query complexity from the simplest to the most sophisticated. We design an Omni-Knowledge Indexing module that extracts and organizes multi-modal information into three databases: (1) a text base built from clip captions, ASR, and OCR; (2) a visual base; and (3) a knowledge graph for deep semantic understanding. This supports hierarchical knowledge access, from naive retrieval to graph-based retrieval, balancing resource cost and reasoning ability. To evaluate deep understanding, we further construct the HiVU benchmark. Experiments show that AdaVideoRAG significantly improves both efficiency and accuracy on long-video QA tasks and can be seamlessly plugged into existing MLLMs through lightweight APIs, establishing a new paradigm for adaptive retrieval-augmented video analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models</title>
<link>https://arxiv.org/abs/2507.00493</link>
<guid>https://arxiv.org/abs/2507.00493</guid>
<content:encoded><![CDATA[
arXiv:2507.00493v3 Announce Type: replace 
Abstract: Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2507.02792</link>
<guid>https://arxiv.org/abs/2507.02792</guid>
<content:encoded><![CDATA[
arXiv:2507.02792v4 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., canny edge) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning-based approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. Through an empirical analysis of existing methods, we identify a key limitation: the sampling schedule of condition features, previously unexplored, fails to account for the evolving interplay between structure preservation and domain alignment throughout diffusion steps. Inspired by this observation, we propose a flexible training-free framework that decouples the sampling schedule of condition features from the denoising process, and systematically investigate the spectrum of feature injection schedules for a higher-quality structure guidance in the feature space. Specifically, we find that condition features sampled from a single timestep are sufficient, yielding a simple yet efficient schedule that balances structure alignment and appearance quality. We further enhance the sampling process by introducing a restart refinement schedule, and improve the visual quality with an appearance-rich prompting strategy. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art results across diverse zero-shot conditioning scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoReMouse: Monocular Reconstruction of Laboratory Mouse</title>
<link>https://arxiv.org/abs/2507.04258</link>
<guid>https://arxiv.org/abs/2507.04258</guid>
<content:encoded><![CDATA[
arXiv:2507.04258v2 Announce Type: replace 
Abstract: Laboratory mice, particularly the C57BL/6 strain, are essential animal models in biomedical research. However, accurate 3D surface motion reconstruction of mice remains a significant challenge due to their complex non-rigid deformations, textureless fur-covered surfaces, and the lack of realistic 3D mesh models. Moreover, existing visual datasets for mice reconstruction only contain sparse viewpoints without 3D geometries. To fill the gap, we introduce MoReMouse, the first monocular dense 3D reconstruction network specifically designed for C57BL/6 mice. To achieve high-fidelity 3D reconstructions, we present three key innovations. First, we create the first high-fidelity, dense-view synthetic dataset for C57BL/6 mice by rendering a realistic, anatomically accurate Gaussian mouse avatar. Second, MoReMouse leverages a transformer-based feedforward architecture combined with triplane representation, enabling high-quality 3D surface generation from a single image, optimized for the intricacies of small animal morphology. Third, we propose geodesic-based continuous correspondence embeddings on the mouse surface, which serve as strong semantic priors, improving surface consistency and reconstruction stability, especially in highly dynamic regions like limbs and tail. Through extensive quantitative and qualitative evaluations, we demonstrate that MoReMouse significantly outperforms existing open-source methods in both accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection</title>
<link>https://arxiv.org/abs/2507.04323</link>
<guid>https://arxiv.org/abs/2507.04323</guid>
<content:encoded><![CDATA[
arXiv:2507.04323v3 Announce Type: replace 
Abstract: Atmospheric Turbulence (AT) degrades the clarity and accuracy of surveillance imagery, posing challenges not only for visualization quality but also for object classification and scene tracking. Deep learning-based methods have been proposed to improve visual quality, but spatio-temporal distortions remain a significant issue. Although deep learning-based object detection performs well under normal conditions, it struggles to operate effectively on sequences distorted by atmospheric turbulence. In this paper, we propose a novel framework that learns to compensate for distorted features while simultaneously improving visualization and object detection. This end-to-end training strategy leverages and exchanges knowledge of low-level distorted features in the AT mitigator with semantic features extracted in the object detector. Specifically, in the AT mitigator a 3D Mamba-based structure is used to handle the spatio-temporal displacements and blurring caused by turbulence. Optimization is achieved through back-propagation in both the AT mitigator and object detector. Our proposed DMAT outperforms state-of-the-art AT mitigation and object detection systems up to a 15% improvement on datasets corrupted by generated turbulence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backdoors in Conditional Diffusion: Threats to Responsible Synthetic Data Pipelines</title>
<link>https://arxiv.org/abs/2507.04726</link>
<guid>https://arxiv.org/abs/2507.04726</guid>
<content:encoded><![CDATA[
arXiv:2507.04726v2 Announce Type: replace 
Abstract: Text-to-image diffusion models achieve high-fidelity image generation from natural language prompts. ControlNets extend these models by enabling conditioning on structural inputs (e.g., edge maps, depth, pose), providing fine-grained control over outputs. Yet their reliance on large, publicly scraped datasets and community fine-tuning makes them vulnerable to data poisoning. We introduce a model-poisoning attack that embeds a covert backdoor into a ControlNet, causing it to produce attacker-specified content when exposed to visual triggers, without textual prompts. Experiments show that poisoning only 1% of the fine-tuning corpus yields a 90-98% attack success rate, while 5% further strengthens the backdoor, all while preserving normal generation quality. To mitigate this risk, we propose clean fine-tuning (CFT): freezing the diffusion backbone and fine-tuning only the ControlNet on a sanitized dataset with a reduced learning rate. CFT lowers attack success rates on held-out data. These results expose a critical security weakness in open-source, ControlNet-guided diffusion pipelines and demonstrate that CFT offers a practical defense for responsible synthetic-data pipelines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</title>
<link>https://arxiv.org/abs/2507.05859</link>
<guid>https://arxiv.org/abs/2507.05859</guid>
<content:encoded><![CDATA[
arXiv:2507.05859v2 Announce Type: replace 
Abstract: Free-Viewpoint Video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representation remains a major challenge. Existing dynamic 3D Gaussian Splatting methods couple reconstruction with optimization-dependent compression and customized motion formats, limiting generalization and standardization. To address this, we propose D-FCGS, a novel Feedforward Compression framework for Dynamic Gaussian Splatting. Key innovations include: (1) a standardized Group-of-Frames (GoF) structure with I-P coding, leveraging sparse control points to extract inter-frame motion tensors; (2) a dual prior-aware entropy model that fuses hyperprior and spatial-temporal priors for accurate rate estimation; (3) a control-point-guided motion compensation mechanism and refinement network to enhance view-consistent fidelity. Trained on Gaussian frames derived from multi-view videos, D-FCGS generalizes across diverse scenes in a zero-shot fashion. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression compared to the baseline while preserving visual quality across viewpoints. This work advances feedforward compression of dynamic 3DGS, facilitating scalable FVV transmission and storage for immersive applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COLI: A Hierarchical Efficient Compressor for Large Images</title>
<link>https://arxiv.org/abs/2507.11443</link>
<guid>https://arxiv.org/abs/2507.11443</guid>
<content:encoded><![CDATA[
arXiv:2507.11443v2 Announce Type: replace 
Abstract: The escalating adoption of high-resolution, large-field-of-view imagery amplifies the need for efficient compression methodologies. Conventional techniques frequently fail to preserve critical image details, while data-driven approaches exhibit limited generalizability. Implicit Neural Representations (INRs) present a promising alternative by learning continuous mappings from spatial coordinates to pixel intensities for individual images, thereby storing network weights rather than raw pixels and avoiding the generalization problem. However, INR-based compression of large images faces challenges including slow compression speed and suboptimal compression ratios. To address these limitations, we introduce COLI (Compressor for Large Images), a novel framework leveraging Neural Representations for Videos (NeRV). First, recognizing that INR-based compression constitutes a training process, we accelerate its convergence through a pretraining-finetuning paradigm, mixed-precision training, and reformulation of the sequential loss into a parallelizable objective. Second, capitalizing on INRs' transformation of image storage constraints into weight storage, we implement Hyper-Compression, a novel post-training technique to substantially enhance compression ratios while maintaining minimal output distortion. Evaluations across two medical imaging datasets demonstrate that COLI consistently achieves competitive or superior PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while accelerating NeRV training by up to 4 times.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PositionIC: Unified Position and Identity Consistency for Image Customization</title>
<link>https://arxiv.org/abs/2507.13861</link>
<guid>https://arxiv.org/abs/2507.13861</guid>
<content:encoded><![CDATA[
arXiv:2507.13861v4 Announce Type: replace 
Abstract: Recent subject-driven image customization excels in fidelity, yet fine-grained instance-level spatial control remains an elusive challenge, hindering real-world applications. This limitation stems from two factors: a scarcity of scalable, position-annotated datasets, and the entanglement of identity and layout by global attention mechanisms. To this end, we introduce \modelname{}, a unified framework for high-fidelity, spatially controllable multi-subject customization. First, we present BMPDS, the first automatic data-synthesis pipeline for position-annotated multi-subject datasets, effectively providing crucial spatial supervision. Second, we design a lightweight, layout-aware diffusion framework that integrates a novel visibility-aware attention mechanism. This mechanism explicitly models spatial relationships via an NeRF-inspired volumetric weight regulation to effectively decouple instance-level spatial embeddings from semantic identity features, enabling precise, occlusion-aware placement of multiple subjects.
  Extensive experiments demonstrate \modelname{} achieves state-of-the-art performance on public benchmarks, setting new records for spatial precision and identity consistency. Our work represents a significant step towards truly controllable, high-fidelity image customization in multi-entity scenarios. Code and data will be publicly released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2508.05264</link>
<guid>https://arxiv.org/abs/2508.05264</guid>
<content:encoded><![CDATA[
arXiv:2508.05264v4 Announce Type: replace 
Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization-Free Style Transfer for 3D Gaussian Splats</title>
<link>https://arxiv.org/abs/2508.05813</link>
<guid>https://arxiv.org/abs/2508.05813</guid>
<content:encoded><![CDATA[
arXiv:2508.05813v2 Announce Type: replace 
Abstract: The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats, allowing for direct stylization on a .ply or .splat file without requiring the original camera views. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This also allows for fast stylization of splats with no additional training, achieving speeds under 2 minutes even on CPU-based consumer hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Find Them All: Unveiling MLLMs for Versatile Person Re-identification</title>
<link>https://arxiv.org/abs/2508.06908</link>
<guid>https://arxiv.org/abs/2508.06908</guid>
<content:encoded><![CDATA[
arXiv:2508.06908v2 Announce Type: replace 
Abstract: Person re-identification (ReID) aims to retrieve images of a target person from the gallery set, with wide applications in medical rehabilitation and public security. However, traditional person ReID models are typically uni-modal, resulting in limited generalizability across heterogeneous data modalities. Recently, the emergence of multi-modal large language models (MLLMs) has shown a promising avenue for addressing this issue. Despite this potential, existing methods merely regard MLLMs as feature extractors or caption generators, leaving their capabilities in person ReID tasks largely unexplored. To bridge this gap, we introduce a novel benchmark for \underline{\textbf{V}}ersatile \underline{\textbf{P}}erson \underline{\textbf{Re}}-\underline{\textbf{ID}}entification, termed VP-ReID. The benchmark includes 257,310 multi-modal queries and gallery images, covering ten diverse person ReID tasks. In addition, we propose two task-oriented evaluation schemes for MLLM-based person ReID. Extensive experiments demonstrate the impressive versatility, effectiveness, and interpretability of MLLMs in various person ReID tasks. Nevertheless, they also have limitations in handling a few modalities, particularly thermal and infrared data. We hope that VP-ReID can facilitate the community in developing more robust and generalizable cross-modal foundation models for person ReID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.08227</link>
<guid>https://arxiv.org/abs/2508.08227</guid>
<content:encoded><![CDATA[
arXiv:2508.08227v2 Announce Type: replace 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) show promising potential in one-step Real-World Image Super-Resolution (Real-ISR). Current one-step Real-ISR methods typically inject the low-quality (LQ) image latent representation at the start or end timestep of the DDPM scheduler. Recent studies have begun to note that the LQ image latent and the pre-trained noisy latent representations are intuitively closer at a mid-timestep. However, a quantitative analysis of these latent representations remains lacking. Considering these latent representations can be decomposed into signal and noise, we propose a method based on the Signal-to-Noise Ratio (SNR) to pre-compute an average optimal mid-timestep for injection. To better approximate the pre-trained noisy latent representation, we further introduce the Latent Representation Refinement (LRR) loss via a LoRA-enhanced VAE encoder. We also fine-tune the backbone of the DDPM-based generative model using LoRA to perform one-step denoising at the average optimal mid-timestep. Based on these components, we present OMGSR, a GAN-based Real-ISR framework that employs a DDPM-based generative model as the generator and a DINOv3-ConvNeXt model with multi-level discriminator heads as the discriminator. We also propose the DINOv3-ConvNeXt DISTS (Dv3CD) loss, which is enhanced for structural perception at varying resolutions. Within the OMGSR framework, we develop OMGSR-S based on SD2.1-base. An ablation study confirms that our pre-computation strategy and LRR loss significantly improve the baseline. Comparative studies demonstrate that OMGSR-S achieves state-of-the-art performance across multiple metrics. Code is available at \hyperlink{Github}{https://github.com/wuer5/OMGSR}.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IAG: Input-aware Backdoor Attack on VLM-based Visual Grounding</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
arXiv:2508.09456v3 Announce Type: replace 
Abstract: Recent advances in vision-language models (VLMs) have significantly enhanced the visual grounding task, which involves locating objects in an image based on natural language queries. Despite these advancements, the security of VLM-based grounding systems has not been thoroughly investigated. This paper reveals a novel and realistic vulnerability: the first multi-target backdoor attack on VLM-based visual grounding. Unlike prior attacks that rely on static triggers or fixed targets, we propose IAG, a method that dynamically generates input-aware, text-guided triggers conditioned on any specified target object description to execute the attack. This is achieved through a text-conditioned UNet that embeds imperceptible target semantic cues into visual inputs while preserving normal grounding performance on benign samples. We further develop a joint training objective that balances language capability with perceptual reconstruction to ensure imperceptibility, effectiveness, and stealth. Extensive experiments on multiple VLMs (e.g., LLaVA, InternVL, Ferret) and benchmarks (RefCOCO, RefCOCO+, RefCOCOg, Flickr30k Entities, and ShowUI) demonstrate that IAG achieves the best ASRs compared with other baselines on almost all settings without compromising clean accuracy, maintaining robustness against existing defenses, and exhibiting transferability across datasets and models. These findings underscore critical security risks in grounding-capable VLMs and highlight the need for further research on trustworthy multimodal understanding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction</title>
<link>https://arxiv.org/abs/2508.10936</link>
<guid>https://arxiv.org/abs/2508.10936</guid>
<content:encoded><![CDATA[
arXiv:2508.10936v2 Announce Type: replace 
Abstract: Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix-game 2.0: An open-source real-time and streaming interactive world model</title>
<link>https://arxiv.org/abs/2508.13009</link>
<guid>https://arxiv.org/abs/2508.13009</guid>
<content:encoded><![CDATA[
arXiv:2508.13009v2 Announce Type: replace 
Abstract: Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion</title>
<link>https://arxiv.org/abs/2509.00062</link>
<guid>https://arxiv.org/abs/2509.00062</guid>
<content:encoded><![CDATA[
arXiv:2509.00062v3 Announce Type: replace 
Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult due to the cubic memory scaling of voxel structures and moreover the significant class imbalance caused by sparsity. We introduce Scaffold Diffusion, a generative model designed for sparse multi-category 3D voxel structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete diffusion language model to generate 3D voxel structures. We show that discrete diffusion language models can be extended beyond inherently sequential domains such as text to generate spatially coherent 3D structures. We evaluate on Minecraft house structures from the 3D-Craft dataset and demonstrate that, unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion produces realistic and coherent structures even when trained on data with over 98% sparsity. We provide an interactive viewer where readers can visualize generated samples and the generation process: https://scaffold.deepexploration.org/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information</title>
<link>https://arxiv.org/abs/2509.01421</link>
<guid>https://arxiv.org/abs/2509.01421</guid>
<content:encoded><![CDATA[
arXiv:2509.01421v3 Announce Type: replace 
Abstract: Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.03277</link>
<guid>https://arxiv.org/abs/2509.03277</guid>
<content:encoded><![CDATA[
arXiv:2509.03277v5 Announce Type: replace 
Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization capabilities to identify 3D anomalies across unseen objects of highly diverse class semantics. To this end, we propose a unified framework to comprehensively detect and segment 3D anomalies by leveraging both point- and pixel-level information. We first design PointAD, which leverages point-pixel correspondence to represent 3D anomalies through their associated rendering pixel representations. This approach is referred to as implicit 3D representation, as it focuses solely on rendering pixel anomalies but neglects the inherent spatial relationships within point clouds. Then, we propose PointAD+ to further broaden the interpretation of 3D anomalies by introducing explicit 3D representation, emphasizing spatial abnormality to uncover abnormal spatial relationships. Hence, we propose G-aggregation to involve geometry information to enable the aggregated point representations spatially aware. To simultaneously capture rendering and spatial abnormality, PointAD+ proposes hierarchical representation learning, incorporating implicit and explicit anomaly semantics into hierarchical text prompts: rendering prompts for the rendering layer and geometry prompts for the geometry layer. A cross-hierarchy contrastive alignment is further introduced to promote the interaction between the rendering and geometry layers, facilitating mutual anomaly learning. Finally, PointAD+ integrates anomaly semantics from both layers to capture the generalized anomaly semantics. During the test, PointAD+ can integrate RGB information in a plug-and-play manner and further improve its detection performance. Extensive experiments demonstrate the superiority of PointAD+ in ZS 3D anomaly detection across unseen objects with highly diverse class semantics, achieving a holistic understanding of abnormality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intraoperative 2D/3D Registration via Spherical Similarity Learning and Differentiable Levenberg-Marquardt Optimization</title>
<link>https://arxiv.org/abs/2509.06890</link>
<guid>https://arxiv.org/abs/2509.06890</guid>
<content:encoded><![CDATA[
arXiv:2509.06890v3 Announce Type: replace 
Abstract: Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Based Decomposition of Reflectance and Shading using a Single Visible-Thermal Image Pair</title>
<link>https://arxiv.org/abs/2509.10388</link>
<guid>https://arxiv.org/abs/2509.10388</guid>
<content:encoded><![CDATA[
arXiv:2509.10388v2 Announce Type: replace 
Abstract: Decomposing an image into its underlying photometric factors--surface reflectance and shading--is a long-standing challenge due to the lack of extensive ground-truth data for real-world scenes. We introduce a novel physics-based approach for intrinsic image decomposition using a pair of visible and thermal images. We leverage the principle that light not reflected from an opaque surface is absorbed and detected as heat by a thermal camera. This allows us to relate the ordinalities (or relative magnitudes) between visible and thermal image intensities to the ordinalities of shading and reflectance, which enables a dense self-supervision of an optimizing neural network to recover shading and reflectance. We perform quantitative evaluations with known reflectance and shading under natural and artificial lighting, and qualitative experiments across diverse scenes. The results demonstrate superior performance over both physics-based and recent learning-based methods, providing a path toward scalable real-world data curation with supervision.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Visual Autonomous Parking via Control-Aided Attention</title>
<link>https://arxiv.org/abs/2509.11090</link>
<guid>https://arxiv.org/abs/2509.11090</guid>
<content:encoded><![CDATA[
arXiv:2509.11090v2 Announce Type: replace 
Abstract: Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details - especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. We train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss - a shift we demonstrate leads to a more robust and generalizable policy. To further strengthen the framework, CAA-Policy incorporates short-horizon waypoint prediction as an auxiliary task to improve temporal consistency of control outputs, a learnable motion prediction module to robustly track target slots over time, and a modified target tokenization scheme for more effective feature fusion. Extensive experiments in the CARLA simulator show that CAA-Policy consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code and Collected Training datasets will be released. Code is released at https://github.com/ai4ce/CAAPolicy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew</title>
<link>https://arxiv.org/abs/2509.12544</link>
<guid>https://arxiv.org/abs/2509.12544</guid>
<content:encoded><![CDATA[
arXiv:2509.12544v3 Announce Type: replace 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it remains challenging as data distributions can be highly heterogeneous. These challenges are further amplified in multi-label scenarios, where data exhibit characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL studies focus on single-label classification, real-world applications, such as in medical imaging, involve multi-label data with highly skewed label distributions across clients. To address this important yet underexplored problem, we propose FedNCA-ML, a novel FL framework that aligns feature distributions across clients and learns discriminative, well-clustered representations inspired by Neural Collapse (NC) theory. NC describes an ideal latent-space geometry where each class's features collapse to their mean, forming a maximally separated simplex. To extend this theory to multi-label settings, we introduce a feature disentanglement module that extracts class-specific representations. The clustering of these disentangled features is guided by a shared NC-inspired structure, mitigating conflicts among client models caused by heterogeneous local data. Furthermore, we design regularisation losses to encourage compact and consistent feature clustering in the latent space. Experiments on four benchmark datasets under eight FL settings demonstrate the effectiveness of the proposed method, achieving improvements of up to 3.92% in class-wise AUC and 4.93% in class-wise F1 score.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming</title>
<link>https://arxiv.org/abs/2509.13504</link>
<guid>https://arxiv.org/abs/2509.13504</guid>
<content:encoded><![CDATA[
arXiv:2509.13504v2 Announce Type: replace 
Abstract: The lack of flexible annotation tools has hindered the deployment of AI models in some scientific areas. Most existing image annotation software requires users to upload a precollected dataset, which limits support for on-demand pipelines and introduces unnecessary steps to acquire images. This constraint is particularly problematic in laboratory environments, where on-site data acquisition from instruments such as microscopes is increasingly common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical user interface that integrates with imaging systems, such as webcams, microscopes, and others, to enable on-site image annotation. LivePyxel is designed to be easy to use through a simple interface that allows users to precisely delimit areas for annotation using tools commonly found in commercial graphics editing software. Of particular interest is the availability of B\'ezier splines and binary masks, and the software's capacity to work with non-destructive layers that enable high-performance editing. LivePyxel also integrates a wide compatibility across video devices, and it's optimized for object detection operations via the use of OpenCV in combination with high-performance libraries designed to handle matrix and linear algebra operations via Numpy effectively. LivePyxel facilitates seamless data collection and labeling, accelerating the development of AI models in experimental workflows. LivePyxel is freely available at https://github.com/UGarCil/LivePyxel
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation</title>
<link>https://arxiv.org/abs/2509.13848</link>
<guid>https://arxiv.org/abs/2509.13848</guid>
<content:encoded><![CDATA[
arXiv:2509.13848v2 Announce Type: replace 
Abstract: Feature caching has recently emerged as a promising method for diffusion model acceleration. It effectively alleviates the inefficiency problem caused by high computational requirements by caching similar features in the inference process of the diffusion model. In this paper, we analyze existing feature caching methods from the perspective of information utilization, and point out that relying solely on historical information will lead to constrained accuracy and speed performance. And we propose a novel paradigm that introduces future information via self-speculation based on the information similarity at the same time step across different iteration times. Based on this paradigm, we present \textit{SpecDiff}, a training-free multi-level feature caching strategy including a cached feature selection algorithm and a multi-level feature classification algorithm. (1) Feature selection algorithm based on self-speculative information. \textit{SpecDiff} determines a dynamic importance score for each token based on self-speculative information and historical information, and performs cached feature selection through the importance score. (2) Multi-level feature classification algorithm based on feature importance scores. \textit{SpecDiff} classifies tokens by leveraging the differences in feature importance scores and introduces a multi-level feature calculation strategy. Extensive experiments show that \textit{SpecDiff} achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow on NVIDIA A800-80GB GPU. By merging speculative and historical information, \textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing the Pareto frontier of speedup and accuracy in the efficient diffusion model inference.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation</title>
<link>https://arxiv.org/abs/2509.16986</link>
<guid>https://arxiv.org/abs/2509.16986</guid>
<content:encoded><![CDATA[
arXiv:2509.16986v2 Announce Type: replace 
Abstract: Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the model's ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at https://github.com/Maplebb/VCE.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment</title>
<link>https://arxiv.org/abs/2509.19659</link>
<guid>https://arxiv.org/abs/2509.19659</guid>
<content:encoded><![CDATA[
arXiv:2509.19659v2 Announce Type: replace 
Abstract: Large vision-language models (VLMs) can jointly interpret images and text, but they are also prone to absorbing and reproducing harmful social stereotypes when visual cues such as age, gender, race, clothing, or occupation are present. To investigate these risks, we introduce a news-image benchmark consisting of 1,343 image-question pairs drawn from diverse outlets, which we annotated with ground-truth answers and demographic attributes (age, gender, race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and employ a large language model (LLM) as judge, with human verification. Our findings show that: (i) visual context systematically shifts model outputs in open-ended settings; (ii) bias prevalence varies across attributes and models, with particularly high risk for gender and occupation; and (iii) higher faithfulness does not necessarily correspond to lower bias. We release the benchmark prompts, evaluation rubric, and code to support reproducible and fairness-aware multimodal assessment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache</title>
<link>https://arxiv.org/abs/2509.21354</link>
<guid>https://arxiv.org/abs/2509.21354</guid>
<content:encoded><![CDATA[
arXiv:2509.21354v2 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment</title>
<link>https://arxiv.org/abs/2509.21609</link>
<guid>https://arxiv.org/abs/2509.21609</guid>
<content:encoded><![CDATA[
arXiv:2509.21609v4 Announce Type: replace 
Abstract: The processes of classification and segmentation utilizing artificial intelligence play a vital role in the automation of disaster assessments. However, contemporary VLMs produce details that are inadequately aligned with the objectives of disaster assessment, primarily due to their deficiency in domain knowledge and the absence of a more refined descriptive process. This research presents the Vision Language Caption Enhancer (VLCE), a dedicated multimodal framework aimed at integrating external semantic knowledge from ConceptNet and WordNet to improve the captioning process. The objective is to produce disaster-specific descriptions that effectively convert raw visual data into actionable intelligence. VLCE utilizes two separate architectures: a CNN-LSTM model that incorporates a ResNet50 backbone, pretrained on EuroSat for satellite imagery (xBD dataset), and a Vision Transformer developed for UAV imagery (RescueNet dataset). In various architectural frameworks and datasets, VLCE exhibits a consistent advantage over baseline models such as LLaVA and QwenVL. Our optimal configuration reaches an impressive 95.33\% on InfoMetIC for UAV imagery while also demonstrating strong performance across satellite imagery. The proposed framework signifies a significant transition from basic visual classification to the generation of comprehensive situational intelligence, demonstrating immediate applicability for implementation in real-time disaster assessment systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-guided Disentangled Representation for Action Recognition</title>
<link>https://arxiv.org/abs/2509.21783</link>
<guid>https://arxiv.org/abs/2509.21783</guid>
<content:encoded><![CDATA[
arXiv:2509.21783v4 Announce Type: replace 
Abstract: Action recognition is a fundamental task in video understanding. Existing methods typically extract unified features to process all actions in one video, which makes it challenging to model the interactions between different objects in multi-action scenarios. To alleviate this issue, we explore disentangling any specified actions from complex scenes as an effective solution. In this paper, we propose Prompt-guided Disentangled Representation for Action Recognition (ProDA), a novel framework that disentangles any specified actions from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs) and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural Network (GPNN) in generating action-specific representations. Furthermore, we design a video-adapted GPNN that aggregates information using dynamic weights. Experiments in video action recognition demonstrate the effectiveness of our approach when compared with the state-of-the-art methods. Our code can be found in https://github.com/iamsnaping/ProDA.git
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiCrafter: High-Fidelity Multi-Subject Generation via Disentangled Attention and Identity-Aware Preference Alignment</title>
<link>https://arxiv.org/abs/2509.21953</link>
<guid>https://arxiv.org/abs/2509.21953</guid>
<content:encoded><![CDATA[
arXiv:2509.21953v2 Announce Type: replace 
Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. Existing In-Context-Learning based methods are limited by their highly coupled training paradigm. These methods attempt to achieve both high subject fidelity and multi-dimensional human preference alignment within a single training stage, relying on a single, indirect reconstruction loss, which is difficult to simultaneously satisfy both these goals. To address this, we propose MultiCrafter, a framework that decouples this task into two distinct training stages. First, in a pre-training stage, we introduce an explicit positional supervision mechanism that effectively resolves attention bleeding and drastically enhances subject fidelity. Second, in a post-training stage, we propose Identity-Preserving Preference Optimization, a novel online reinforcement learning framework. We feature a scoring mechanism to accurately assess multi-subject fidelity based on the Hungarian matching algorithm, which allows the model to optimize for aesthetics and prompt alignment while ensuring subject fidelity achieved in the first stage. Experiments validate that our decoupling framework significantly improves subject fidelity while aligning with human preferences better.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sim-DETR: Unlock DETR for Temporal Sentence Grounding</title>
<link>https://arxiv.org/abs/2509.23867</link>
<guid>https://arxiv.org/abs/2509.23867</guid>
<content:encoded><![CDATA[
arXiv:2509.23867v2 Announce Type: replace 
Abstract: Temporal sentence grounding aims to identify exact moments in a video that correspond to a given textual query, typically addressed with detection transformer (DETR) solutions. However, we find that typical strategies designed to enhance DETR do not improve, and may even degrade, its performance in this task. We systematically analyze and identify the root causes of this abnormal behavior: (1) conflicts between queries from similar target moments and (2) internal query conflicts due to the tension between global semantics and local localization. Building on these insights, we propose a simple yet powerful baseline, Sim-DETR, which extends the standard DETR with two minor modifications in the decoder layers: (1) constraining self-attention between queries based on their semantic and positional overlap and (2) adding query-to-frame alignment to bridge the global and local contexts. Experiments demonstrate that Sim-DETR unlocks the full potential of DETR for temporal sentence grounding, offering a strong baseline for future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Situ Tweedie Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2510.01047</link>
<guid>https://arxiv.org/abs/2510.01047</guid>
<content:encoded><![CDATA[
arXiv:2510.01047v2 Announce Type: replace 
Abstract: While diffusion models excel at generating continuous data such as images, adapting them to discrete tasks has relied on indirect approaches that either operate in continuous embedding spaces or use token masking mechanisms, both of which deviate from modeling the true discrete data distribution that can be theoretically guaranteed by Tweedie's formula. We propose in-situ Tweedie Discrete Diffusion (TDD), a framework that performs diffusion guaranteed by Tweedie's formula directly within the discrete one-hot space, hence "in-situ." Unlike prior methods that diffuse continuous embeddings or mask tokens, TDD directly corrupts one-hot vectors with Gaussian noise and performs iterative denoising through a timestep-conditioned cross-entropy objective rather than mean-squared-error reconstruction. At each denoising step, the model predicts class probabilities, applies argmax to obtain discrete predictions, converts them to one-hot vectors, and feeds them into the next iteration with progressively reduced noise. This process naturally unifies discriminative classification and generative modeling under a single framework. Experiments demonstrate that TDD achieves strong performance on both image classification and text generation tasks, with extensive ablation studies confirming the effectiveness of each design component. Our work establishes a principled approach to discrete diffusion that preserves the core characteristics of diffusion models while operating natively in discrete space.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.08562</link>
<guid>https://arxiv.org/abs/2510.08562</guid>
<content:encoded><![CDATA[
arXiv:2510.08562v2 Announce Type: replace 
Abstract: End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of robust driving logic, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes and simplifies the learning task by predicting the residual deviation from a deterministic inertial reference. This inertial reference serves as a strong physical prior, compelling the model to move beyond simple pattern-matching and instead focus its capacity on learning the necessary, context-driven deviations (e.g., traffic rules, obstacles) from this default, inertially-guided path. To mitigate the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. This technique re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. On the NAVSIM v1 and v2 benchmarks, ResAD achieves state-of-the-art results of 88.8 PDMS and 85.5 EPDMS with only two denoising steps, demonstrating that ResAD significantly simplifies the learning task and improves planning performance. The code will be released to facilitate further research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImmerIris: A Large-Scale Dataset and Benchmark for Off-Axis and Unconstrained Iris Recognition in Immersive Applications</title>
<link>https://arxiv.org/abs/2510.10113</link>
<guid>https://arxiv.org/abs/2510.10113</guid>
<content:encoded><![CDATA[
arXiv:2510.10113v2 Announce Type: replace 
Abstract: Recently, iris recognition is regaining prominence in immersive applications such as extended reality as a means of seamless user identification. This application scenario introduces unique challenges compared to traditional iris recognition under controlled setups, as the ocular images are primarily captured off-axis and less constrained, causing perspective distortion, intra-subject variation, and quality degradation in iris textures. Datasets capturing these challenges remain limited. This paper fills this gap by presenting a large-scale iris dataset collected via head-mounted displays, termed ImmerIris. It contains 499,791 ocular images from 564 subjects, and is, to our knowledge, the largest public iris dataset to date and among the first dedicated to immersive applications. It is accompanied by a comprehensive set of evaluation protocols that benchmark recognition systems under various challenging conditions. This paper also draws attention to a shared obstacle of current recognition methods, the reliance on a pre-processing, normalization stage, which is fallible in off-axis and unconstrained setups. To this end, this paper further proposes a normalization-free paradigm that directly learns from minimally adjusted ocular images. Despite its simplicity, it outperforms normalization-based prior arts, indicating a promising direction for robust iris recognition.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAGLFNet: Deep Feature Attention Guided Global and Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</title>
<link>https://arxiv.org/abs/2510.10471</link>
<guid>https://arxiv.org/abs/2510.10471</guid>
<content:encoded><![CDATA[
arXiv:2510.10471v2 Announce Type: replace 
Abstract: Environmental perception systems are crucial for high-precision mapping and autonomous navigation, with LiDAR serving as a core sensor providing accurate 3D point cloud data. Efficiently processing unstructured point clouds while extracting structured semantic information remains a significant challenge. In recent years, numerous pseudo-image-based representation methods have emerged to balance efficiency and performance by fusing 3D point clouds with 2D grids. However, the fundamental inconsistency between the pseudo-image representation and the original 3D information critically undermines 2D-3D feature fusion, posing a primary obstacle for coherent information fusion and leading to poor feature discriminability. This work proposes DAGLFNet, a pseudo-image-based semantic segmentation framework designed to extract discriminative features. It incorporates three key components: first, a Global-Local Feature Fusion Encoding (GL-FFE) module to enhance intra-set local feature correlation and capture global contextual information; second, a Multi-Branch Feature Extraction (MB-FE) network to capture richer neighborhood information and improve the discriminability of contour features; and third, a Feature Fusion via Deep Feature-guided Attention (FFDFA) mechanism to refine cross-channel feature fusion precision. Experimental evaluations demonstrate that DAGLFNet achieves mean Intersection-over-Union (mIoU) scores of 69.9% and 78.7% on the validation sets of SemanticKITTI and nuScenes, respectively. The method achieves an excellent balance between accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSCloudCAM: Multi-Scale Context Adaptation with Convolutional Cross-Attention for Multispectral Cloud Segmentation</title>
<link>https://arxiv.org/abs/2510.10802</link>
<guid>https://arxiv.org/abs/2510.10802</guid>
<content:encoded><![CDATA[
arXiv:2510.10802v3 Announce Type: replace 
Abstract: Clouds remain a major obstacle in optical satellite imaging, limiting accurate environmental and climate analysis. To address the strong spectral variability and the large scale differences among cloud types, we propose MSCloudCAM, a novel multi-scale context adapter network with convolution based cross-attention tailored for multispectral and multi-sensor cloud segmentation. A key contribution of MSCloudCAM is the explicit modeling of multiple complementary multi-scale context extractors. And also, rather than simply stacking or concatenating their outputs, our formulation uses one extractor's fine-resolution features and the other extractor's global contextual representations enabling dynamic, scale-aware feature selection. Building on this idea, we design a new convolution-based cross attention adapter that effectively fuses localized, detailed information with broader multi-scale context. Integrated with a hierarchical vision backbone and refined through channel and spatial attention mechanisms, MSCloudCAM achieves strong spectral-spatial discrimination. Experiments on various multisensor datatsets e.g. CloudSEN12 (Sentinel-2) and L8Biome (Landsat-8) show that MSCloudCAM outperforms recent state-of-the-art models while maintaining competitive model complexity, highlighting the novelty and effectiveness of the proposed design for large-scale Earth observation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control</title>
<link>https://arxiv.org/abs/2510.13186</link>
<guid>https://arxiv.org/abs/2510.13186</guid>
<content:encoded><![CDATA[
arXiv:2510.13186v2 Announce Type: replace 
Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients and trains a global GS model at the edge server, is an emerging paradigm for scene reconstruction. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments unveil that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. It is found that the GS-oriented objective can be accurately predicted with low sampling ratios (e.g.,10%), and our method achieves an excellent tradeoff between view contributions and communication costs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification</title>
<link>https://arxiv.org/abs/2510.16822</link>
<guid>https://arxiv.org/abs/2510.16822</guid>
<content:encoded><![CDATA[
arXiv:2510.16822v2 Announce Type: replace 
Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as climate change, underscoring the urgent need for scalable, automated monitoring. We introduce ReefNet, a large public coral reef image dataset with point-label annotations mapped to the World Register of Marine Species (WoRMS). ReefNet aggregates imagery from 76 curated CoralNet sources and an additional site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level hard coral annotations with expert-verified labels. Unlike prior datasets, which are often limited by size, geography, or coarse labels and are not ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global scale to WoRMS. We propose two evaluation settings: (i) a within-source benchmark that partitions each source's images for localized evaluation, and (ii) a cross-source benchmark that withholds entire sources to test domain generalization. We analyze both supervised and zero-shot classification performance on ReefNet and find that while supervised within-source performance is promising, supervised performance drops sharply across domains, and performance is low across the board for zero-shot models, especially for rare and visually similar genera. This provides a challenging benchmark intended to catalyze advances in domain generalization and fine-grained coral classification. We will release our dataset, benchmarking code, and pretrained models to advance robust, domain-adaptive, global coral reef monitoring and conservation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUPID: Generative 3D Reconstruction via Joint Object and Pose Modeling</title>
<link>https://arxiv.org/abs/2510.20776</link>
<guid>https://arxiv.org/abs/2510.20776</guid>
<content:encoded><![CDATA[
arXiv:2510.20776v2 Announce Type: replace 
Abstract: We introduce Cupid, a generative 3D reconstruction framework that jointly models the full distribution over both canonical objects and camera poses. Our two-stage flow-based model first generates a coarse 3D structure and 2D-3D correspondences to estimate the camera pose robustly. Conditioned on this pose, a refinement stage injects pixel-aligned image features directly into the generative process, marrying the rich prior of a generative model with the geometric fidelity of reconstruction. This strategy achieves exceptional faithfulness, outperforming state-of-the-art reconstruction methods by over 3 dB PSNR and 10% in Chamfer Distance. As a unified generative model that decouples the object and camera pose, Cupid naturally extends to multi-view and scene-level reconstruction tasks without requiring post-hoc optimization or fine-tuning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.21171</link>
<guid>https://arxiv.org/abs/2510.21171</guid>
<content:encoded><![CDATA[
arXiv:2510.21171v3 Announce Type: replace 
Abstract: Adapting CLIP for anomaly detection on unseen objects has shown strong potential in a zero-shot manner. However, existing methods typically rely on a single textual space to align with visual semantics across diverse objects and domains. The indiscriminate alignment hinders the model from accurately capturing varied anomaly semantics. We propose TokenCLIP, a token-wise adaptation framework that enables dynamic alignment between visual and learnable textual spaces for fine-grained anomaly learning. Rather than mapping all visual tokens to a single, token-agnostic textual space, TokenCLIP aligns each token with a customized textual subspace that represents its visual characteristics. Explicitly assigning a unique learnable textual space to each token is computationally intractable and prone to insufficient optimization. We instead expand the token-agnostic textual space into a set of orthogonal subspaces, and then dynamically assign each token to a subspace combination guided by semantic affinity, which jointly supports customized and efficient token-wise adaptation. To this end, we formulate dynamic alignment as an optimal transport problem, where all visual tokens in an image are transported to textual subspaces based on semantic similarity. The transport constraints of OT ensure sufficient optimization across subspaces and encourage them to focus on different semantics. Solving the problem yields a transport plan that adaptively assigns each token to semantically relevant subspaces. A top-k masking is then applied to sparsify the plan and specialize subspaces for distinct visual regions. Extensive experiments demonstrate the superiority of TokenCLIP.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance</title>
<link>https://arxiv.org/abs/2510.21590</link>
<guid>https://arxiv.org/abs/2510.21590</guid>
<content:encoded><![CDATA[
arXiv:2510.21590v2 Announce Type: replace 
Abstract: Current image super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce TIGER (Text-Image Guided supEr-Resolution), a novel two-stage framework that breaks this trade-off through a "text-first, image-later" paradigm. TIGER explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and uses them to guide full-image super-resolution. This ensures high fidelity and readability. To support comprehensive training and evaluation, we present the UZ-ST (UltraZoom-Scene Text) dataset, the first Chinese scene text dataset with extreme zoom. Extensive experiments show TIGER achieves state-of-the-art performance, enhancing readability and image quality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</title>
<link>https://arxiv.org/abs/2510.23594</link>
<guid>https://arxiv.org/abs/2510.23594</guid>
<content:encoded><![CDATA[
arXiv:2510.23594v3 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on vision-language tasks, yet their reasoning processes remain sometimes unreliable. We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDocLayout: Towards Diverse Document Layout Generation via Coarse-to-Fine LLM Learning</title>
<link>https://arxiv.org/abs/2510.26213</link>
<guid>https://arxiv.org/abs/2510.26213</guid>
<content:encoded><![CDATA[
arXiv:2510.26213v2 Announce Type: replace 
Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, layout generation, remains underexplored. Distinct from traditional graphic layout design and room layout planning, document layout generation typically involves a larger number of elements per page and exhibits greater structural diversity and complexity. Currently, a major obstacle lies in the scarcity of diverse document layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniDocLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniDocLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm:1) learning universal layout principles from our dataset with coarse category definitions, and 2) transferring the knowledge to a specific domain with few fine-grained annotated samples. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^6$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, dataset, and models will be publicly released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOCUS: Efficient Keyframe Selection for Long Video Understanding</title>
<link>https://arxiv.org/abs/2510.27280</link>
<guid>https://arxiv.org/abs/2510.27280</guid>
<content:encoded><![CDATA[
arXiv:2510.27280v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) represent images and video frames as visual tokens. Scaling from single images to hour-long videos, however, inflates the token budget far beyond practical limits. Popular pipelines therefore either uniformly subsample or apply keyframe selection with retrieval-style scoring using smaller vision-language models. However, these keyframe selection methods still rely on pre-filtering before selection to reduce the inference cost and can miss the most informative moments. We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a training-free, model-agnostic keyframe selection module that selects query-relevant frames under a strict token budget. FOCUS formulates keyframe selection as a combinatorial pure-exploration (CPE) problem in multi-armed bandits: it treats short temporal clips as arms, and uses empirical means and Bernstein confidence radius to identify informative regions while preserving exploration of uncertain areas. The resulting two-stage exploration-exploitation procedure reduces from a sequential policy with theoretical guarantees, first identifying high-value temporal regions, then selecting top-scoring frames within each region. On two long-video question-answering benchmarks, FOCUS delivers substantial accuracy improvements while processing less than 2% of video frames. For videos longer than 20 minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating its effectiveness as a keyframe selection method and providing a simple and general solution for scalable long-video understanding with MLLMs. Code is available at https://github.com/NUS-HPC-AI-Lab/FOCUS.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefVTON: person-to-person Try on with Additional Unpaired Visual Reference</title>
<link>https://arxiv.org/abs/2511.00956</link>
<guid>https://arxiv.org/abs/2511.00956</guid>
<content:encoded><![CDATA[
arXiv:2511.00956v3 Announce Type: replace 
Abstract: We introduce RefTON, a flux-based person-to-person virtual try-on framework that enhances garment realism through unpaired visual references. Unlike conventional approaches that rely on complex auxiliary inputs such as body parsing and warped mask or require finely designed extract branches to process various input conditions, RefTON streamlines the process by directly generating try-on results from a source image and a target garment, without the need for structural guidance or auxiliary components to handle diverse inputs. Moreover, inspired by human clothing selection behavior, RefTON leverages additional reference images (the target garment worn on different individuals) to provide powerful guidance for refining texture alignment and maintaining the garment details. To enable this capability, we built a dataset containing unpaired reference images for training. Extensive experiments on public benchmarks demonstrate that RefTON achieves competitive or superior performance compared to state-of-the-art methods, while maintaining a simple and efficient person-to-person design.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniREditBench: A Unified Reasoning-based Image Editing Benchmark</title>
<link>https://arxiv.org/abs/2511.01295</link>
<guid>https://arxiv.org/abs/2511.01295</guid>
<content:encoded><![CDATA[
arXiv:2511.01295v2 Announce Type: replace 
Abstract: Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pressure2Motion: Hierarchical Human Motion Reconstruction from Ground Pressure with Text Guidance</title>
<link>https://arxiv.org/abs/2511.05038</link>
<guid>https://arxiv.org/abs/2511.05038</guid>
<content:encoded><![CDATA[
arXiv:2511.05038v2 Announce Type: replace 
Abstract: We present Pressure2Motion, a novel motion capture algorithm that reconstructs human motion from a ground pressure sequence and text prompt. At inference time, Pressure2Motion requires only a pressure mat, eliminating the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminacy of pressure signals with respect to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint to resolve ambiguities. Specifically, our model adopts a dual-level feature extractor to accurately interpret pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion estimation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion reconstruction, and the established MPL benchmark is the first benchmark for this novel motion capture task. Experiments show that our method generates high-fidelity, physically plausible motions, establishing a new state of the art for this task. The codes and benchmarks will be publicly released upon publication.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field</title>
<link>https://arxiv.org/abs/2511.06299</link>
<guid>https://arxiv.org/abs/2511.06299</guid>
<content:encoded><![CDATA[
arXiv:2511.06299v3 Announce Type: replace 
Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT</title>
<link>https://arxiv.org/abs/2511.06625</link>
<guid>https://arxiv.org/abs/2511.06625</guid>
<content:encoded><![CDATA[
arXiv:2511.06625v3 Announce Type: replace 
Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning</title>
<link>https://arxiv.org/abs/2511.11025</link>
<guid>https://arxiv.org/abs/2511.11025</guid>
<content:encoded><![CDATA[
arXiv:2511.11025v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title>
<link>https://arxiv.org/abs/2511.11030</link>
<guid>https://arxiv.org/abs/2511.11030</guid>
<content:encoded><![CDATA[
arXiv:2511.11030v3 Announce Type: replace 
Abstract: Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.11266</link>
<guid>https://arxiv.org/abs/2511.11266</guid>
<content:encoded><![CDATA[
arXiv:2511.11266v2 Announce Type: replace 
Abstract: Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\% increase in driving score for LMDrive and 17.5\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation</title>
<link>https://arxiv.org/abs/2511.11483</link>
<guid>https://arxiv.org/abs/2511.11483</guid>
<content:encoded><![CDATA[
arXiv:2511.11483v2 Announce Type: replace 
Abstract: Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2511.13533</link>
<guid>https://arxiv.org/abs/2511.13533</guid>
<content:encoded><![CDATA[
arXiv:2511.13533v2 Announce Type: replace 
Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data. Code is available at https://github.com/jwen307/multi_target_minimax.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpha Divergence Losses for Biometric Verification</title>
<link>https://arxiv.org/abs/2511.13621</link>
<guid>https://arxiv.org/abs/2511.13621</guid>
<content:encoded><![CDATA[
arXiv:2511.13621v3 Announce Type: replace 
Abstract: Performance in face and speaker verification is largely driven by margin-based softmax losses such as CosFace and ArcFace. Recently introduced $\alpha$-divergence loss functions offer a compelling alternative, particularly due to their ability to induce sparse solutions (when $\alpha>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find that this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $\alpha$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a training instability in A3M-caused by sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is critical for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount. Finally, the sparsity of $\alpha$-divergence-based posteriors enables memory-efficient training, which is crucial for datasets with millions of identities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Autonomous Driving: DepthSense with Radar and Spatial Attention</title>
<link>https://arxiv.org/abs/2109.05265</link>
<guid>https://arxiv.org/abs/2109.05265</guid>
<content:encoded><![CDATA[
arXiv:2109.05265v4 Announce Type: replace-cross 
Abstract: Depth perception is crucial for spatial understanding and has traditionally been achieved through stereoscopic imaging. However, the precision of depth estimation using stereoscopic methods depends on the accurate calibration of binocular vision sensors. Monocular cameras, while more accessible, often suffer from reduced accuracy, especially under challenging imaging conditions. Optical sensors, too, face limitations in adverse environments, leading researchers to explore radar technology as a reliable alternative. Although radar provides coarse but accurate signals, its integration with fine-grained monocular camera data remains underexplored. In this research, we propose DepthSense, a novel radar-assisted monocular depth enhancement approach. DepthSense employs an encoder-decoder architecture, a Radar Residual Network, feature fusion with a spatial attention mechanism, and an ordinal regression layer to deliver precise depth estimations. We conducted extensive experiments on the nuScenes dataset to validate the effectiveness of DepthSense. Our methodology not only surpasses existing approaches in quantitative performance but also reduces parameter complexity and inference times. Our findings demonstrate that DepthSense represents a significant advancement over traditional stereo methods, offering a robust and efficient solution for depth estimation in autonomous driving. By leveraging the complementary strengths of radar and monocular camera data, DepthSense sets a new benchmark in the field, paving the way for more reliable and accurate spatial perception systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FCDM: A Physics-Guided Bidirectional Frequency Aware Convolution and Diffusion-Based Model for Sinogram Inpainting</title>
<link>https://arxiv.org/abs/2409.06714</link>
<guid>https://arxiv.org/abs/2409.06714</guid>
<content:encoded><![CDATA[
arXiv:2409.06714v5 Announce Type: replace-cross 
Abstract: Computed tomography (CT) is widely used in scientific imaging systems such as synchrotron and laboratory-based nano-CT, but acquiring full-view sinograms requires high radiation dose and long scan times. Sparse-view CT alleviates this burden but yields incomplete sinograms with structured signal loss, hampering accurate reconstruction. Unlike RGB images, sinograms encode overlapping features along projection paths and exhibit distinct directional spectral patterns, which make conventional RGB-oriented inpainting approaches--including diffusion models--ineffective for sinogram restoration, as they disregard the angular dependencies and physical constraints inherent to tomographic data. To overcome these limitations, we propose FCDM, a diffusion-based framework tailored for sinograms, which restores global structure through bidirectional frequency reasoning and angular-aware masking, while enforcing physical plausibility via physics-guided constraints and frequency-adaptive noise control. Experiments on real-world datasets show that FCDM consistently outperforms baselines, achieving SSIM over 0.93 and PSNR above 31 dB across diverse sparse-view scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AsynEIO: Asynchronous Monocular Event-Inertial Odometry Using Gaussian Process Regression</title>
<link>https://arxiv.org/abs/2411.12175</link>
<guid>https://arxiv.org/abs/2411.12175</guid>
<content:encoded><![CDATA[
arXiv:2411.12175v2 Announce Type: replace-cross 
Abstract: Event cameras, when combined with inertial sensors, show significant potential for motion estimation in challenging scenarios, such as high-speed maneuvers and low-light environments. There are many methods for producing such estimations, but most boil down to a synchronous discrete-time fusion problem. However, the asynchronous nature of event cameras and their unique fusion mechanism with inertial sensors remain underexplored. In this paper, we introduce a monocular event-inertial odometry method called AsynEIO, designed to fuse asynchronous event and inertial data within a unified Gaussian Process (GP) regression framework. Our approach incorporates an event-driven frontend that tracks feature trajectories directly from raw event streams at a high temporal resolution. These tracked feature trajectories, along with various inertial factors, are integrated into the same GP regression framework to enable asynchronous fusion. With deriving analytical residual Jacobians and noise models, our method constructs a factor graph that is iteratively optimized and pruned using a sliding-window optimizer. Comparative assessments highlight the performance of different inertial fusion strategies, suggesting optimal choices for varying conditions. Experimental results on both public datasets and our own event-inertial sequences indicate that AsynEIO outperforms existing methods, especially in high-speed and low-illumination scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffBreak: Is Diffusion-Based Purification Robust?</title>
<link>https://arxiv.org/abs/2411.16598</link>
<guid>https://arxiv.org/abs/2411.16598</guid>
<content:encoded><![CDATA[
arXiv:2411.16598v4 Announce Type: replace-cross 
Abstract: Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, accrediting it two critical factors: inaccurate gradients and improper evaluation protocols that test only a single random purification of the AE. We show that when accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient mismatches that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Reward Gap Optimization for Mitigating VLM Hallucinations</title>
<link>https://arxiv.org/abs/2411.17265</link>
<guid>https://arxiv.org/abs/2411.17265</guid>
<content:encoded><![CDATA[
arXiv:2411.17265v4 Announce Type: replace-cross 
Abstract: The success of Direct Preference Optimization (DPO) in mitigating hallucinations in Vision Language Models (VLMs) critically hinges on the true reward gaps within preference pairs. However, current methods, typically relying on ranking or rewriting strategies, often struggle to optimize these reward gaps in a systematic way during data curation. A core difficulty lies in precisely characterizing and strategically manipulating the overall reward gap configuration, that is, the deliberate design of how to shape these reward gaps within each preference pair across the data. To address this, we introduce Topic-level Preference Rewriting(TPR), a novel framework designed for the systematic optimization of reward gap configuration. Through selectively replacing semantic topics within VLM responses with model's own resampled candidates for targeted rewriting, TPR can provide topic-level control over fine-grained semantic details. This precise control enables advanced data curation strategies, such as progressively adjusting the difficulty of rejected responses, thereby sculpting an effective reward gap configuration that guides the model to overcome challenging hallucinations. Comprehensive experiments demonstrate TPR achieves state-of-the-art performance on multiple hallucination benchmarks, outperforming previous methods by an average of 20%. Notably, it significantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also exhibits superior data efficiency towards robust and cost-effective VLM alignment. Code and datasets are available at https://tpr-dpo.github.io .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshCone: Second-Order Cone Programming for Geometrically-Constrained Mesh Enhancement</title>
<link>https://arxiv.org/abs/2412.08484</link>
<guid>https://arxiv.org/abs/2412.08484</guid>
<content:encoded><![CDATA[
arXiv:2412.08484v3 Announce Type: replace-cross 
Abstract: Modern geometric generation methods rely heavily on deep learning methods that, while powerful, often lack interpretability and require extensive training data. This work introduces MeshCone, a convex optimization framework for mesh enhancement from partially deformed meshes that requires no training data. We formulate the problem as a second-order cone program where vertex positions are optimized to align with target geometry while enforcing smoothness through convex edge-length regularization. Our convex relaxation enables deterministic, interpretable solutions with proven convergence properties via the Splitting Conic Solver (SCS). We demonstrate robust performance across 56 diverse object categories from ShapeNet and ThreeDScans, achieving superior refinement quality compared to classical baselines while maintaining sub-second inference times. This work establishes a principled baseline demonstrating what convex optimization alone can achieve, providing mathematical guarantees and interpretability that complement data-driven approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Full-scale Representation Guided Network for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2501.18921</link>
<guid>https://arxiv.org/abs/2501.18921</guid>
<content:encoded><![CDATA[
arXiv:2501.18921v2 Announce Type: replace-cross 
Abstract: The U-Net architecture and its variants have remained state-of-the-art (SOTA) for retinal vessel segmentation over the past decade. In this study, we introduce a Full-Scale Guided Network (FSG-Net), where a novel feature representation module using modernized convolution blocks effectively captures full-scale structural information, while a guided convolution block subsequently refines this information. Specifically, we introduce an attention-guided filter within the guided convolution block, leveraging its similarity to unsharp masking to enhance fine vascular structures. Passing full-scale information to the attention block facilitates the generation of more contextually relevant attention maps, which are then passed to the attention-guided filter, providing further refinement to the segmentation performance. The structure preceding the guided convolution block can be replaced by any U-Net variant, ensuring flexibility and scalability across various segmentation tasks. For a fair comparison, we re-implemented recent studies available in public repositories to evaluate their scalability and reproducibility. Our experiments demonstrate that, despite its compact architecture, FSG-Net delivers performance competitive with SOTA methods across multiple public datasets. Ablation studies further demonstrate that each proposed component meaningfully contributes to this competitive performance. Our code is available on https://github.com/ZombaSY/FSG-Net-pytorch.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient Contrastive Pre-training under Non-Stationary Drift</title>
<link>https://arxiv.org/abs/2502.07620</link>
<guid>https://arxiv.org/abs/2502.07620</guid>
<content:encoded><![CDATA[
arXiv:2502.07620v3 Announce Type: replace-cross 
Abstract: The remarkable success of large-scale contrastive pre-training has been largely driven by by vast yet static datasets. However, as the scaling paradigm evolves, this paradigm encounters a fundamental challenge when applied to dynamic data streams characterized by concept drift - unpredictable changes in the underlying data distribution. This paper aims to advance robust pre-training under such non-stationary environments. We begin by revealing that conventional contrastive pre-training methods are highly susceptible to concept drift, resulting in significant substantial bias and instability within the learned feature representations. To systematically analyze these effects, we develop a structural causal model that elucidates how drift acts as a confounder, distorting the learned representations. Based on these causal insights, we propose Resilient Contrastive Pre-training (RCP), a novel method that incorporates causal intervention. RCP formulates a causally-informed objective to mitigate drift-induced biases through targeted interventions. The method is designed for simple and scalable implementation and exhibits notable adaptability, promoting robust and autonomous pre-training on non-stationary data. Comprehensive experiments across various downstream tasks consistently demonstrate that RCP effectively alleviates the detrimental impact of concept drift, yielding more resilient and generalizable representations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v5 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering</title>
<link>https://arxiv.org/abs/2504.14135</link>
<guid>https://arxiv.org/abs/2504.14135</guid>
<content:encoded><![CDATA[
arXiv:2504.14135v2 Announce Type: replace-cross 
Abstract: High-fidelity simulation is essential for robotics research, enabling safe and efficient testing of perception, control, and navigation algorithms. However, achieving both photorealistic rendering and accurate physics modeling remains a challenge. This paper presents a novel simulation framework, the Unreal Robotics Lab (URL), that integrates the advanced rendering capabilities of the Unreal Engine with MuJoCo's high-precision physics simulation. Our approach enables realistic robotic perception while maintaining accurate physical interactions, facilitating benchmarking and dataset generation for vision-based robotics applications. The system supports complex environmental effects, such as smoke, fire, and water dynamics, which are critical to evaluating robotic performance under adverse conditions. We benchmark visual navigation and SLAM methods within our framework, demonstrating its utility for testing real-world robustness in controlled yet diverse scenarios. By bridging the gap between physics accuracy and photorealistic rendering, our framework provides a powerful tool for advancing robotics research and sim-to-real transfer. Our open-source framework is available at https://unrealroboticslab.github.io/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Drive Anywhere with Model-Based Reannotation</title>
<link>https://arxiv.org/abs/2505.05592</link>
<guid>https://arxiv.org/abs/2505.05592</guid>
<content:encoded><![CDATA[
arXiv:2505.05592v3 Announce Type: replace-cross 
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[
arXiv:2505.22633v3 Announce Type: replace-cross 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning</title>
<link>https://arxiv.org/abs/2506.06659</link>
<guid>https://arxiv.org/abs/2506.06659</guid>
<content:encoded><![CDATA[
arXiv:2506.06659v3 Announce Type: replace-cross 
Abstract: Autonomous vehicles must navigate safely in complex driving environments. Imitating a single expert trajectory, as in regression-based approaches, usually does not explicitly assess the safety of the predicted trajectory. Selection-based methods address this by generating and scoring multiple trajectory candidates and predicting the safety score for each. However, they face optimization challenges in precisely selecting the best option from thousands of candidates and distinguishing subtle but safety-critical differences, especially in rare and challenging scenarios. We propose DriveSuprim to overcome these challenges and advance the selection-based paradigm through a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training. DriveSuprim achieves state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, with 83.02 Driving Score and 60.00 Success Rate on the Bench2Drive benchmark, demonstrating superior planning capabilities in various driving scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v3 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding</title>
<link>https://arxiv.org/abs/2507.00416</link>
<guid>https://arxiv.org/abs/2507.00416</guid>
<content:encoded><![CDATA[
arXiv:2507.00416v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results. In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model. This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System</title>
<link>https://arxiv.org/abs/2507.16621</link>
<guid>https://arxiv.org/abs/2507.16621</guid>
<content:encoded><![CDATA[
arXiv:2507.16621v2 Announce Type: replace-cross 
Abstract: Extrinsic Calibration represents the cornerstone of autonomous driving. Its accuracy plays a crucial role in the perception pipeline, as any errors can have implications for the safety of the vehicle. Modern sensor systems collect different types of data from the environment, making it harder to align the data. To this end, we propose a target-based extrinsic calibration system tailored for a multi-LiDAR and multi-camera sensor suite. This system enables cross-calibration between LiDARs and cameras with limited prior knowledge using a custom ChArUco board and a tailored nonlinear optimization method. We test the system with real-world data gathered in a warehouse. Results demonstrated the effectiveness of the proposed method, highlighting the feasibility of a unique pipeline tailored for various types of sensors.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Cortical Computation: Manifold Disentanglement and Predictive Dynamics in VCNet</title>
<link>https://arxiv.org/abs/2508.02995</link>
<guid>https://arxiv.org/abs/2508.02995</guid>
<content:encoded><![CDATA[
arXiv:2508.02995v3 Announce Type: replace-cross 
Abstract: Despite their success, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. These shortcomings can be traced to a lack of inductive biases that reflect the inherent geometric structure of the visual world. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural and computational principles,which evolved to internalize these structures,may offer a blueprint for more capable artificial vision. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet is framed as a geometric framework that emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation for learning disentangled representations, and top-down predictive feedback for representation refinement. We interpret these mechanisms through the lens of geometry and dynamical systems, positing that they guide the learning of structured, low-dimensional neural manifolds. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset, which probes sensitivity to natural textures, and a light field image classification task, which requires processing higher-dimensional visual data. Our results show that VCNet achieves state-of-the-art accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating high-level neuroscientific principles, viewed through a geometric lens, can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to See and Act: Task-Aware Virtual View Exploration for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2508.05186</link>
<guid>https://arxiv.org/abs/2508.05186</guid>
<content:encoded><![CDATA[
arXiv:2508.05186v4 Announce Type: replace-cross 
Abstract: Recent vision-language-action (VLA) models for multi-task robotic manipulation commonly rely on static viewpoints and shared visual encoders, which limit 3D perception and cause task interference, hindering robustness and generalization. In this work, we propose Task-aware Virtual View Exploration (TVVE), a framework designed to overcome these challenges by integrating virtual view exploration with task-specific representation learning. TVVE employs an efficient exploration policy, accelerated by a novel pseudo-environment, to acquire informative views. Furthermore, we introduce a Task-aware Mixture-of-Experts (TaskMoE) visual encoder to disentangle features across different tasks, boosting both representation fidelity and task generalization. By learning to see the world in a task-aware way, TVVE generates more complete and discriminative visual representations, demonstrating significantly enhanced action prediction across a wide array of manipulation challenges. To further validate the robustness and generalization capability of TVVE under out-of-distribution (OOD) settings, we construct a challenging benchmark, RLBench-OG, covering various visual perturbations and camera pose variations. Extensive experiments on RLBench and RLBench-OG show that our TVVE achieves superior performance over state-of-the-art approaches. In real-robot experiments, TVVE demonstrates exceptional performance and generalizes robustly in multiple OOD settings, including visual disturbances and unseen instructions. Visual results and code are provided at: https://hcplab-sysu.github.io/TAVP.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2508.05224</link>
<guid>https://arxiv.org/abs/2508.05224</guid>
<content:encoded><![CDATA[
arXiv:2508.05224v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy by keeping data local. Traditional FL approaches rely on a centralized, star-shaped topology, where a central server aggregates model updates from clients. However, this architecture introduces several limitations, including a single point of failure, limited personalization, and poor robustness to distribution shifts or vulnerability to malfunctioning clients. Moreover, update selection in centralized FL often relies on low-level parameter differences, which can be unreliable when client data is not independent and identically distributed, and offer clients little control. In this work, we propose a decentralized, peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P topology to enable each client to identify and aggregate a personalized set of trustworthy and beneficial updates.This framework is the Local Inference Guided Aggregation for Heterogeneous Training Environments to Yield Enhancement Through Agreement and Regularization (LIGHTYEAR). Central to our method is an agreement score, computed on a local validation set, which quantifies the semantic alignment of incoming updates in the function space with respect to the clients reference model. Each client uses this score to select a tailored subset of updates and performs aggregation with a regularization term that further stabilizes the training. Our empirical evaluation across five datasets shows that the proposed approach consistently outperforms both, centralized baselines and existing P2P methods in terms of client-level performance, particularly under adversarial and heterogeneous conditions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</title>
<link>https://arxiv.org/abs/2508.06859</link>
<guid>https://arxiv.org/abs/2508.06859</guid>
<content:encoded><![CDATA[
arXiv:2508.06859v2 Announce Type: replace-cross 
Abstract: Timely and accurate forecasts of severe weather events are essential for early warning and for constraining downstream analysis and decision-making. Since severe weather events prediction still depends on subjective, time-consuming expert interpretation, end-to-end "AI weather station" systems are emerging but face three major challenges: (1) scarcity of severe weather event samples; (2) imperfect alignment between high-dimensional meteorological data and textual warnings; (3) current multimodal language models cannot effectively process high-dimensional meteorological inputs or capture their complex spatiotemporal dependencies. To address these challenges, we introduce MP-Bench, the first large-scale multimodal dataset for severe weather events prediction, comprising 421,363 pairs of raw multi-year meteorological data and corresponding text caption, covering a wide range of severe weather scenarios. On top of this dataset, we develop a Meteorology Multimodal Large Model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is designed to accommodate the unique characteristics of 4D meteorological data flow, incorporating three plug-and-play adaptive fusion modules that enable dynamic feature extraction and integration across temporal sequences, vertical pressure layers, and spatial dimensions. Extensive experiments on MP-Bench show that MMLM achieves strong performance across multiple tasks, demonstrating effective severe weather understanding and representing a key step toward automated, AI-driven severe weather events forecasting systems. Our source code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained GRPO for Precise Preference Alignment in Flow Models</title>
<link>https://arxiv.org/abs/2510.01982</link>
<guid>https://arxiv.org/abs/2510.01982</guid>
<content:encoded><![CDATA[
arXiv:2510.01982v3 Announce Type: replace-cross 
Abstract: The incorporation of online reinforcement learning (RL) into diffusion and flow-based generative models has recently gained attention as a powerful paradigm for aligning model behavior with human preferences. By leveraging stochastic sampling via Stochastic Differential Equations (SDEs) during the denoising phase, these models can explore a variety of denoising trajectories, enhancing the exploratory capacity of RL. However, despite their ability to discover potentially high-reward samples, current approaches often struggle to effectively align with preferences due to the sparsity and narrowness of reward feedback. To overcome this limitation, we introduce a novel framework called Granular-GRPO (G$^2$RPO), which enables fine-grained and comprehensive evaluation of sampling directions in the RL training of flow models. Specifically, we propose a Singular Stochastic Sampling mechanism that supports step-wise stochastic exploration while ensuring strong correlation between injected noise and reward signals, enabling more accurate credit assignment to each SDE perturbation. Additionally, to mitigate the bias introduced by fixed-granularity denoising, we design a Multi-Granularity Advantage Integration module that aggregates advantages computed across multiple diffusion scales, resulting in a more robust and holistic assessment of sampling trajectories. Extensive experiments on various reward models, including both in-domain and out-of-domain settings, demonstrate that our G$^2$RPO outperforms existing flow-based GRPO baselines, highlighting its effectiveness and generalization capability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"It's trained by non-disabled people": Evaluating How Image Quality Affects Product Captioning with VLMs</title>
<link>https://arxiv.org/abs/2511.08917</link>
<guid>https://arxiv.org/abs/2511.08917</guid>
<content:encoded><![CDATA[
arXiv:2511.08917v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) are increasingly used by blind and low-vision (BLV) people to identify and understand products in their everyday lives, such as food, personal products, and household goods. Despite their prevalence, we lack an empirical understanding of how common image quality issues, like blur and misframing of items, affect the accuracy of VLM-generated captions and whether resulting captions meet BLV people's information needs. Grounded in a survey with 86 BLV people, we systematically evaluate how image quality issues affect captions generated by VLMs. We show that the best model recognizes products in images with no quality issues with 98% accuracy, but drops to 75% accuracy overall when quality issues are present, worsening considerably as issues compound. We discuss the need for model evaluations that center on disabled people's experiences throughout the process and offer concrete recommendations for HCI and ML researchers to make VLMs more reliable for BLV people.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</title>
<link>https://arxiv.org/abs/2511.12609</link>
<guid>https://arxiv.org/abs/2511.12609</guid>
<content:encoded><![CDATA[
arXiv:2511.12609v2 Announce Type: replace-cross 
Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.13278</link>
<guid>https://arxiv.org/abs/2511.13278</guid>
<content:encoded><![CDATA[
<div> Lightweight buildings, 3D Gaussian Splatting, multi-view images, edge-consistency pruning, Delaunay triangulation<br /><br />Summary:<br /><br />This paper introduces SF-Recon, a novel method for reconstructing lightweight building surface models directly from multi-view images, addressing the limitations of traditional multi-view geometry pipelines that rely heavily on dense reconstruction, meshing, and subsequent simplification. Initially, SF-Recon trains a 3D Gaussian Splatting (3DGS) field to obtain a consistent representation across views. The method then extracts building structures by optimizing Gaussian primitives guided by normal gradients, focusing on alignment with roof and wall boundaries, enhancing accuracy in structural detail. To improve clarity and suppress artifacts, a multi-view edge-consistency pruning step is applied without needing external supervision. Finally, the structured Gaussian field is converted into a lightweight and faithful building mesh using a multi-view depth-constrained Delaunay triangulation. Experiments conducted on a newly proposed SF dataset reveal that SF-Recon achieves significantly reduced model complexity, with fewer faces and vertices, while maintaining computational efficiency and preserving structural fidelity. This approach offers a streamlined and effective solution for digital city modeling, navigation, and geospatial analytics without extensive post-processing. More details and resources are available on the project website. <div>
arXiv:2511.13278v2 Announce Type: replace 
Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection</title>
<link>https://arxiv.org/abs/2511.13344</link>
<guid>https://arxiv.org/abs/2511.13344</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, YOLOv9-T, object detection, adaptive routing, mean Average Precision<br /><br />Summary:<br /><br />This paper introduces a novel Mixture-of-Experts (MoE) framework aimed at improving object detection performance. The approach integrates multiple YOLOv9-T expert models within a single system. A key innovation is the use of adaptive routing, which dynamically directs features to specific experts, allowing for specialized processing tailored to different object characteristics. This method leverages the strengths of multiple experts rather than relying on a single YOLOv9-T model, enhancing the overall detection capability. Experimental results show that this framework achieves higher mean Average Precision (mAP), a critical metric measuring the accuracy of object detectors, compared to baseline single-model approaches. Moreover, the system attains better Average Recall (AR), indicating improved ability to detect a larger proportion of objects in various scenarios. The combination of adaptive routing and the MoE design allows the model to dynamically allocate resources based on input complexity, improving efficiency and detection quality. This framework offers a promising direction for advancing object detection architectures and could be extended to other vision tasks that benefit from dynamic expert specialization. <div>
arXiv:2511.13344v3 Announce Type: replace 
Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, Chain-of-Thought, reasoning transparency, evaluation benchmarks, future directions<br /><br />Summary:<br /><br />This paper addresses the enhancement of complex reasoning capabilities in Multimodal Large Language Models (MLLMs), which have achieved significant success in perception tasks but still face challenges like opaque reasoning paths and limited generalization. It introduces the concept of Multimodal Chain-of-Thought (MCoT), inspired by the Chain-of-Thought (CoT) reasoning approach in language models that improves reasoning transparency and interpretability. The paper systematically reviews the background and theoretical motivations behind MCoT, considering both technical evolution and task demands. It categorizes mainstream MCoT methods based on three key stages: the CoT paradigms themselves, the post-training phase, and the inference phase, while analyzing the mechanisms driving these methods. Additionally, the work summarizes existing evaluation benchmarks and metrics used to assess MCoT performance and discusses various application scenarios where MCoT can be applied. Lastly, it highlights the current challenges facing MCoT development, such as improving generalization and interpretability, and offers perspectives on future research directions to advance this promising area of multimodal reasoning. <div>
arXiv:2511.12861v3 Announce Type: replace-cross 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The persistence of painting styles</title>
<link>https://arxiv.org/abs/2511.16695</link>
<guid>https://arxiv.org/abs/2511.16695</guid>
<content:encoded><![CDATA[
<div> Persistent homology, topological data analysis, artistic styles, AI-generated art, statistical differentiation

<br /><br />Summary: This article introduces the use of persistent homology (PH), a concept from topological data analysis, as a novel quantitative tool for analyzing and distinguishing artistic styles. It challenges the traditional reliance on art historians and critics, who use subjective visual intuition, by providing a mathematical and statistically rigorous method to assess artworks. The study demonstrates that PH can objectively identify differences between artists from various artistic movements, as well as differentiate between artists within the same movement. Moreover, the approach is sensitive enough to distinguish genuine artworks by an artist from AI-generated images created in that artist’s style. This establishes PH as both an interpretable and reliable technique, contributing to art studies by adding a structured framework for style recognition. The findings suggest that integrating mathematical tools like PH into art analysis can enhance the understanding of visual culture and creativity, offering new possibilities for authentication and critique in the era of AI-generated art. <div>
arXiv:2511.16695v1 Announce Type: new 
Abstract: Art is a deeply personal and expressive medium, where each artist brings their own style, technique, and cultural background into their work. Traditionally, identifying artistic styles has been the job of art historians or critics, relying on visual intuition and experience. However, with the advancement of mathematical tools, we can explore art through more structured lens. In this work, we show how persistent homology (PH), a method from topological data analysis, provides objective and interpretable insights on artistic styles. We show how PH can, with statistical certainty, differentiate between artists, both from different artistic currents and from the same one, and distinguish images of an artist from an AI-generated image in the artist's style.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions</title>
<link>https://arxiv.org/abs/2511.16711</link>
<guid>https://arxiv.org/abs/2511.16711</guid>
<content:encoded><![CDATA[
<div> Keywords: macaque faces, StyleGAN2, facial expression generation, data augmentation, generative AI<br /><br />Summary:<br /><br />Generating animal faces using generative AI is difficult due to the limited quantity and variability of training images, especially concerning facial expressions across different individuals. This study centers on macaque monkeys, an important model in neuroscience and evolutionary research, and introduces a method to generate their facial expressions using a style-based generative model, StyleGAN2. To overcome data scarcity, the authors apply three key strategies: first, they augment data by synthesizing new facial expressions using motion transfer techniques that animate still images with computer graphics; second, they conduct sample selection based on latent representations from an initially trained StyleGAN2 to ensure training data variation and uniform sampling; third, they refine the loss function to accurately capture subtle facial movements like eye movements. The proposed method successfully generates diverse facial expressions for multiple macaque individuals and outperforms models trained only on original still images. Furthermore, the model facilitates style-based image editing where distinct style parameters correspond to specific facial movements. These results highlight the model's capability to disentangle motion components as style parameters, presenting a valuable tool for studying macaque facial expressions in research. <div>
arXiv:2511.16711v1 Announce Type: new 
Abstract: Generating animal faces using generative AI techniques is challenging because the available training images are limited both in quantity and variation, particularly for facial expressions across individuals. In this study, we focus on macaque monkeys, widely studied in systems neuroscience and evolutionary research, and propose a method to generate their facial expressions using a style-based generative image model (i.e., StyleGAN2). To address data limitations, we implemented: 1) data augmentation by synthesizing new facial expression images using a motion transfer to animate still images with computer graphics, 2) sample selection based on the latent representation of macaque faces from an initially trained StyleGAN2 model to ensure the variation and uniform sampling in training dataset, and 3) loss function refinement to ensure the accurate reproduction of subtle movements, such as eye movements. Our results demonstrate that the proposed method enables the generation of diverse facial expressions for multiple macaque individuals, outperforming models trained solely on original still images. Additionally, we show that our model is effective for style-based image editing, where specific style parameters correspond to distinct facial movements. These findings underscore the model's potential for disentangling motion components as style parameters, providing a valuable tool for research on macaque facial expressions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation</title>
<link>https://arxiv.org/abs/2511.16712</link>
<guid>https://arxiv.org/abs/2511.16712</guid>
<content:encoded><![CDATA[
<div> Personalized portrait, dual-person generation, dataset, deep learning, facial consistency<br /><br />Summary:<br /><br />1. This paper addresses the challenge of personalized dual-person portrait customization, which is valuable for applications like preserving emotional memories and wedding photography planning.  
2. The authors identify the lack of a dedicated benchmark dataset for dual-person portrait generation as a key limiting factor in achieving high-quality outputs.  
3. To overcome this, they introduce PairHuman, the first large-scale benchmark dataset tailored specifically for dual-person portrait generation, comprising over 100,000 images with diverse scenes, attire, and interpersonal interactions.  
4. PairHuman includes extensive metadata such as detailed image descriptions, person localization data, human keypoints, and attribute tags, which support nuanced personalization and scene understanding.  
5. Additionally, the paper presents DHumanDiff, a baseline model designed for dual-person portrait generation that emphasizes improved facial consistency and balances the generation of personalized individuals with semantic scene creation.  
6. Experimental results demonstrate that the combination of the PairHuman dataset and DHumanDiff produces highly customized dual-person portraits with superior visual quality aligned with human preferences.  
7. The dataset is made publicly available to the research community via GitHub, facilitating further innovation in this domain. <div>
arXiv:2511.16712v1 Announce Type: new 
Abstract: Personalized dual-person portrait customization has considerable potential applications, such as preserving emotional memories and facilitating wedding photography planning. However, the absence of a benchmark dataset hinders the pursuit of high-quality customization in dual-person portrait generation. In this paper, we propose the PairHuman dataset, which is the first large-scale benchmark dataset specifically designed for generating dual-person portraits that meet high photographic standards. The PairHuman dataset contains more than 100K images that capture a variety of scenes, attire, and dual-person interactions, along with rich metadata, including detailed image descriptions, person localization, human keypoints, and attribute tags. We also introduce DHumanDiff, which is a baseline specifically crafted for dual-person portrait generation that features enhanced facial consistency and simultaneously balances in personalized person generation and semantic-driven scene creation. Finally, the experimental results demonstrate that our dataset and method produce highly customized portraits with superior visual quality that are tailored to human preferences. Our dataset is publicly available at https://github.com/annaoooo/PairHuman.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images</title>
<link>https://arxiv.org/abs/2511.16717</link>
<guid>https://arxiv.org/abs/2511.16717</guid>
<content:encoded><![CDATA[
<div> Keywords: neutron imaging, inertial confinement fusion, denoising, autoencoder, wavelet transform  

<br /><br />Summary:  
Neutron imaging is crucial for analyzing inertial confinement fusion (ICF) experiments, such as those at the National Ignition Facility, helping to optimize and improve fusion platforms. However, neutron images often suffer from mixed Gaussian and Poisson noise, which obscures fine details and blurs edges. These noise types are challenging to separate and remove using conventional filtering and thresholding methods. Preserving image fidelity during noise removal is therefore essential for accurate interpretation of neutron source images. Traditional methods rely on combinations of filtering and thresholding, but machine learning approaches have rarely been applied due to the lack of ground truth data in neutron imaging. Recent advances in synthetic data generation have enabled new opportunities for applying supervised and unsupervised machine learning for denoising in fusion imaging. This study implements an unsupervised autoencoder incorporating a Cohen-Daubechies-Feauveau (CDF 97) wavelet transform within its latent space to address mixed Gaussian-Poisson noise. The proposed network effectively denoises neutron images, showing lower reconstruction errors and better edge preservation compared to non-machine learning methods such as BM3D filtering. This approach marks a significant advancement in noise reduction and three-dimensional reconstruction analysis for ICF neutron imaging experiments. <div>
arXiv:2511.16717v1 Announce Type: new 
Abstract: Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM 3: Segment Anything with Concepts</title>
<link>https://arxiv.org/abs/2511.16719</link>
<guid>https://arxiv.org/abs/2511.16719</guid>
<content:encoded><![CDATA[
<div> Segment Anything Model, promptable concept segmentation, image and video tracking, large-scale dataset, SA-Co benchmark<br /><br />Summary:<br /><br />1. This paper introduces Segment Anything Model (SAM) 3, a unified framework designed for detecting, segmenting, and tracking objects in both images and videos using concept prompts. These prompts can be short noun phrases, image exemplars, or a combination, enabling versatile input modes.  
2. The method, called Promptable Concept Segmentation (PCS), processes concept prompts to generate segmentation masks and assign unique identities to all object instances that match the given concepts.  
3. To support training and evaluation, the authors have developed a scalable data engine producing a high-quality dataset containing 4 million unique concept labels, including difficult negative samples, spanning images and videos, thus significantly enhancing diversity and robustness.  
4. The SAM 3 architecture includes an image-level object detector and a memory-based video tracker that share a unified backbone, with decoupled recognition and localization via a presence head to improve detection accuracy.  
5. Quantitative results demonstrate that SAM 3 achieves double the accuracy of prior systems for both image and video PCS tasks while enhancing previous SAM capabilities in visual segmentation. The model and the new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation are publicly released to foster further research. <div>
arXiv:2511.16719v1 Announce Type: new 
Abstract: We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge</title>
<link>https://arxiv.org/abs/2511.16743</link>
<guid>https://arxiv.org/abs/2511.16743</guid>
<content:encoded><![CDATA[
<div> Safety, Vision-Language Models, Fine-Tuning, Semantic Alignment, Zero-Shot Accuracy<br /><br />Summary:<br /><br />This paper addresses the common issue in fine-tuning vision-language models like CLIP, where improving safety often leads to significant drops in generalization performance. The authors identify that the primary cause of this trade-off is the use of rigid alignment strategies that force unsafe concepts to map onto fixed, predefined safe targets, which disrupts the semantic structure learned by the models. To overcome this, they propose a proximity-aware fine-tuning approach that redirects unsafe concepts to their semantically closest safe alternatives, minimizing changes in the model’s internal representations. They introduce SaFeR-CLIP, a novel fine-tuning framework based on this principle of minimal intervention, successfully improving safety while recovering up to 8.0% in zero-shot accuracy compared to prior methods. Additionally, to better evaluate safety under distributional shifts, the authors contribute a new benchmark dataset, NSFW-Caps, consisting of 1,000 highly-aligned pairs designed for rigorous safety testing. Overall, the work demonstrates that respecting the geometric relationships in pretrained model representations is crucial for achieving improved safety in vision-language models without compromising their generalization capabilities. <div>
arXiv:2511.16743v1 Announce Type: new 
Abstract: Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVG360: Multi-View SVG Generation with Geometric and Color Consistency from a Single SVG</title>
<link>https://arxiv.org/abs/2511.16766</link>
<guid>https://arxiv.org/abs/2511.16766</guid>
<content:encoded><![CDATA[
<div> SVG, multi-view consistency, 3D representation, Segment Anything 2, raster to vector conversion<br /><br />Summary:<br /><br />This paper addresses the challenge of generating multi-view consistent Scalable Vector Graphics (SVGs) from a single-view SVG input, which has been an underexplored area especially for single object SVGs. The authors propose a three-stage framework to achieve geometric and color consistency across different views. First, the input SVG is rasterized and converted into a 3D representation, which is then rendered from multiple camera poses to create multi-view images. Second, they extend the temporal memory mechanism of Segment Anything 2 (SAM2) into a spatial memory bank that links parts across adjacent views, enabling cleaner and more consistent vector paths and color assignments without additional training. Finally, during the raster to vector reconstruction, the framework consolidates paths and optimizes the structure to minimize redundancy while preserving semantic boundaries and fine details. The generated SVGs maintain strong geometric and color consistency between views, reduce redundant paths significantly, and retain important structural finer details. This work thus bridges generative modeling techniques with structured vector graphic representation, offering a scalable solution for generating multi-view SVGs from a single input. Applications include asset creation and semantic vector editing in design workflows. <div>
arXiv:2511.16766v1 Announce Type: new 
Abstract: Scalable Vector Graphics (SVGs) are central to modern design workflows, offering scaling without distortion and precise editability. However, for single object SVGs, generating multi-view consistent SVGs from a single-view input remains underexplored. We present a three stage framework that produces multi-view SVGs with geometric and color consistency from a single SVG input. First, the rasterized input is lifted to a 3D representation and rendered under target camera poses, producing multi-view images of the object. Next, we extend the temporal memory mechanism of Segment Anything 2 (SAM2) to the spatial domain, constructing a spatial memory bank that establishes part level correspondences across neighboring views, yielding cleaner and more consistent vector paths and color assignments without retraining. Finally, during the raster to vector conversion, we perform path consolidation and structural optimization to reduce redundancy while preserving boundaries and semantics. The resulting SVGs exhibit strong geometric and color consistency across views, significantly reduce redundant paths, and retain fine structural details. This work bridges generative modeling and structured vector representation, providing a scalable route to single input, object level multi-view SVG generation and supporting applications such as asset creation and semantic vector editing.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation</title>
<link>https://arxiv.org/abs/2511.16807</link>
<guid>https://arxiv.org/abs/2511.16807</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D meshes, autoregressive models, Mesh RAG, point cloud segmentation, incremental editing<br /><br />Summary:<br /><br />3D meshes play a vital role in numerous fields, including industrial design, gaming, simulation, and robotics, but traditional manual creation is time-consuming and not scalable. Autoregressive models have been utilized to automate mesh generation, yet these models often face a trade-off between quality and speed due to their sequential generation process, which also limits incremental editing capabilities. To address these challenges, the authors introduce Mesh RAG, a novel, training-free, and plug-and-play framework designed to enhance autoregressive mesh generation models. Mesh RAG draws inspiration from retrieval-augmented generation (RAG) methods used in language models and integrates techniques such as point cloud segmentation, spatial transformation, and point cloud registration. By relying on a retrieval-based approach, Mesh RAG decouples generation from strict sequential dependencies, allowing for efficient and parallel inference. The framework is compatible with various foundational autoregressive mesh generation models and significantly improves mesh quality while accelerating generation speed compared to traditional sequential part prediction. Furthermore, Mesh RAG enables incremental editing without the need to retrain the underlying models, enhancing flexibility and practicality in mesh generation workflows. <div>
arXiv:2511.16807v1 Announce Type: new 
Abstract: 3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldGen: From Text to Traversable and Interactive 3D Worlds</title>
<link>https://arxiv.org/abs/2511.16825</link>
<guid>https://arxiv.org/abs/2511.16825</guid>
<content:encoded><![CDATA[
<div> WorldGen, 3D worlds, text-to-3D, procedural generation, generative AI<br /><br />Summary:<br /><br />1. WorldGen is a novel system designed to automatically create large-scale, interactive 3D worlds directly from natural language text prompts. 2. The system converts textual descriptions into fully textured, traversable environments that can be explored or edited in standard game engines without requiring manual modeling or specialized 3D skills. 3. WorldGen integrates multiple advanced technologies, including large language model (LLM)-driven scene layout reasoning, procedural generation techniques, diffusion-based 3D content creation, and object-aware scene decomposition to produce coherent and functional virtual spaces. 4. The system is modular and offers fine-grained control over important parameters such as layout, scale, and visual style, ensuring that the output worlds are geometrically consistent, visually rich, and optimized for real-time rendering. 5. This work pushes forward the capabilities of 3D generative AI, making accessible, large-scale, generative world-building feasible for applications in gaming, simulation, and immersive social environments, reducing the barrier for creators to design complex virtual worlds. <div>
arXiv:2511.16825v1 Announce Type: new 
Abstract: We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation</title>
<link>https://arxiv.org/abs/2511.16853</link>
<guid>https://arxiv.org/abs/2511.16853</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, Earth Observation, Regression Tasks, REO-Instruct, Biomass Estimation  

<br /><br />Summary:  
This paper addresses the limited exploration of vision language models (VLMs) for scientific regression tasks in Earth Observation (EO), noting that most existing EO datasets focus on semantic understanding rather than quantitative prediction. To bridge this gap, the authors introduce REO-Instruct, the first unified benchmark tailored for both descriptive and regression challenges in EO. The dataset centers on a forest ecological scenario encompassing human activity detection, land-cover classification, ecological patch counting, and above-ground biomass (AGB) regression, thereby creating a cognitively interpretable logical progression from qualitative descriptions to quantitative measurements. REO-Instruct combines co-registered Sentinel-2 and ALOS-2 satellite imagery with structured textual annotations produced and validated using a hybrid human-AI approach, ensuring data quality and relevance. Comprehensive evaluation protocols along with baseline tests on generic VLMs demonstrate that current models encounter significant difficulties in numeric reasoning within this context. This finding underscores a critical need to enhance VLMs for scientific inference tasks. By offering a standardized benchmark, REO-Instruct aims to facilitate the development and assessment of next-generation geospatial models capable of integrating both descriptive understanding and rigorous scientific regression, advancing the field of Earth Observation analysis. The dataset and project details are publicly accessible on GitHub. <div>
arXiv:2511.16853v1 Announce Type: new 
Abstract: Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOP-ASK: Object-Interaction Reasoning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.16857</link>
<guid>https://arxiv.org/abs/2511.16857</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, spatial reasoning, object interaction, BOP-ASK dataset, 3D localization<br /><br />Summary:<br /><br />1. Vision Language Models (VLMs) have shown strong results on spatial reasoning benchmarks but often fail in detailed understanding of object interactions, such as precise 3D localization, physical compatibility, affordances, and multi-step spatial planning.<br />2. Existing benchmarks test only high-level spatial relations (e.g., 'left of,' 'behind') and neglect fine-grained spatial understanding essential for real-world applications.<br />3. The authors introduce BOP-ASK, a large-scale dataset designed for object interaction reasoning to both train and benchmark VLMs, containing over 150,000 images and 33 million question-answer pairs across six tasks, including four novel tasks.<br />4. BOP-ASK is created using 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets, enriched with detailed annotations such as grasp poses, relative poses, path planning trajectories, and object-to-object spatial relationships.<br />5. Evaluation of proprietary and open-source VLMs on BOP-ASK-core shows significant improvement over baselines and reveals emergent capabilities like precise object and grasp pose estimation, trajectory planning, and fine-grained spatial reasoning in cluttered scenes.<br />6. Additionally, BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, provides a means to test models’ generalization.<br />7. The dataset and the generation pipeline will be publicly released to support further research in detailed vision-language spatial reasoning. <div>
arXiv:2511.16857v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parts-Mamba: Augmenting Joint Context with Part-Level Scanning for Occluded Human Skeleton</title>
<link>https://arxiv.org/abs/2511.16860</link>
<guid>https://arxiv.org/abs/2511.16860</guid>
<content:encoded><![CDATA[
<div> Keywords: Skeleton Action Recognition, Graph Convolutional Networks, Occlusion, Parts-Mamba, Contextual Information<br /><br />Summary:<br /><br />1. This paper addresses the challenge of skeleton action recognition, which involves identifying human actions by analyzing human skeleton data captured from videos. 2. Traditional Graph Convolutional Networks (GCNs), while effective, suffer performance degradation when skeleton data is incomplete or occluded due to missing joints or frames in real-world scenarios. 3. To overcome this limitation, the authors introduce Parts-Mamba, a hybrid model combining GCN with Mamba, designed to better capture and retain contextual information from non-adjacent joints in the skeleton. 4. Parts-Mamba features a parts-specific scanning mechanism to extract detailed part-level information and a parts-body fusion module to integrate distant joint contexts, thus preserving critical local and global skeleton information despite occlusions. 5. Experimental results on benchmark datasets NTU RGB+D 60 and NTU RGB+D 120 under various occlusion conditions demonstrate that Parts-Mamba achieves up to 12.9% improvement in recognition accuracy over existing methods, highlighting its robustness and effectiveness in handling incomplete skeleton data. <div>
arXiv:2511.16860v1 Announce Type: new 
Abstract: Skeleton action recognition involves recognizing human action from human skeletons. The use of graph convolutional networks (GCNs) has driven major advances in this recognition task. In real-world scenarios, the captured skeletons are not always perfect or complete because of occlusions of parts of the human body or poor communication quality, leading to missing parts in skeletons or videos with missing frames. In the presence of such non-idealities, existing GCN models perform poorly due to missing local context. To address this limitation, we propose Parts-Mamba, a hybrid GCN-Mamba model designed to enhance the ability to capture and maintain contextual information from distant joints. The proposed Parts-Mamba model effectively captures part-specific information through its parts-specific scanning feature and preserves non-neighboring joint context via a parts-body fusion module. Our proposed model is evaluated on the NTU RGB+D 60 and NTU RGB+D 120 datasets under different occlusion settings, achieving up to 12.9% improvement in accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Joint Gromov Wasserstein Objective for Multiple Object Matching</title>
<link>https://arxiv.org/abs/2511.16868</link>
<guid>https://arxiv.org/abs/2511.16868</guid>
<content:encoded><![CDATA[
<div> Keywords: Gromov-Wasserstein distance, Joint Gromov-Wasserstein, multiple object matching, optimal transport, biomolecular complexes<br /><br />Summary:<br /><br />The paper introduces the Joint Gromov-Wasserstein (JGW) objective to extend the traditional Gromov-Wasserstein (GW) framework beyond pairwise matching, enabling simultaneous matching among collections of objects. This generalization addresses limitations in scenarios requiring multiple-to-one or multiple-to-multiple object matching. The JGW objective serves as a non-negative dissimilarity measure capable of identifying partially isomorphic distributions of metric measure spaces (mm-spaces), with guaranteed point sampling convergence. The authors demonstrate that this objective can be applied to point cloud representations by adapting existing Optimal Transport algorithms, including incorporating entropic regularization to improve computational efficiency. Comprehensive benchmarking against other GW variants for partial matching tasks shows that the method achieves superior accuracy and faster computation. Additionally, experiments conducted on synthetic and real-world datasets validate the approach in multiple shape matching problems, covering both geometric shapes and biomolecular complexes. The results indicate promising potential applications of JGW in complex matching challenges across diverse fields such as computer graphics and structural biology. <div>
arXiv:2511.16868v1 Announce Type: new 
Abstract: The Gromov-Wasserstein (GW) distance serves as a powerful tool for matching objects in metric spaces. However, its traditional formulation is constrained to pairwise matching between single objects, limiting its utility in scenarios and applications requiring multiple-to-one or multiple-to-multiple object matching. In this paper, we introduce the Joint Gromov-Wasserstein (JGW) objective and extend the original framework of GW to enable simultaneous matching between collections of objects. Our formulation provides a non-negative dissimilarity measure that identifies partially isomorphic distributions of mm-spaces, with point sampling convergence. We also show that the objective can be formulated and solved for point cloud object representations by adapting traditional algorithms in Optimal Transport, including entropic regularization. Our benchmarking with other variants of GW for partial matching indicates superior performance in accuracy and computational efficiency of our method, while experiments on both synthetic and real-world datasets show its effectiveness for multiple shape matching, including geometric shapes and biomolecular complexes, suggesting promising applications for solving complex matching problems across diverse domains, including computer graphics and structural biology.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Align &amp; Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representational Alignment</title>
<link>https://arxiv.org/abs/2511.16870</link>
<guid>https://arxiv.org/abs/2511.16870</guid>
<content:encoded><![CDATA[
<div> Keywords: representation alignment, diffusion models, inverse problems, self-supervised encoders, image reconstruction  

<br /><br />Summary: This paper explores the application of representation alignment (REPA) between diffusion or flow-based generative models and pretrained self-supervised visual encoders like DINOv2 to enhance inverse problem-solving in imaging tasks. First, it extends previous work on aligning internal model representations to improve convergence and sample quality, applying this concept specifically at inference time where ground truth is unavailable. Second, the authors demonstrate that aligning the internal representations of generative models with approximate target features enhances reconstruction fidelity and perceptual realism in tasks such as super-resolution, inpainting, and deblurring. Third, the paper provides theoretical results connecting REPA regularization to a divergence measure within the embedding space of the visual encoder and explains how REPA updates guide the generative model's representations toward those of clean images. Fourth, the approach is shown to be general and can be integrated seamlessly into multiple state-of-the-art inverse problem solvers. Finally, extensive experiments validate the method’s effectiveness, showing consistent improvements in reconstruction quality across different tasks while also achieving efficiency gains by reducing the number of discretization steps needed without degrading solver performance. <div>
arXiv:2511.16870v1 Announce Type: new 
Abstract: Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a pretrained self-supervised visual encoder, such as DINOv2, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we show that aligning model representations with approximate target features can substantially enhance reconstruction fidelity and perceptual realism. We provide theoretical results showing (a) the relation between the REPA regularization and a divergence measure in the DINOv2 embedding space, and (b) how REPA updates steer the model's internal representations toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by integrating it into multiple state-of-the-art inverse problem solvers. Extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirm that our method consistently improves reconstruction quality across tasks, while also providing substantial efficiency gains by reducing the number of required discretization steps without compromising the performance of the underlying solver.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery</title>
<link>https://arxiv.org/abs/2511.16887</link>
<guid>https://arxiv.org/abs/2511.16887</guid>
<content:encoded><![CDATA[
<div> glass surface detection, reflection dynamics, flash/no-flash imagery, Reflection Contrast Mining Module, Reflection Guided Attention Module<br /><br />Summary:<br /><br />1. Glass surfaces are common in daily life but challenging to detect in images due to their colorless and transparent nature, as well as the lack of distinctive features.<br /><br />2. Existing detection methods typically rely on boundary cues like frames or reflection cues, but they do not fully leverage the intrinsic optical properties of glass, leading to limited accuracy.<br /><br />3. The authors observe that illumination differences across glass surfaces cause reflection variations, especially when using flash/no-flash image pairs; reflections disappear when flashing from the bright side to the dark side and appear when flashing the opposite direction.<br /><br />4. Based on this phenomenon, they propose NFGlassNet, which utilizes flash/no-flash reflection dynamics to improve glass surface detection, featuring two modules: Reflection Contrast Mining Module (RCMM) to extract reflections and Reflection Guided Attention Module (RGAM) to fuse reflection and surface features.<br /><br />5. They created a new dataset of 3,300 flash/no-flash image pairs with annotated glass surfaces to train and evaluate their model.<br /><br />6. Extensive experiments demonstrate that NFGlassNet outperforms existing state-of-the-art methods in accuracy.<br /><br />7. The authors intend to release their code, model, and dataset upon paper acceptance for further research and application. <div>
arXiv:2511.16887v1 Announce Type: new 
Abstract: Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios</title>
<link>https://arxiv.org/abs/2511.16901</link>
<guid>https://arxiv.org/abs/2511.16901</guid>
<content:encoded><![CDATA[
<div> Keywords: audio-visual reasoning, spatio-temporal annotation, multimodal large language models, reinforcement learning, dataset

<br /><br />Summary: Recently, significant progress has been made in multimodal large language models (MLLMs) focusing on video understanding, but existing work mainly addresses simplistic video scenarios, lacking the complexity found in real-world audio-visual events. To address this, the paper introduces R-AVST, a new dataset designed for audio-visual reasoning with detailed spatio-temporal annotations. The dataset creation pipeline integrates LLM-based key object extraction, automatic spatial annotation, and manual quality checks, producing over 5,000 untrimmed videos containing 27,000 objects spanning 100 types of audio-visual events. Using R-AVST, the authors define three essential tasks for spatio-temporal reasoning in audio-visual contexts and generate over 8,000 balanced, high-quality question-answer pairs to benchmark models effectively. Additionally, the paper proposes AVST-Zero, a reinforcement learning-based model that eliminates intermediate supervision by optimizing directly with multi-dimensional reward signals, aiming to improve reasoning capabilities. Extensive experiments demonstrate that R-AVST significantly advances the study of audio-visual spatio-temporal reasoning, and AVST-Zero achieves competitive performance relative to existing models. This work marks the first dataset crafted for real-world audio-visual spatio-temporal reasoning and introduces a novel methodological approach to future challenges in this field. <div>
arXiv:2511.16901v1 Announce Type: new 
Abstract: Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models</title>
<link>https://arxiv.org/abs/2511.16904</link>
<guid>https://arxiv.org/abs/2511.16904</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion probabilistic models, hot diffusion, cold diffusion, Blur-Noise Mixture Diffusion Model, image generation<br /><br />Summary:<br /><br />This paper addresses two prevailing types of diffusion probabilistic models in image generation: hot diffusion, which relies solely on noise, and cold diffusion, which uses only blurring without noise. It points out that hot diffusion overlooks the intrinsic correlation between high-frequency image details and low-frequency structures, resulting in randomness during early generative steps. On the other hand, cold diffusion leverages these image correlations but ignores the essential role of noise in defining the data manifold, which causes out-of-manifold generation issues and leads to lower performance. To combine the strengths of both paradigms, the authors introduce Warm Diffusion, a novel Blur-Noise Mixture Diffusion Model (BNMD) that simultaneously controls blurring and noise. This approach uses a divide-and-conquer strategy exploiting spectral dependencies in images to separate the denoising and deblurring tasks, simplifying the training of the score model. Additionally, the paper presents a Blur-to-Noise Ratio (BNR) framework based on spectral analysis to study the trade-offs between learning dynamics and manifold changes. Extensive empirical evaluations demonstrate that Warm Diffusion outperforms previous methods on multiple image generation benchmarks, validating its effectiveness and theoretical contributions. <div>
arXiv:2511.16904v1 Announce Type: new 
Abstract: Diffusion probabilistic models have achieved remarkable success in generative tasks across diverse data types. While recent studies have explored alternative degradation processes beyond Gaussian noise, this paper bridges two key diffusion paradigms: hot diffusion, which relies entirely on noise, and cold diffusion, which uses only blurring without noise. We argue that hot diffusion fails to exploit the strong correlation between high-frequency image detail and low-frequency structures, leading to random behaviors in the early steps of generation. Conversely, while cold diffusion leverages image correlations for prediction, it neglects the role of noise (randomness) in shaping the data manifold, resulting in out-of-manifold issues and partially explaining its performance drop. To integrate both strengths, we propose Warm Diffusion, a unified Blur-Noise Mixture Diffusion Model (BNMD), to control blurring and noise jointly. Our divide-and-conquer strategy exploits the spectral dependency in images, simplifying score model estimation by disentangling the denoising and deblurring processes. We further analyze the Blur-to-Noise Ratio (BNR) using spectral analysis to investigate the trade-off between model learning dynamics and changes in the data manifold. Extensive experiments across benchmarks validate the effectiveness of our approach for image generation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content</title>
<link>https://arxiv.org/abs/2511.16908</link>
<guid>https://arxiv.org/abs/2511.16908</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated images, quality assessment, realism, plausibility, multi-modal large language models<br /><br />Summary:<br /><br />1. This paper addresses the need for more detailed quality assessment of AI-generated content, highlighting that existing datasets usually provide only a single coarse quality score which limits targeted improvements in generative models.<br /><br />2. The authors identify realism and plausibility as two critical evaluation dimensions specifically important for AI-generated images and for enhancing the capabilities of unified generation-understanding models.<br /><br />3. They introduce Q-Real, a novel dataset containing 3,088 images from popular text-to-image models, accompanied by detailed annotations of entity locations and a set of judgment questions and attribution descriptions focused on realism and plausibility.<br /><br />4. To effectively leverage recent advances in multi-modal large language models (MLLMs), the authors construct the Q-Real Bench benchmarking framework, which evaluates models on judgment and grounding with reasoning tasks using the dataset.<br /><br />5. The paper also proposes a fine-tuning framework for improving MLLM capabilities, supported by extensive experiments demonstrating the quality, relevance, and comprehensive nature of the dataset and benchmark. The dataset and code will be publicly released upon publication. <div>
arXiv:2511.16908v1 Announce Type: new 
Abstract: Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.16917</link>
<guid>https://arxiv.org/abs/2511.16917</guid>
<content:encoded><![CDATA[
<div> Keywords: UniModel, unified diffusion transformer, pixel-to-pixel framework, multimodal learning, visual-text representations<br /><br />Summary:<br /><br />1. The article introduces UniModel, a unified generative model designed to handle both visual understanding and visual generation within a single pixel-to-pixel diffusion framework, aiming for unification across model, tasks, and representations. <br /><br />2. At the representation level, the model removes modality differences by converting text prompts into painted text images on a clean canvas so that all input and output data are treated as RGB pixel images, enabling a fully vision-native multimodal learning approach. <br /><br />3. At the task level, various vision-language tasks are expressed as pixel-to-pixel transformations: for understanding, the model converts RGB images into painted text images encoding semantic predictions; for generation, painted text images guide image synthesis, treating captioning and text-to-image generation as two directions of a visual translation process. <br /><br />4. At the model level, UniModel uses a single Unified Diffusion Transformer trained with rectified flow on pixel-space data, employing a shared backbone for bidirectional mappings between images and painted text images, directed by lightweight task embeddings. <br /><br />5. Experiments demonstrate strong cross-modal alignment and controllability, including cycle-consistent image-caption-image loops, indicating that unifying model, tasks, and representations in a single visual space is a promising approach for general-purpose multimodal intelligence. <div>
arXiv:2511.16917v1 Announce Type: new 
Abstract: We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution</title>
<link>https://arxiv.org/abs/2511.16920</link>
<guid>https://arxiv.org/abs/2511.16920</guid>
<content:encoded><![CDATA[
<div> Anomaly generation, zero-shot, diffusion models, localization, latent inpainting<br /><br />Summary:<br /><br />This paper addresses anomaly generation without access to real anomalous samples or prior training, a critical challenge as existing methods rely on few-shot fine-tuning with scarce anomaly data. The authors introduce Delta-Denoising (DeltaDeno), a zero-shot, training-free approach utilizing diffusion models with two contrasting branches guided by a minimal prompt pair and a shared denoising schedule. DeltaDeno localizes defects by accumulating per-step denoising deltas into an image-specific localization map, which creates a mask used for latent inpainting at later diffusion steps, enabling the generation of realistic local anomalies while preserving surrounding context. To enhance control and stability, the method refines prompts at the token level, aligning shared content and emphasizing anomaly-related tokens. Additionally, a spatial attention bias restricted to anomaly tokens focuses generation on predicted defective regions. Experiments on public datasets demonstrate that DeltaDeno produces high-quality, realistic anomaly generations that consistently boost performance in downstream anomaly detection tasks. The paper also promises to release the code publicly, enabling further research and practical application. Overall, DeltaDeno offers an innovative, effective solution for zero-shot anomaly generation leveraging diffusion models without training data. <div>
arXiv:2511.16920v1 Announce Type: new 
Abstract: Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Diffusion Model-Based Video Super-Resolution: Leveraging Dense Guidance from Aligned Features</title>
<link>https://arxiv.org/abs/2511.16928</link>
<guid>https://arxiv.org/abs/2511.16928</guid>
<content:encoded><![CDATA[
<div> Diffusion Model, Video Super-Resolution, Feature Alignment, Optical Guided Warping, Temporal Consistency

<br /><br />Summary: This paper addresses challenges in Diffusion Model (DM)-based Video Super-Resolution (VSR) methods, which, despite producing high perceptual quality, face issues like error accumulation, spatial artifacts, and a trade-off between perceptual quality and fidelity. The authors analyze the alignment and compensation processes between adjacent video frames and make two key observations: firstly, compensating in the feature domain is more effective than the pixel domain due to stronger spatial and temporal correlations; secondly, warping at an upscaled resolution better preserves high-frequency details, although this advantage is not strictly monotonic. Based on these insights, the paper introduces DGAF-VSR—a novel Dense Guided diffusion model with Aligned Features—incorporating an Optical Guided Warping Module (OGWM) designed to maintain high-frequency information in aligned features, and a Feature-wise Temporal Condition Module (FTCM) that provides dense guidance within the feature domain. Comprehensive experiments conducted on both synthetic and real-world datasets reveal that DGAF-VSR outperforms state-of-the-art VSR methods across multiple metrics, achieving a 35.82% reduction in DISTS for perceptual quality, a 0.20 dB gain in PSNR for fidelity, and a 30.37% decrease in tLPIPS for temporal consistency. These results demonstrate the effectiveness of the proposed approach in enhancing video super-resolution performance while mitigating common issues associated with prior methods. <div>
arXiv:2511.16928v1 Announce Type: new 
Abstract: Diffusion model (DM) based Video Super-Resolution (VSR) approaches achieve impressive perceptual quality. However, they suffer from error accumulation, spatial artifacts, and a trade-off between perceptual quality and fidelity, primarily caused by inaccurate alignment and insufficient compensation between video frames. In this paper, within the DM-based VSR pipeline, we revisit the role of alignment and compensation between adjacent video frames and reveal two crucial observations: (a) the feature domain is better suited than the pixel domain for information compensation due to its stronger spatial and temporal correlations, and (b) warping at an upscaled resolution better preserves high-frequency information, but this benefit is not necessarily monotonic. Therefore, we propose a novel Densely Guided diffusion model with Aligned Features for Video Super-Resolution (DGAF-VSR), with an Optical Guided Warping Module (OGWM) to maintain high-frequency details in the aligned features and a Feature-wise Temporal Condition Module (FTCM) to deliver dense guidance in the feature domain. Extensive experiments on synthetic and real-world datasets demonstrate that DGAF-VSR surpasses state-of-the-art methods in key aspects of VSR, including perceptual quality (35.82\% DISTS reduction), fidelity (0.20 dB PSNR gain), and temporal consistency (30.37\% tLPIPS reduction).
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape-preserving Tooth Segmentation from CBCT Images Using Deep Learning with Semantic and Shape Awareness</title>
<link>https://arxiv.org/abs/2511.16936</link>
<guid>https://arxiv.org/abs/2511.16936</guid>
<content:encoded><![CDATA[
<div> tooth segmentation, cone beam computed tomography, deep learning, shape preservation, multi-task learning<br /><br />Summary:<br /><br />Accurate segmentation of teeth from cone beam computed tomography (CBCT) images is essential for advancing digital dentistry, yet this process is complicated by interdental adhesions that distort tooth anatomy. To tackle this challenge, the authors propose a deep learning framework that integrates both semantic and shape awareness to maintain shape fidelity during segmentation. A key innovation is the target-tooth-centroid prompted multi-label learning approach, designed to capture semantic relationships between individual teeth and reduce shape ambiguities that arise in complex dental structures. Complementing this, the method incorporates a tooth-shape-aware learning mechanism that enforces morphological constraints, thereby preserving the anatomical boundaries of each tooth with higher accuracy. These two components are effectively combined through a multi-task learning strategy, which simultaneously optimizes segmentation precision and shape preservation. Extensive evaluations conducted on both internal and external datasets reveal that this approach significantly outperforms current state-of-the-art methods. Consequently, the proposed framework successfully mitigates common shape distortions from CBCT images and produces anatomically faithful tooth boundaries, promising robust applications in digital dentistry workflows. <div>
arXiv:2511.16936v1 Announce Type: new 
Abstract: Background:Accurate tooth segmentation from cone beam computed tomography (CBCT) images is crucial for digital dentistry but remains challenging in cases of interdental adhesions, which cause severe anatomical shape distortion.
  Methods:
  To address this, we propose a deep learning framework that integrates semantic and shape awareness for shape-preserving segmentation. Our method introduces a target-tooth-centroid prompted multi-label learning strategy to model semantic relationships between teeth, reducing shape ambiguity. Additionally, a tooth-shape-aware learning mechanism explicitly enforces morphological constraints to preserve boundary integrity. These components are unified via multi-task learning, jointly optimizing segmentation and shape preservation.
  Results: Extensive evaluations on internal and external datasets demonstrate that our approach significantly outperforms existing methods.
  Conclusions: Our approach effectively mitigates shape distortions and providing anatomically faithful tooth boundaries.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios</title>
<link>https://arxiv.org/abs/2511.16937</link>
<guid>https://arxiv.org/abs/2511.16937</guid>
<content:encoded><![CDATA[
<div> Spatio-Temporal Video Grounding, OmniGround, Forward-Backward-Refinement, DeepSTG, PG-TAF  

<br /><br />Summary:  
This paper addresses the challenges in Spatio-Temporal Video Grounding (STVG), which involves localizing objects in videos based on natural language descriptions. Current Multimodal Large Language Models underperform due to limited benchmark scope, causing issues like category bias, oversimplified reasoning, and lack of linguistic robustness. To overcome these, the authors introduce OmniGround, a comprehensive benchmark containing 3,475 videos across 81 categories with complex real-world queries to better represent diverse scenarios. They develop the Forward-Backward-Refinement annotation pipeline that uses multi-directional tracking combined with intelligent error correction to produce high-quality annotations. Additionally, they propose DeepSTG, an evaluation framework that measures dataset quality across four dimensions beyond simple statistics, providing deeper insights into dataset challenges. Experimental results show a significant 10.4% average performance drop when models are tested on complex real-world scenes, especially with small or occluded objects and complex spatial relations. To improve performance, the authors present PG-TAF, a training-free two-stage framework that decomposes STVG into high-level temporal grounding followed by fine-grained spatio-temporal propagation. PG-TAF achieves substantial improvements of 25.6% in mean temporal Intersection over Union (m_tIoU) and 35.6% in mean video Intersection over Union (m_vIoU) on OmniGround, with consistent gains demonstrated across four established benchmarks. <div>
arXiv:2511.16937v1 Announce Type: new 
Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.16940</link>
<guid>https://arxiv.org/abs/2511.16940</guid>
<content:encoded><![CDATA[
<div> Privacy, Vision-Language Models, Privacy Reasoning, Benchmark, Dataset<br /><br />Summary:<br /><br />1. Modern Vision-Language Models (VLMs) have advanced reasoning capabilities that increase privacy risks, moving beyond simple attribute perception to the more critical threat of individual-level linkage. <br />2. Current privacy benchmarks are inadequate as they mainly assess privacy perception and do not evaluate a VLM's ability to reason across distributed information to infer personal profiles. <br />3. To fill this gap, the authors propose MultiPriv, the first comprehensive benchmark focused on evaluating individual-level privacy reasoning in VLMs, supported by the Privacy Perception and Reasoning (PPR) framework.<br />4. MultiPriv includes a novel bilingual multimodal dataset featuring synthetic individual profiles that meticulously link identifiers such as faces and names to sensitive attributes, enabling nine challenging tasks from attribute detection to cross-image re-identification and chained inference.<br />5. A large-scale evaluation across over 50 foundational and commercial VLMs reveals significant unmeasured reasoning-based privacy risks, shows that perception-level metrics poorly predict these reasoning risks, and highlights that current safety alignments are inconsistent and largely ineffective against such privacy reasoning attacks.<br /><br />MultiPriv therefore exposes systemic vulnerabilities in existing VLMs and provides a necessary framework to advance the development of robust, privacy-preserving vision-language models. <div>
arXiv:2511.16940v1 Announce Type: new 
Abstract: Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow-Guided Implicit Neural Representation for Motion-Aware Dynamic MRI Reconstruction</title>
<link>https://arxiv.org/abs/2511.16948</link>
<guid>https://arxiv.org/abs/2511.16948</guid>
<content:encoded><![CDATA[
<div> Dynamic MRI, implicit neural representation, optical flow, motion compensation, joint optimization  

<br /><br />Summary:  
This paper addresses the challenges in dynamic magnetic resonance imaging (dMRI) reconstruction caused by limited sampling and motion-induced artifacts. Traditional motion-compensated approaches rely on pre-estimated optical flow, which suffers from inaccuracies under undersampling and negatively impacts reconstruction quality. To overcome this, the authors propose a novel implicit neural representation (INR) framework that simultaneously models both the dynamic image sequence and its underlying motion field. The approach uses one INR to parameterize the spatiotemporal image content and another INR to represent the optical flow, coupling them through a physics-inspired optical flow equation as a regularization term. Additionally, a data consistency loss ensures alignment with k-space measurements, enabling a joint optimization process that recovers temporally coherent images and motion fields without prior flow estimation. Experiments on cardiac dMRI datasets demonstrate that this method outperforms current state-of-the-art motion-compensated and deep learning reconstruction techniques, achieving higher image quality, more accurate motion estimation, and improved temporal fidelity. The work highlights the promise of implicit joint modeling with flow-regularized constraints as a powerful tool to advance dynamic MRI reconstruction. <div>
arXiv:2511.16948v1 Announce Type: new 
Abstract: Dynamic magnetic resonance imaging (dMRI) captures temporally-resolved anatomy but is often challenged by limited sampling and motion-induced artifacts. Conventional motion-compensated reconstructions typically rely on pre-estimated optical flow, which is inaccurate under undersampling and degrades reconstruction quality. In this work, we propose a novel implicit neural representation (INR) framework that jointly models both the dynamic image sequence and its underlying motion field. Specifically, one INR is employed to parameterize the spatiotemporal image content, while another INR represents the optical flow. The two are coupled via the optical flow equation, which serves as a physics-inspired regularization, in addition to a data consistency loss that enforces agreement with k-space measurements. This joint optimization enables simultaneous recovery of temporally coherent images and motion fields without requiring prior flow estimation. Experiments on dynamic cardiac MRI datasets demonstrate that the proposed method outperforms state-of-the-art motion-compensated and deep learning approaches, achieving superior reconstruction quality, accurate motion estimation, and improved temporal fidelity. These results highlight the potential of implicit joint modeling with flow-regularized constraints for advancing dMRI reconstruction.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FingerCap: Fine-grained Finger-level Hand Motion Captioning</title>
<link>https://arxiv.org/abs/2511.16951</link>
<guid>https://arxiv.org/abs/2511.16951</guid>
<content:encoded><![CDATA[
<div> Finger-level hand motion, hand motion captioning, FingerCap-40K dataset, FiGOP, Video-MLLM evaluation<br /><br />Summary:<br /><br />1. This work introduces Fine-grained Finger-level Hand Motion Captioning (FingerCap), a novel task focused on generating detailed textual descriptions that accurately capture finger-level semantics during hand actions.<br />2. To facilitate research in this area, the authors curate FingerCap-40K, a large-scale dataset containing 40,000 paired videos and captions, developed from two sources: concise instruction-style finger motions and naturalistic hand-object interactions, ensuring variety and depth.<br />3. For evaluation, they propose HandJudge, an LLM-based rubric designed to assess finger-level correctness and motion completeness, which provides a fine-grained, automated metric tailored to the task.<br />4. Recognizing the challenge of temporal sparsity in video-based large multimodal models (Video-MLLMs), especially since sparse RGB sampling misses subtle finger dynamics, they introduce FiGOP (Finger Group-of-Pictures). FiGOP pairs each RGB keyframe with subsequent hand keypoints until the next RGB frame, and encodes these via a lightweight temporal module to recover fine temporal cues.<br />5. Experiments on the FingerCap-40K dataset demonstrate that existing open- and closed-source Video-MLLMs struggle with finger-level understanding, while incorporating FiGOP yields consistent performance improvements as measured by HandJudge and human evaluations, offering a computationally efficient and effective solution. <div>
arXiv:2511.16951v1 Announce Type: new 
Abstract: Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point-Supervised Facial Expression Spotting with Gaussian-Based Instance-Adaptive Intensity Modeling</title>
<link>https://arxiv.org/abs/2511.16952</link>
<guid>https://arxiv.org/abs/2511.16952</guid>
<content:encoded><![CDATA[
<div> Keywords: facial expression spotting, point-supervised learning, Gaussian-based intensity modeling, apex classification, contrastive loss  

<br /><br />Summary: This paper addresses the challenge of automatic facial expression spotting in untrimmed videos, focusing on point-supervised learning that requires only a single timestamp annotation per expression instance for training, reducing the annotation burden compared to fully-supervised methods. The authors propose a novel two-branch framework for point-supervised facial expression spotting (P-FES). The first branch introduces a Gaussian-based instance-adaptive intensity modeling (GIM) module that models expression intensity distributions using soft pseudo-labeling, which helps differentiate neutral frames from expressions by detecting pseudo-apex frames and estimating expression durations. The second branch is a class-aware apex classification component that categorizes macro- and micro-expressions based solely on detected apex frames. During inference, these branches function independently, with one generating class-agnostic expression proposals and the other classifying expression types. Additionally, an intensity-aware contrastive loss is designed to improve feature discrimination by contrasting expression frames of varying intensities against neutral frames, effectively suppressing neutral noise. Comprehensive experiments on SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets validate the framework’s effectiveness, demonstrating improved facial expression spotting performance under point-supervised conditions, offering a practical and less costly annotation alternative. <div>
arXiv:2511.16952v1 Announce Type: new 
Abstract: Automatic facial expression spotting, which aims to identify facial expression instances in untrimmed videos, is crucial for facial expression analysis. Existing methods primarily focus on fully-supervised learning and rely on costly, time-consuming temporal boundary annotations. In this paper, we investigate point-supervised facial expression spotting (P-FES), where only a single timestamp annotation per instance is required for training. We propose a unique two-branch framework for P-FES. First, to mitigate the limitation of hard pseudo-labeling, which often confuses neutral and expression frames with various intensities, we propose a Gaussian-based instance-adaptive intensity modeling (GIM) module to model instance-level expression intensity distribution for soft pseudo-labeling. By detecting the pseudo-apex frame around each point label, estimating the duration, and constructing an instance-level Gaussian distribution, GIM assigns soft pseudo-labels to expression frames for more reliable intensity supervision. The GIM module is incorporated into our framework to optimize the class-agnostic expression intensity branch. Second, we design a class-aware apex classification branch that distinguishes macro- and micro-expressions solely based on their pseudo-apex frames. During inference, the two branches work independently: the class-agnostic expression intensity branch generates expression proposals, while the class-aware apex-classification branch is responsible for macro- and micro-expression classification.Furthermore, we introduce an intensity-aware contrastive loss to enhance discriminative feature learning and suppress neutral noise by contrasting neutral frames with expression frames with various intensities. Extensive experiments on the SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets demonstrate the effectiveness of our proposed framework.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</title>
<link>https://arxiv.org/abs/2511.16955</link>
<guid>https://arxiv.org/abs/2511.16955</guid>
<content:encoded><![CDATA[
<div> Keywords: Group Relative Policy Optimization, flow matching models, deterministic ODE sampling, Neighbor GRPO, contrastive learning<br /><br />Summary:<br /><br />This paper addresses challenges in applying Group Relative Policy Optimization (GRPO) to modern flow matching generative models, which rely on deterministic sampling through Ordinary Differential Equations (ODEs). Existing approaches convert ODEs to Stochastic Differential Equations (SDEs) to introduce necessary stochasticity, but this results in inefficient credit assignment and incompatibility with high-order solvers designed for fewer-step sampling. The authors first reinterpret SDE-based GRPO methods from a distance optimization perspective, identifying their mechanism as akin to contrastive learning. Building on this insight, they propose Neighbor GRPO, a new alignment algorithm that avoids the drawbacks of SDEs by generating diverse candidate trajectories through perturbations of initial noise in ODE sampling. Neighbor GRPO uses a softmax distance-based surrogate leaping policy to optimize the model, thereby preserving the efficiency and solver compatibility of deterministic ODE sampling. The paper also introduces symmetric anchor sampling to improve computational efficiency and group-wise quasi-norm reweighting to mitigate reward flattening issues. Extensive experiments demonstrate that Neighbor GRPO outperforms SDE-based GRPO methods in training cost, speed of convergence, and quality of generated images and videos, offering a theoretically grounded and practically superior approach to aligning generative models with human preferences. <div>
arXiv:2511.16955v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatPedia: A Universal Generative Foundation for High-Fidelity Material Synthesis</title>
<link>https://arxiv.org/abs/2511.16957</link>
<guid>https://arxiv.org/abs/2511.16957</guid>
<content:encoded><![CDATA[
<div> Physically-based rendering, material synthesis, joint RGB-PBR representation, video diffusion, text-to-material generation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating Physically-Based Rendering (PBR) materials, which are essential for photorealistic graphics but traditionally require significant labor and expertise.<br />2. Existing generative models fall short due to the lack of a unified representation that connects natural RGB image appearance with PBR material properties, resulting in fragmented and task-specific pipelines.<br />3. The authors introduce MatPedia, a foundation model that employs a novel joint RGB-PBR representation encoding materials into two interconnected latent spaces—one representing RGB appearance and the other encoding four PBR maps reflecting physical properties.<br />4. MatPedia models these latents as a 5-frame sequence and utilizes video diffusion architectures to capture correlations effectively while leveraging visual priors from RGB generation models.<br />5. By training on MatHybrid-410K, a combined dataset of PBR and large-scale RGB images, MatPedia supports multiple material generation tasks—text-to-material, image-to-material, and intrinsic decomposition—within a unified framework, producing high-resolution (1024×1024) outputs with superior quality and diversity compared to previous methods. <div>
arXiv:2511.16957v1 Announce Type: new 
Abstract: Physically-based rendering (PBR) materials are fundamental to photorealistic graphics, yet their creation remains labor-intensive and requires specialized expertise. While generative models have advanced material synthesis, existing methods lack a unified representation bridging natural image appearance and PBR properties, leading to fragmented task-specific pipelines and inability to leverage large-scale RGB image data. We present MatPedia, a foundation model built upon a novel joint RGB-PBR representation that compactly encodes materials into two interdependent latents: one for RGB appearance and one for the four PBR maps encoding complementary physical properties. By formulating them as a 5-frame sequence and employing video diffusion architectures, MatPedia naturally captures their correlations while transferring visual priors from RGB generation models. This joint representation enables a unified framework handling multiple material tasks--text-to-material generation, image-to-material generation, and intrinsic decomposition--within a single architecture. Trained on MatHybrid-410K, a mixed corpus combining PBR datasets with large-scale RGB images, MatPedia achieves native $1024\times1024$ synthesis that substantially surpasses existing approaches in both quality and diversity.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Heads Better than One: Dual Degradation Representation for Blind Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16963</link>
<guid>https://arxiv.org/abs/2511.16963</guid>
<content:encoded><![CDATA[
<div> Keywords: blind super-resolution, degradation extractor, noise embedding, blur embedding, state-of-the-art

<br /><br />Summary:  
This paper addresses the challenge of blind single image super-resolution (SISR), where the degradation process is unknown and may include noise and blur, unlike traditional methods assuming known and fixed degradations such as bicubic downsampling. The authors propose a Dual Branch Degradation Extractor Network that predicts two unsupervised degradation embeddings capturing blurry and noisy aspects of the degradation separately. This dual embedding enables the super-resolution network to adapt distinctively to both blur and noise, improving restoration quality under diverse degradations. Additionally, the degradation extractor serves as a regularizer by leveraging differences between super-resolved (SR) and high-resolution (HR) images, enhancing model robustness. Extensive experiments conducted on multiple standard benchmarks validate the approach, demonstrating its superior performance compared to existing blind SR methods. Overall, the proposed method achieves state-of-the-art results in handling blind degradation scenarios by explicitly modeling and disentangling noise and blur components within an unsupervised learning framework, allowing more accurate and flexible super-resolution reconstruction. <div>
arXiv:2511.16963v1 Announce Type: new 
Abstract: Previous methods have demonstrated remarkable performance in single image super-resolution (SISR) tasks with known and fixed degradation (e.g., bicubic downsampling). However, when the actual degradation deviates from these assumptions, these methods may experience significant declines in performance. In this paper, we propose a Dual Branch Degradation Extractor Network to address the blind SR problem. While some blind SR methods assume noise-free degradation and others do not explicitly consider the presence of noise in the degradation model, our approach predicts two unsupervised degradation embeddings that represent blurry and noisy information. The SR network can then be adapted to blur embedding and noise embedding in distinct ways. Furthermore, we treat the degradation extractor as a regularizer to capitalize on differences between SR and HR images. Extensive experiments on several benchmarks demonstrate our method achieves SOTA performance in the blind SR problem.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices</title>
<link>https://arxiv.org/abs/2511.16965</link>
<guid>https://arxiv.org/abs/2511.16965</guid>
<content:encoded><![CDATA[
<div> Keywords: cooking progression, edge devices, image synthesis, culinary image similarity, doneness levels

<br /><br />Summary: This paper addresses the challenge of synthesizing realistic cooked food images from raw inputs directly on edge devices, a task complicated by the need to capture intricate changes in texture, color, and structure during cooking. The authors introduce the first oven-based cooking-progression dataset annotated by chefs with doneness levels, which serves as a valuable resource for training and evaluating models in this domain. To enable user-preferred visual outcomes rather than fixed presets, they propose an edge-efficient generator guided by recipe and cooking state inputs that conditions the image synthesis process on the raw food image. Ensuring both temporal consistency and culinary plausibility, they develop a domain-specific Culinary Image Similarity (CIS) metric; this novel metric functions as both a training loss and a means to monitor cooking progression. The proposed model demonstrates superior performance compared to existing baselines, achieving significant reductions in Fréchet Inception Distance (FID) scores, with a 30% improvement on their own dataset and a 60% improvement on public datasets. Overall, this work contributes a practical and effective approach to realistic food image synthesis suitable for deployment on resource-constrained edge devices. <div>
arXiv:2511.16965v1 Announce Type: new 
Abstract: Synthesizing realistic cooked food images from raw inputs on edge devices is a challenging generative task, requiring models to capture complex changes in texture, color and structure during cooking. Existing image-to-image generation methods often produce unrealistic results or are too resource-intensive for edge deployment. We introduce the first oven-based cooking-progression dataset with chef-annotated doneness levels and propose an edge-efficient recipe and cooking state guided generator that synthesizes realistic food images conditioned on raw food image. This formulation enables user-preferred visual targets rather than fixed presets. To ensure temporal consistency and culinary plausibility, we introduce a domain-specific \textit{Culinary Image Similarity (CIS)} metric, which serves both as a training loss and a progress-monitoring signal. Our model outperforms existing baselines with significant reductions in FID scores (30\% improvement on our dataset; 60\% on public datasets)
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Finer the Better: Towards Granular-aware Open-set Domain Generalization</title>
<link>https://arxiv.org/abs/2511.16979</link>
<guid>https://arxiv.org/abs/2511.16979</guid>
<content:encoded><![CDATA[
<div> Open-Set Domain Generalization, CLIP, Semantic enhancement, Contrastive learning, Diffusion module<br /><br />Summary: The paper addresses Open-Set Domain Generalization (OSDG), focusing on scenarios where models encounter both domain shifts and novel categories. It highlights the challenge of balancing structural risk for known classes with open-space risk for unknowns, especially with "hard unknowns" that closely resemble known classes. To tackle this, the authors propose Semantic-enhanced CLIP (SeeCLIP), which introduces fine-grained semantic enhancement to improve vision-language alignment beyond broad category labels. SeeCLIP includes a semantic-aware prompt enhancement module that decomposes images into discriminative semantic tokens for more nuanced representation. The model employs duplex contrastive learning with two complementary objectives: repulsion to keep unknowns separate from known classes, and cohesion to maintain semantic closeness within unknown prompts. Additionally, a semantic-guided diffusion module synthesizes pseudo-unknown samples by perturbing semantic tokens, creating challenging hard negatives that resemble known classes but contain subtle local differences. These hard negatives help the model learn finer decision boundaries. Experiments on five benchmarks show that SeeCLIP consistently outperforms state-of-the-art approaches, achieving improvements of 3% in accuracy and 5% in H-score, demonstrating its effectiveness in handling OSDG challenges. <div>
arXiv:2511.16979v1 Announce Type: new 
Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Driven Natural Selection for Compact 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.16980</link>
<guid>https://arxiv.org/abs/2511.16980</guid>
<content:encoded><![CDATA[
<div> 3DGS, pruning, opacity decay, natural selection, rendering quality<br /><br />Summary:<br /><br />This paper addresses the high storage and computational costs of 3DGS, which uses numerous Gaussian primitives to represent 3D scenes. Existing pruning methods either depend on manually designed criteria or introduce extra learnable parameters, leading to suboptimal pruning outcomes. To overcome these limitations, the authors propose a novel pruning framework inspired by natural selection. This approach models survival pressure as a regularization gradient field applied to the opacity of Gaussians, allowing optimization gradients—driven by the goal of maximizing rendering quality—to autonomously decide which Gaussians to keep or prune. This fully learnable framework requires no human intervention. Additionally, the authors introduce an opacity decay technique guided by a finite opacity prior, which speeds up the pruning process without reducing its effectiveness. Experimental results show that compared to the original 3DGS method, their approach achieves over 0.6 dB PSNR improvement under a 15% budget constraint, demonstrating state-of-the-art performance for compact 3DGS representations. The project page with further details is available at https://xiaobin2001.github.io/GNS-web. <div>
arXiv:2511.16980v1 Announce Type: new 
Abstract: 3DGS employs a large number of Gaussian primitives to fit scenes, resulting in substantial storage and computational overhead. Existing pruning methods rely on manually designed criteria or introduce additional learnable parameters, yielding suboptimal results. To address this, we propose an natural selection inspired pruning framework that models survival pressure as a regularization gradient field applied to opacity, allowing the optimization gradients--driven by the goal of maximizing rendering quality--to autonomously determine which Gaussians to retain or prune. This process is fully learnable and requires no human intervention. We further introduce an opacity decay technique with a finite opacity prior, which accelerates the selection process without compromising pruning effectiveness. Compared to 3DGS, our method achieves over 0.6 dB PSNR gain under 15\% budgets, establishing state-of-the-art performance for compact 3DGS. Project page https://xiaobin2001.github.io/GNS-web.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection</title>
<link>https://arxiv.org/abs/2511.16982</link>
<guid>https://arxiv.org/abs/2511.16982</guid>
<content:encoded><![CDATA[
<div> Keywords: Plant disease detection, Deep neural network ensembles, Ensemble diversity, Synergistic Diversity (SQ), Image-based analysis  

<br /><br />Summary:  
Plant diseases are a major threat to global agriculture, causing significant economic losses exceeding $220 billion annually and endangering food security. Accurate and timely detection of diseases from plant leaf images is essential to reduce these impacts. Deep neural network ensembles (Deep Ensembles) have become a powerful method to boost prediction accuracy by combining different deep neural networks (DNNs). However, identifying which member models to include in an ensemble remains difficult due to challenges in effectively measuring ensemble diversity. This paper addresses these challenges by first analyzing the shortcomings of existing ensemble diversity metrics, known as Q metrics, which often fail to select the best ensemble combinations. The authors then propose a novel diversity metric called the Synergistic Diversity (SQ) framework, designed to measure the synergy between ensemble members more effectively and to correlate consistently with ensemble accuracy. Extensive experiments on a plant leaf image dataset validate the SQ metric, showing that it improves the selection of ensemble members and leads to enhanced detection accuracy. These results highlight the SQ framework’s potential to enable more reliable and efficient image-based plant disease detection methods in agricultural applications. <div>
arXiv:2511.16982v1 Announce Type: new 
Abstract: Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadioKMoE: Knowledge-Guided Radiomap Estimation with Kolmogorov-Arnold Networks and Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.16986</link>
<guid>https://arxiv.org/abs/2511.16986</guid>
<content:encoded><![CDATA[
<div> Radiomap Estimation, Kolmogorov-Arnold Networks, Mixture-of-Experts, Wireless Networks, Signal Propagation<br /><br />Summary:  
Radiomap estimation (RME) is crucial for managing and deploying wireless networks by providing spatial insights into signal coverage and propagation. However, the complexity of radio propagation and environmental factors makes accurate estimation challenging. This paper introduces RadioKMoE, a knowledge-guided framework that combines Kolmogorov-Arnold Networks (KAN) with a Mixture-of-Experts (MoE) architecture to overcome these difficulties. First, the KAN module predicts an initial coarse coverage map by leveraging its ability to approximate physics-based models and capture overall radio propagation patterns. Next, the coarse map, complemented by environmental data, informs the Mixture-of-Experts network, which consists of multiple specialized expert networks. Each expert specializes in specific radiomap patterns, enabling the model to refine local details while maintaining global consistency. Comprehensive experiments on both single- and multi-band radiomap estimation tasks demonstrate that RadioKMoE achieves higher accuracy and robustness compared to conventional deep learning approaches. This approach thus successfully integrates domain knowledge with specialized neural network components to improve radiomap modeling in complex wireless environments. <div>
arXiv:2511.16986v1 Announce Type: new 
Abstract: Radiomap serves as a vital tool for wireless network management and deployment by providing powerful spatial knowledge of signal propagation and coverage. However, increasingly complex radio propagation behavior and surrounding environments pose strong challenges for radiomap estimation (RME). In this work, we propose a knowledge-guided RME framework that integrates Kolmogorov-Arnold Networks (KAN) with Mixture-of-Experts (MoE), namely RadioKMoE. Specifically, we design a KAN module to predict an initial coarse coverage map, leveraging KAN's strength in approximating physics models and global radio propagation patterns. The initial coarse map, together with environmental information, drives our MoE network for precise radiomap estimation. Unlike conventional deep learning models, the MoE module comprises expert networks specializing in distinct radiomap patterns to improve local details while preserving global consistency. Experimental results in both multi- and single-band RME demonstrate the enhanced accuracy and robustness of the proposed RadioKMoE in radiomap estimation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction</title>
<link>https://arxiv.org/abs/2511.16991</link>
<guid>https://arxiv.org/abs/2511.16991</guid>
<content:encoded><![CDATA[
<div> Keywords: visual complexity, self-supervised learning, DINOv3, ResNet-50, image prediction<br /><br />Summary:<br /><br />1. The paper addresses the problem of predicting visual complexity of images, which is significant in computer vision tasks like image compression, retrieval, and classification, and also relevant to cognitive science understanding of human perception.<br />2. It introduces DReX (DINO-ResNet Fusion), a vision-only model that combines self-supervised transformer features from DINOv3 ViT-S/16 with hierarchical features from a convolutional ResNet-50 using a learnable attention mechanism.<br />3. This fusion approach captures both low-level textures and high-level semantic structures, enabling improved prediction of image complexity.<br />4. DReX achieves state-of-the-art performance on the IC9600 benchmark with a Pearson correlation of 0.9581, outperforming prior methods including multimodal image-text models, while using about 21.5 times fewer learnable parameters.<br />5. The model generalizes well across various datasets and evaluation metrics, such as Pearson and Spearman correlations, RMSE, and MAE. Ablation studies and attention analysis reveal that combining both backbones provides complementary benefits, with DINOv3’s [CLS] token notably boosting sensitivity to complexity.<br />6. Overall, the findings demonstrate that visual features alone, when effectively fused between self-supervised transformers and supervised CNNs, suffice for robust, human-aligned image complexity prediction. <div>
arXiv:2511.16991v1 Announce Type: new 
Abstract: Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether language information is necessary for this task. We propose DReX (DINO-ResNet Fusion), a vision-only model that fuses self-supervised and convolutional representations through a learnable attention mechanism to predict image complexity. Our architecture integrates multi-scale hierarchical features from ResNet-50 with semantically rich representations from DINOv3 ViT-S/16, enabling the model to capture both low-level texture patterns and high-level semantic structure. DReX achieves state-of-the-art performance on the IC9600 benchmark (Pearson r = 0.9581), surpassing previous methods--including those trained on multimodal image-text data--while using approximately 21.5x fewer learnable parameters. Furthermore, DReX generalizes robustly across multiple datasets and metrics, achieving superior results on Pearson and Spearman correlation, Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Ablation and attention analyses confirm that DReX leverages complementary cues from both backbones, with the DINOv3 [CLS] token enhancing sensitivity to visual complexity. Our findings suggest that visual features alone can be sufficient for human-aligned complexity prediction and that, when properly fused, self-supervised transformers and supervised deep convolutional neural networks offer complementary and synergistic benefits for this task.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DepthFocus: Controllable Depth Estimation for See-Through Scenes</title>
<link>https://arxiv.org/abs/2511.16993</link>
<guid>https://arxiv.org/abs/2511.16993</guid>
<content:encoded><![CDATA[
<div> Keywords: DepthFocus, Vision Transformer, multi-depth estimation, transmissive materials, synthetic dataset  

<br /><br />Summary:  
DepthFocus is a novel steerable Vision Transformer designed to address the challenges of depth estimation in scenes with transmissive materials that create layered depth ambiguities. Unlike traditional methods that passively output a single static depth map focused on the nearest surface, DepthFocus incorporates an intent-driven approach by conditioning its depth estimation on a scalar depth preference, allowing dynamic adaptation to focus on user-specified depths within complex scenes. The model is trained primarily on a newly constructed large-scale synthetic dataset containing 500,000 multi-layered images, crafted specifically to capture diverse see-through and layered effects common in real-world environments. DepthFocus achieves state-of-the-art results on single-depth benchmarks like BOOSTER, which contains many transparent and reflective objects, demonstrating superior performance over existing methods. Additionally, it quantitatively verifies its ability to produce intent-aligned depth estimations on both new synthetic and real multi-depth datasets introduced by the authors. The model also shows strong generalization capabilities when tested on unseen transmissive scenes, indicating robustness and potential for broad applicability. This work represents a significant advance toward more active, human-like 3D perception systems that can selectively perceive and interpret multiple depth layers rather than being constrained to a single depth plane. <div>
arXiv:2511.16993v1 Announce Type: new 
Abstract: Depth in the real world is rarely singular. Transmissive materials create layered ambiguities that confound conventional perception systems. Existing models remain passive, attempting to estimate static depth maps anchored to the nearest surface, while humans actively shift focus to perceive a desired depth. We introduce DepthFocus, a steerable Vision Transformer that redefines stereo depth estimation as intent-driven control. Conditioned on a scalar depth preference, the model dynamically adapts its computation to focus on the intended depth, enabling selective perception within complex scenes. The training primarily leverages our newly constructed 500k multi-layered synthetic dataset, designed to capture diverse see-through effects. DepthFocus not only achieves state-of-the-art performance on conventional single-depth benchmarks like BOOSTER, a dataset notably rich in transparent and reflective objects, but also quantitatively demonstrates intent-aligned estimation on our newly proposed real and synthetic multi-depth datasets. Moreover, it exhibits strong generalization capabilities on unseen see-through scenes, underscoring its robustness as a significant step toward active and human-like 3D perception.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions</title>
<link>https://arxiv.org/abs/2511.16998</link>
<guid>https://arxiv.org/abs/2511.16998</guid>
<content:encoded><![CDATA[
<div> Memory-Enhanced Visual-Language Recovery, adverse weather restoration, implicit memory bank, visual-language model, dynamic cross-attention<br /><br />Summary:<br /><br />This paper addresses the challenge of reliable visual perception in adverse weather conditions like rain, haze, and snow, critical for autonomous driving and outdoor robotics. It introduces the Memory-Enhanced Visual-Language Recovery (MVLR) model designed to restore images degraded by various weather effects across different severity levels. MVLR uniquely combines a lightweight encoder-decoder architecture with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM applies chain-of-thought inference to generate weather degradation priors, which query the IMB that holds continuous latent representations of degradation patterns. These retrieved prototypes are adaptively integrated with multi-scale visual features using dynamic cross-attention mechanisms, enhancing restoration precision while ensuring computational efficiency. Experimental evaluations on four severe-weather benchmarks demonstrate that MVLR outperforms traditional single-branch approaches and Mixture-of-Experts baselines in Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). The findings suggest that MVLR strikes an effective balance between compact model size and expressive capability, making it suitable for real-time deployment in diverse outdoor environments with challenging weather conditions. <div>
arXiv:2511.16998v1 Announce Type: new 
Abstract: Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Language Models are Confused Tourists</title>
<link>https://arxiv.org/abs/2511.17004</link>
<guid>https://arxiv.org/abs/2511.17004</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, cultural robustness, adversarial evaluation, image perturbation, multimodal understanding<br /><br />Summary:<br />1. The paper addresses the cultural dimension in Vision-Language Models (VLMs), focusing on the stability of these models across diverse and mixed cultural inputs, which is critical for supporting diverse and multicultural societies.  
2. Current evaluation methods generally test VLMs on benchmarks featuring singular cultural cues per image, neglecting more complex scenarios where multiple, possibly unrelated, cultural elements coexist in a single visual input.  
3. To fill this evaluation gap, the authors introduce ConfusedTourist, a novel adversarial robustness suite specifically designed to assess how VLMs handle perturbed geographical and cultural cues through image stacking perturbations and image-generation-based variants.  
4. Experimental results reveal a significant vulnerability in state-of-the-art VLMs, where accuracy drastically decreases under these perturbations, and performance worsens when image-generation-based disturbances are applied.  
5. Interpretability analyses show that these failures arise from systematic attention shifts within the models, which prioritize distracting cultural cues over the intended focus, highlighting a major challenge for current multimodal understanding systems and emphasizing the urgent need for more culturally robust VLMs. <div>
arXiv:2511.17004v1 Announce Type: new 
Abstract: Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLUID: Training-Free Face De-identification via Latent Identity Substitution</title>
<link>https://arxiv.org/abs/2511.17005</link>
<guid>https://arxiv.org/abs/2511.17005</guid>
<content:encoded><![CDATA[
<div> identity editing, diffusion models, face de-identification, latent space, attribute preservation<br /><br />Summary:<br /><br />1. The paper introduces FLUID, a novel training-free framework designed for face de-identification by manipulating identity directly in the latent space of pretrained diffusion models.<br /><br />2. FLUID draws inspiration from chemical substitution mechanisms to reinterpret identity editing as a semantic displacement within the latent h-space of an unconditional diffusion model.<br /><br />3. The framework identifies effective identity-editing directions through an optimization process that incorporates novel reagent losses, aiming to suppress identity while preserving other facial attributes.<br /><br />4. The authors propose two editing approaches—linear and geodesic (tangent-based)—to navigate the complex latent manifold more effectively.<br /><br />5. Experiments conducted on popular face datasets CelebA-HQ and FFHQ demonstrate that FLUID achieves a better balance between identity suppression and attribute preservation compared to state-of-the-art face de-identification methods, as validated by both qualitative assessments and quantitative metrics. <div>
arXiv:2511.17005v1 Announce Type: new 
Abstract: We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites</title>
<link>https://arxiv.org/abs/2511.17014</link>
<guid>https://arxiv.org/abs/2511.17014</guid>
<content:encoded><![CDATA[
<div> Keywords: camera lens blur, circle of confusion, depth estimation, neural reblurring network, mixed reality<br /><br />Summary:<br /><br />This paper addresses the challenge of achieving consistent and natural camera lens blur for blending 3D virtual objects seamlessly into real photographs. Traditional methods rely heavily on camera parameters such as focal length, focus distance, aperture size, and scene depth to compute the circle of confusion (CoC), which dictates realistic blur levels. These requirements limit usability for ordinary users who often lack access to such metadata. The authors propose a novel compositing approach that bypasses the need for explicit depth or camera information by directly estimating the CoC map from a single RGB image. The CoC for virtual objects is inferred through a linear relationship with the signed CoC map and depth, allowing adaptive blur levels corresponding to object positioning in the scene. Realistic lens blur is then rendered using a neural reblurring network, enhancing the visual fidelity of mixed reality compositions. The method offers a flexible and practical solution for real-world applications, enabling users without detailed camera data to produce high-quality, defocus-consistent composites. Experimental results demonstrate that this approach outperforms current state-of-the-art methods in both qualitative visual quality and quantitative metrics, making it an effective tool for augmented reality and image compositing tasks. <div>
arXiv:2511.17014v1 Announce Type: new 
Abstract: Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis</title>
<link>https://arxiv.org/abs/2511.17045</link>
<guid>https://arxiv.org/abs/2511.17045</guid>
<content:encoded><![CDATA[
<div> RacketVision, sports analytics, racket pose estimation, ball tracking, trajectory forecasting<br /><br />Summary:<br /><br />We introduce RacketVision, a comprehensive dataset and benchmark designed to advance computer vision applications in sports analytics, specifically focusing on table tennis, tennis, and badminton. This novel dataset uniquely combines fine-grained annotated data for racket poses with traditional ball position annotations, supporting research on complex human-object interactions. RacketVision targets three interconnected core tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Evaluation with established baseline models highlighted a significant insight regarding multimodal fusion: simply concatenating racket pose features to ball tracking inputs actually harms trajectory prediction performance. Instead, implementing a CrossAttention mechanism to integrate these multimodal features substantially improves results, surpassing strong unimodal baseline methods. This demonstrates the critical importance of sophisticated fusion strategies in leveraging multimodal data effectively. The dataset and benchmark offer a versatile resource and a promising starting point for future exploration in dynamic object tracking, conditional motion forecasting, and multimodal analysis within the domain of sports. The project, including code and data, is publicly available at the provided GitHub repository, fostering further research and development in this emerging field. <div>
arXiv:2511.17045v1 Announce Type: new 
Abstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation</title>
<link>https://arxiv.org/abs/2511.17048</link>
<guid>https://arxiv.org/abs/2511.17048</guid>
<content:encoded><![CDATA[
<div> 3D Room Generation, Language-driven Planning, Spatial Arrangement, Rendering Optimization, Indoor Scene Synthesis  

<br /><br />Summary:  
This paper introduces RoomPlanner, a pioneering fully automatic framework for generating realistic 3D indoor scenes from short text inputs without requiring manual layout design or panorama images. The approach employs a hierarchical language-driven agent planner system that interprets ambiguous prompts into detailed scene descriptions, including semantic and spatial attributes for objects and backgrounds. Using these descriptions, initial 3D point clouds are created. RoomPlanner ensures rational spatial placement through two arrangement constraints that iteratively optimize object positioning within bounded environments, guaranteeing collision-free and accessible layouts. For the rendering stage, novel strategies like AnyReach Sampling for camera trajectories and Interval Timestep Flow Sampling (ITFS) for refining the 3D Gaussian scene representation are introduced to boost efficiency. These optimizations reduce total scene generation time to under 30 minutes. Experimental results demonstrate that RoomPlanner produces geometrically reasonable indoor scenes that outperform existing methods in both speed and visual quality while maintaining scene editability. The authors plan to release the code soon, making the framework accessible for broader use in automated indoor scene synthesis. <div>
arXiv:2511.17048v1 Announce Type: new 
Abstract: In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning</title>
<link>https://arxiv.org/abs/2511.17052</link>
<guid>https://arxiv.org/abs/2511.17052</guid>
<content:encoded><![CDATA[
<div> Analyzing whole-slide images, PathAgent, large language model, chain-of-thought, zero-shot generalization<br /><br />Summary: This paper introduces PathAgent, a novel framework designed to analyze whole-slide images (WSIs) by mimicking the stepwise, reflective reasoning process of human pathologists. Unlike traditional computational methods that often provide opaque results, PathAgent explicitly replicates the dynamic and evidence-driven approach used by experts to zoom, refocus, and self-correct during diagnosis. The system integrates three main modules: Navigator, which autonomously identifies significant micro-regions within WSIs; Perceptor, which extracts morphological visual cues from these regions; and Executor, which compiles observations into evolving natural language narratives or chain-of-thoughts. This approach produces fully interpretable predictions, enhancing transparency and trustworthiness. The framework operates without the need for additional training, leveraging large language models for zero-shot generalization. PathAgent was rigorously tested on five challenging datasets where it outperformed specialized baseline models in both open-ended and constrained visual question-answering tasks. Furthermore, collaborative evaluations with human pathologists demonstrated its clinical relevance and potential as a diagnostic assistant. Overall, PathAgent presents a promising step toward transparent, interpretable, and clinically grounded AI tools for computational pathology. <div>
arXiv:2511.17052v1 Announce Type: new 
Abstract: Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding</title>
<link>https://arxiv.org/abs/2511.17053</link>
<guid>https://arxiv.org/abs/2511.17053</guid>
<content:encoded><![CDATA[
<div> Keywords: LVLM, Pedestrian Tracking, OmniPT, Reinforcement Learning, Semantic Understanding<br /><br />Summary:<br /><br />This paper addresses the challenge of improving pedestrian tracking by leveraging Large Vision-Language Models (LVLMs), which currently excel in image-level tasks but lag in instance-level tasks like tracking. The authors propose a unified pedestrian tracking framework named OmniPT that can handle standard tracking, reference-based tracking, and generate semantic understanding of tracked pedestrians interactively. They focus on two main issues: adapting the tracking problem so that foundation LVLMs can perform it effectively, and ensuring the model outputs bounding box data in a fixed, supervisable format. The training pipeline involves several stages: an initial Reinforcement Learning (RL) phase to enable formatted bounding box outputs, a mid-training phase using diverse pedestrian datasets to enhance domain knowledge, supervised fine-tuning on specific pedestrian tracking datasets, and a final RL phase to boost tracking accuracy and instruction-following capabilities. The methodology integrates concepts from recent tasks combining object tracking with natural language, such as Referring Multiple Object Tracking (MOT) and Semantic MOT, capitalizing on the semantic understanding strengths of LVLMs. Experimental results on established tracking benchmarks demonstrate that OmniPT outperforms previous specialized methods, indicating the effectiveness of this LVLM-based approach in advanced pedestrian tracking scenarios. <div>
arXiv:2511.17053v1 Announce Type: new 
Abstract: LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-AD-Net: Reinforcement Learning Guided Adaptive Displacement in Latent Space for Refined Point Cloud Completion</title>
<link>https://arxiv.org/abs/2511.17054</link>
<guid>https://arxiv.org/abs/2511.17054</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud completion, reinforcement learning, geometric consistency, latent space refinement, PointNN selector<br /><br />Summary: This paper introduces RL-AD-Net, a novel reinforcement learning (RL) framework designed to refine point cloud completions in the latent space of a pretrained point autoencoder. The autoencoder converts completed shapes into compact global feature vectors (GFVs), which the RL agent adjusts selectively to enhance local geometric fidelity. To ensure robustness, the framework uses a lightweight, non-parametric PointNN selector to evaluate and retain the better reconstruction between the original completion and the RL-refined output. When ground truth data is available, both Chamfer Distance and geometric consistency metrics guide the refinement process. Since RL training is unsupervised and dynamic, training is performed separately for each category to address convergence issues encountered in highly diverse datasets. However, the authors acknowledge the potential for future extension towards multi-category refinement. Experiments on the ShapeNetCore-2048 dataset demonstrate that existing baseline completion networks, while effective under their original training-style cropping, struggle under random cropping scenarios. In contrast, RL-AD-Net consistently improves performance in both settings by employing RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it compatible with a wide variety of completion networks without requiring retraining, highlighting its practical applicability. <div>
arXiv:2511.17054v1 Announce Type: new 
Abstract: Recent point cloud completion models, including transformer-based, denoising-based, and other state-of-the-art approaches, generate globally plausible shapes from partial inputs but often leave local geometric inconsistencies. We propose RL-AD-Net, a reinforcement learning (RL) refinement framework that operates in the latent space of a pretrained point autoencoder. The autoencoder encodes completions into compact global feature vectors (GFVs), which are selectively adjusted by an RL agent to improve geometric fidelity. To ensure robustness, a lightweight non-parametric PointNN selector evaluates the geometric consistency of both the original completion and the RL-refined output, retaining the better reconstruction. When ground truth is available, both Chamfer Distance and geometric consistency metrics guide refinement. Training is performed separately per category, since the unsupervised and dynamic nature of RL makes convergence across highly diverse categories challenging. Nevertheless, the framework can be extended to multi-category refinement in future work. Experiments on ShapeNetCore-2048 demonstrate that while baseline completion networks perform reasonable under their training-style cropping, they struggle in random cropping scenarios. In contrast, RL-AD-Net consistently delivers improvements across both settings, highlighting the effectiveness of RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it applicable to a wide range of completion networks without requiring retraining.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.17059</link>
<guid>https://arxiv.org/abs/2511.17059</guid>
<content:encoded><![CDATA[
<div> articulated objects, surface reconstruction, joint parameter estimation, temporal geometry constraint, planar Gaussian splatting<br /><br />
Summary:<br /><br />
1. The paper addresses the challenge of part-level surface reconstruction and joint parameter estimation of articulated objects common in everyday environments, such as drawers and refrigerators.<br /><br />
2. It builds upon the existing REArtGS framework, which is category-agnostic and uses multi-view RGB images captured at two different states, but identifies limitations in handling screw-joint or multi-part objects and the lack of geometric constraints for states not seen during training.<br /><br />
3. The proposed method, REArtGS++, introduces improvements by modeling decoupled screw motions for each joint without relying on joint type priors, enabling more flexible joint representation.<br /><br />
4. REArtGS++ jointly optimizes part-aware Gaussian representations along with joint parameters via a novel part motion blending technique to better capture articulated motion.<br /><br />
5. A key innovation is the introduction of time-continuous geometric constraints by encouraging Gaussian components to be planar and employing a temporally consistent regularization of planar normals and depth using a Taylor first-order expansion.<br /><br />
6. Extensive experiments on synthetic and real-world articulated objects demonstrate that REArtGS++ outperforms existing methods in terms of generalizable part-level surface reconstruction and accurate joint parameter estimation.<br /><br />
7. The project site with detailed resources and code is available at https://sites.google.com/view/reartgs2/home. <div>
arXiv:2511.17059v1 Announce Type: new 
Abstract: Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS~\cite{wu2025reartgs} introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</title>
<link>https://arxiv.org/abs/2511.17068</link>
<guid>https://arxiv.org/abs/2511.17068</guid>
<content:encoded><![CDATA[
<div> Magnetic Resonance Imaging, Computed Tomography, Brain MRI reconstruction, Diffusion model, Retrieval-augmented synthesis

<br /><br />Summary:  
This paper addresses the challenge of synthesizing high-quality brain MRI scans from sparse, low-dose CT volumes characterized by poor through-plane resolution. The authors propose a novel framework named ReBrain, which leverages a Brownian Bridge Diffusion Model (BBDM) for synthesizing 2D MRI slices from available 3D CT inputs with limited slices. To ensure structural and pathological consistency, ReBrain incorporates a retrieval-augmented mechanism by accessing a large database of CT slices, retrieving similar examples via a fine-tuned model. These retrieved CT slices serve as references and are integrated into the synthesis process through a ControlNet branch, guiding intermediate MRI slice generation and preserving structural continuity. The framework also handles cases of retrieval failure by applying spherical linear interpolation as fallback guidance, enhancing robustness. Extensive experiments on two benchmark datasets, SynthRAD2023 and BraTS, validate ReBrain’s effectiveness, showing it achieves state-of-the-art results in cross-modal brain MRI reconstruction despite input sparsity. This work demonstrates significant potential for improvement in clinical scenarios where MRI acquisition is limited or infeasible, facilitating accurate brain disease diagnosis from CT scans. <div>
arXiv:2511.17068v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diversity Has Always Been There in Your Visual Autoregressive Models</title>
<link>https://arxiv.org/abs/2511.17074</link>
<guid>https://arxiv.org/abs/2511.17074</guid>
<content:encoded><![CDATA[
<div> Visual Autoregressive models, generative diversity, diversity collapse, feature map, DiverseVAR<br /><br />Summary:<br /><br />1. Visual Autoregressive (VAR) models have recently become popular for next-scale image prediction, providing better inference efficiency and image quality than traditional multi-step autoregressive and diffusion models.  
2. Despite their advantages, VAR models often face a problem called diversity collapse, where the variability of generated outputs significantly reduces, similar to issues seen in few-step distilled diffusion models.  
3. The paper proposes DiverseVAR, a straightforward and effective method to restore the generative diversity of VAR models without the need for additional training.  
4. Analysis identifies the pivotal component of the feature map as a critical factor influencing diversity formation, particularly at early scales in the generation process.  
5. DiverseVAR works by suppressing this pivotal component in the model's input and amplifying it in the output, which unlocks the model’s inherent generative potential while maintaining high-fidelity image synthesis.  
6. Experimental results demonstrate that DiverseVAR significantly improves generative diversity while only minimally affecting overall model performance.  
7. The code for DiverseVAR will be made publicly available on GitHub to encourage further research and application. <div>
arXiv:2511.17074v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spanning Tree Autoregressive Visual Generation</title>
<link>https://arxiv.org/abs/2511.17089</link>
<guid>https://arxiv.org/abs/2511.17089</guid>
<content:encoded><![CDATA[
<div> Spanning Tree Autoregressive, image generation, sequence order flexibility, breadth-first search, rejection sampling

<br /><br />Summary:  
The paper introduces Spanning Tree Autoregressive (STAR) modeling designed to improve image generation by integrating prior knowledge like center bias and locality. Unlike conventional autoregressive models that rely on randomly permuted sequences—often causing performance drops or limiting sequence order flexibility during inference—STAR leverages traversal orders derived from uniform spanning trees of the image patch lattice. Using breadth-first search, STAR constructs spanning trees whose traversal orders make partial observations appear as prefixes via rejection sampling. This structured randomization strategy preserves the ability of postfix completion, maintaining high sampling performance without demanding significant changes to the language autoregressive model architecture. STAR’s method enables flexible sequence ordering during image editing at inference time, effectively balancing flexibility and performance. Its approach ensures efficient sampling and better utilization of image positional context, addressing the limitations of bidirectional context modeling in visual generation. Overall, STAR provides a novel framework that enhances both the flexibility and quality of image autoregressive sampling, with minimal architectural adjustments. <div>
arXiv:2511.17089v1 Announce Type: new 
Abstract: We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPAGS: Sparse-View Articulated Object Reconstruction from Single State via Planar Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.17092</link>
<guid>https://arxiv.org/abs/2511.17092</guid>
<content:encoded><![CDATA[
arXiv:2511.17092v1 Announce Type: new 
Abstract: Articulated objects are ubiquitous in daily environments, and their 3D reconstruction holds great significance across various fields. However, existing articulated object reconstruction methods typically require costly inputs such as multi-stage and multi-view observations. To address the limitations, we propose a category-agnostic articulated object reconstruction framework via planar Gaussian Splatting, which only uses sparse-view RGB images from a single state. Specifically, we first introduce a Gaussian information field to perceive the optimal sparse viewpoints from candidate camera poses. Then we compress 3D Gaussians into planar Gaussians to facilitate accurate estimation of normal and depth. The planar Gaussians are optimized in a coarse-to-fine manner through depth smooth regularization and few-shot diffusion. Moreover, we introduce a part segmentation probability for each Gaussian primitive and update them by back-projecting part segmentation masks of renderings. Extensive experimental results demonstrate that our method achieves higher-fidelity part-level surface reconstruction on both synthetic and real-world data than existing methods. Codes will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models</title>
<link>https://arxiv.org/abs/2511.17094</link>
<guid>https://arxiv.org/abs/2511.17094</guid>
<content:encoded><![CDATA[
arXiv:2511.17094v1 Announce Type: new 
Abstract: Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\% and 16.04\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs</title>
<link>https://arxiv.org/abs/2511.17103</link>
<guid>https://arxiv.org/abs/2511.17103</guid>
<content:encoded><![CDATA[
arXiv:2511.17103v1 Announce Type: new 
Abstract: Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the "affective gap", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the "affective gap". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the "affective gap" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better</title>
<link>https://arxiv.org/abs/2511.17106</link>
<guid>https://arxiv.org/abs/2511.17106</guid>
<content:encoded><![CDATA[
arXiv:2511.17106v1 Announce Type: new 
Abstract: Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\%$ and shortening output token length by $24.5\%$.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.17116</link>
<guid>https://arxiv.org/abs/2511.17116</guid>
<content:encoded><![CDATA[
arXiv:2511.17116v1 Announce Type: new 
Abstract: Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color</title>
<link>https://arxiv.org/abs/2511.17133</link>
<guid>https://arxiv.org/abs/2511.17133</guid>
<content:encoded><![CDATA[
arXiv:2511.17133v1 Announce Type: new 
Abstract: Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs</title>
<link>https://arxiv.org/abs/2511.17135</link>
<guid>https://arxiv.org/abs/2511.17135</guid>
<content:encoded><![CDATA[
arXiv:2511.17135v1 Announce Type: new 
Abstract: Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\%$ to $6.3\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.17138</link>
<guid>https://arxiv.org/abs/2511.17138</guid>
<content:encoded><![CDATA[
arXiv:2511.17138v1 Announce Type: new 
Abstract: Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Look Closer: A New Instance-Wise Loss for Small Cerebral Lesion Segmentation</title>
<link>https://arxiv.org/abs/2511.17146</link>
<guid>https://arxiv.org/abs/2511.17146</guid>
<content:encoded><![CDATA[
arXiv:2511.17146v1 Announce Type: new 
Abstract: Traditional loss functions in medical image segmentation, such as Dice, often under-segment small lesions because their small relative volume contributes negligibly to the overall loss. To address this, instance-wise loss functions and metrics have been proposed to evaluate segmentation quality on a per-lesion basis. We introduce CC-DiceCE, a loss function based on the CC-Metrics framework, and compare it with the existing blob loss. Both are benchmarked against a DiceCE baseline within the nnU-Net framework, which provides a robust and standardized setup. We find that CC-DiceCE loss increases detection (recall) with minimal to no degradation in segmentation performance, albeit at the cost of slightly more false positives. Furthermore, our multi-dataset study shows that CC-DiceCE generally outperforms blob loss.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight detector for real-time detection of remote sensing images</title>
<link>https://arxiv.org/abs/2511.17147</link>
<guid>https://arxiv.org/abs/2511.17147</guid>
<content:encoded><![CDATA[
arXiv:2511.17147v1 Announce Type: new 
Abstract: Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.17150</link>
<guid>https://arxiv.org/abs/2511.17150</guid>
<content:encoded><![CDATA[
arXiv:2511.17150v1 Announce Type: new 
Abstract: Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network</title>
<link>https://arxiv.org/abs/2511.17155</link>
<guid>https://arxiv.org/abs/2511.17155</guid>
<content:encoded><![CDATA[
arXiv:2511.17155v1 Announce Type: new 
Abstract: The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused. To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting. However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy. To address this limitation, we propose UI-Styler, a novel ultrasound-specific, class-aware image style transfer framework. UI-Styler leverages a pattern-matching mechanism to transfer texture patterns embedded in the target images onto source images while preserving the source structural content. In addition, we introduce a class-aware prompting strategy guided by pseudo labels of the target domain, which enforces accurate semantic alignment with diagnostic categories. Extensive experiments on ultrasound cross-device tasks demonstrate that UI-Styler consistently outperforms existing UIT methods, achieving state-of-the-art performance in distribution distance and downstream tasks, such as classification and segmentation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle</title>
<link>https://arxiv.org/abs/2511.17171</link>
<guid>https://arxiv.org/abs/2511.17171</guid>
<content:encoded><![CDATA[
arXiv:2511.17171v1 Announce Type: new 
Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating self-supervised representations for audio-visual deepfake detection</title>
<link>https://arxiv.org/abs/2511.17181</link>
<guid>https://arxiv.org/abs/2511.17181</guid>
<content:encoded><![CDATA[
arXiv:2511.17181v1 Announce Type: new 
Abstract: Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition</title>
<link>https://arxiv.org/abs/2511.17183</link>
<guid>https://arxiv.org/abs/2511.17183</guid>
<content:encoded><![CDATA[
arXiv:2511.17183v1 Announce Type: new 
Abstract: Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention</title>
<link>https://arxiv.org/abs/2511.17185</link>
<guid>https://arxiv.org/abs/2511.17185</guid>
<content:encoded><![CDATA[
arXiv:2511.17185v1 Announce Type: new 
Abstract: We propose PostCam, a framework for novel-view video generation that enables post-capture editing of camera trajectories in dynamic scenes. We find that existing video recapture methods suffer from suboptimal camera motion injection strategies; such suboptimal designs not only limit camera control precision but also result in generated videos that fail to preserve fine visual details from the source video. To achieve more accurate and flexible motion manipulation, PostCam introduces a query-shared cross-attention module. It integrates two distinct forms of control signals: the 6-DoF camera poses and the 2D rendered video frames. By fusing them into a unified representation within a shared feature space, our model can extract underlying motion cues, which enhances both control precision and generation quality. Furthermore, we adopt a two-stage training strategy: the model first learns coarse camera control from pose inputs, and then incorporates visual information to refine motion accuracy and enhance visual fidelity. Experiments on both real-world and synthetic datasets demonstrate that PostCam outperforms state-of-the-art methods by over 20% in camera control precision and view consistency, while achieving the highest video generation quality. Our project webpage is publicly available at: https://cccqaq.github.io/PostCam.github.io/
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real Noise Decoupling for Hyperspectral Image Denoising</title>
<link>https://arxiv.org/abs/2511.17196</link>
<guid>https://arxiv.org/abs/2511.17196</guid>
<content:encoded><![CDATA[
arXiv:2511.17196v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) denoising is a crucial step in enhancing the quality of HSIs. Noise modeling methods can fit noise distributions to generate synthetic HSIs to train denoising networks. However, the noise in captured HSIs is usually complex and difficult to model accurately, which significantly limits the effectiveness of these approaches. In this paper, we propose a multi-stage noise-decoupling framework that decomposes complex noise into explicitly modeled and implicitly modeled components. This decoupling reduces the complexity of noise and enhances the learnability of HSI denoising methods when applied to real paired data. Specifically, for explicitly modeled noise, we utilize an existing noise model to generate paired data for pre-training a denoising network, equipping it with prior knowledge to handle the explicitly modeled noise effectively. For implicitly modeled noise, we introduce a high-frequency wavelet guided network. Leveraging the prior knowledge from the pre-trained module, this network adaptively extracts high-frequency features to target and remove the implicitly modeled noise from real paired HSIs. Furthermore, to effectively eliminate all noise components and mitigate error accumulation across stages, a multi-stage learning strategy, comprising separate pre-training and joint fine-tuning, is employed to optimize the entire framework. Extensive experiments on public and our captured datasets demonstrate that our proposed framework outperforms state-of-the-art methods, effectively handling complex real-world noise and significantly enhancing HSI quality.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.17199</link>
<guid>https://arxiv.org/abs/2511.17199</guid>
<content:encoded><![CDATA[
arXiv:2511.17199v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning</title>
<link>https://arxiv.org/abs/2511.17201</link>
<guid>https://arxiv.org/abs/2511.17201</guid>
<content:encoded><![CDATA[
arXiv:2511.17201v1 Announce Type: new 
Abstract: In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</title>
<link>https://arxiv.org/abs/2511.17207</link>
<guid>https://arxiv.org/abs/2511.17207</guid>
<content:encoded><![CDATA[
arXiv:2511.17207v1 Announce Type: new 
Abstract: Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis. To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework. The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications. SING3R-SLAM first builds locally consistent submaps through our lightweight tracking and reconstruction module, and then progressively aligns and fuses them into a global Gaussian map that enforces cross-view geometric consistency. This global map, in turn, provides feedback to correct local drift and enhance the robustness of tracking. Extensive experiments demonstrate that SING3R-SLAM achieves state-of-the-art tracking, 3D reconstruction, and novel view rendering, resulting in over 12% improvement in tracking and producing finer, more detailed geometry, all while maintaining a compact and memory-efficient global representation on real-world datasets.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers</title>
<link>https://arxiv.org/abs/2511.17209</link>
<guid>https://arxiv.org/abs/2511.17209</guid>
<content:encoded><![CDATA[
arXiv:2511.17209v1 Announce Type: new 
Abstract: We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FisheyeGaussianLift: BEV Feature Lifting for Surround-View Fisheye Camera Perception</title>
<link>https://arxiv.org/abs/2511.17210</link>
<guid>https://arxiv.org/abs/2511.17210</guid>
<content:encoded><![CDATA[
arXiv:2511.17210v1 Announce Type: new 
Abstract: Accurate BEV semantic segmentation from fisheye imagery remains challenging due to extreme non-linear distortion, occlusion, and depth ambiguity inherent to wide-angle projections. We present a distortion-aware BEV segmentation framework that directly processes multi-camera high-resolution fisheye images,utilizing calibrated geometric unprojection and per-pixel depth distribution estimation. Each image pixel is lifted into 3D space via Gaussian parameterization, predicting spatial means and anisotropic covariances to explicitly model geometric uncertainty. The projected 3D Gaussians are fused into a BEV representation via differentiable splatting, producing continuous, uncertainty-aware semantic maps without requiring undistortion or perspective rectification. Extensive experiments demonstrate strong segmentation performance on complex parking and urban driving scenarios, achieving IoU scores of 87.75% for drivable regions and 57.26% for vehicles under severe fisheye distortion and diverse environmental conditions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-domain Adaptation Networks for Realistic Image Super-resolution</title>
<link>https://arxiv.org/abs/2511.17217</link>
<guid>https://arxiv.org/abs/2511.17217</guid>
<content:encoded><![CDATA[
arXiv:2511.17217v1 Announce Type: new 
Abstract: Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QueryOcc: Query-based Self-Supervision for 3D Semantic Occupancy</title>
<link>https://arxiv.org/abs/2511.17221</link>
<guid>https://arxiv.org/abs/2511.17221</guid>
<content:encoded><![CDATA[
arXiv:2511.17221v1 Announce Type: new 
Abstract: Learning 3D scene geometry and semantics from images is a core challenge in computer vision and a key capability for autonomous driving. Since large-scale 3D annotation is prohibitively expensive, recent work explores self-supervised learning directly from sensor data without manual labels. Existing approaches either rely on 2D rendering consistency, where 3D structure emerges only implicitly, or on discretized voxel grids from accumulated lidar point clouds, limiting spatial precision and scalability. We introduce QueryOcc, a query-based self-supervised framework that learns continuous 3D semantic occupancy directly through independent 4D spatio-temporal queries sampled across adjacent frames. The framework supports supervision from either pseudo-point clouds derived from vision foundation models or raw lidar data. To enable long-range supervision and reasoning under constant memory, we introduce a contractive scene representation that preserves near-field detail while smoothly compressing distant regions. QueryOcc surpasses previous camera-based methods by 26% in semantic RayIoU on the self-supervised Occ3D-nuScenes benchmark while running at 11.6 FPS, demonstrating that direct 4D query supervision enables strong self-supervised occupancy learning. https://research.zenseact.com/publications/queryocc/
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17242</link>
<guid>https://arxiv.org/abs/2511.17242</guid>
<content:encoded><![CDATA[
arXiv:2511.17242v1 Announce Type: new 
Abstract: This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.
  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.
  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Deconvolution for Color Images Using Normalized Quaternion Kernels</title>
<link>https://arxiv.org/abs/2511.17253</link>
<guid>https://arxiv.org/abs/2511.17253</guid>
<content:encoded><![CDATA[
arXiv:2511.17253v1 Announce Type: new 
Abstract: In this work, we address the challenging problem of blind deconvolution for color images. Existing methods often convert color images to grayscale or process each color channel separately, which overlooking the relationships between color channels. To handle this issue, we formulate a novel quaternion fidelity term designed specifically for color image blind deconvolution. This fidelity term leverages the properties of quaternion convolution kernel, which consists of four kernels: one that functions similarly to a non-negative convolution kernel to capture the overall blur, and three additional convolution kernels without constraints corresponding to red, green and blue channels respectively model their unknown interdependencies. In order to preserve image intensity, we propose to use the normalized quaternion kernel in the blind deconvolution process. Extensive experiments on real datasets of blurred color images show that the proposed method effectively removes artifacts and significantly improves deblurring effect, demonstrating its potential as a powerful tool for color image deconvolution.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats</title>
<link>https://arxiv.org/abs/2511.17254</link>
<guid>https://arxiv.org/abs/2511.17254</guid>
<content:encoded><![CDATA[
arXiv:2511.17254v1 Announce Type: new 
Abstract: Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback</title>
<link>https://arxiv.org/abs/2511.17255</link>
<guid>https://arxiv.org/abs/2511.17255</guid>
<content:encoded><![CDATA[
arXiv:2511.17255v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing</title>
<link>https://arxiv.org/abs/2511.17269</link>
<guid>https://arxiv.org/abs/2511.17269</guid>
<content:encoded><![CDATA[
arXiv:2511.17269v1 Announce Type: new 
Abstract: Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.17282</link>
<guid>https://arxiv.org/abs/2511.17282</guid>
<content:encoded><![CDATA[
arXiv:2511.17282v1 Announce Type: new 
Abstract: Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17300</link>
<guid>https://arxiv.org/abs/2511.17300</guid>
<content:encoded><![CDATA[
arXiv:2511.17300v1 Announce Type: new 
Abstract: Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiFingerPose: Bimodal Finger Pose Estimation for Touch Devices</title>
<link>https://arxiv.org/abs/2511.17306</link>
<guid>https://arxiv.org/abs/2511.17306</guid>
<content:encoded><![CDATA[
arXiv:2511.17306v1 Announce Type: new 
Abstract: Finger pose offers promising opportunities to expand human computer interaction capability of touchscreen devices. Existing finger pose estimation algorithms that can be implemented in portable devices predominantly rely on capacitive images, which are currently limited to estimating pitch and yaw angles and exhibit reduced accuracy when processing large-angle inputs (especially when it is greater than 45 degrees). In this paper, we propose BiFingerPose, a novel bimodal based finger pose estimation algorithm capable of simultaneously and accurately predicting comprehensive finger pose information. A bimodal input is explored, including a capacitive image and a fingerprint patch obtained from the touchscreen with an under-screen fingerprint sensor. Our approach leads to reliable estimation of roll angle, which is not achievable using only a single modality. In addition, the prediction performance of other pose parameters has also been greatly improved. The evaluation of a 12-person user study on continuous and discrete interaction tasks further validated the advantages of our approach. Specifically, BiFingerPose outperforms previous SOTA methods with over 21% improvement in prediction performance, 2.5 times higher task completion efficiency, and 23% better user operation accuracy, demonstrating its practical superiority. Finally, we delineate the application space of finger pose with respect to enhancing authentication security and improving interactive experiences, and develop corresponding prototypes to showcase the interaction potential. Our code will be available at https://github.com/XiongjunGuan/DualFingerPose.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion</title>
<link>https://arxiv.org/abs/2511.17308</link>
<guid>https://arxiv.org/abs/2511.17308</guid>
<content:encoded><![CDATA[
arXiv:2511.17308v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuM: Multi-View Masked Image Modeling for 3D Vision</title>
<link>https://arxiv.org/abs/2511.17309</link>
<guid>https://arxiv.org/abs/2511.17309</guid>
<content:encoded><![CDATA[
arXiv:2511.17309v1 Announce Type: new 
Abstract: Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior</title>
<link>https://arxiv.org/abs/2511.17322</link>
<guid>https://arxiv.org/abs/2511.17322</guid>
<content:encoded><![CDATA[
arXiv:2511.17322v1 Announce Type: new 
Abstract: In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation. This method significantly improves the quality of initial poses. Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refracting Reality: Generating Images with Realistic Transparent Objects</title>
<link>https://arxiv.org/abs/2511.17340</link>
<guid>https://arxiv.org/abs/2511.17340</guid>
<content:encoded><![CDATA[
arXiv:2511.17340v1 Announce Type: new 
Abstract: Generative image models can produce convincingly real images, with plausible shapes, textures, layouts and lighting. However, one domain in which they perform notably poorly is in the synthesis of transparent objects, which exhibit refraction, reflection, absorption and scattering. Refraction is a particular challenge, because refracted pixel rays often intersect with surfaces observed in other parts of the image, providing a constraint on the color. It is clear from inspection that generative models have not distilled the laws of optics sufficiently well to accurately render refractive objects. In this work, we consider the problem of generating images with accurate refraction, given a text prompt. We synchronize the pixels within the object's boundary with those outside by warping and merging the pixels using Snell's Law of Refraction, at each step of the generation trajectory. For those surfaces that are not directly observed in the image, but are visible via refraction or reflection, we recover their appearance by synchronizing the image with a second generated image -- a panorama centered at the object -- using the same warping and merging procedure. We demonstrate that our approach generates much more optically-plausible images that respect the physical constraints.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loomis Painter: Reconstructing the Painting Process</title>
<link>https://arxiv.org/abs/2511.17344</link>
<guid>https://arxiv.org/abs/2511.17344</guid>
<content:encoded><![CDATA[
arXiv:2511.17344v1 Announce Type: new 
Abstract: Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2511.17345</link>
<guid>https://arxiv.org/abs/2511.17345</guid>
<content:encoded><![CDATA[
arXiv:2511.17345v1 Announce Type: new 
Abstract: Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture</title>
<link>https://arxiv.org/abs/2511.17354</link>
<guid>https://arxiv.org/abs/2511.17354</guid>
<content:encoded><![CDATA[
arXiv:2511.17354v1 Announce Type: new 
Abstract: Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification</title>
<link>https://arxiv.org/abs/2511.17355</link>
<guid>https://arxiv.org/abs/2511.17355</guid>
<content:encoded><![CDATA[
arXiv:2511.17355v1 Announce Type: new 
Abstract: Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&amp;E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation</title>
<link>https://arxiv.org/abs/2511.17361</link>
<guid>https://arxiv.org/abs/2511.17361</guid>
<content:encoded><![CDATA[
arXiv:2511.17361v1 Announce Type: new 
Abstract: Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\% reduction in memory footprint, 124\% faster inference, and a 5.9\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</title>
<link>https://arxiv.org/abs/2511.17362</link>
<guid>https://arxiv.org/abs/2511.17362</guid>
<content:encoded><![CDATA[
arXiv:2511.17362v1 Announce Type: new 
Abstract: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVRecon: Sparse Voxel Rasterization for Surface Reconstruction</title>
<link>https://arxiv.org/abs/2511.17364</link>
<guid>https://arxiv.org/abs/2511.17364</guid>
<content:encoded><![CDATA[
arXiv:2511.17364v1 Announce Type: new 
Abstract: We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon. Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization. Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial. To address this challenge, we promote coherent and smooth voxel-wise structure through (1) robust geometric initialization using a visual geometry model and (2) a spatial smoothness loss that enforces coherent relationships across parent-child and sibling voxel groups. Extensive experiments across various benchmarks show that our method achieves strong reconstruction accuracy while having consistently speedy convergence. The code will be made public.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions</title>
<link>https://arxiv.org/abs/2511.17380</link>
<guid>https://arxiv.org/abs/2511.17380</guid>
<content:encoded><![CDATA[
arXiv:2511.17380v1 Announce Type: new 
Abstract: Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration</title>
<link>https://arxiv.org/abs/2511.17392</link>
<guid>https://arxiv.org/abs/2511.17392</guid>
<content:encoded><![CDATA[
arXiv:2511.17392v1 Announce Type: new 
Abstract: Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks</title>
<link>https://arxiv.org/abs/2511.17393</link>
<guid>https://arxiv.org/abs/2511.17393</guid>
<content:encoded><![CDATA[
arXiv:2511.17393v1 Announce Type: new 
Abstract: Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment</title>
<link>https://arxiv.org/abs/2511.17397</link>
<guid>https://arxiv.org/abs/2511.17397</guid>
<content:encoded><![CDATA[
arXiv:2511.17397v1 Announce Type: new 
Abstract: Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?</title>
<link>https://arxiv.org/abs/2511.17400</link>
<guid>https://arxiv.org/abs/2511.17400</guid>
<content:encoded><![CDATA[
arXiv:2511.17400v1 Announce Type: new 
Abstract: Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: "Is it necessary to model all channel interactions?". Inspired by the philosophy of Sparse Mixture-of-Experts ($\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers</title>
<link>https://arxiv.org/abs/2511.17421</link>
<guid>https://arxiv.org/abs/2511.17421</guid>
<content:encoded><![CDATA[
arXiv:2511.17421v1 Announce Type: new 
Abstract: Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing</title>
<link>https://arxiv.org/abs/2511.17442</link>
<guid>https://arxiv.org/abs/2511.17442</guid>
<content:encoded><![CDATA[
arXiv:2511.17442v1 Announce Type: new 
Abstract: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17448</link>
<guid>https://arxiv.org/abs/2511.17448</guid>
<content:encoded><![CDATA[
arXiv:2511.17448v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning with Sketch-Guided Verification for Physics-Aware Video Generation</title>
<link>https://arxiv.org/abs/2511.17450</link>
<guid>https://arxiv.org/abs/2511.17450</guid>
<content:encoded><![CDATA[
arXiv:2511.17450v1 Announce Type: new 
Abstract: Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition</title>
<link>https://arxiv.org/abs/2511.17454</link>
<guid>https://arxiv.org/abs/2511.17454</guid>
<content:encoded><![CDATA[
arXiv:2511.17454v1 Announce Type: new 
Abstract: We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift</title>
<link>https://arxiv.org/abs/2511.17455</link>
<guid>https://arxiv.org/abs/2511.17455</guid>
<content:encoded><![CDATA[
arXiv:2511.17455v1 Announce Type: new 
Abstract: Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPR-OdomNet: Difference and Similarity-Driven Odometry Estimation Network for Ground Penetrating Radar-Based Localization</title>
<link>https://arxiv.org/abs/2511.17457</link>
<guid>https://arxiv.org/abs/2511.17457</guid>
<content:encoded><![CDATA[
arXiv:2511.17457v1 Announce Type: new 
Abstract: When performing robot/vehicle localization using ground penetrating radar (GPR) to handle adverse weather and environmental conditions, existing techniques often struggle to accurately estimate distances when processing B-scan images with minor distinctions. This study introduces a new neural network-based odometry method that leverages the similarity and difference features of GPR B-scan images for precise estimation of the Euclidean distances traveled between the B-scan images. The new custom neural network extracts multi-scale features from B-scan images taken at consecutive moments and then determines the Euclidean distance traveled by analyzing the similarities and differences between these features. To evaluate our method, an ablation study and comparison experiments have been conducted using the publicly available CMU-GPR dataset. The experimental results show that our method consistently outperforms state-of-the-art counterparts in all tests. Specifically, our method achieves a root mean square error (RMSE), and achieves an overall weighted RMSE of 0.449 m across all data sets, which is a 10.2\% reduction in RMSE when compared to the best state-of-the-art method.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual World Models via Digital Twin-conditioned Video Diffusion</title>
<link>https://arxiv.org/abs/2511.17481</link>
<guid>https://arxiv.org/abs/2511.17481</guid>
<content:encoded><![CDATA[
arXiv:2511.17481v1 Announce Type: new 
Abstract: World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as "what would happen if this object was removed?", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions</title>
<link>https://arxiv.org/abs/2511.17484</link>
<guid>https://arxiv.org/abs/2511.17484</guid>
<content:encoded><![CDATA[
arXiv:2511.17484v1 Announce Type: new 
Abstract: Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Artificial Intelligence Framework for Measuring Human Spine Aging Using MRI</title>
<link>https://arxiv.org/abs/2511.17485</link>
<guid>https://arxiv.org/abs/2511.17485</guid>
<content:encoded><![CDATA[
arXiv:2511.17485v1 Announce Type: new 
Abstract: The human spine is a complex structure composed of 33 vertebrae. It holds the body and is important for leading a healthy life. The spine is vulnerable to age-related degenerations that can be identified through magnetic resonance imaging (MRI). In this paper we propose a novel computer-vison-based deep learning method to estimate spine age using images from over 18,000 MRI series. Data are restricted to subjects with only age-related spine degeneration. Eligibility criteria are created by identifying common age-based clusters of degenerative spine conditions using uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN). Model selection is determined using a detailed ablation study on data size, loss, and the effect of different spine regions. We evaluate the clinical utility of our model by calculating the difference between actual spine age and model-predicted age, the spine age gap (SAG), and examining the association between these differences and spine degenerative conditions and lifestyle factors. We find that SAG is associated with conditions including disc bulges, disc osteophytes, spinal stenosis, and fractures, as well as lifestyle factors like smoking and physically demanding work, and thus may be a useful biomarker for measuring overall spine health.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</title>
<link>https://arxiv.org/abs/2511.17487</link>
<guid>https://arxiv.org/abs/2511.17487</guid>
<content:encoded><![CDATA[
arXiv:2511.17487v1 Announce Type: new 
Abstract: Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</title>
<link>https://arxiv.org/abs/2511.17490</link>
<guid>https://arxiv.org/abs/2511.17490</guid>
<content:encoded><![CDATA[
arXiv:2511.17490v1 Announce Type: new 
Abstract: Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvDiff: High Quality Video with an Event Camera</title>
<link>https://arxiv.org/abs/2511.17492</link>
<guid>https://arxiv.org/abs/2511.17492</guid>
<content:encoded><![CDATA[
arXiv:2511.17492v1 Announce Type: new 
Abstract: As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native 3D Editing with Full Attention</title>
<link>https://arxiv.org/abs/2511.17501</link>
<guid>https://arxiv.org/abs/2511.17501</guid>
<content:encoded><![CDATA[
arXiv:2511.17501v1 Announce Type: new 
Abstract: Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Augmented Reality: Paradigms, Technologies, and Future Applications</title>
<link>https://arxiv.org/abs/2511.16783</link>
<guid>https://arxiv.org/abs/2511.16783</guid>
<content:encoded><![CDATA[
arXiv:2511.16783v1 Announce Type: cross 
Abstract: This paper introduces Generative Augmented Reality (GAR) as a next-generation paradigm that reframes augmentation as a process of world re-synthesis rather than world composition by a conventional AR engine. GAR replaces the conventional AR engine's multi-stage modules with a unified generative backbone, where environmental sensing, virtual content, and interaction signals are jointly encoded as conditioning inputs for continuous video generation. We formalize the computational correspondence between AR and GAR, survey the technical foundations that make real-time generative augmentation feasible, and outline prospective applications that leverage its unified inference model. We envision GAR as a future AR paradigm that delivers high-fidelity experiences in terms of realism, interactivity, and immersion, while eliciting new research challenges on technologies, content ecosystems, and the ethical and societal implications.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach</title>
<link>https://arxiv.org/abs/2511.16786</link>
<guid>https://arxiv.org/abs/2511.16786</guid>
<content:encoded><![CDATA[
arXiv:2511.16786v1 Announce Type: cross 
Abstract: Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI Super-Resolution with Deep Learning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2511.16854</link>
<guid>https://arxiv.org/abs/2511.16854</guid>
<content:encoded><![CDATA[
arXiv:2511.16854v1 Announce Type: cross 
Abstract: High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.
  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots</title>
<link>https://arxiv.org/abs/2511.16949</link>
<guid>https://arxiv.org/abs/2511.16949</guid>
<content:encoded><![CDATA[
arXiv:2511.16949v1 Announce Type: cross 
Abstract: Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation</title>
<link>https://arxiv.org/abs/2511.17031</link>
<guid>https://arxiv.org/abs/2511.17031</guid>
<content:encoded><![CDATA[
arXiv:2511.17031v1 Announce Type: cross 
Abstract: The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Vision-Language Models Understand Visual Persuasiveness?</title>
<link>https://arxiv.org/abs/2511.17036</link>
<guid>https://arxiv.org/abs/2511.17036</guid>
<content:encoded><![CDATA[
arXiv:2511.17036v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays</title>
<link>https://arxiv.org/abs/2511.17043</link>
<guid>https://arxiv.org/abs/2511.17043</guid>
<content:encoded><![CDATA[
arXiv:2511.17043v1 Announce Type: cross 
Abstract: Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation</title>
<link>https://arxiv.org/abs/2511.17126</link>
<guid>https://arxiv.org/abs/2511.17126</guid>
<content:encoded><![CDATA[
arXiv:2511.17126v1 Announce Type: cross 
Abstract: Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the added value of pretherapeutic MR descriptors in predicting breast cancer pathologic complete response to neoadjuvant chemotherapy</title>
<link>https://arxiv.org/abs/2511.17158</link>
<guid>https://arxiv.org/abs/2511.17158</guid>
<content:encoded><![CDATA[
arXiv:2511.17158v1 Announce Type: cross 
Abstract: Objectives: To evaluate the association between pretreatment MRI descriptors and breast cancer (BC) pathological complete response (pCR) to neoadjuvant chemotherapy (NAC). Materials \& Methods: Patients with BC treated by NAC with a breast MRI between 2016 and 2020 were included in this retrospective observational single-center study. MR studies were described using the standardized BI-RADS and breast edema score on T2-weighted MRI. Univariable and multivariable logistic regression analyses were performed to assess variables association with pCR according to residual cancer burden. Random forest classifiers were trained to predict pCR on a random split including 70% of the database and were validated on the remaining cases. Results: Among 129 BC, 59 (46%) achieved pCR after NAC (luminal (n=7/37, 19%), triple negative (TN) (n=30/55, 55%), HER2+ (n=22/37, 59%). Clinical and biological items associated with pCR were BC subtype (p<0.001), T stage 0/I/II (p=0.008), higher Ki67 (p=0.005) and higher tumor-infiltrating lymphocytes levels (p=0.016). Univariate analysis showed that the following MRI features, oval or round shape (p=0.047), unifocality (p=0.026), non-spiculated margins (p=0.018), no associated non-mass enhancement (NME) (p = 0.024) and a lower MRI size (p = 0.031) were significantly associated with pCR. Unifocality and non-spiculated margins remained independently associated with pCR at multivariable analysis. Adding significant MRI features to clinicobiological variables in random forest classifiers significantly increased sensitivity (0.67 versus 0.62), specificity (0.69 versus 0.67) and precision (0.71 versus 0.67) for pCR prediction. Conclusion: Non-spiculated margins and unifocality are independently associated with pCR and can increase models performance to predict BC response to NAC. Clinical Relevance Statement: A multimodal approach integrating pretreatment MRI features with clinicobiological predictors, including TILs, could be employed to develop machine learning models for identifying patients at risk of non-response. This may enable consideration of alternative therapeutic strategies to optimize treatment outcomes
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism</title>
<link>https://arxiv.org/abs/2511.17198</link>
<guid>https://arxiv.org/abs/2511.17198</guid>
<content:encoded><![CDATA[
arXiv:2511.17198v1 Announce Type: cross 
Abstract: LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making</title>
<link>https://arxiv.org/abs/2511.17225</link>
<guid>https://arxiv.org/abs/2511.17225</guid>
<content:encoded><![CDATA[
arXiv:2511.17225v1 Announce Type: cross 
Abstract: In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</title>
<link>https://arxiv.org/abs/2511.17238</link>
<guid>https://arxiv.org/abs/2511.17238</guid>
<content:encoded><![CDATA[
arXiv:2511.17238v1 Announce Type: cross 
Abstract: The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data</title>
<link>https://arxiv.org/abs/2511.17276</link>
<guid>https://arxiv.org/abs/2511.17276</guid>
<content:encoded><![CDATA[
arXiv:2511.17276v1 Announce Type: cross 
Abstract: This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM</title>
<link>https://arxiv.org/abs/2511.17335</link>
<guid>https://arxiv.org/abs/2511.17335</guid>
<content:encoded><![CDATA[
arXiv:2511.17335v1 Announce Type: cross 
Abstract: Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Latent Transmission and Glare Maps for Lens Veiling Glare Removal</title>
<link>https://arxiv.org/abs/2511.17353</link>
<guid>https://arxiv.org/abs/2511.17353</guid>
<content:encoded><![CDATA[
arXiv:2511.17353v1 Announce Type: cross 
Abstract: Beyond the commonly recognized optical aberrations, the imaging performance of compact optical systems-including single-lens and metalens designs-is often further degraded by veiling glare caused by stray-light scattering from non-ideal optical surfaces and coatings, particularly in complex real-world environments. This compound degradation undermines traditional lens aberration correction yet remains underexplored. A major challenge is that conventional scattering models (e.g., for dehazing) fail to fit veiling glare due to its spatial-varying and depth-independent nature. Consequently, paired high-quality data are difficult to prepare via simulation, hindering application of data-driven veiling glare removal models. To this end, we propose VeilGen, a generative model that learns to simulate veiling glare by estimating its underlying optical transmission and glare maps in an unsupervised manner from target images, regularized by Stable Diffusion (SD)-based priors. VeilGen enables paired dataset generation with realistic compound degradation of optical aberrations and veiling glare, while also providing the estimated latent optical transmission and glare maps to guide the veiling glare removal process. We further introduce DeVeiler, a restoration network trained with a reversibility constraint, which utilizes the predicted latent maps to guide an inverse process of the learned scattering model. Extensive experiments on challenging compact optical systems demonstrate that our approach delivers superior restoration quality and physical fidelity compared with existing methods. These suggest that VeilGen reliably synthesizes realistic veiling glare, and its learned latent maps effectively guide the restoration process in DeVeiler. All code and datasets will be publicly released at https://github.com/XiaolongQian/DeVeiler.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2511.17366</link>
<guid>https://arxiv.org/abs/2511.17366</guid>
<content:encoded><![CDATA[
arXiv:2511.17366v1 Announce Type: cross 
Abstract: Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation</title>
<link>https://arxiv.org/abs/2511.17384</link>
<guid>https://arxiv.org/abs/2511.17384</guid>
<content:encoded><![CDATA[
arXiv:2511.17384v1 Announce Type: cross 
Abstract: While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the "collision rate" and "warning rate" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning by Curvature Alignment</title>
<link>https://arxiv.org/abs/2511.17426</link>
<guid>https://arxiv.org/abs/2511.17426</guid>
<content:encoded><![CDATA[
arXiv:2511.17426v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation</title>
<link>https://arxiv.org/abs/2511.17432</link>
<guid>https://arxiv.org/abs/2511.17432</guid>
<content:encoded><![CDATA[
arXiv:2511.17432v1 Announce Type: cross 
Abstract: Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaCells. Teaching Machines the Language of Lines: Per Point Attribute Scores for Face-Sketch Classification</title>
<link>https://arxiv.org/abs/2102.11361</link>
<guid>https://arxiv.org/abs/2102.11361</guid>
<content:encoded><![CDATA[
arXiv:2102.11361v3 Announce Type: replace 
Abstract: FaCells is a method, and an exhibition, that turns model internals into line based artworks. Aligned face photographs (CelebA, 260k images, 40 attributes) are translated into vector sketches suitable for an XY plotter. We study how to 'write' these drawings for a sequence model, comparing absolute vs. relative point encodings and random vs. travel-minimizing stroke order. A bidirectional LSTM is trained for attribute prediction; a minimal architectural change, removing the global average over the sequence and applying a Dense layer at each point, yields per point attribute scores. Aggregating points whose score exceeds an attribute specific threshold across many portraits produces new drawings we call FaCells: statistical abstractions of attributes such as Eyeglasses, Wavy Hair, or Bangs. Across ablations, absolute coordinates with travel-minimizing order and a global average readout perform best; this configuration is then adapted to produce per-point scores. Multilabel training over 40 attributes is stable, and attributes reaching at least 50% balanced accuracy are visualized as FaCells. Complementary notions (e.g., No_Beard) are constructed by selecting points below a negative threshold. FaCells foregrounds interpretability as a creative tool: the resulting works are plotter ready, reproducible, and inexpensive to realize, yet materially present. Presented at Spectrum Miami 2025, the project bridges data, model, and paper while acknowledging the limits of the labels and the biases of the dataset.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colo-ReID: Discriminative Representation Embedding with Meta-learning for Colonoscopic Polyp Re-Identification</title>
<link>https://arxiv.org/abs/2308.00929</link>
<guid>https://arxiv.org/abs/2308.00929</guid>
<content:encoded><![CDATA[
arXiv:2308.00929v3 Announce Type: replace 
Abstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras and plays an important role in the prevention and treatment of colorectal cancer. However, traditional methods for object ReID directly adopting CNN models trained on the ImageNet dataset usually produce unsatisfactory retrieval performance on colonoscopic datasets due to the large domain gap. Additionally, these methods neglect to explore the potential of self-discrepancy among intra-class or inter-class relations in the colonoscopic polyp dataset, which remains an open research problem in the medical community. To solve this dilemma, we propose a simple but effective training method named Colo-ReID, which can help our model learn more general and discriminative knowledge based on the meta-learning strategy in scenarios with fewer samples. Based on this, a dynamic Meta-Learning Regulation mechanism called MLR is introduced to further boost the performance of polyp re-identification. Our experimental results show that Colo-ReID consistently outperforms second-best method in terms of mAP performance by +2.3% on polyp re-identification task. Our source code is also publicly available at https://github.com/JeremyXSC/Colo-ReID.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos</title>
<link>https://arxiv.org/abs/2311.02733</link>
<guid>https://arxiv.org/abs/2311.02733</guid>
<content:encoded><![CDATA[
arXiv:2311.02733v2 Announce Type: replace 
Abstract: Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multimodal models that can exploit both pieces of information simultaneously. However, previous methods mainly adopt unimodal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multimodal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multimodal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A statistical method for crack pre-detection in 3D concrete images</title>
<link>https://arxiv.org/abs/2402.16126</link>
<guid>https://arxiv.org/abs/2402.16126</guid>
<content:encoded><![CDATA[
arXiv:2402.16126v2 Announce Type: replace 
Abstract: In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. Classical image-processing techniques and modern deep-learning models both face substantial computational challenges when applied directly to high resolution big data volumes. This paper introduces a statistical framework for crack pre-localization, whose purpose is not to replace or compete with segmentation networks, but to identify, with controlled error rates, the regions of a 3D CT image that are most likely to contain cracks. The method combines a simple Hessian-based filter, geometric descriptors computed on a regular spatial partition, and a spatial multiple testing procedure to detect anomalous regions while relying only on minimal calibration data, rather than large annotated datasets. Experiments on semi-synthetic and real 3D CT scans demonstrate that the proposed approach reliably highlights regions likely to contain cracks while preserving linear computational complexity. By restricting subsequent high resolution segmentation to these localized regions, deep-learning models can be trained and operate more efficiently, reducing both training runtime as well as resource consumption. The framework thus offers a practical and interpretable preprocessing step for large-scale CT inspection pipelines.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindShot: A Few-Shot Brain Decoding Framework via Transferring Cross-Subject Prior and Distilling Frequency Domain Knowledge</title>
<link>https://arxiv.org/abs/2405.15278</link>
<guid>https://arxiv.org/abs/2405.15278</guid>
<content:encoded><![CDATA[
arXiv:2405.15278v2 Announce Type: replace 
Abstract: Aiming to reconstruct visual stimuli from brain signals, brain decoding has recently made significant progress using functional magnetic resonance imaging (fMRI). However, it still has challenging issues such as substantial individual differences and high data collection costs. To simplify these problems, most methods adopt the per-subject-per-model paradigm, but this greatly limits their applications. In this paper, we design a few-shot brain decoding setting specifically for potential clinical scenarios and propose a novel two-stage decoding framework named MindShot, comprising a Multi-Subject Pretraining (MSP) stage and Fourier-based cross-subject Knowledge Distillation (FKD) stage. Firstly, a MSP framework based on multi-modal contrastive learning is constructed to mine the cross-subject prior. Secondly, the FKD is presented to decrease inter-individual differences while improving the decoding adaptability to new individuals. Our approach achieves high semantic fidelity in visual reconstruction on the largest dataset and has the potential to reduce scanning time by up to 99%. Remarkably, MindShot achieves a CLIP accuracy of 83.6% using only 1.8% of the fMRI-image pairs, surpassing the 77.4% accuracy of the method trained on the entire NSD dataset. This makes it feasible to train large-scale brain decoding frameworks that require less data, facilitating practical applications. The code is available at https://github.com/JSinBUPT/MindShot.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Cooperative Network Architecture: Learning Structured Networks as Representation of Sensory Patterns</title>
<link>https://arxiv.org/abs/2407.05650</link>
<guid>https://arxiv.org/abs/2407.05650</guid>
<content:encoded><![CDATA[
arXiv:2407.05650v5 Announce Type: replace 
Abstract: We introduce the Cooperative Network Architecture (CNA), a model that represents sensory signals using structured, recurrently connected networks of neurons, termed "nets." Nets are dynamically assembled from overlapping net fragments, which are learned based on statistical regularities in sensory input. This architecture offers robustness to noise, deformation, and generalization to out-of-distribution data, addressing challenges in current vision systems from a novel perspective. We demonstrate that net fragments can be learned without supervision and flexibly recombined to encode novel patterns, enabling figure completion and resilience to noise. Our findings establish CNA as a promising paradigm for developing neural representations that integrate local feature processing with global structure formation, providing a foundation for future research on invariant object recognition.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition</title>
<link>https://arxiv.org/abs/2407.07026</link>
<guid>https://arxiv.org/abs/2407.07026</guid>
<content:encoded><![CDATA[
arXiv:2407.07026v2 Announce Type: replace 
Abstract: With the proliferation of social media posts in recent years, the need to detect sentiments in multimodal (image-text) content has grown rapidly. Since posts are user-generated, the image and text from the same post can express different or even contradictory sentiments, leading to potential \textbf{sentiment discrepancy}. However, existing works mainly adopt a single-branch fusion structure that primarily captures the consistent sentiment between image and text. The ignorance or implicit modeling of discrepant sentiment results in compromised unimodal encoding and limited performance. In this paper, we propose a semantics Completion and Decomposition (CoDe) network to resolve the above issue. In the semantics completion module, we complement image and text representations with the semantics of the in-image text, helping bridge the sentiment gap. In the semantics decomposition module, we decompose image and text representations with exclusive projection and contrastive learning, thereby explicitly capturing the discrepant sentiment between modalities. Finally, we fuse image and text representations by cross-attention and combine them with the learned discrepant sentiment for final classification. Extensive experiments on four datasets demonstrate the superiority of CoDe and the effectiveness of each proposed module.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpotFormer: Multi-Scale Spatio-Temporal Transformer for Facial Expression Spotting</title>
<link>https://arxiv.org/abs/2407.20799</link>
<guid>https://arxiv.org/abs/2407.20799</guid>
<content:encoded><![CDATA[
arXiv:2407.20799v2 Announce Type: replace 
Abstract: Facial expression spotting, identifying periods where facial expressions occur in a video, is a significant yet challenging task in facial expression analysis. The issues of irrelevant facial movements and the challenge of detecting subtle motions in micro-expressions remain unresolved, hindering accurate expression spotting. In this paper, we propose an efficient framework for facial expression spotting. First, we propose a Sliding Window-based multi-temporal-resolution Optical flow (SW-MRO) feature, which calculates multi-temporal-resolution optical flow of the input image sequence within compact sliding windows. The window length is tailored to perceive complete micro-expressions and distinguish between general macro- and micro-expressions. SW-MRO can effectively reveal subtle motions while avoiding the optical flow being dominated by head movements. Second, we propose SpotFormer, a multi-scale spatio-temporal Transformer that simultaneously encodes spatio-temporal relationships of the SW-MRO features for accurate frame-level probability estimation. In SpotFormer, we use the proposed Facial Local Graph Pooling (FLGP) operation and convolutional layers to extract multi-scale spatio-temporal features. We show the validity of the architecture of SpotFormer by comparing it with several model variants. Third, we introduce supervised contrastive learning into SpotFormer to enhance the discriminability between different types of expressions. Extensive experiments on SAMM-LV, CAS(ME)^2, and CAS(ME)^3 show that our method outperforms state-of-the-art models, particularly in micro-expression spotting.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction</title>
<link>https://arxiv.org/abs/2411.16898</link>
<guid>https://arxiv.org/abs/2411.16898</guid>
<content:encoded><![CDATA[
arXiv:2411.16898v4 Announce Type: replace 
Abstract: Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</title>
<link>https://arxiv.org/abs/2502.07160</link>
<guid>https://arxiv.org/abs/2502.07160</guid>
<content:encoded><![CDATA[
arXiv:2502.07160v3 Announce Type: replace 
Abstract: Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complementary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving index map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Adaptation via Side Graph Convolution for Efficient Fine-tuning of 3D Point Cloud Transformers</title>
<link>https://arxiv.org/abs/2502.14142</link>
<guid>https://arxiv.org/abs/2502.14142</guid>
<content:encoded><![CDATA[
arXiv:2502.14142v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud Transformers has emerged as a promising technique for 3D point cloud analysis. While existing PEFT methods attempt to minimize the number of tunable parameters, they often suffer from high temporal and spatial computational costs during fine-tuning. This paper proposes a novel PEFT algorithm called Side Token Adaptation on a neighborhood Graph (STAG) to achieve superior temporal and spatial efficiency. STAG employs a graph convolutional side network operating in parallel with a frozen backbone Transformer to adapt tokens to downstream tasks. Through efficient graph convolution, parameter sharing, and reduced gradient computation, STAG significantly reduces both temporal and spatial costs for fine-tuning. We also present Point Cloud Classification 13 (PCC13), a new benchmark comprising diverse publicly available 3D point cloud datasets to facilitate comprehensive evaluation. Extensive experiments using multiple pre-trained models and PCC13 demonstrates the effectiveness of STAG. Specifically, STAG maintains classification accuracy comparable to existing methods while reducing tunable parameters to only 0.43M and achieving significant reductions in both computation time and memory consumption for fine-tuning. Code and benchmark will be available at: https://github.com/takahikof/STAG.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation</title>
<link>https://arxiv.org/abs/2502.17429</link>
<guid>https://arxiv.org/abs/2502.17429</guid>
<content:encoded><![CDATA[
arXiv:2502.17429v3 Announce Type: replace 
Abstract: While 3D instance segmentation (3DIS) has advanced significantly, most existing methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new classes emerge gradually and exhibit natural imbalance. Although some approaches address the emergence of new classes, they often overlook class imbalance, which leads to suboptimal performance, particularly on rare categories. To tackle this, we propose \ourmethodbf, a unified framework for \textbf{CL}ass-incremental \textbf{Imb}alance-aware \textbf{3D}IS. Building upon established exemplar replay (ER) strategies, we show that ER alone is insufficient to achieve robust performance under memory constraints. To mitigate this, we introduce a novel pseudo-label generator (PLG) that extends supervision to previously learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies from pseudo-labels and dynamically adjusts training bias, without requiring access to past data. We design and evaluate three incremental scenarios for 3DIS on the challenging ScanNet200 dataset and additionally validate our method for semantic segmentation on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by up to 16.76\% mAP for instance segmentation and approximately 30\% mIoU for semantic segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at: https://github.com/vgthengane/CLIMB3D
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackGS: Optimizing COLMAP-Free 3D Gaussian Splatting with Global Track Constraints</title>
<link>https://arxiv.org/abs/2502.19800</link>
<guid>https://arxiv.org/abs/2502.19800</guid>
<content:encoded><![CDATA[
arXiv:2502.19800v3 Announce Type: replace 
Abstract: We present TrackGS, a novel method to integrate global feature tracks with 3D Gaussian Splatting (3DGS) for COLMAP-free novel view synthesis. While 3DGS delivers impressive rendering quality, its reliance on accurate precomputed camera parameters remains a significant limitation. Existing COLMAP-free approaches depend on local constraints that fail in complex scenarios. Our key innovation lies in leveraging feature tracks to establish global geometric constraints, enabling simultaneous optimization of camera parameters and 3D Gaussians. Specifically, we: (1) introduce track-constrained Gaussians that serve as geometric anchors, (2) propose novel 2D and 3D track losses to enforce multi-view consistency, and (3) derive differentiable formulations for camera intrinsics optimization. Extensive experiments on challenging real-world and synthetic datasets demonstrate state-of-the-art performance, with much lower pose error than previous methods while maintaining superior rendering quality. Our approach eliminates the need for COLMAP preprocessing, making 3DGS more accessible for practical applications.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</title>
<link>https://arxiv.org/abs/2503.00972</link>
<guid>https://arxiv.org/abs/2503.00972</guid>
<content:encoded><![CDATA[
arXiv:2503.00972v3 Announce Type: replace 
Abstract: Point cloud registration is important in computer-aided interventions (CAI). While learning-based point cloud registration methods have been developed, their clinical application is hampered by issues of generalizability and explainability. Therefore, classical point cloud registration methods, including Iterative Closest Point (ICP), are still widely applied in CAI. ICP methods fail to consider that: (1) the points have well-defined semantic meaning, in that each point can be related to a specific anatomical label; (2) the deformation required for registration needs to follow biomechanical energy constraints. In this paper, we present a novel non-rigid semantic ICP (SemICP) method that handles multiple point labels and uses linear elastic energy regularization. We use semantic labels to improve the robustness of closest point matching and propose a novel point cloud deformation representation that incorporates explicit biomechanical energy regularization. Our experiments on four datasets show that our method significantly improves the Hausdorff distance and mean surface distance compared with other point cloud registration methods. We also demonstrate that integrating deep learning segmentation models with our registration pipeline enables effective alignment of US and MR point clouds.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints</title>
<link>https://arxiv.org/abs/2503.06677</link>
<guid>https://arxiv.org/abs/2503.06677</guid>
<content:encoded><![CDATA[
arXiv:2503.06677v5 Announce Type: replace 
Abstract: Articulated objects, as prevalent entities in human life, their 3D representations play crucial roles across various applications. However, achieving both high-fidelity textured surface reconstruction and dynamic generation for articulated objects remains challenging for existing methods. In this paper, we present REArtGS, a novel framework that introduces additional geometric and motion constraints to 3D Gaussian primitives, enabling realistic surface reconstruction and generation for articulated objects. Specifically, given multi-view RGB images of arbitrary two states of articulated objects, we first introduce an unbiased Signed Distance Field (SDF) guidance to regularize Gaussian opacity fields, enhancing geometry constraints and improving surface reconstruction quality. Then we establish deformable fields for 3D Gaussians constrained by the kinematic structures of articulated objects, achieving unsupervised generation of surface meshes in unseen states. Extensive experiments on both synthetic and real datasets demonstrate our approach achieves high-quality textured surface reconstruction for given states, and enables high-fidelity surface generation for unseen states. Project site: https://sites.google.com/view/reartgs/home.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning</title>
<link>https://arxiv.org/abs/2503.12972</link>
<guid>https://arxiv.org/abs/2503.12972</guid>
<content:encoded><![CDATA[
arXiv:2503.12972v3 Announce Type: replace 
Abstract: Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond</title>
<link>https://arxiv.org/abs/2503.21692</link>
<guid>https://arxiv.org/abs/2503.21692</guid>
<content:encoded><![CDATA[
arXiv:2503.21692v4 Announce Type: replace 
Abstract: The integration of multi-view imaging and pose estimation represents a significant advance in computer vision applications, offering new possibilities for understanding human movement and interactions. This work presents a new algorithm that improves multi-view multi-person pose estimation, focusing on fast triangulation speeds and good generalization capabilities. The approach extends to whole-body pose estimation, capturing details from facial expressions to finger movements across multiple individuals and viewpoints. Adaptability to different settings is demonstrated through strong performance across unseen datasets and configurations. To support further progress in this field, all of this work is publicly accessible.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model</title>
<link>https://arxiv.org/abs/2503.23463</link>
<guid>https://arxiv.org/abs/2503.23463</guid>
<content:encoded><![CDATA[
arXiv:2503.23463v2 Announce Type: replace 
Abstract: We present OpenDriveVLA, a Vision Language Action model designed for end-to-end autonomous driving, built upon open-source large language models. OpenDriveVLA generates spatially grounded driving actions by leveraging multimodal inputs, including 2D and 3D instance-aware visual representations, ego vehicle states, and language commands. To bridge the modality gap between driving visual representations and language embeddings, we introduce a hierarchical vision language alignment process, projecting both 2D and 3D structured visual tokens into a unified semantic space. Furthermore, we incorporate structured agent environment ego interaction modeling into the autoregressive decoding process, enabling the model to capture fine-grained spatial dependencies and behavior-aware dynamics critical for reliable trajectory planning. Extensive experiments on the nuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art results across open-loop trajectory planning and driving-related question answering tasks. Qualitative analyses further illustrate its capability to follow high-level driving commands and generate trajectories under challenging scenarios, highlighting its potential for next-generation end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ISS-Geo142: A Benchmark for Geolocating Astronaut Photography from the International Space Station</title>
<link>https://arxiv.org/abs/2504.21194</link>
<guid>https://arxiv.org/abs/2504.21194</guid>
<content:encoded><![CDATA[
arXiv:2504.21194v2 Announce Type: replace 
Abstract: This paper introduces ISS-Geo142, a curated benchmark for geolocating astronaut photography captured from the International Space Station (ISS). Although the ISS position at capture time is known precisely, the specific Earth locations depicted in these images are typically not directly georeferenced, making automated localization non-trivial. ISS-Geo142 consists of 142 images with associated metadata and manually determined geographic locations, spanning a range of spatial scales and scene types.
  On top of this benchmark, we implement and evaluate three geolocation pipelines: a neural network based approach (NN-Geo) using VGG16 features and cross-correlation over map-derived Areas of Interest (AOIs), a Scale-Invariant Feature Transform based pipeline (SIFT-Match) using sliding-window feature matching on stitched high-resolution AOIs, and TerraByte, an AI system built around a GPT-4 model with vision capabilities that jointly reasons over image content and ISS coordinates. On ISS-Geo142, NN-Geo achieves a match for 75.52\% of the images under our evaluation protocol, SIFT-Match attains high precision on structurally rich scenes at substantial computational cost, and TerraByte establishes the strongest overall baseline, correctly geolocating approximately 90\% of the images while also producing human-readable geographic descriptions.
  The methods and experiments were originally developed in 2023; this manuscript is a revised and extended version that situates the work relative to subsequent advances in cross-view geo-localization and remote-sensing vision--language models. Taken together, ISS-Geo142 and these three pipelines provide a concrete, historically grounded benchmark for future work on ISS image geolocation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis</title>
<link>https://arxiv.org/abs/2505.21502</link>
<guid>https://arxiv.org/abs/2505.21502</guid>
<content:encoded><![CDATA[
arXiv:2505.21502v2 Announce Type: replace 
Abstract: We propose GRGS, a generalizable and relightable 3D Gaussian framework for high-fidelity human novel view synthesis under diverse lighting conditions. Unlike existing methods that rely on per-character optimization or ignore physical constraints, GRGS adopts a feed-forward, fully supervised strategy projecting geometry, material, and illumination cues from multi-view 2D observations into 3D Gaussian representations. To recover accurate geometry under diverse lighting conditions, we introduce a Lighting-robust Geometry Refinement (LGR) module trained on synthetically relit data to predict precise depth and surface normals. Based on the high-quality geometry, a Physically Grounded Neural Rendering (PGNR) module is further proposed to integrate neural prediction with physics-based shading, supporting editable relighting with shadows and indirect illumination. Moreover, we design a 2D-to-3D projection training scheme leveraging differentiable supervision from ambient occlusion, direct, and indirect lighting maps, alleviating the computational cost of ray tracing. Extensive experiments demonstrate that GRGS achieves superior visual quality, geometric consistency, and generalization across characters and lighting conditions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAR: Function-preserving Attention Replacement for IMC-friendly Inference</title>
<link>https://arxiv.org/abs/2505.21535</link>
<guid>https://arxiv.org/abs/2505.21535</guid>
<content:encoded><![CDATA[
arXiv:2505.21535v3 Announce Type: replace 
Abstract: While transformers dominate modern vision and language models, their attention mechanism remains poorly suited for in-memory computing (IMC) devices due to intensive activation-to-activation multiplications and non-local memory access, leading to substantial latency and bandwidth overhead on ReRAM-based accelerators. To address this mismatch, we propose FAR, a Function-preserving Attention Replacement framework that substitutes all attention in pretrained DeiTs with sequential modules inherently compatible with IMC dataflows. Specifically, FAR replaces self-attention with a multi-head bidirectional LSTM architecture via block-wise distillation to retain functional equivalence while enabling linear-time computation and localized weight reuse. We further incorporate structured pruning on FAR models, enabling flexible adaptation to resource-constrained IMC arrays while maintaining functional fidelity. Evaluations on the DeiT family demonstrate that FAR maintains comparable accuracy to the original attention-based models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships learned by attention while improving computational efficiency, highlighting its potential for energy-efficient transformer inference on IMC-based edge accelerators.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantFace: Efficient Quantization for Face Restoration</title>
<link>https://arxiv.org/abs/2506.00820</link>
<guid>https://arxiv.org/abs/2506.00820</guid>
<content:encoded><![CDATA[
arXiv:2506.00820v2 Announce Type: replace 
Abstract: Diffusion models have been achieving remarkable performance in face restoration. However, the heavy computations hamper the widespread adoption of these models. In this work, we propose QuantFace, a novel low-bit quantization framework for face restoration models, where the full-precision (i.e., 32-bit) weights and activations are quantized to 4~6-bit. We first analyze the data distribution within activations and find that it is highly variant. To preserve the original data information, we employ rotation-scaling channel balancing. Furthermore, we propose Quantization-Distillation Low-Rank Adaptation (QD-LoRA), which jointly optimizes for quantization and distillation performance. Finally, we propose an adaptive bit-width allocation strategy. We formulate such a strategy as an integer programming problem that combines quantization error and perceptual metrics to find a satisfactory resource allocation. Extensive experiments on the synthetic and real-world datasets demonstrate the effectiveness of QuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over recent leading low-bit quantization methods for face restoration. The code is available at https://github.com/jiatongli2024/QuantFace.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORV: 4D Occupancy-centric Robot Video Generation</title>
<link>https://arxiv.org/abs/2506.03079</link>
<guid>https://arxiv.org/abs/2506.03079</guid>
<content:encoded><![CDATA[
arXiv:2506.03079v2 Announce Type: replace 
Abstract: Recent embodied intelligence suffers from data scarcity, while conventional simulators lack visual realism. Controllable video generation is emerging as a promising data engine, yet current action-conditioned methods still fall short: generated videos are limited in fidelity and temporal consistency, poorly aligned with controls, and often constrained to singleview settings. We attribute these issues to the representational gap between sparse control inputs and dense pixel outputs. Thus, we introduce ORV, a 4D occupancy-centric framework for robot video generation that couples action priors with occupancy-derived visual priors. Concretely, we align chunked 7-DoF actions with video latents via an Action-Expert AdaLN modulation, and inject 2D renderings of 4D semantic occupancy into the generation process as soft guidance. Meanwhile, a central obstacle is the lack of occupancy data for embodied scenarios; we therefore curate ORV-Data, a large-scale, high-quality 4D semantic occupancy dataset of robot manipulation. Across BridgeV2, DROID, and RT-1, ORV improves video generation quality and controllability, achieving 18.8% lower FVD than state of the art, +3.5% success rate on visual planning, and +6.4% success rate on policy learning. Beyond singleview generation, ORV natively supports multiview consistent synthesis and enables simulation-to-real transfer despite significant domain gaps. Code, models, and data are at: https://orangesodahub.github.io/ORV
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-Set Domain Generalization through Spectral-Spatial Uncertainty Disentanglement for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2506.09460</link>
<guid>https://arxiv.org/abs/2506.09460</guid>
<content:encoded><![CDATA[
arXiv:2506.09460v2 Announce Type: replace 
Abstract: Open-set domain generalization (OSDG) tackles the dual challenge of recognizing unknown classes while simultaneously striving to generalize across unseen domains without using target data during training. In this article, an OSDG framework for hyperspectral image classification is proposed, centered on a new Spectral-Spatial Uncertainty Disentanglement mechanism. It has been designed to address the domain shift influencing both spectral, spatial and combined feature extraction pathways using evidential deep learning, after which the most reliable pathway for each sample is adaptively selected. The proposed framework is further integrated with frequency-domain feature extraction for domain-invariant representation learning, dual-channel residual networks for spectral-spatial feature extraction, and evidential deep learning based uncertainty quantification. Experiments conducted on three cross scene hyperspectral datasets, show that performance comparable to state-of-the-art domain adaptation methods can be achieved despite no access to target data, while high unknown-class rejection and known-class accuracy levels are maintained. The implementation will be available at github.com/amir-khb/UGOSDG upon acceptance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loss-Oriented Ranking for Automated Visual Prompting in LVLMs</title>
<link>https://arxiv.org/abs/2506.16112</link>
<guid>https://arxiv.org/abs/2506.16112</guid>
<content:encoded><![CDATA[
arXiv:2506.16112v2 Announce Type: replace 
Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we develop an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experiments indicate that AutoV enhances the performance of various LVLMs across multiple image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\textbf{10.2}\%$ accuracy gain on VizWiz, and AutoV boosts Qwen2.5-VL by $\textbf{3.8}\%$ on MMMU, highlighting its potential as an optimal visual prompting method.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Video Temporal Grounding with Generative Multi-modal Large Language Models</title>
<link>https://arxiv.org/abs/2506.18883</link>
<guid>https://arxiv.org/abs/2506.18883</guid>
<content:encoded><![CDATA[
arXiv:2506.18883v2 Announce Type: replace 
Abstract: This paper presents a computational model for universal video temporal grounding, which accurately localizes temporal moments in videos based on natural language queries (e.g., questions or descriptions). Unlike existing methods that are often limited to specific video domains or durations, we propose UniTime, a robust and universal video grounding model leveraging the strong vision-language understanding capabilities of generative Multi-modal Large Language Models (MLLMs). Our model effectively handles videos of diverse views, genres, and lengths while comprehending complex language queries. The key contributions include: (i) We consider steering strong MLLMs for temporal grounding in videos. To enable precise timestamp outputs, we incorporate temporal information by interleaving timestamp tokens with video tokens. (ii) By training the model to handle videos with different input granularities through adaptive frame scaling, our approach achieves robust temporal grounding for both short and long videos. (iii) Comprehensive experiments show that UniTime outperforms state-of-the-art approaches in both zero-shot and dataset-specific finetuned settings across five public temporal grounding benchmarks. (iv) When employed as a preliminary moment retriever for long-form video question-answering (VideoQA), UniTime significantly improves VideoQA accuracy, highlighting its value for complex video understanding tasks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Training-Free Style-Personalization via SVD-Based Feature Decomposition</title>
<link>https://arxiv.org/abs/2507.04482</link>
<guid>https://arxiv.org/abs/2507.04482</guid>
<content:encoded><![CDATA[
arXiv:2507.04482v2 Announce Type: replace 
Abstract: We present a training-free framework for style-personalized image generation that operates during inference using a scale-wise autoregressive model. Our method generates a stylized image guided by a single reference style while preserving semantic consistency and mitigating content leakage. Through a detailed step-wise analysis of the generation process, we identify a pivotal step where the dominant singular values of the internal feature encode style-related components. Building upon this insight, we introduce two lightweight control modules: Principal Feature Blending, which enables precise modulation of style through SVD-based feature reconstruction, and Structural Attention Correction, which stabilizes structural consistency by leveraging content-guided attention correction across fine stages. Without any additional training, extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video</title>
<link>https://arxiv.org/abs/2508.03100</link>
<guid>https://arxiv.org/abs/2508.03100</guid>
<content:encoded><![CDATA[
arXiv:2508.03100v2 Announce Type: replace 
Abstract: Multimodal reasoning over long-horizon video is challenging due to the need for precise spatiotemporal fusion and alignment across modalities. While recent methods such as Group Relative Policy Optimization (GRPO) have shown promise in this domain, they suffer from three key limitations: (1) data inefficiency from their on-policy design, (2) a vanishing advantage problem, where identical or near-identical rewards within a group eliminate the learning signal by producing zero-valued advantages, and (3) uniform credit assignment that fails to emphasize critical reasoning steps. We introduce $\textbf{AVATAR}$ ($\textbf{A}$udio-$\textbf{V}$ideo $\textbf{A}$gen$\textbf{t}$ for $\textbf{A}$lignment and $\textbf{R}$easoning), a framework that addresses these limitations through two core components: (1) an off-policy training architecture that improves sample efficiency and resolves vanishing advantages by reusing past experiences with greater reward diversity, and (2) Temporal Advantage Shaping (TAS), a novel credit assignment strategy that upweights key reasoning phases during learning. $\textbf{AVATAR}$ achieves strong performance across various benchmarks, outperforming the Qwen2.5-Omni baseline by $\mathbf{+5.4}$ on MMVU, $\mathbf{+4.9}$ on OmniBench, and $\mathbf{+4.5}$ on Video-Holmes, while demonstrating $\textbf{$5$$\times$ sample efficiency}$, requiring $80\%$ fewer generated completions to reach target performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composed Object Retrieval: Object-level Retrieval via Composed Expressions</title>
<link>https://arxiv.org/abs/2508.04424</link>
<guid>https://arxiv.org/abs/2508.04424</guid>
<content:encoded><![CDATA[
arXiv:2508.04424v2 Announce Type: replace 
Abstract: Retrieving fine-grained visual content based on user intent remains a challenge in multi-modal systems. Although current Composed Image Retrieval (CIR) methods combine reference images with retrieval texts, they are constrained to image-level matching and cannot localize specific objects. To this end, we propose Composed Object Retrieval (COR), a brand-new task that goes beyond image-level retrieval to achieve object-level precision, allowing the retrieval and segmentation of target objects based on composed expressions combining reference objects and retrieval texts. COR presents significant challenges in retrieval flexibility, which requires systems to identify arbitrary objects satisfying composed expressions while avoiding semantically similar but irrelevant negative objects within the same scene. We construct COR127K, the first large-scale COR benchmark that contains 127,166 retrieval triplets with various semantic transformations in 408 categories. We also present CORE, a unified end-to-end model that integrates reference region encoding, adaptive visual-textual interaction, and region-level contrastive learning. Extensive experiments demonstrate that CORE significantly outperforms existing models in both base and novel categories, establishing a simple and effective baseline for this challenging task while opening new directions for fine-grained multi-modal retrieval research. We will publicly release both the dataset and the model at https://github.com/wangtong627/COR.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.06869</link>
<guid>https://arxiv.org/abs/2508.06869</guid>
<content:encoded><![CDATA[
arXiv:2508.06869v3 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) demonstrate exceptional performance in vision-language tasks, yet their processing of long videos is constrained by input context length and high computational costs. Sparse frame sampling thus becomes a necessary preprocessing step, with sampled frame quality directly impacting downstream performance. Existing keyframe search algorithms achieve a balance between efficiency and sampled frame quality but heavily rely on the visual modality alone. This makes them difficult to adapt to text-related tasks and often leads to retrieval results deviating from core semantic content. To address this, we propose the VISUAL-SUBTITLE INTEGRATION (VSI), a multimodal keyframe retrieval framework. It employs a dual-branch collaborative retrieval approach combining Video Search and Subtitle Match to fuse complementary visual and textual information for precise localization. Experiments on LongVideoBench and VideoMME demonstrate that VSI achieves state-of-the-art accuracy in keyframe retrieval while delivering breakthrough performance in text-related tasks and exhibiting strong generalization across other tasks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model</title>
<link>https://arxiv.org/abs/2508.09327</link>
<guid>https://arxiv.org/abs/2508.09327</guid>
<content:encoded><![CDATA[
arXiv:2508.09327v2 Announce Type: replace 
Abstract: Generative artificial intelligence (AI) has been playing an important role in various domains. Leveraging its high capability to generate high-fidelity and diverse synthetic data, generative AI is widely applied in diagnostic tasks, such as lung cancer diagnosis using computed tomography (CT). However, existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability. To address these drawbacks, we propose Lung-DDPM+, an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver, enabling the method to focus on lesion areas while achieving a better trade-off between sampling efficiency and quality. Evaluation results on the public LIDC-IDRI dataset suggest that the proposed method achieves 8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks. We also conducted a Visual Turing Test by an experienced radiologist, showing the advanced quality and fidelity of synthetic samples generated by the proposed method. These experimental results demonstrate that Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging. The code and pretrained models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.16069</link>
<guid>https://arxiv.org/abs/2508.16069</guid>
<content:encoded><![CDATA[
arXiv:2508.16069v2 Announce Type: replace 
Abstract: Recent advances in point cloud object detection have increasingly adopted Transformer-based and State Space Models (SSMs), demonstrating strong performance. However, voxelbased representations in these models require strict consistency in input and output dimensions due to their serialized processing, which limits the spatial diffusion capability typically offered by convolutional operations. This limitation significantly affects detection accuracy. Inspired by CNN-based object detection architectures, we propose a novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and diffusion in point cloud data. VDM is composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. To ensure computational efficiency, the output feature maps are downsampled to one-fourth of the original input resolution. VDM serves two primary functions: (1) diffusing foreground voxel features through sparse 3D convolutions to enrich spatial context, and (2) aggregating fine-grained spatial information to strengthen voxelwise feature representation. The enhanced voxel features produced by VDM can be seamlessly integrated into mainstream Transformer- or SSM-based detection models for accurate object classification and localization, highlighting the generalizability of our method. We evaluate VDM on several benchmark datasets by embedding it into both Transformerbased and SSM-based models. Experimental results show that our approach consistently improves detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new stateof-the-art performance across all datasets. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment</title>
<link>https://arxiv.org/abs/2509.14001</link>
<guid>https://arxiv.org/abs/2509.14001</guid>
<content:encoded><![CDATA[
arXiv:2509.14001v4 Announce Type: replace 
Abstract: Personalized object detection aims to adapt a general-purpose detector to recognize user-specific instances from only a few examples. Lightweight models often struggle in this setting due to their weak semantic priors, while large vision-language models (VLMs) offer strong object-level understanding but are too computationally demanding for real-time or on-device applications. We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a distillation framework that transfers multimodal region-level knowledge from a frozen VLM teacher into a lightweight vision-only detector. MOCHA extracts fused visual and textual teacher's embeddings and uses them to guide student training through a dual-objective loss that enforces accurate local alignment and global relational consistency across regions. This process enables efficient transfer of semantics without the need for teacher modifications or textual input at inference. MOCHA consistently outperforms prior baselines across four personalized detection benchmarks under strict few-shot regimes, yielding a +10.1 average improvement, with minimal inference cost.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EatGAN: An Edge-Attention Guided Generative Adversarial Network for Single Image Super-Resolution</title>
<link>https://arxiv.org/abs/2509.14550</link>
<guid>https://arxiv.org/abs/2509.14550</guid>
<content:encoded><![CDATA[
arXiv:2509.14550v2 Announce Type: replace 
Abstract: Single-image super-resolution (SISR) is an important task in image processing, aiming to enhance the resolution of imaging systems. Recently, SISR has made a significant leap and achieved promising results with deep learning. GAN-based models stand out among all the deep learning models because of their excellent performance in perceiving quality. However, it is rather difficult for them to reconstruct realistic high-frequency details and achieve stable training. To solve these issues, we introduce an Edge-Attention guided Generative Adversarial Network (EatGAN), the first GAN-based SISR model that simultaneously leverages edge priors both explicitly and implicitly inside the generator, which (i) proposes a Normalized Edge Attention (NEA) mechanism based on channel-affine and spatial gating that transforms edge prior into lightweight, learnable modulation parameters and injects and fuses them multiple times in a (ii) edge-guided hybrid residual block, which progressively enforces structural consistency across scales; and (iii) a composite generator objective combining pixel, perceptual, edge-gradient, and adversarial terms. Experiments show consistent state-of-the-art across distortion-oriented benchmarks and perception oriented benchmarks. Notably, our model achieves 40.87 dB and 0.073 (LPIPS) on Manga 109, which indicates that reframing image priors from passive guidance into a controllable modulation primitive for generators can chart a practical path toward trustworthy, high-fidelity Super-Resolution.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mask2IV: Interaction-Centric Video Generation via Mask Trajectories</title>
<link>https://arxiv.org/abs/2510.03135</link>
<guid>https://arxiv.org/abs/2510.03135</guid>
<content:encoded><![CDATA[
arXiv:2510.03135v2 Announce Type: replace 
Abstract: Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning. However, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use. To overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories. This design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process. Furthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues. To support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios. Extensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation</title>
<link>https://arxiv.org/abs/2510.08318</link>
<guid>https://arxiv.org/abs/2510.08318</guid>
<content:encoded><![CDATA[
arXiv:2510.08318v2 Announce Type: replace 
Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding</title>
<link>https://arxiv.org/abs/2510.09110</link>
<guid>https://arxiv.org/abs/2510.09110</guid>
<content:encoded><![CDATA[
arXiv:2510.09110v3 Announce Type: replace 
Abstract: Visual grouping -- operationalized through tasks such as instance segmentation, visual grounding, and object detection -- enables applications ranging from robotic perception to photo editing. These fundamental problems in computer vision are powered by large-scale, painstakingly annotated datasets. Despite their impact, these datasets are costly to build, biased in coverage, and difficult to scale. Synthetic datasets offer a promising alternative but struggle with flexibility, accuracy, and compositional diversity.
  We introduce Synthetic Object Compositions (SOC), an accurate and scalable data synthesis pipeline via a novel object-centric composition strategy. It composes high-quality synthetic object segments into new images using 3D geometric layout augmentation and camera configuration augmentation with generative harmonization and mask-area-weighted blending, yielding accurate and diverse masks, boxes, and referring expressions.
  Models trained on just 100K of our synthetic images outperform those trained on larger real datasets (GRIT 20M, V3Det 200K) and synthetic pipelines (Copy-Paste, X-Paste, SynGround, SegGen) by +24-36% -- achieving +10.9 AP on LVIS and +8.4 NAcc on gRefCOCO. Beyond the general open-vocabulary setup, SOC also enables controllable dataset construction for different use cases and boosts performance in both low-data and closed-vocabulary scenarios.
  Augmenting LVIS and COCO with synthetic object segments delivers strong performance across different real-data scales and yields even greater improvements under extremely limited real-data conditions, including +6.59 AP on a 1% COCO data setup. Furthermore, this controllability enables targeted data generation for intra-class referring, a diagnostic grounding task we propose that requires fine-grained attribute discrimination.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model</title>
<link>https://arxiv.org/abs/2510.14528</link>
<guid>https://arxiv.org/abs/2510.14528</guid>
<content:encoded><![CDATA[
arXiv:2510.14528v3 Announce Type: replace 
Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography</title>
<link>https://arxiv.org/abs/2510.25522</link>
<guid>https://arxiv.org/abs/2510.25522</guid>
<content:encoded><![CDATA[
arXiv:2510.25522v3 Announce Type: replace 
Abstract: Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The model's superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the region's most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2510.26601</link>
<guid>https://arxiv.org/abs/2510.26601</guid>
<content:encoded><![CDATA[
arXiv:2510.26601v2 Announce Type: replace 
Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
<link>https://arxiv.org/abs/2511.00456</link>
<guid>https://arxiv.org/abs/2511.00456</guid>
<content:encoded><![CDATA[
arXiv:2511.00456v3 Announce Type: replace 
Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia affected regions. Furthermore, we evaluate seven pre-trained deep learning models including a Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations in this study confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation</title>
<link>https://arxiv.org/abs/2511.00511</link>
<guid>https://arxiv.org/abs/2511.00511</guid>
<content:encoded><![CDATA[
arXiv:2511.00511v3 Announce Type: replace 
Abstract: Significant progress has been achieved in high-fidelity video synthesis, yet current paradigms often fall short in effectively integrating identity information from multiple subjects. This leads to semantic conflicts and suboptimal performance in preserving identities and interactions, limiting controllability and applicability. To tackle this issue, we introduce ID-Crafter, a framework for multi-subject video generation that achieves superior identity preservation and semantic coherence. ID-Crafter integrates three key components: (i) a hierarchical identity-preserving attention mechanism that progressively aggregates features at intra-subject, inter-subject, and cross-modal levels; (ii) a semantic understanding module powered by a pretrained Vision-Language Model (VLM) to provide fine-grained guidance and capture complex inter-subject relationships; and (iii) an online reinforcement learning phase to further refine the model for critical concepts. Furthermore, we construct a new dataset to facilitate robust training and evaluation. Extensive experiments demonstrate that ID-Crafter establishes new state-of-the-art performance on multi-subject video generation benchmarks, excelling in identity preservation, temporal consistency, and overall video quality.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Future Anatomies: Longitudinal Brain Mri-to-Mri Prediction</title>
<link>https://arxiv.org/abs/2511.02558</link>
<guid>https://arxiv.org/abs/2511.02558</guid>
<content:encoded><![CDATA[
arXiv:2511.02558v2 Announce Type: replace 
Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition</title>
<link>https://arxiv.org/abs/2511.03725</link>
<guid>https://arxiv.org/abs/2511.03725</guid>
<content:encoded><![CDATA[
arXiv:2511.03725v2 Announce Type: replace 
Abstract: Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models</title>
<link>https://arxiv.org/abs/2511.10629</link>
<guid>https://arxiv.org/abs/2511.10629</guid>
<content:encoded><![CDATA[
arXiv:2511.10629v2 Announce Type: replace 
Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Draft and Refine with Visual Experts</title>
<link>https://arxiv.org/abs/2511.11005</link>
<guid>https://arxiv.org/abs/2511.11005</guid>
<content:encoded><![CDATA[
arXiv:2511.11005v2 Announce Type: replace 
Abstract: While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems. Code is available at https://github.com/EavnJeong/Draft-and-Refine-with-Visual-Experts.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</title>
<link>https://arxiv.org/abs/2511.11236</link>
<guid>https://arxiv.org/abs/2511.11236</guid>
<content:encoded><![CDATA[
arXiv:2511.11236v2 Announce Type: replace 
Abstract: In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding</title>
<link>https://arxiv.org/abs/2511.11313</link>
<guid>https://arxiv.org/abs/2511.11313</guid>
<content:encoded><![CDATA[
arXiv:2511.11313v3 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code and Model are available in https://github.com/Tanveer81/DocSLM.git.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis</title>
<link>https://arxiv.org/abs/2502.09779</link>
<guid>https://arxiv.org/abs/2502.09779</guid>
<content:encoded><![CDATA[
arXiv:2502.09779v4 Announce Type: replace-cross 
Abstract: Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly available tools that could be consistently used across different applications. To mitigate this gap, we present a publicly accessible, end-to-end segmentation and feature calculation model specifically for CT body composition analysis. Our model performs segmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and visceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in axial CT images. It also provides various body composition metrics, including muscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle area/volume, and skeletal muscle index (SMI), supporting both 2D and 3D assessments. To evaluate the model, the segmentation was applied to both internal and external datasets, with body composition metrics analyzed across different age, sex, and race groups. The model achieved high dice coefficients on both internal and external datasets, exceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model outperforms the benchmark by 2.10% on skeletal muscle and 8.6% on SAT compared to the manual annotations given by the publicly available dataset. Body composition metrics show mean relative absolute errors (MRAEs) under 10% for all measures. Our model with weights is publicly available at https://github.com/mazurowski-lab/CT-Muscle-and-Fat-Segmentation.git.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning</title>
<link>https://arxiv.org/abs/2503.17987</link>
<guid>https://arxiv.org/abs/2503.17987</guid>
<content:encoded><![CDATA[
arXiv:2503.17987v3 Announce Type: replace-cross 
Abstract: Text-to-Image(T2I) models typically deploy safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods manually design instructions for the LLM to generate adversarial prompts, which effectively bypass safety filters while producing sensitive images, exposing safety vulnerabilities of T2I models. However, due to the LLM's limited understanding of the T2I model and its safety filters, existing methods require numerous queries to achieve a successful attack, limiting their practical applicability. To address this issue, we propose Reason2Attack(R2A), which aims to enhance the LLM's reasoning capabilities in generating adversarial prompts by incorporating the jailbreaking attack into the post-training process of the LLM. Specifically, we first propose a CoT example synthesis pipeline based on Frame Semantics, which generates adversarial prompts by identifying related terms and corresponding context illustrations. Using CoT examples generated by the pipeline, we fine-tune the LLM to understand the reasoning path and format the output structure. Subsequently, we incorporate the jailbreaking attack task into the reinforcement learning process of the LLM and design an attack process reward that considers prompt length, prompt stealthiness, and prompt effectiveness, aiming to further enhance reasoning accuracy. Extensive experiments on various T2I models show that R2A achieves a better attack success ratio while requiring fewer queries than baselines. Moreover, our adversarial prompts demonstrate strong attack transferability across both open-source and commercial T2I models.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals</title>
<link>https://arxiv.org/abs/2505.12114</link>
<guid>https://arxiv.org/abs/2505.12114</guid>
<content:encoded><![CDATA[
arXiv:2505.12114v2 Announce Type: replace-cross 
Abstract: AI-enhanced personality assessments are increasingly shaping hiring decisions, using affective computing to predict traits from the Big Five (OCEAN) model. However, integrating AI into these assessments raises ethical concerns, especially around bias amplification rooted in training data. These biases can lead to discriminatory outcomes based on protected attributes like gender, ethnicity, and age. To address this, we introduce a counterfactual-based framework to systematically evaluate and quantify bias in AI-driven personality assessments. Our approach employs generative adversarial networks (GANs) to generate counterfactual representations of job applicants by altering protected attributes, enabling fairness analysis without access to the underlying model. Unlike traditional bias assessments that focus on unimodal or static data, our method supports multimodal evaluation-spanning visual, audio, and textual features. This comprehensive approach is particularly important in high-stakes applications like hiring, where third-party vendors often provide AI systems as black boxes. Applied to a state-of-the-art personality prediction model, our method reveals significant disparities across demographic groups. We also validate our framework using a protected attribute classifier to confirm the effectiveness of our counterfactual generation. This work provides a scalable tool for fairness auditing of commercial AI hiring platforms, especially in black-box settings where training data and model internals are inaccessible. Our results highlight the importance of counterfactual approaches in improving ethical transparency in affective computing.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly</title>
<link>https://arxiv.org/abs/2506.08708</link>
<guid>https://arxiv.org/abs/2506.08708</guid>
<content:encoded><![CDATA[
arXiv:2506.08708v2 Announce Type: replace-cross 
Abstract: While vision-language models (VLMs) have demonstrated promising capabilities in reasoning and planning for embodied agents, their ability to comprehend physical phenomena, particularly within structured 3D environments, remains severely limited. To close this gap, we introduce PhyBlock, a progressive benchmark designed to assess VLMs on physical understanding and planning through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level cognitive hierarchy assembly task alongside targeted Visual Question Answering (VQA) samples, collectively aimed at evaluating progressive spatial reasoning and fundamental physical comprehension, including object properties, spatial relationships, and holistic scene understanding. PhyBlock includes 2600 block tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three key dimensions: partial completion, failure diagnosis, and planning robustness. We benchmark 21 state-of-the-art VLMs, highlighting their strengths and limitations in physically grounded, multi-step planning. Our empirical findings indicate that the performance of VLMs exhibits pronounced limitations in high-level planning and reasoning capabilities, leading to a notable decline in performance for the growing complexity of the tasks. Error analysis reveals persistent difficulties in spatial orientation and dependency reasoning. Surprisingly, chain-of-thought prompting offers minimal improvements, suggesting spatial tasks heavily rely on intuitive model comprehension. We position PhyBlock as a unified testbed to advance embodied reasoning, bridging vision-language understanding and real-world physical problem-solving.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HazeMatching: Dehazing Light Microscopy Images with Guided Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2506.22397</link>
<guid>https://arxiv.org/abs/2506.22397</guid>
<content:encoded><![CDATA[
arXiv:2506.22397v5 Announce Type: replace-cross 
Abstract: Fluorescence microscopy is a major driver of scientific progress in the life sciences. Although high-end confocal microscopes are capable of filtering out-of-focus light, cheaper and more accessible microscopy modalities, such as widefield microscopy, can not, which consequently leads to hazy image data. Computational dehazing is trying to combine the best of both worlds, leading to cheap microscopy but crisp-looking images. The perception-distortion trade-off tells us that we can optimize either for data fidelity, e.g. low MSE or high PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID. Existing methods either prioritize fidelity at the expense of realism, or produce perceptually convincing results that lack quantitative accuracy. In this work, we propose HazeMatching, a novel iterative method for dehazing light microscopy images, which effectively balances these objectives. Our goal was to find a balanced trade-off between the fidelity of the dehazing results and the realism of individual predictions (samples). We achieve this by adapting the conditional flow matching framework by guiding the generative process with a hazy observation in the conditional velocity field. We evaluate HazeMatching on 5 datasets, covering both synthetic and real data, assessing both distortion and perceptual quality. Our method is compared against 11 baselines, achieving a consistent balance between fidelity and realism on average. Additionally, with calibration analysis, we show that HazeMatching produces well-calibrated predictions. Note that our method does not need an explicit degradation operator to exist, making it easily applicable on real microscopy data. All data used for training and evaluation and our code will be publicly available under a permissive license.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2507.20973</link>
<guid>https://arxiv.org/abs/2507.20973</guid>
<content:encoded><![CDATA[
arXiv:2507.20973v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling</title>
<link>https://arxiv.org/abs/2508.04270</link>
<guid>https://arxiv.org/abs/2508.04270</guid>
<content:encoded><![CDATA[
arXiv:2508.04270v2 Announce Type: replace-cross 
Abstract: The primate visual cortex exhibits topographic organization, where functionally similar neurons are spatially clustered, a structure widely believed to enhance neural processing efficiency. While prior works have demonstrated that conventional deep ANNs can develop topographic representations, these models largely neglect crucial temporal dynamics. This oversight often leads to significant performance degradation in tasks like object recognition and compromises their biological fidelity. To address this, we leverage spiking neural networks (SNNs), which inherently capture spike-based temporal dynamics and offer enhanced biological plausibility. We propose a novel Spatio-Temporal Constraints (STC) loss function for topographic deep spiking neural networks (TDSNNs), successfully replicating the hierarchical spatial functional organization observed in the primate visual cortex from low-level sensory input to high-level abstract representations. Our results show that STC effectively generates representative topographic features across simulated visual cortical areas. While introducing topography typically leads to significant performance degradation in ANNs, our spiking architecture exhibits a remarkably small performance drop (No drop in ImageNet top-1 accuracy, compared to a 3% drop observed in TopoNet, which is the best-performing topographic ANN so far) and outperforms topographic ANNs in brain-likeness. We also reveal that topographic organization facilitates efficient and stable temporal information processing via the spike mechanism in TDSNNs, contributing to model robustness. These findings suggest that TDSNNs offer a compelling balance between computational performance and brain-like features, providing not only a framework for interpreting neural science phenomena but also novel insights for designing more efficient and robust deep learning models.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology Aware Neural Interpolation of Scalar Fields</title>
<link>https://arxiv.org/abs/2508.17995</link>
<guid>https://arxiv.org/abs/2508.17995</guid>
<content:encoded><![CDATA[
arXiv:2508.17995v2 Announce Type: replace-cross 
Abstract: This paper presents a neural scheme for the topology-aware interpolation of time-varying scalar fields. Given a time-varying sequence of persistence diagrams, along with a sparse temporal sampling of the corresponding scalar fields, denoted as keyframes, our interpolation approach aims at "inverting" the non-keyframe diagrams to produce plausible estimations of the corresponding, missing data. For this, we rely on a neural architecture which learns the relation from a time value to the corresponding scalar field, based on the keyframe examples, and reliably extends this relation to the non-keyframe time steps. We show how augmenting this architecture with specific topological losses exploiting the input diagrams both improves the geometrical and topological reconstruction of the non-keyframe time steps. At query time, given an input time value for which an interpolation is desired, our approach instantaneously produces an output, via a single propagation of the time input through the network. Experiments interpolating 2D and 3D time-varying datasets show our approach superiority, both in terms of data and topological fitting, with regard to reference interpolation schemes. Our implementation is available at this GitHub link : https://github.com/MohamedKISSI/Topology-Aware-Neural-Interpolation-of-Scalar-Fields.git.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance of Conformal Prediction in Capturing Aleatoric Uncertainty</title>
<link>https://arxiv.org/abs/2509.05826</link>
<guid>https://arxiv.org/abs/2509.05826</guid>
<content:encoded><![CDATA[
arXiv:2509.05826v2 Announce Type: replace-cross 
Abstract: Conformal prediction is a model-agnostic approach to generating prediction sets that cover the true class with a high probability. Although its prediction set size is expected to capture aleatoric uncertainty, there is a lack of evidence regarding its effectiveness. The literature presents that prediction set size can upper-bound aleatoric uncertainty or that prediction sets are larger for difficult instances and smaller for easy ones, but a validation of this attribute of conformal predictors is missing. This work investigates how effectively conformal predictors quantify aleatoric uncertainty, specifically the inherent ambiguity in datasets caused by overlapping classes. We perform this by measuring the correlation between prediction set sizes and the number of distinct labels assigned by human annotators per instance. We further assess the similarity between prediction sets and human-provided annotations. We use three conformal prediction approaches to generate prediction sets for eight deep learning models trained on four datasets. The datasets contain annotations from multiple human annotators (ranging from five to fifty participants) per instance, enabling the identification of class overlap. We show that the vast majority of the conformal prediction outputs show a very weak to weak correlation with human annotations, with only a few showing moderate correlation. These findings underscore the necessity of critically reassessing the prediction sets generated using conformal predictors. While they can provide a higher coverage of the true classes, their capability in capturing aleatoric uncertainty and generating sets that align with human annotations remains limited.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</title>
<link>https://arxiv.org/abs/2510.22300</link>
<guid>https://arxiv.org/abs/2510.22300</guid>
<content:encoded><![CDATA[
arXiv:2510.22300v2 Announce Type: replace-cross 
Abstract: Using risky text prompts, such as pornography and violent prompts, to test the safety of text-to-image (T2I) models is a critical task. However, existing risky prompt datasets are limited in three key areas: 1) limited risky categories, 2) coarse-grained annotation, and 3) low effectiveness. To address these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark designed for evaluating safety-related tasks in T2I models. Specifically, we first develop a hierarchical risk taxonomy, which consists of 6 primary categories and 14 fine-grained subcategories. Building upon this taxonomy, we construct a pipeline to collect and annotate risky prompts. Finally, we obtain 6,432 effective risky prompts, where each prompt is annotated with both hierarchical category labels and detailed risk reasons. Moreover, to facilitate the evaluation, we propose a reason-driven risky image detection method that explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt, we conduct a comprehensive evaluation of eight T2I models, nine defense methods, five safety filters, and five attack strategies, offering nine key insights into the strengths and limitations of T2I model safety. Finally, we discuss potential applications of T2I-RiskyPrompt across various research fields. The dataset and code are provided in https://github.com/datar001/T2I-RiskyPrompt.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization</title>
<link>https://arxiv.org/abs/2511.01588</link>
<guid>https://arxiv.org/abs/2511.01588</guid>
<content:encoded><![CDATA[
arXiv:2511.01588v2 Announce Type: replace-cross 
Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Analysis of Prenatal Ultrasound for Identification of Ventriculomegaly</title>
<link>https://arxiv.org/abs/2511.07827</link>
<guid>https://arxiv.org/abs/2511.07827</guid>
<content:encoded><![CDATA[
arXiv:2511.07827v2 Announce Type: replace-cross 
Abstract: The proposed study aimed to develop a deep learning model capable of detecting ventriculomegaly on prenatal ultrasound images. Ventriculomegaly is a prenatal condition characterized by dilated cerebral ventricles of the fetal brain and is important to diagnose early, as it can be associated with an increased risk for fetal aneuploidies and/or underlying genetic syndromes. An Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), recently developed by our group, was fine-tuned for a binary classification task to distinguish fetal brain ultrasound images as either normal or showing ventriculomegaly. The USF-MAE incorporates a Vision Transformer encoder pretrained on more than 370,000 ultrasound images from the OpenUS-46 corpus. For this study, the pretrained encoder was adapted and fine-tuned on a curated dataset of fetal brain ultrasound images to optimize its performance for ventriculomegaly detection. Model evaluation was conducted using 5-fold cross-validation and an independent test cohort, and performance was quantified using accuracy, precision, recall, specificity, F1-score, and area under the receiver operating characteristic curve (AUC). The proposed USF-MAE model reached an F1-score of 91.76% on the 5-fold cross-validation and 91.78% on the independent test set, with much higher scores than those obtained by the baseline models by 19.37% and 16.15% compared to VGG-19, 2.31% and 2.56% compared to ResNet-50, and 5.03% and 11.93% compared to ViT-B/16, respectively. The model also showed a high mean test precision of 94.47% and an accuracy of 97.24%. The Eigen-CAM (Eigen Class Activation Map) heatmaps showed that the model was focusing on the ventricle area for the diagnosis of ventriculomegaly, which has explainability and clinical plausibility.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI</title>
<link>https://arxiv.org/abs/2504.12197</link>
<guid>https://arxiv.org/abs/2504.12197</guid>
<content:encoded><![CDATA[
<div> Prototype learning, interpretability, concept mining, robustness, AI alignment  
<br /><br />Summary:  
1. The article addresses the challenge of limited interpretability in deep AI models, emphasizing the need for decisions that are understandable and aligned with human expectations.  
2. Existing post-hoc methods like GradCAM offer heatmaps but limited conceptual insight, while prototype-based methods provide example-based explanations but often suffer from rigid region selection and lack semantic consistency.  
3. To overcome these issues, the authors propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without requiring extra supervision.  
4. PCMNet clusters these prototypes into concept groups and extracts concept activation vectors, thereby providing structured, concept-level explanations that enhance interpretability.  
5. The method also improves robustness to occlusion and challenging visual conditions, crucial for reliable AI systems.  
6. Experimental results across multiple image classification benchmarks demonstrate that PCMNet outperforms state-of-the-art techniques in interpretability, stability, and robustness.  
7. This work advances AI alignment by improving transparency, controllability, and trustworthiness in AI models.  
8. The authors have made their code publicly available on GitHub to support further research and application. <div>
arXiv:2504.12197v3 Announce Type: replace 
Abstract: As AI systems grow more capable, it becomes increasingly important that their decisions remain understandable and aligned with human expectations. A key challenge is the limited interpretability of deep models. Post-hoc methods like GradCAM offer heatmaps but provide limited conceptual insight, while prototype-based approaches offer example-based explanations but often rely on rigid region selection and lack semantic consistency.
  To address these limitations, we propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without additional supervision. By clustering these prototypes into concept groups and extracting concept activation vectors, PCMNet provides structured, concept-level explanations and enhances robustness to occlusion and challenging conditions, which are both critical for building reliable and aligned AI systems.
  Experiments across multiple image classification benchmarks show that PCMNet outperforms state-of-the-art methods in interpretability, stability, and robustness. This work contributes to AI alignment by enhancing transparency, controllability, and trustworthiness in AI systems. Our code is available at: https://github.com/alehdaghi/PCMNet.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2510.22946</link>
<guid>https://arxiv.org/abs/2510.22946</guid>
<content:encoded><![CDATA[
<div> Keywords: unified multimodal models, double fusion mechanism, multimodal self-attention, text-to-image generation, image editing<br /><br />Summary: This paper addresses the challenge of training unified multimodal models, which typically require extensive computational resources when trained from scratch. The authors propose an efficient approach that strategically fuses publicly available pre-trained models designed specifically for generation or understanding tasks. The core innovation, called the double fusion mechanism, involves retaining the original model blocks while interleaving new multimodal self-attention blocks throughout the network. This design not only preserves the strengths of the individual base models but also enhances multimodal fusion by combining high-level semantic representations from the understanding encoder with low-level spatial information from the generation encoder. The method achieves competitive performance across several benchmarks using only around 35 billion training tokens. Notably, the approach attains a score of 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing tasks. The authors also release all associated code, model weights, and datasets to foster further research on unified multimodal modeling. <div>
arXiv:2510.22946v4 Announce Type: replace 
Abstract: Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment</title>
<link>https://arxiv.org/abs/2511.15831</link>
<guid>https://arxiv.org/abs/2511.15831</guid>
<content:encoded><![CDATA[
<div> Keywords: virtual try-on, multimodal large language model, semantic alignment, progressive training, data scarcity<br /><br />Summary:<br /><br />Image-based virtual try-on (VTON) aims to generate photorealistic images of people wearing designated garments, but creating a universal framework capable of managing diverse and complex scenarios remains challenging. Recent multi-task VTON approaches use textual instructions for guidance but face two main issues: the semantic gap between text and images and the scarcity of data in complicated cases. To solve these problems, the authors propose UniFit, a universal VTON system driven by a Multimodal Large Language Model (MLLM). The core innovation is the MLLM-Guided Semantic Alignment Module (MGSA), which combines multimodal inputs via an MLLM and learnable queries, employing a semantic alignment loss to capture cross-modal semantics and enhance coherence, thereby reducing the semantic gap. UniFit employs a two-stage progressive training strategy coupled with a self-synthesis pipeline, enabling it to effectively learn complex tasks from limited data. Experimental results demonstrate that UniFit supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, while achieving state-of-the-art performance. The authors have released their source code and pretrained models at the provided GitHub repository to facilitate further research and application. <div>
arXiv:2511.15831v1 Announce Type: new 
Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3</title>
<link>https://arxiv.org/abs/2511.15833</link>
<guid>https://arxiv.org/abs/2511.15833</guid>
<content:encoded><![CDATA[
<div> Segment Anything Model 3, Promptable Concept Segmentation, EfficientSAM3, Progressive Hierarchical Distillation, On-device segmentation<br /><br />Summary:<br /><br />The paper introduces EfficientSAM3, a family of efficient models designed to enable on-device visual understanding by distilling the capabilities of the Segment Anything Model 3 (SAM3) into lightweight student networks. The core approach, Progressive Hierarchical Distillation (PHD), transfers knowledge from SAM3 to students in three stages: first, Encoder Distillation aligns image features through prompt-in-the-loop training on the SA-1B dataset; second, Temporal Memory Distillation replaces SAM3’s dense memory with a compact Perceiver-based module trained on SA-V for efficient spatiotemporal feature compression and retrieval; third, End-to-End Fine-Tuning refines the entire model pipeline using official SAM3 PCS data to retain concept-level segmentation accuracy. This methodology enables creating multiple EfficientSAM3 student variants with RepViT, TinyViT, and EfficientViT backbones, striking a balance between performance and computational efficiency suitable for mobile or edge devices. The models are benchmarked on popular video object segmentation (VOS) datasets, demonstrating competitive performance while significantly reducing computational costs compared to the original unified SAM3 architecture. The work offers a practical solution to bring promptable concept segmentation and tracking to resource-constrained environments without substantial loss of fidelity to the original teacher model’s behavior. <div>
arXiv:2511.15833v1 Announce Type: new 
Abstract: The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion</title>
<link>https://arxiv.org/abs/2511.15874</link>
<guid>https://arxiv.org/abs/2511.15874</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, occlusion, dynamic sampling, multi-hypothesis inference, iterative refinement<br /><br />Summary:<br /><br />Accurate 6D object pose estimation is essential for applications such as robotics, augmented reality, and scene understanding. Traditional methods often rely on access to CAD models and a multi-stage pipeline involving detection, initial pose proposal, and refinement, but this approach struggles with occlusion, which causes errors early on that propagate and reduce accuracy. To tackle this, the authors introduce four key improvements to model-based 6D pose estimation techniques: (i) a dynamic non-uniform dense sampling strategy focusing computational resources on visible parts of the object, thereby reducing errors caused by occlusion; (ii) a multi-hypothesis inference mechanism that maintains several confidence-ranked pose candidates, preventing failures from relying on a single estimate; (iii) iterative refinement steps that progressively enhance pose accuracy; and (iv) occlusion-centered data augmentations during training to improve robustness and generalization. Additionally, they propose a new evaluation metric weighted by visibility to better assess performance under occlusion and reduce bias in existing benchmarks. Experimental results demonstrate that their approach significantly boosts accuracy by over 5% on the ICBIN dataset and more than 2% on BOP benchmarks, while also achieving approximately three times faster inference speed compared to previous methods. <div>
arXiv:2511.15874v1 Announce Type: new 
Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation</title>
<link>https://arxiv.org/abs/2511.15875</link>
<guid>https://arxiv.org/abs/2511.15875</guid>
<content:encoded><![CDATA[
<div> Keywords: historical maps, synthetic data, deep learning, cartographic style transfer, semantic segmentation<br /><br />Summary:<br /><br />1. This paper addresses the challenge of automating the analysis of historical maps using deep learning, which typically requires large annotated datasets that are scarce for specific and homogeneous historical map corpora.<br /><br />2. The authors propose generating synthetic historical map data by transferring the cartographic style of an original map corpus onto vector data, thus creating an effectively unlimited dataset that preserves both style and structure.<br /><br />3. Two methods are introduced to simulate the visual uncertainty and noise present in historical map scans: an automatic deep generative approach and a manual stochastic degradation technique, aimed at emulating data-dependent uncertainty.<br /><br />4. To validate the proposed data bootstrapping methods, the generated datasets were employed for domain-adaptive semantic segmentation using a Self-Constructing Graph Convolutional Network, demonstrating the applicability and effectiveness of synthetic data.<br /><br />5. Overall, the work provides a practical solution to overcome data scarcity for deep learning applied to historical maps by synthesizing realistic and diverse training samples, improving the performance of land-cover interpretation tasks within homogeneous historical map corpora. <div>
arXiv:2511.15875v1 Announce Type: new 
Abstract: The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes</title>
<link>https://arxiv.org/abs/2511.15884</link>
<guid>https://arxiv.org/abs/2511.15884</guid>
<content:encoded><![CDATA[
<div> 6D pose estimation, warehouse automation, RGB-D, category-level method, Box6D<br /><br />Summary:<br /><br />Accurate and efficient 6D pose estimation of novel objects in cluttered and occluded environments is essential for robotic manipulation tasks in warehouse automation, bin picking, logistics, and e-commerce fulfillment. Existing methods are divided into three groups: model-based approaches that rely on exact CAD models but struggle with environmental transfer; model-free approaches that use few reference images but fail under challenging conditions; and category-level approaches which balance flexibility and accuracy but lack environment and object priors, reducing industrial practicability. This paper introduces Box6D, a category-level 6D pose estimation technique specifically designed for storage boxes in warehouse contexts. Box6D uses a single RGB-D observation to infer box dimensions rapidly via binary search and estimates poses using a generic category CAD template instead of instance-specific models. It incorporates a depth-based plausibility filter and an early-stopping strategy to reject implausible pose hypotheses, significantly reducing computational load. Evaluations on real-world storage settings and public benchmarks demonstrate that Box6D achieves competitive or superior 6D pose accuracy compared to prior methods while reducing inference time by approximately 76%, making it suitable for practical industrial applications. <div>
arXiv:2511.15884v1 Announce Type: new 
Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification</title>
<link>https://arxiv.org/abs/2511.15923</link>
<guid>https://arxiv.org/abs/2511.15923</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, domain-specific video classification, rationale gap, self-generated rationales, fine-tuning  

<br /><br />Summary: Vision Language Models (VLMs) often face challenges in domain-specific video classification, especially when data is limited, due to a rationale gap that hinders effective semantic alignment between spatio-temporal content and abstract labels. To address this, the paper introduces a two-stage self-improvement method that does not require additional annotations. First, VLMs are prompted to generate detailed textual rationales for each video, allowing the model to internally articulate domain-specific reasoning. These self-generated rationales serve as intermediate supervision during a fine-tuning stage, aligning the model’s internal representations with domain nuances. Second, a conventional supervised fine-tuning is applied on task labels, leveraging the enhanced domain reasoning gained from the previous stage. Experimental results across multiple diverse datasets confirm that this approach substantially outperforms standard direct supervised fine-tuning. The paper validates self-generated rationale as an effective and annotation-efficient strategy to adapt VLMs for specialized video analysis tasks, bridging the semantic gap without extra labeled data and improving model performance in low-resource settings. <div>
arXiv:2511.15923v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Medical Visual Understanding From Multi-Granular Language Learning</title>
<link>https://arxiv.org/abs/2511.15943</link>
<guid>https://arxiv.org/abs/2511.15943</guid>
<content:encoded><![CDATA[
<div> Multi-Granular Language Learning, contrastive learning, multi-label alignment, cross-granularity, medical imaging<br /><br />Summary:<br /><br />This paper addresses the limitations of existing Contrastive Language-Image Pretraining (CLIP) methods, which primarily focus on single-label and single-granularity alignment, restricting their performance in complex fields like medical imaging. The authors propose Multi-Granular Language Learning (MGLL), a new contrastive learning framework designed to handle multi-label and cross-granularity alignment effectively. MGLL utilizes structured multi-label supervision, integrating textual descriptions from different annotation granularities, such as diagnostic descriptions and clinical explanations. A novel feature of MGLL is the introduction of soft-label supervision combined with point-wise constraints to improve the alignment between images and textual data. To maintain consistency across different granularities, MGLL employs a smooth Kullback-Leibler (KL) divergence, which also ensures computational efficiency. The framework is designed as a plug-and-play module, making it easily integrable with existing vision-language models. MGLL was pretrained on a large-scale multi-granular dataset constructed by the authors and evaluated across several downstream tasks, where it demonstrated superior performance compared to state-of-the-art methods. The authors have made the code publicly available to encourage further research and development in this area. <div>
arXiv:2511.15943v1 Announce Type: new 
Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Interpretable 2D Video Extraction from 3D Echocardiography</title>
<link>https://arxiv.org/abs/2511.15946</link>
<guid>https://arxiv.org/abs/2511.15946</guid>
<content:encoded><![CDATA[
<div> 3D echocardiography, automated 2D view selection, deep learning classifier, cardiac ultrasound, clinical validation<br /><br />Summary:<br /><br />1. Traditional cardiac ultrasound imaging relies heavily on 2D videos despite the heart's complex three-dimensional structure, limiting comprehensive assessment.<br /><br />2. 3D echocardiography provides higher-quality volumetric images that can improve diagnostic capability and streamline image acquisition, but clinicians are accustomed to interpreting standard 2D views.<br /><br />3. The authors propose an automated method that extracts standard 2D echocardiography views from 3D ultrasound volumes by combining a deep learning-based view classifier with heuristic rules based on anatomical landmarks and expert cardiologist input.<br /><br />4. This methodology was rigorously validated through blinded evaluation by three cardiologists on a dataset of 1,600 videos from two hospitals, achieving 96% accuracy in identifying standard views.<br /><br />5. Further validation demonstrated that the extracted 2D videos preserved crucial spatial and diagnostic features, enabling accurate real-world interpretation and effective use with AI models (EchoPrime, PanEcho) for detecting cardiac abnormalities and clinical measurement tools (EchoNet-Measurement).<br /><br />6. The authors publicly released both their code and a dataset of 29 3D echocardiography videos to promote further research and clinical translation. <div>
arXiv:2511.15946v1 Announce Type: new 
Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click</title>
<link>https://arxiv.org/abs/2511.15948</link>
<guid>https://arxiv.org/abs/2511.15948</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Scene Graph Generation, interactive framework, panoptic segmentation, human guidance, semantic reasoning<br /><br />Summary:<br /><br />This paper introduces Click2Graph, the first interactive framework designed for Panoptic Video Scene Graph Generation (PVSG) that integrates user interaction with spatial, temporal, and semantic reasoning. Unlike existing VSGG systems that operate as closed, feed-forward models without user input, Click2Graph allows precise user prompting, such as a click or bounding box, to guide the segmentation and tracking of subjects across video frames. The framework autonomously identifies interacting objects and predicts triplets to create a temporally consistent scene graph that captures dynamic relationships. Click2Graph features two innovative components: a Dynamic Interaction Discovery Module that generates object prompts conditioned on the selected subject, and a Semantic Classification Head that jointly reasons over entities and their predicates, enabling deeper semantic understanding. Experimental results on the OpenPVSG benchmark demonstrate that this approach effectively combines panoptic grounding, relational inference, and human prompting, establishing a new paradigm for controllable and interpretable video scene understanding. This work contributes a foundation for future research in user-guided scene graph generation in complex video environments by uniting visual prompting with comprehensive semantic and temporal analysis. <div>
arXiv:2511.15948v1 Announce Type: new 
Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts  triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer</title>
<link>https://arxiv.org/abs/2511.15967</link>
<guid>https://arxiv.org/abs/2511.15967</guid>
<content:encoded><![CDATA[
<div> CLIP, Open-vocabulary, Semantic Segmentation, Mutual Information, Fine-tuning  

<br /><br />Summary:  
This paper addresses the challenge of fine-tuning CLIP for open-vocabulary semantic segmentation without losing its strong vision-language alignment. The authors propose InfoCLIP, a novel approach grounded in information theory to stabilize modality alignment during fine-tuning. InfoCLIP introduces two mutual information-based objectives: the first compresses the pixel-text modality alignment from the pretrained CLIP model to reduce noise caused by coarse-grained local semantic features. The second objective maximizes mutual information between the pretrained CLIP's alignment knowledge and the fine-tuned model to facilitate the transfer of compact local semantic relations that better suit segmentation tasks. This method effectively mitigates overfitting on limited categories commonly seen in existing fine-tuning approaches. Extensive experiments on multiple benchmarks demonstrate that InfoCLIP enhances the generalization and adaptability of CLIP for open-vocabulary semantic segmentation. The results highlight InfoCLIP's superiority in asymmetric knowledge transfer and its ability to preserve the pretrained vision-language relationships while improving segmentation accuracy across unseen categories. <div>
arXiv:2511.15967v1 Announce Type: new 
Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation</title>
<link>https://arxiv.org/abs/2511.15968</link>
<guid>https://arxiv.org/abs/2511.15968</guid>
<content:encoded><![CDATA[
<div> Multi-task learning, breast ultrasound, tumor segmentation, consistency regularization, BI-RADS features<br /><br />Summary:  
This study addresses the challenge of destructive task interference in multi-task learning, where models trained to perform multiple tasks simultaneously often underperform compared to single-task baselines and exhibit limited generalization. To overcome this issue in the context of breast ultrasound-based tumor segmentation, the authors propose a novel consistency regularization approach. This method integrates differentiable BI-RADS-inspired morphological features to reduce interference between segmentation and classification tasks during training. The approach was developed and validated using the BrEaST dataset from Poland and tested for generalization on three external datasets from Spain and Egypt: UDIAT, BUSI, and BUS-UCLM. The results show statistically significant improvements (p < 0.001) in segmentation accuracy as measured by the Dice coefficient, with scores increasing from 0.59 to 0.81 on UDIAT, 0.56 to 0.66 on BUSI, and 0.49 to 0.69 on BUS-UCLM when comparing the proposed multi-task method to the baseline. Furthermore, the proposed approach achieves state-of-the-art segmentation performance on the rigorously validated UDIAT external dataset. These findings highlight the effectiveness of consistency regularization using morphological features in improving multi-task learning for medical image analysis. <div>
arXiv:2511.15968v1 Announce Type: new 
Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition</title>
<link>https://arxiv.org/abs/2511.15984</link>
<guid>https://arxiv.org/abs/2511.15984</guid>
<content:encoded><![CDATA[
<div> Keywords: visual semantic understanding, detection-guided generative framework, hierarchical category, attribute recognition, e-commerce datasets<br /><br />Summary:<br /><br />This paper addresses the challenge of achieving visual semantic understanding by proposing a unified framework that integrates object detection, category prediction, and attribute recognition. Unlike existing methods that depend on global similarity measures and often fail to differentiate fine-grained categories and diverse attributes, especially in large-scale e-commerce contexts, the authors introduce a novel detection-guided generative framework. The approach extracts refined region of interest (ROI)-level features for each detected object and leverages a BART-based generative model to sequentially produce semantic tokens. These tokens represent coarse-to-fine hierarchical categories and detailed property-value attribute pairs, enabling property-conditioned attribute recognition. The framework is designed to capture subtle distinctions and variations within categories more effectively than traditional similarity-based or multi-stage classification pipelines. Extensive experiments on proprietary large-scale e-commerce datasets, as well as open-source datasets, validate the superiority of this method. Results show significant improvements in fine-grained recognition accuracy and more coherent unified inference compared to existing solutions. This work contributes a powerful model that advances the state-of-the-art in combined object detection and semantic attribute prediction with broad applicability in e-commerce visual understanding. <div>
arXiv:2511.15984v1 Announce Type: new 
Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection</title>
<link>https://arxiv.org/abs/2511.15986</link>
<guid>https://arxiv.org/abs/2511.15986</guid>
<content:encoded><![CDATA[
<div> Medical imaging, fairness, multimodal large language models, in-context learning, demonstration selection<br /><br />Summary:<br /><br />1. Multimodal large language models (MLLMs) have demonstrated strong potential in medical image reasoning but face significant challenges related to fairness across diverse demographic groups.  
2. Existing debiasing methods often require large labeled datasets or extensive fine-tuning, which are impractical for foundation-scale models due to resource constraints.  
3. The authors propose leveraging In-Context Learning (ICL) as a lightweight and tuning-free alternative to improve fairness in MLLMs.  
4. Conventional demonstration selection (DS) strategies inadequately address fairness because they select exemplars that are demographically imbalanced, leading to biased outcomes.  
5. To counter this, the study introduces Fairness-Aware Demonstration Selection (FADS), a clustering-based sampling method that ensures demonstrations are both demographically balanced and semantically relevant.  
6. Experimental evaluation on various medical imaging benchmarks shows FADS consistently reduces disparities related to gender, race, and ethnicity while maintaining high accuracy in medical image reasoning tasks.  
7. The results highlight the potential of fairness-aware in-context learning as an efficient, scalable, and data-efficient approach to achieving equitable medical image reasoning in foundation-scale models. <div>
arXiv:2511.15986v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.16015</link>
<guid>https://arxiv.org/abs/2511.16015</guid>
<content:encoded><![CDATA[
<div> out-of-distribution detection, long-tailed datasets, graph convolutional networks, Gaussianization, vision benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting out-of-distribution (OOD) data in deep neural networks, especially within the context of long-tailed in-distribution (ID) datasets that cause high false positive rates and poor accuracy for tail classes.<br /><br />2. It proposes leveraging inter-sample relationships through a graph-based representation, constructed in the feature space of a pre-trained model, to enhance OOD detection performance.<br /><br />3. The method incorporates Gaussianization to adjust for distribution differences between the activation layers of the pre-trained model and the training data, aligning activations closer to a standard normal distribution.<br /><br />4. The initial graph structure is refined using graph convolutional networks (GCNs), resulting in a feature space that is more effective for OOD detection in long-tailed scenarios.<br /><br />5. Experimental results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks demonstrate significant improvements over state-of-the-art methods, notably reducing false positive rates and enhancing classification accuracy for tail classes. <div>
arXiv:2511.16015v1 Announce Type: new 
Abstract: Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial textures, human detection, sequence-level optimization, physical garments, cross-model transferability<br /><br />Summary:<br />1. The paper addresses the vulnerability of deep neural networks for human detection to adversarial attacks in real-world surveillance settings.<br />2. It introduces a sequence-level optimization framework to generate natural, printable adversarial textures for clothing items such as shirts, trousers, and hats.<br />3. The approach maps product images to UV space, converts them into a compact palette with control-point parameterization, and uses ICC locking to ensure printable colors.<br />4. A physically based human-garment simulation pipeline models motion, multi-angle camera views, cloth dynamics, and changing illumination.<br />5. The method uses an expectation-over-transformation objective with temporal weighting to minimize detection confidence across entire walking video sequences.<br />6. Experimental results demonstrate strong, stable concealment with robustness to viewpoint changes and effective transferability across different models.<br />7. Physical garments created through sublimation printing reliably suppress detection both indoors and outdoors, confirming the practical feasibility of the approach. <div>
arXiv:2511.16020v1 Announce Type: new 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16024</link>
<guid>https://arxiv.org/abs/2511.16024</guid>
<content:encoded><![CDATA[
<div> MoE, Real-world Image Super-Resolution, Mixture-of-Ranks, LoRA, Degradation Estimation<br /><br />Summary:  
This paper explores the application of sparsely-gated Mixture-of-Experts (MoE) architectures to real-world image super-resolution (Real-ISR), addressing the limitations of existing dense models. The authors propose a novel Mixture-of-Ranks (MoR) framework that treats each rank in Low-Rank Adaptation (LoRA) as an independent expert, enabling fine-grained knowledge recombination while preserving shared common-sense features. To dynamically guide expert activation depending on image degradation, a degradation estimation module is introduced, which utilizes CLIP embeddings combined with predefined positive-negative text pairs to assess relative degradation severity. The framework also incorporates zero-expert slots and a degradation-aware load-balancing loss, allowing adaptive allocation of computational resources by adjusting the number of active experts based on the complexity of degraded inputs. This approach enhances flexibility in capturing heterogeneous and complex real-world degradations and facilitates more efficient knowledge sharing under fixed computational budgets. Comprehensive experiments demonstrate the proposed MoR architecture achieves state-of-the-art performance in single-step Real-ISR tasks, validating its effectiveness and robustness compared to prior dense and fine-tuned diffusion-based models. <div>
arXiv:2511.16024v1 Announce Type: new 
Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning</title>
<link>https://arxiv.org/abs/2511.16026</link>
<guid>https://arxiv.org/abs/2511.16026</guid>
<content:encoded><![CDATA[
<div> Keywords: laser cutting, speckle sensing, material classification, deep learning, convolutional neural network  

<br /><br />Summary:  
This paper addresses the challenge of monitoring and controlling the laser cutting process by proposing a material classification technique based on speckle pattern analysis combined with deep learning. Laser cutting, widely used in various industries, produces dust and aerosols that pose environmental and health risks, making precise process control crucial. The proposed method leverages a convolutional neural network (CNN) trained on laser speckle pattern images to identify different material types in real-time, enhancing cutting safety and efficiency. Unlike previous methods that struggled with variations in laser color, this model achieves high classification accuracy even when the laser wavelength changes. Experimental results show the model reaches 98.30% accuracy on the training data and 96.88% on validation data. Additionally, when tested on a large, diverse dataset of 3000 new images across 30 material types, the model attains a strong F1-score of 0.9643. These results demonstrate the robustness and effectiveness of the technique, offering a reliable solution for material-aware laser cutting applications that can adapt to different operational conditions while maintaining high performance. <div>
arXiv:2511.16026v1 Announce Type: new 
Abstract: Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis</title>
<link>https://arxiv.org/abs/2511.16030</link>
<guid>https://arxiv.org/abs/2511.16030</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, sparse-view reconstruction, curriculum learning, student views, multi-signal metric<br /><br />Summary:<br /><br />1. The paper addresses the challenge of 3D scene reconstruction and rendering using 3D Gaussian Splatting (3DGS) in sparse-view settings, where limited viewpoint coverage causes supervision scarcity and overfitting.  
2. It proposes CuriGS, a curriculum-guided framework designed to improve sparse-view 3D reconstruction leveraging 3DGS by introducing the concept of student views—pseudo-views sampled near ground-truth (teacher) poses.  
3. Multiple groups of student views with varying levels of perturbation are generated for each teacher pose, and a curriculum schedule progressively unlocks higher perturbation levels during training to enhance learning stability.  
4. Each student view undergoes regularization based on depth correlation and co-regularization and is evaluated using a combined multi-signal metric integrating SSIM, LPIPS, and an image-quality measure to ensure training robustness and quality.  
5. The best-performing students at each perturbation level are periodically retained, and those meeting a quality threshold are promoted into the training set, resulting in stable augmentation and improved sparse training views, which leads to superior rendering fidelity and geometric consistency compared to state-of-the-art baselines on synthetic and real datasets. <div>
arXiv:2511.16030v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossmodal learning for Crop Canopy Trait Estimation</title>
<link>https://arxiv.org/abs/2511.16031</link>
<guid>https://arxiv.org/abs/2511.16031</guid>
<content:encoded><![CDATA[
<div> Keywords: plant phenotyping, UAV, satellite imagery, cross-modal learning, crop canopy trait estimation<br /><br />Summary:  
Recent advances in plant phenotyping have promoted the use of multi-sensor platforms for collecting crop canopy reflectance data, particularly employing UAVs due to their high resolution and effectiveness in crop monitoring and prediction. Satellite imagery, while useful for agricultural tasks, suffers from limited spatial resolution, which restricts its applicability in modern micro-plot management in farming systems. This study introduces a cross-modal learning approach designed to enhance low-resolution satellite images by infusing them with the detailed visual information typically available from UAV imagery. The research is based on a dataset of approximately co-registered satellite and UAV image pairs from replicated plots of 84 hybrid maize varieties collected across five different locations in the U.S. Corn Belt. The model learns fine-grained spatial and spectral correspondences between the two sensing modalities. Experimental results demonstrate that this method can generate UAV-like representations from satellite inputs that consistently outperform original satellite images in downstream tasks, including yield and nitrogen content prediction. This work highlights the potential of leveraging cross-modal correspondence learning to bridge the resolution gap between satellites and UAVs, thereby advancing agricultural monitoring accuracy and utility. <div>
arXiv:2511.16031v1 Announce Type: new 
Abstract: Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets</title>
<link>https://arxiv.org/abs/2511.16037</link>
<guid>https://arxiv.org/abs/2511.16037</guid>
<content:encoded><![CDATA[
<div> keywords: food recognition, domain-shift, long-tailed distribution, large language models, fine-grained classification<br /><br />Summary: Training models for food recognition faces significant challenges due to domain-shift problems where internet-crawled images differ visually from user-captured photos in real-life settings. Additionally, real-world food datasets often have a long-tailed distribution, meaning some categories have very few samples, and many dishes are visually similar, making fine-grained classification difficult. This paper proposes a novel framework that leverages large language models (LLMs) to tackle these issues. First, LLMs are utilized to analyze and interpret food images to generate descriptive food titles and ingredient lists. Second, the generated textual information and food images from multiple domains are projected into a shared embedding space, where their similarity is maximized. Finally, the model uses these aligned multimodal features to perform the recognition task. This approach unifies the benefits of multimodal representation learning to address domain adaptation, data imbalance, and subtle inter-class variations simultaneously. Experimental results demonstrate that this simple yet effective framework outperforms existing state-of-the-art methods designed specifically for long-tailed distributions, domain adaptation, and fine-grained food classification on two benchmark food datasets. This indicates the powerful role of LLMs and multimodal alignment in enhancing real-world food recognition systems. <div>
arXiv:2511.16037v1 Announce Type: new 
Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers</title>
<link>https://arxiv.org/abs/2511.16047</link>
<guid>https://arxiv.org/abs/2511.16047</guid>
<content:encoded><![CDATA[
<div> Visual autoregressive modeling, next-scale prediction, KV caching, multi-scale image generation, cache optimization  

<br /><br />Summary:  
This paper addresses the challenges of Key and Value (KV) caching in Visual Autoregressive (VAR) models using next-scale prediction for image generation. First, the authors identify that attending to tokens from local scales is crucial for maintaining high generation quality. Second, they observe that allocating limited KV memory to the coarsest scales, called condensed scales, helps stabilize multi-scale image generation. Third, they distinguish two types of layers in VAR transformers: cache-efficient layers, which show strong KV similarity across finer scales, and cache-demanding layers that exhibit weaker inter-scale similarity, leading to higher cache requirements. Based on these insights, the authors propose AMS-KV, a scale-adaptive KV caching policy prioritizing the storage of KVs from condensed and local scales to preserve relevant tokens. AMS-KV further improves cache utilization and computational efficiency by identifying cache-demanding layers through inter-scale similarity analysis. Experimental results show that AMS-KV reduces KV cache usage by up to 84.83% and decreases self-attention latency by 60.48% compared to baseline next-scale VAR models. Moreover, this method enables stable scaling to batch sizes of 256, doubling the batch size previously limited to 128 due to out-of-memory errors, and also improves throughput, demonstrating significant scalability and efficiency benefits for VAR-based image generation. <div>
arXiv:2511.16047v1 Announce Type: new 
Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.16049</link>
<guid>https://arxiv.org/abs/2511.16049</guid>
<content:encoded><![CDATA[
<div> 4D LiDAR, generative model, Hybrid-Cylindrical-Spherical representation, Spatio-Temporal Attention, controllable synthesis  

<br /><br />Summary: Synthesizing high-fidelity and controllable 4D LiDAR data is essential for scalable simulation environments in autonomous driving. The process is challenging due to the unique spherical geometry of sensors, temporal sparsity of point clouds, and dynamic scene complexity. The authors propose LiSTAR, a generative world model that operates on the sensor’s native geometry to improve data fidelity. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to reduce quantization artifacts commonly seen with Cartesian grids. To capture complex temporal dynamics from sparse data, it employs a Spatio-Temporal Attention with a Ray-Centric Transformer (START), which models feature evolution along sensor rays for improved temporal coherence. For controllable data synthesis, the method includes a novel 4D point cloud-aligned voxel layout for conditioning and a discrete Masked Generative START (MaskSTART) framework that creates a compact tokenized scene representation, enabling efficient, high-resolution, and layout-guided compositional generation. Extensive experiments demonstrate that LiSTAR achieves state-of-the-art results in 4D LiDAR reconstruction, prediction, and conditional generation, showing significant performance improvements such as a 76% reduction in generation MMD, a 32% increase in reconstruction IoU, and a 50% decrease in prediction L1 Median error. These advances establish a strong foundation for realistic and controllable simulations in autonomous systems. <div>
arXiv:2511.16049v1 Announce Type: new 
Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.16077</link>
<guid>https://arxiv.org/abs/2511.16077</guid>
<content:encoded><![CDATA[
<div> Keywords: video reasoning segmentation, reinforcement learning, frame sampler, reasoning chains, mask propagation<br /><br />Summary:<br /><br />The article introduces VideoSeg-R1, a novel framework that applies reinforcement learning to video reasoning segmentation, a task traditionally reliant on supervised fine-tuning with limited generalization capabilities. The framework features a decoupled architecture that tackles the problem by combining referring image segmentation with video mask propagation. VideoSeg-R1 operates in three distinct stages: first, a hierarchical text-guided frame sampler mimics human attention to select frames intelligently; second, a reasoning model generates spatial cues alongside explicit reasoning chains to facilitate interpretable and structured analysis; third, a segmentation-propagation module utilizes advanced tools SAM2 and XMem to extend segmentation masks across video frames. Additionally, the system incorporates a task difficulty-aware mechanism that dynamically adjusts the reasoning length, optimizing both efficiency and segmentation accuracy. Extensive experiments on various benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation settings. The proposed approach enhances generalization to out-of-distribution scenarios, offers explicit reasoning for better interpretability, and accelerates processing through adaptive control. The authors plan to release the code publicly to support further research and development in this field. <div>
arXiv:2511.16077v1 Announce Type: new 
Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectralTrain: A Universal Framework for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.16084</link>
<guid>https://arxiv.org/abs/2511.16084</guid>
<content:encoded><![CDATA[
<div> hyperspectral image classification, curriculum learning, PCA spectral downsampling, training efficiency, remote sensing<br /><br />Summary:<br /><br />1. This study addresses the computational challenges in hyperspectral image (HSI) classification by proposing SpectralTrain, a universal and architecture-agnostic training framework.<br /><br />2. SpectralTrain integrates curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling to gradually introduce spectral complexity while retaining crucial information.<br /><br />3. The approach improves learning efficiency by enabling models to capture spectral-spatial patterns with significantly reduced computational cost, achieving 2-7x faster training times.<br /><br />4. The framework is compatible with various architectures, optimizers, and loss functions, demonstrating flexibility across classical and state-of-the-art deep learning models.<br /><br />5. Extensive experiments on Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 datasets reveal strong generalization across different spatial scales, spectral properties, and application domains.<br /><br />6. Application to cloud classification highlights the framework’s potential for climate-related remote sensing tasks.<br /><br />7. SpectralTrain emphasizes training strategy optimization as a powerful complement to architectural design in improving HSI model performance.<br /><br />8. The authors provide open-source code to facilitate adoption and further research at https://github.com/mh-zhou/SpectralTrain. <div>
arXiv:2511.16084v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</title>
<link>https://arxiv.org/abs/2511.16091</link>
<guid>https://arxiv.org/abs/2511.16091</guid>
<content:encoded><![CDATA[
<div> radar SLAM, 3D Gaussian, dynamic object masking, large-scale mapping, outdoor reconstruction<br /><br />Summary:<br /><br />Rad-GS is a novel 4D radar-camera SLAM system tailored for kilometer-scale outdoor environments, which leverages 3D Gaussian functions as a differentiable spatial representation. It integrates raw radar point clouds enriched with Doppler information and geometrically enhanced point clouds to improve dynamic object masking in synchronized images, reducing rendering artifacts and increasing localization accuracy. The system also utilizes unsynchronized image frames for global refinement of the 3D Gaussian model, enhancing both texture consistency and novel view synthesis quality. To efficiently manage large-scale environments, Rad-GS employs a global octree data structure combined with a strategic approach to Gaussian primitive management, significantly suppressing noise while minimizing memory consumption. Extensive experimental evaluations and ablation studies show that Rad-GS performs on par with traditional 3D Gaussian methods based on camera or LiDAR data, establishing its robustness and reliability for outdoor mapping. Finally, real-world reconstructions at kilometer scales validate Rad-GS’s capability for detailed and scalable large-scale scene reconstruction using 4D mmWave radar, highlighting its potential as a practical solution for robust outdoor SLAM applications. <div>
arXiv:2511.16091v1 Announce Type: new 
Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs</title>
<link>https://arxiv.org/abs/2511.16107</link>
<guid>https://arxiv.org/abs/2511.16107</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, visual in-context learning, vision-language models, cross-task learning, prompt generation<br /><br />Summary: In this paper, the authors explore the capabilities of vision-language models (VLMs) to perform visual in-context learning (VICL) when the visual prompt and target images come from different tasks, a challenge known as cross-task VICL. They introduce T2T-VICL, a fully collaborative pipeline designed to investigate and enable cross-task VICL in unified VLMs. A key contribution is their mechanism to automatically generate and select text prompts that effectively describe the differences between two distinct low-level vision tasks, enabling better contextual understanding. Additionally, they create the first cross-task VICL dataset to benchmark and study these scenarios systematically. To enhance inference, the authors propose a novel framework that merges perceptual score-based reasoning with traditional evaluation metrics to achieve more reliable cross-task VICL performance. The approach demonstrates strong empirical results, achieving top-tier performance in nine cross-task scenarios and second-best results in ten others. This work expands the boundaries of VICL in VLMs by showing that these models can generalize and adapt across different visual tasks with appropriate prompting and evaluation strategies. <div>
arXiv:2511.16107v1 Announce Type: new 
Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustered Error Correction with Grouped 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.16112</link>
<guid>https://arxiv.org/abs/2511.16112</guid>
<content:encoded><![CDATA[
<div> 4D Gaussian Splatting, dynamic scene reconstruction, error correction, temporal consistency, perceptual rendering  

<br /><br />Summary:  
The paper addresses the challenges faced by existing 4D Gaussian Splatting (4DGS) methods in accurately reconstructing dynamic scenes, particularly issues with ambiguous pixel correspondences and insufficient densification in dynamic regions. The authors propose a novel method consisting of two main components: (1) Elliptical Error Clustering and Error Correcting Splat Addition, which identifies dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting, which enhances the consistency of mapping between splats and dynamic objects. The method classifies rendering errors into two types — missing-color and occlusion errors — and applies specific corrections such as backprojection or foreground splitting, guided by cross-view color consistency to ensure accuracy. Experimental evaluations on Neural 3D Video and Technicolor datasets reveal significant improvements in temporal consistency and perceptual rendering quality, achieving a 0.39 dB PSNR increase on the Technicolor Light Field dataset. Visualization results demonstrate better alignment of splats with dynamic objects and the effectiveness of the error correction techniques in detecting errors and initializing new splats properly. The authors also provide their implementation details and source code publicly via GitHub for reproducibility and further research. <div>
arXiv:2511.16112v1 Announce Type: new 
Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Complexity from Scale in Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2511.16117</link>
<guid>https://arxiv.org/abs/2511.16117</guid>
<content:encoded><![CDATA[
<div> Keywords: latent diffusion models, scale-independent latent space, hierarchical tokens, visual generation, coarse-to-fine generation<br /><br />Summary:<br /><br />Existing latent diffusion models typically link the scale of the data (image resolution or video frame rate) with content complexity by increasing latent tokens accordingly. However, this approach overlooks that the true latent capacity needed depends primarily on content complexity rather than scale, which merely sets an upper bound. Motivated by this insight, the paper introduces DCS-LDM, a novel visual generation framework that decouples information complexity from scale. DCS-LDM builds a hierarchical, scale-independent latent space using multi-level tokens that effectively model the complexity of samples. This approach enables decoding into arbitrary resolutions and frame rates without changing the underlying fixed latent representation. As a result, DCS-LDM offers a more flexible computation-quality tradeoff compared to traditional models. Additionally, the method separates structural and detailed visual information across different hierarchy levels, enabling a progressive coarse-to-fine generation paradigm. Experiments demonstrate that DCS-LDM achieves performance on par with state-of-the-art latent diffusion models. Importantly, it supports flexible generation across diverse scales and visual qualities, making it a versatile tool for visual synthesis tasks where varying output resolutions and frame rates are needed. <div>
arXiv:2511.16117v1 Announce Type: new 
Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTinker: Guided Flow Upsampling and Texture Mapping for High-Resolution Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2511.16124</link>
<guid>https://arxiv.org/abs/2511.16124</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Frame Interpolation, guided flow upsampling, texture mapping, motion estimation, high-resolution frames  

<br /><br />Summary:  
1. Estimating motion in high-resolution video frames is challenging due to large pixel movements and high computational costs.  
2. Conventional flow-based Video Frame Interpolation (VFI) methods typically predict bidirectional optical flows at low resolution and then upsample them (e.g., via bilinear interpolation) to the high-resolution scale, which often leads to blurred or mosaic-like artifacts around the flow edges.  
3. Low-resolution motion estimation fails to capture the fine pixel motions accurately at high resolutions, causing misaligned flows and resulting in ghosting and discontinuities in the interpolated frames.  
4. The proposed VTinker framework introduces two key components: Guided Flow Upsampling (GFU), which uses input frames as guidance to sharpen flow edges and reduce blurring during upsampling, and Texture Mapping, which creates an intermediate proxy frame to enable selecting and mapping clear texture blocks from input frames, thereby minimizing pixel-level artifacts.  
5. VTinker’s novel approach significantly improves the quality of interpolated video frames, achieving state-of-the-art performance according to extensive experimental results. The implementation codes are publicly available on GitHub. <div>
arXiv:2511.16124v1 Announce Type: new 
Abstract: Due to large pixel movement and high computational cost, estimating the motion of high-resolution frames is challenging. Thus, most flow-based Video Frame Interpolation (VFI) methods first predict bidirectional flows at low resolution and then use high-magnification upsampling (e.g., bilinear) to obtain the high-resolution ones. However, this kind of upsampling strategy may cause blur or mosaic at the flows' edges. Additionally, the motion of fine pixels at high resolution cannot be adequately captured in motion estimation at low resolution, which leads to the misalignment of task-oriented flows. With such inaccurate flows, input frames are warped and combined pixel-by-pixel, resulting in ghosting and discontinuities in the interpolated frame. In this study, we propose a novel VFI pipeline, VTinker, which consists of two core components: guided flow upsampling (GFU) and Texture Mapping. After motion estimation at low resolution, GFU introduces input frames as guidance to alleviate the blurring details in bilinear upsampling flows, which makes flows' edges clearer. Subsequently, to avoid pixel-level ghosting and discontinuities, Texture Mapping generates an initial interpolated frame, referred to as the intermediate proxy. The proxy serves as a cue for selecting clear texture blocks from the input frames, which are then mapped onto the proxy to facilitate producing the final interpolated frame via a reconstruction module. Extensive experiments demonstrate that VTinker achieves state-of-the-art performance in VFI. Codes are available at: https://github.com/Wucy0519/VTinker.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Noise Benefits AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.16136</link>
<guid>https://arxiv.org/abs/2511.16136</guid>
<content:encoded><![CDATA[
<div> generative models, AI-generated image detection, out-of-distribution generalization, positive-incentive noise, CLIP<br /><br />Summary: The paper addresses the challenge of detecting AI-generated images, especially under out-of-distribution scenarios where current detection methods struggle due to reliance on spurious shortcuts during training. It identifies that small perturbations in the feature space can reduce the dominance of these shortcuts and improve generalization. To leverage this insight, the authors propose PiN-CLIP, a novel approach combining a noise generator and a detection network trained jointly under a variational positive-incentive principle. This principle guides the creation of positive-incentive noise through cross-attention fusion of visual features and categorical semantic information, injecting this noise into the feature space to fine-tune the visual encoder. The tuning process suppresses shortcut-sensitive directions while enhancing stable forensic cues, allowing for more robust and generalized extraction of artifact representations indicative of synthetic images. The effectiveness of PiN-CLIP is demonstrated on a diverse open-world dataset containing synthetic images produced by 42 different generative models, showing a substantial improvement in detection accuracy with a 5.4% increase over previous state-of-the-art methods. This advancement highlights the method's potential to improve the reliability of AI-generated image detection in real-world applications. <div>
arXiv:2511.16136v1 Announce Type: new 
Abstract: The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video</title>
<link>https://arxiv.org/abs/2511.16137</link>
<guid>https://arxiv.org/abs/2511.16137</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality Enhancement, Compressed Video, Blind Methods, Degradation Representation, Hierarchical Termination<br /><br />Summary:<br /><br />This paper addresses the challenge of Quality Enhancement for Compressed Video (QECV) where existing methods mostly rely on known Quantization Parameters (QPs) using separate models for each QP, classified as non-blind methods. In real-world applications, QPs may be unknown, motivating the need for blind QECV approaches. Current blind methods generate degradation vectors through classification with cross-entropy loss, serving as channel attention for artifact removal; however, these vectors only capture global degradation and lack spatial detail, limiting their effectiveness across varying artifact patterns. To overcome this, the authors propose a pretrained Degradation Representation Learning (DRL) module that extracts high-dimensional, multiscale degradation features from video content to better guide artifact removal. Additionally, recognizing that both blind and non-blind methods use uniform architectures regardless of QP, they introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on compression level, aligning computational effort with the degree of degradation. Experiments demonstrate significant performance improvements, with PSNR gains of 110% (from 0.31 dB to 0.65 dB) over state-of-the-art blind methods at QP = 22 and a halving of average inference time at QP = 22 compared to QP = 42, indicating both enhanced quality and efficiency. <div>
arXiv:2511.16137v1 Announce Type: new 
Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time 3D Object Detection with Inference-Aligned Learning</title>
<link>https://arxiv.org/abs/2511.16140</link>
<guid>https://arxiv.org/abs/2511.16140</guid>
<content:encoded><![CDATA[
<div> Spatial-prioritized, Rank-aware, 3D object detection, Point clouds, Real-time  

<br /><br />Summary: Real-time 3D object detection from point clouds is critical for dynamic scene understanding in applications such as augmented reality, robotics, and navigation. This work presents a novel framework called SR3D (Spatial-prioritized and Rank-aware 3D object detection) designed specifically for indoor point cloud data. The main motivation is to address the training-inference gap caused by inconsistencies between spatial reliability and ranking awareness during training versus the ranking-based prediction selection used during inference. SR3D introduces two key components: a spatial-prioritized optimal transport assignment that emphasizes well-located and spatially reliable samples dynamically during training, and a rank-aware adaptive self-distillation scheme that incorporates ranking perception into the model via a self-distillation paradigm. These components are tailored to the spatial characteristics of point cloud data, helping the model to better align training procedures with inference-time behavior. Extensive experiments conducted on benchmark datasets ScanNet V2 and SUN RGB-D demonstrate that SR3D successfully bridges the training-inference gap, significantly improving detection accuracy without sacrificing real-time performance. Overall, this framework advances the state-of-the-art in 3D object detection for indoor point clouds by simultaneously enhancing spatial reliability and ranking awareness during training. <div>
arXiv:2511.16140v1 Announce Type: new 
Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection</title>
<link>https://arxiv.org/abs/2511.16143</link>
<guid>https://arxiv.org/abs/2511.16143</guid>
<content:encoded><![CDATA[
<div> Keywords: Water Body Change Detection, High Spatial Resolution, Deep Learning, Spatial Semantics, Attention Module<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting changes in water bodies from bi-temporal remote sensing images, highlighting the limitation of existing datasets with insufficient spatial resolution for accurate urban and rural analysis. To overcome this, the authors introduce a new dataset named HSRW-CD, featuring image pairs with spatial resolution higher than 3 meters, encompassing diverse types of water bodies. A novel Spatial Semantics and Continuity Perception (SSCP) attention module is proposed to improve deep learning models by better exploiting spatial semantic and structural features in change detection networks. The SSCP module consists of three components: Multi-Semantic spatial Attention (MSA), which enhances spatial semantic features and provides priors for channel attention; Structural Relation-aware Global Attention (SRGA), which captures spatial continuity of water bodies; and Channel-wise Self-Attention (CSA), which computes similarity across channels by leveraging the priors from MSA and SRGA. Designed as a plug-and-play module, SSCP can be integrated into existing WBCD models. Extensive experiments on the new HSRW-CD dataset and the Water-CD dataset validate that SSCP improves discrimination of water bodies and generalizes well. The dataset and code are publicly available at the provided GitHub repository. <div>
arXiv:2511.16143v1 Announce Type: new 
Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM</title>
<link>https://arxiv.org/abs/2511.16144</link>
<guid>https://arxiv.org/abs/2511.16144</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, SLAM, Language Embeddings, Real-time Mapping, Semantic Pruning  

<br /><br />Summary:  
This paper presents LEGO-SLAM, a novel framework integrating language understanding with 3D Gaussian Splatting (3DGS) for Simultaneous Localization and Mapping (SLAM). Current SLAM systems, despite generating photorealistic maps, lack open-vocabulary semantic understanding, limiting robotic interaction capabilities. The challenge lies in incorporating high-dimensional language features without incurring excessive memory usage and rendering overhead. LEGO-SLAM addresses this by employing a scene-adaptive encoder-decoder that compresses high-dimensional language embeddings into a 16-dimensional feature space, reducing memory requirements and improving rendering speed for real-time application. Unlike static models, LEGO-SLAM's encoder adjusts dynamically to new environments, enhancing adaptability. Additionally, the compact language features enable a pruning strategy that eliminates semantic redundancies, cutting the number of map Gaussians by over 60% without degrading visual quality. The framework also introduces a language-based loop detection method that leverages these compact features, removing the need for separate detection models. Experimental results validate that LEGO-SLAM maintains competitive mapping and tracking performance while providing open-vocabulary semantic capabilities at 15 frames per second, marking a significant advance for real-time, semantically-aware SLAM systems. <div>
arXiv:2511.16144v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2511.16150</link>
<guid>https://arxiv.org/abs/2511.16150</guid>
<content:encoded><![CDATA[
<div> Multimodal embeddings, Multimodal Large Language Models, Reasoning Guided Embeddings, contrastive training, multimodal retrieval<br /><br />Summary:<br /><br />This paper addresses the limitation in current methods for extracting multimodal embeddings from Multimodal Large Language Models (MLLMs), which typically treat embedding extraction as a direct encoding task without leveraging the models' generative reasoning capabilities. The authors propose a novel approach called Reasoning Guided Embeddings (RGE), which explicitly incorporates the generative reasoning process into embedding extraction. RGE works by conditioning the model to generate structured rationales according to given instructions before extracting embeddings, effectively preserving the reasoning steps within the representation. This approach enhances the quality of embeddings by integrating context-conditional inference signals, which standard direct encoding methods overlook. The method is trained using a contrastive learning framework that aligns reasoning outputs with improved embeddings. Experiments conducted on the MMEB benchmark demonstrate that RGE improves multimodal retrieval performance, achieving a 4.9% increase over non-reasoning baselines. Overall, the study confirms that embedding quality benefits significantly from the explicit inclusion of reasoning processes enabled by MLLMs, suggesting a new direction for enhancing multimodal representation learning by bridging generative reasoning and embedding extraction. <div>
arXiv:2511.16150v1 Announce Type: new 
Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.16156</link>
<guid>https://arxiv.org/abs/2511.16156</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, Structured Pruning, Knowledge Distillation, Image Generation, Resource Efficiency  

<br /><br />Summary:  
This paper addresses the high computational cost of Diffusion Transformers (DiTs) in image generation by introducing Pluggable Pruning with Contiguous Layer Distillation (PPCL), a novel pruning framework optimized for DiT architectures. First, the method identifies redundant layers via a combination of linear probing and first-order differential trend analysis of similarity metrics, enabling targeted pruning of model depth intervals. Second, PPCL implements a unique teacher-student alternating distillation scheme that simultaneously integrates both depth-wise and width-wise pruning during a single training phase, allowing flexible knowledge transfer across various pruning ratios without requiring retraining for each configuration. Experimental validation on multiple Multi-Modal Diffusion Transformer models demonstrates that PPCL can reduce model parameters by 50% with less than 3% degradation in key performance metrics, maintaining high-quality image generation. This significant compression makes DiTs more practical for deployment in environments with limited computational resources. Additionally, the framework is plug-and-play and versatile, supporting diverse pruning strategies efficiently. The authors have also made their code and pretrained checkpoints publicly available, facilitating further research and adoption within the community. <div>
arXiv:2511.16156v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2511.16160</link>
<guid>https://arxiv.org/abs/2511.16160</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial intelligence, Multimodal Large Language Models, Video2Layout, cognitive maps, spatial reasoning<br /><br />Summary:<br /><br />This paper addresses the challenge of enhancing spatial intelligence in Multimodal Large Language Models (MLLMs) by proposing a new framework called Video2Layout. Unlike existing methods that rely on discretized grid-based cognitive maps, Video2Layout reconstructs spatial layouts using continuous object boundary coordinates, allowing for more precise metric grounding of object sizes and inter-object distances. The approach is motivated by the limitations of raster representations in enabling fine-grained spatial reasoning. The method consists of two main stages: first, a supervised fine-tuning stage using a high-quality dataset generated from the AI2THOR simulator, which teaches the model to accurately map visual inputs to boundary coordinates; second, a reinforcement fine-tuning stage to improve the model’s generalization to real-world scenarios. To evaluate how the quantity of image inputs impacts spatial reasoning accuracy, the authors introduce QVS-Bench, a diagnostic benchmark specifically designed for this purpose. Experimental results on QVS-Bench and other common spatial reasoning benchmarks indicate that Video2Layout’s model, V2LO-7B, outperforms models trained on grid maps by an average margin of 4.92%, demonstrating the effectiveness of continuous coordinate representations for spatial understanding. The code for the framework is publicly accessible on GitHub. <div>
arXiv:2511.16160v1 Announce Type: new 
Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</title>
<link>https://arxiv.org/abs/2511.16161</link>
<guid>https://arxiv.org/abs/2511.16161</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud completion, symmetry priors, diffusion models, distribution learning, hierarchical upsampling  

<br /><br />Summary:  
Point cloud completion is a critical task in 3D vision involving reconstructing missing parts of 3D shapes while maintaining detailed local geometry and overall global structure. Existing methods that rely on direct regression of local symmetry transformations help preserve geometric details but have significant drawbacks. First, regression-based techniques often overfit by memorizing instance-specific transformations rather than learning generalizable geometric priors. Second, they are highly sensitive to noise in input data due to point-wise transformation regression, which diminishes robustness and generalization. To overcome these challenges, the paper introduces Simba, a novel framework that transforms point-wise regression into a distribution learning problem. Simba leverages symmetry priors integrated with diffusion models’ generative strengths, which prevents memorization of instant-specific features and better captures robust geometric structures. The authors also propose a hierarchical Mamba-based architecture designed to enable high-fidelity upsampling of point clouds, improving detail preservation. The approach is extensively evaluated on popular datasets including PCN, ShapeNet, and KITTI benchmarks, where it demonstrates state-of-the-art performance, validating its effectiveness and robustness in point cloud completion tasks. <div>
arXiv:2511.16161v1 Announce Type: new 
Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.16162</link>
<guid>https://arxiv.org/abs/2511.16162</guid>
<content:encoded><![CDATA[
<div> robustness, adversarial training, medical image segmentation, wavelet reconstruction, noise injection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of maintaining stable segmentation models under distribution shifts and perturbations in clinical medical imaging. 2. It critiques adversarial training (AT) as the mainstream method for robustness, highlighting its inherent clean-robustness trade-off and high costs that hinder its scalability and maintainability. 3. The authors propose a novel method called Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR), which injects small zero-mean noise at multiple layers during training to learn a frequency-bias prior, steering representations away from noise-sensitive directions. 4. LNG-SWR applies selective wavelet reconstruction on input/features to adapt frequencies by suppressing noise-sensitive bands, enhancing directional structures and shape cues, stabilizing boundary responses, and maintaining spectral consistency. 5. This approach is backbone-agnostic, adds minimal inference overhead, and can complement or replace adversarial training. 6. Experiments on CT and ultrasound datasets demonstrate consistent improvements in clean Dice/IoU scores and reduced performance drops under strong adversarial attacks using PGD and SSAH. 7. Combining LNG-SWR with adversarial training yields further additive robustness gains without compromising clean accuracy. 8. Overall, LNG-SWR offers a simple, effective, and engineering-friendly solution for robust medical image segmentation in both adversarial and standard training settings. <div>
arXiv:2511.16162v1 Announce Type: new 
Abstract: Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs</title>
<link>https://arxiv.org/abs/2511.16163</link>
<guid>https://arxiv.org/abs/2511.16163</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, verbose generation, adversarial perturbations, reinforcement learning, output token maximization<br /><br />Summary:<br />1. This paper addresses the efficiency challenges of Vision-Language Models (VLMs), focusing on their generation of lengthy and low-information-density outputs that increase energy consumption, latency, and token costs. <br />2. Existing methods to prolong VLM output rely on delaying the end-of-sequence (EOS) token occurrence but lack explicit optimization, resulting in poor control and stability. <br />3. The authors propose a verbose-text induction attack (VTIA), which injects imperceptible adversarial perturbations into benign images through a two-stage framework to maximize output token length. <br />4. The first stage performs adversarial prompt search using reinforcement learning to find prompt embeddings that induce the model’s language component to generate verbose outputs. <br />5. The second stage optimizes vision-aligned perturbations on input images to maximize similarity to the adversarial prompt’s visual embeddings, creating malicious images that trigger verbose generation. <br />6. Experiments on four popular VLMs demonstrate that VTIA substantially improves effectiveness, efficiency, and generalization in inducing verbose outputs, highlighting potential risks in deploying VLMs without robustness considerations. <div>
arXiv:2511.16163v1 Announce Type: new 
Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoVLA: Self-Evolving Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2511.16166</link>
<guid>https://arxiv.org/abs/2511.16166</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, long-horizon manipulation, stage hallucination, self-supervised learning, sim-to-real transfer<br /><br />Summary: EvoVLA is a novel self-supervised Vision-Language-Action (VLA) framework designed to tackle the challenge of long-horizon robotic manipulation tasks, which prior models struggle with due to stage hallucination where agents falsely report progress on multi-step tasks. EvoVLA employs three key components: Stage-Aligned Reward (SAR), which leverages triplet contrastive learning with Gemini-generated hard negatives to eliminate visual shortcuts; Pose-Based Object Exploration (POE), which enhances curiosity-driven exploration using relative object-gripper pose rather than raw pixel data; and Long-Horizon Memory, which stabilizes intrinsic shaping during extended task rollouts through selective context retention and gated fusion. Evaluations on the Discoverse-L benchmark involving three multi-stage tasks demonstrate a 10.2 percentage point improvement in average task success over the best baseline (OpenVLA-OFT), achieving 69.2% success. The model also exhibits 1.5 times greater sample efficiency and decreases stage hallucination from 38.5% to 14.8%. Real-world trials with physical robots show a 54.6% average success rate across four manipulation tasks, outperforming OpenVLA-OFT by 11 percentage points, indicating strong sim-to-real transfer and generalization capabilities. The project’s code and additional resources are publicly available online. <div>
arXiv:2511.16166v1 Announce Type: new 
Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective</title>
<link>https://arxiv.org/abs/2511.16170</link>
<guid>https://arxiv.org/abs/2511.16170</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary semantic segmentation, CLIP, multimodal alignment, distraction tokens, ReFocusing CLIP (RF-CLIP)  

<br /><br />Summary: This paper addresses the challenge of enhancing pixel-level multimodal alignment for open-vocabulary semantic segmentation (OVSS), which uses vision-language alignment to associate textual prompts with image pixels. The authors conduct an interpretability-driven investigation into CLIP's internal mechanisms, discovering a phenomenon analogous to human distraction where CLIP’s attention is diverted from relevant target regions to irrelevant tokens. These distraction tokens arise due to dimension-specific over-activation within CLIP's model. By analyzing and filtering out these distraction tokens, the authors demonstrate improved dense prediction performance. Building on this insight, they propose ReFocusing CLIP (RF-CLIP), a novel training-free method that mimics human distraction-refocusing behavior. RF-CLIP redirects CLIP’s attention from irrelevant distraction tokens back to target areas, thereby refining the granularity of multimodal pixel-level alignment. This approach significantly enhances the accuracy and quality of OVSS without incurring additional training costs. Evaluations on eight semantic segmentation benchmarks show that RF-CLIP achieves state-of-the-art performance while maintaining high inference efficiency. The work highlights the benefit of interpretability analyses in improving large pretrained vision-language models for dense prediction tasks. <div>
arXiv:2511.16170v1 Announce Type: new 
Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</title>
<link>https://arxiv.org/abs/2511.16175</link>
<guid>https://arxiv.org/abs/2511.16175</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Disentangled Visual Foresight, diffusion Transformer, meta queries, robot manipulation<br /><br />Summary:<br /><br />1. This paper introduces Mantis, a novel framework designed to improve Vision-Language-Action (VLA) models by addressing challenges related to predicting high-dimensional visual states and maintaining strong language comprehension.<br />2. Mantis features a Disentangled Visual Foresight (DVF) mechanism that decouples visual foresight prediction from the main backbone using a combination of meta queries and a diffusion Transformer (DiT) head.<br />3. By feeding the current visual state into the DiT via a residual connection, Mantis uses a next-state prediction objective that enables meta queries to automatically capture latent actions shaping the visual trajectory, thereby enhancing explicit action learning.<br />4. This disentanglement reduces the workload on the VLA backbone, preserving its ability to perform comprehension and reasoning through language supervision.<br />5. Empirical results demonstrate that Mantis, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, achieves a 96.7% success rate on the LIBERO benchmark after fine-tuning, outperforming strong baselines with faster convergence.<br />6. Real-world tests show that Mantis surpasses the leading open-source model π₀.₅ in instruction-following, generalization to unseen instructions, and reasoning capabilities.<br />7. The authors have released code and pretrained weights to facilitate further research and development in the VLA community. <div>
arXiv:2511.16175v1 Announce Type: new 
Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\pi_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.16184</link>
<guid>https://arxiv.org/abs/2511.16184</guid>
<content:encoded><![CDATA[
<div> Visible-Infrared Re-Identification, Unsupervised Domain Adaptation, Domain Discrepancies, Domain-Shared Learning, Gradual Alignment<br /><br />Summary:  
The paper addresses challenges in Visible-Infrared person Re-Identification (VI-ReID), focusing on the gap between public datasets and real-world applications, where most algorithms fail due to domain differences. It investigates Unsupervised Domain Adaptation VI-ReID (UDA-VI-ReID) to transfer knowledge from labeled source data to unlabeled target data without requiring new annotations, maintaining accuracy. Two core challenges are identified: inter-domain modality discrepancies (differences between source and target domains) and intra-domain modality discrepancies (differences between visible and infrared data within the same domain). To overcome these, the authors propose a novel two-stage model called Domain-Shared Learning and Gradual Alignment (DSLGA). In the first pre-training stage, the Domain-Shared Learning Strategy (DSLS) reduces ineffective pre-training by exploiting shared information between domains. In the second fine-tuning stage, the Gradual Alignment Strategy (GAS) progressively aligns cross-modality features via a cluster-to-holistic approach, addressing large intra-domain discrepancies. Additionally, a new testing protocol named CMDA-XD is introduced for evaluating UDA-VI-ReID models. Extensive experiments show the proposed method surpasses existing domain adaptation methods and even some supervised VI-ReID techniques, proving its effectiveness under various setups. <div>
arXiv:2511.16184v1 Announce Type: new 
Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2511.16186</link>
<guid>https://arxiv.org/abs/2511.16186</guid>
<content:encoded><![CDATA[
<div> Keywords: organ reconstruction, topology-preserving, template-based, deep learning, anatomical consistency<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reconstructing human organs composed of interconnected substructures whose geometries and spatial relationships are mutually constrained. Traditional deep-learning approaches tend to treat these parts independently, leading to anatomically implausible results.<br /><br />2. The authors introduce PrIntMesh, a novel framework that performs template-based, topology-preserving reconstruction of organs as unified systems rather than separate components.<br /><br />3. PrIntMesh begins with a connected organ template and jointly deforms its substructures to fit patient-specific anatomies. This joint deformation explicitly preserves internal boundaries and enforces smooth, artifact-free surfaces.<br /><br />4. The method is validated on multiple organs including the heart, hippocampus, and lungs, demonstrating high geometric accuracy, correct topological representation, and robustness even when trained with limited or noisy data.<br /><br />5. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency across substructures, and provides a data-efficient solution with strong clinical applicability. <div>
arXiv:2511.16186v1 Announce Type: new 
Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.16203</link>
<guid>https://arxiv.org/abs/2511.16203</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, adversarial robustness, multimodal attacks, cross-modal misalignment, embodied AI<br /><br />Summary: This paper presents VLA-Fool, a comprehensive investigation into the adversarial robustness of Vision-Language-Action (VLA) models, which integrate perception, reasoning, and action in embodied environments. The study addresses a gap in current research that largely neglects adversarial vulnerabilities under realistic multimodal and black-box conditions, focusing instead on single-modality perturbations. VLA-Fool systematically explores three categories of adversarial attacks: textual perturbations using gradient-based and prompt-based methods, visual perturbations through patch and noise distortions, and novel cross-modal misalignment attacks that disrupt the semantic alignment between visual perception and textual instruction. Additionally, the authors introduce a VLA-aware semantic space within linguistic prompts, establishing the first automated, semantically guided prompting framework tailored to enhance robustness evaluation. Experimental validation on the LIBERO benchmark with a fine-tuned OpenVLA model shows that even minimal multimodal perturbations cause substantial behavioral deviations, emphasizing the fragility of multimodal alignment in these embodied agents. The findings highlight critical vulnerabilities in current VLA systems and suggest that robust multimodal alignment is essential for reliable embodied AI performance under adversarial conditions. <div>
arXiv:2511.16203v1 Announce Type: new 
Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles</title>
<link>https://arxiv.org/abs/2511.16213</link>
<guid>https://arxiv.org/abs/2511.16213</guid>
<content:encoded><![CDATA[
<div> Clustering, Unsupervised Learning, Image Classification, Cluster Ensembling, Nearest Neighbor Selection<br /><br />Summary:<br /><br />This paper addresses the challenge of unsupervised image classification, also known as image clustering, which involves grouping unlabeled images into meaningful semantic categories. Unlike early methods that combined representation learning with clustering iteratively, the authors focus solely on clustering due to advancements in foundational models that provide strong frozen backbones. The proposed method, ICCE (Image Clustering through Cluster Ensembles), enhances a recent multi-head clustering framework by incorporating adaptive nearest neighbor selection and cluster ensembling strategies. Initially, ICCE trains multiple clustering heads on a frozen backbone to generate diverse clusterings of the image data. These varying cluster results are then unified via a cluster ensembling technique that resolves conflicts and achieves a consensus clustering. Finally, the consensus clustering output is used as pseudo-labels to train an image classifier. ICCE demonstrates state-of-the-art performance across ten benchmark datasets, reaching 99.3% accuracy on CIFAR10, 89% on CIFAR100, and notably exceeding 70% accuracy on the large-scale ImageNet dataset. This marks the first fully unsupervised image classification approach to surpass the 70% accuracy threshold on ImageNet, effectively narrowing the gap with fully supervised methods. <div>
arXiv:2511.16213v1 Announce Type: new 
Abstract: Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, "Image Clustering through Cluster Ensembles" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2511.16221</link>
<guid>https://arxiv.org/abs/2511.16221</guid>
<content:encoded><![CDATA[
<div> Deception Detection, Multimodal Large Language Models, Social Reasoning, Dataset, Memory Module<br /><br />Summary:<br /><br />1. The paper identifies a critical limitation of current state-of-the-art Multimodal Large Language Models (MLLMs): their inability to 'read the room' and detect deception in complex social interactions. 2. To address this, the authors introduce a new task called Multimodal Interactive Deception Assessment (MIDA) and provide a novel dataset that couples synchronized video and text with verified truthfulness labels for each statement, enabling rigorous evaluation. 3. A comprehensive benchmark evaluating 12 leading MLLMs reveals a significant performance gap, with even powerful models like GPT-4o struggling to reliably distinguish truthful from deceptive statements. 4. Analysis of failure modes shows these models struggle to ground language in multimodal social cues and lack mechanisms to model others’ knowledge, beliefs, or intentions, highlighting the need for new approaches. 5. As a solution, the authors propose a Social Chain-of-Thought (SoCoT) reasoning pipeline combined with a Dynamic Social Epistemic Memory (DSEM) module, which together improve performance on this difficult task and point toward more perceptive AI systems capable of human-like social reasoning. <div>
arXiv:2511.16221v1 Announce Type: new 
Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwiTrack: Tri-State Switch for Cross-Modal Object Tracking</title>
<link>https://arxiv.org/abs/2511.16227</link>
<guid>https://arxiv.org/abs/2511.16227</guid>
<content:encoded><![CDATA[
<div> Cross-modal tracking, RGB-NIR, state-switching framework, trajectory prediction, template reconstruction  

<br /><br />Summary:  
This paper addresses the challenge of cross-modal object tracking (CMOT), specifically tracking targets across alternating RGB and near-infrared (NIR) video frames in a scenario where only one modality is available per frame. Existing approaches that connect RGB and NIR inputs through a shared backbone are limited in capturing modality-specific features and are prone to object drift, particularly with unreliable inputs. To overcome these limitations, the authors propose SwiTrack, a novel state-switching framework that employs three specialized processing streams tailored to the modality state. RGB frames are directly processed by a visual encoder, while NIR frames are refined via a gated adapter integrated with the visual encoder to better calibrate shared latent features, improving robustness. For invalid or unreliable modality inputs, a consistency trajectory prediction module uses spatial and temporal cues to estimate target movement and prevent drift. Additionally, the framework incorporates dynamic template reconstruction, which iteratively updates the template features, and applies a similarity alignment loss to enhance cross-modal feature consistency. Experiments on recent benchmarks demonstrate that SwiTrack achieves state-of-the-art performance, improving precision and success rates by 7.2% and 4.3%, respectively, while running at real-time speeds of 65 frames per second. The code and models are publicly available online. <div>
arXiv:2511.16227v1 Announce Type: new 
Abstract: Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs</title>
<link>https://arxiv.org/abs/2511.16264</link>
<guid>https://arxiv.org/abs/2511.16264</guid>
<content:encoded><![CDATA[
<div> Keywords: full-body tracking, AR/VR, multi-layer perceptron, Memory-Block, multi-task learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving realistic and smooth full-body tracking for immersive AR/VR applications, where existing systems focus mainly on head and hand tracking via HMDs and controllers, resulting in incomplete 3D full-body reconstruction.<br /><br />2. The authors propose a novel neural network-based method using a multi-layer perceptron (MLP) enhanced with residual connections and a new component called Memory-Block to handle missing sensor data.<br /><br />3. Memory-Block represents missing sensor inputs with trainable code-vectors and integrates these with sparse signals from previous time instances, which improves temporal consistency in the full-body motion tracking.<br /><br />4. The approach is formulated as a multi-task learning problem, enabling the MLP backbone to learn robust feature representations that enhance prediction accuracy.<br /><br />5. Experimental results demonstrate that this method outperforms state-of-the-art baselines by significantly reducing prediction errors and achieves an efficient runtime of 72 frames per second on mobile HMDs, thereby optimizing the accuracy and computational tradeoff for real-time AR/VR applications. <div>
arXiv:2511.16264v1 Announce Type: new 
Abstract: Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TetraSDF: Precise Mesh Extraction with Multi-resolution Tetrahedral Grid</title>
<link>https://arxiv.org/abs/2511.16273</link>
<guid>https://arxiv.org/abs/2511.16273</guid>
<content:encoded><![CDATA[
arXiv:2511.16273v1 Announce Type: new 
Abstract: Extracting meshes that exactly match the zero-level set of neural signed distance functions (SDFs) remains challenging. Sampling-based methods introduce discretization error, while continuous piecewise affine (CPWA) analytic approaches apply only to plain ReLU MLPs. We present TetraSDF, a precise analytic meshing framework for SDFs represented by a ReLU MLP composed with a multi-resolution tetrahedral positional encoder. The encoder's barycentric interpolation preserves global CPWA structure, enabling us to track ReLU linear regions within an encoder-induced polyhedral complex. A fixed analytic input preconditioner derived from the encoder's metric further reduces directional bias and stabilizes training. Across multiple benchmarks, TetraSDF matches or surpasses existing grid-based encoders in SDF reconstruction accuracy, and its analytic extractor produces highly self-consistent meshes that remain faithful to the learned isosurfaces, all with practical runtime and memory efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</title>
<link>https://arxiv.org/abs/2511.16282</link>
<guid>https://arxiv.org/abs/2511.16282</guid>
<content:encoded><![CDATA[
arXiv:2511.16282v1 Announce Type: new 
Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability</title>
<link>https://arxiv.org/abs/2511.16294</link>
<guid>https://arxiv.org/abs/2511.16294</guid>
<content:encoded><![CDATA[
arXiv:2511.16294v1 Announce Type: new 
Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing 3D Gaussian Splattering for Mobile GPUs</title>
<link>https://arxiv.org/abs/2511.16298</link>
<guid>https://arxiv.org/abs/2511.16298</guid>
<content:encoded><![CDATA[
arXiv:2511.16298v1 Announce Type: new 
Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling</title>
<link>https://arxiv.org/abs/2511.16301</link>
<guid>https://arxiv.org/abs/2511.16301</guid>
<content:encoded><![CDATA[
arXiv:2511.16301v1 Announce Type: new 
Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders are Topic Models</title>
<link>https://arxiv.org/abs/2511.16309</link>
<guid>https://arxiv.org/abs/2511.16309</guid>
<content:encoded><![CDATA[
arXiv:2511.16309v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks</title>
<link>https://arxiv.org/abs/2511.16315</link>
<guid>https://arxiv.org/abs/2511.16315</guid>
<content:encoded><![CDATA[
arXiv:2511.16315v1 Announce Type: new 
Abstract: ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaTex: Seamless Texture Generation as Latent Color Diffusion</title>
<link>https://arxiv.org/abs/2511.16317</link>
<guid>https://arxiv.org/abs/2511.16317</guid>
<content:encoded><![CDATA[
arXiv:2511.16317v1 Announce Type: new 
Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WWE-UIE: A Wavelet &amp; White Balance Efficient Network for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2511.16321</link>
<guid>https://arxiv.org/abs/2511.16321</guid>
<content:encoded><![CDATA[
arXiv:2511.16321v1 Announce Type: new 
Abstract: Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2511.16322</link>
<guid>https://arxiv.org/abs/2511.16322</guid>
<content:encoded><![CDATA[
arXiv:2511.16322v1 Announce Type: new 
Abstract: Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks</title>
<link>https://arxiv.org/abs/2511.16341</link>
<guid>https://arxiv.org/abs/2511.16341</guid>
<content:encoded><![CDATA[
arXiv:2511.16341v1 Announce Type: new 
Abstract: Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach</title>
<link>https://arxiv.org/abs/2511.16343</link>
<guid>https://arxiv.org/abs/2511.16343</guid>
<content:encoded><![CDATA[
arXiv:2511.16343v1 Announce Type: new 
Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</title>
<link>https://arxiv.org/abs/2511.16349</link>
<guid>https://arxiv.org/abs/2511.16349</guid>
<content:encoded><![CDATA[
arXiv:2511.16349v1 Announce Type: new 
Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Order Matching Network for Alignment-Free Depth Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16361</link>
<guid>https://arxiv.org/abs/2511.16361</guid>
<content:encoded><![CDATA[
arXiv:2511.16361v1 Announce Type: new 
Abstract: Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration</title>
<link>https://arxiv.org/abs/2511.16364</link>
<guid>https://arxiv.org/abs/2511.16364</guid>
<content:encoded><![CDATA[
arXiv:2511.16364v1 Announce Type: new 
Abstract: Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement</title>
<link>https://arxiv.org/abs/2511.16378</link>
<guid>https://arxiv.org/abs/2511.16378</guid>
<content:encoded><![CDATA[
arXiv:2511.16378v1 Announce Type: new 
Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss</title>
<link>https://arxiv.org/abs/2511.16418</link>
<guid>https://arxiv.org/abs/2511.16418</guid>
<content:encoded><![CDATA[
arXiv:2511.16418v1 Announce Type: new 
Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation</title>
<link>https://arxiv.org/abs/2511.16428</link>
<guid>https://arxiv.org/abs/2511.16428</guid>
<content:encoded><![CDATA[
arXiv:2511.16428v1 Announce Type: new 
Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360{\deg} field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks for Surgical Scene Segmentation</title>
<link>https://arxiv.org/abs/2511.16430</link>
<guid>https://arxiv.org/abs/2511.16430</guid>
<content:encoded><![CDATA[
arXiv:2511.16430v1 Announce Type: new 
Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation</title>
<link>https://arxiv.org/abs/2511.16435</link>
<guid>https://arxiv.org/abs/2511.16435</guid>
<content:encoded><![CDATA[
arXiv:2511.16435v1 Announce Type: new 
Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreetView-Waste: A Multi-Task Dataset for Urban Waste Management</title>
<link>https://arxiv.org/abs/2511.16440</link>
<guid>https://arxiv.org/abs/2511.16440</guid>
<content:encoded><![CDATA[
arXiv:2511.16440v1 Announce Type: new 
Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</title>
<link>https://arxiv.org/abs/2511.16449</link>
<guid>https://arxiv.org/abs/2511.16449</guid>
<content:encoded><![CDATA[
arXiv:2511.16449v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs</title>
<link>https://arxiv.org/abs/2511.16454</link>
<guid>https://arxiv.org/abs/2511.16454</guid>
<content:encoded><![CDATA[
arXiv:2511.16454v1 Announce Type: new 
Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry</title>
<link>https://arxiv.org/abs/2511.16471</link>
<guid>https://arxiv.org/abs/2511.16471</guid>
<content:encoded><![CDATA[
arXiv:2511.16471v1 Announce Type: new 
Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow and Depth Assisted Video Prediction with Latent Transformer</title>
<link>https://arxiv.org/abs/2511.16484</link>
<guid>https://arxiv.org/abs/2511.16484</guid>
<content:encoded><![CDATA[
arXiv:2511.16484v1 Announce Type: new 
Abstract: Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</title>
<link>https://arxiv.org/abs/2511.16494</link>
<guid>https://arxiv.org/abs/2511.16494</guid>
<content:encoded><![CDATA[
arXiv:2511.16494v1 Announce Type: new 
Abstract: Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI</title>
<link>https://arxiv.org/abs/2511.16498</link>
<guid>https://arxiv.org/abs/2511.16498</guid>
<content:encoded><![CDATA[
arXiv:2511.16498v1 Announce Type: new 
Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras</title>
<link>https://arxiv.org/abs/2511.16521</link>
<guid>https://arxiv.org/abs/2511.16521</guid>
<content:encoded><![CDATA[
arXiv:2511.16521v1 Announce Type: new 
Abstract: Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization</title>
<link>https://arxiv.org/abs/2511.16524</link>
<guid>https://arxiv.org/abs/2511.16524</guid>
<content:encoded><![CDATA[
arXiv:2511.16524v1 Announce Type: new 
Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive vision-language learning with paraphrasing and negation</title>
<link>https://arxiv.org/abs/2511.16527</link>
<guid>https://arxiv.org/abs/2511.16527</guid>
<content:encoded><![CDATA[
arXiv:2511.16527v1 Announce Type: new 
Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration</title>
<link>https://arxiv.org/abs/2511.16532</link>
<guid>https://arxiv.org/abs/2511.16532</guid>
<content:encoded><![CDATA[
arXiv:2511.16532v1 Announce Type: new 
Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation</title>
<link>https://arxiv.org/abs/2511.16535</link>
<guid>https://arxiv.org/abs/2511.16535</guid>
<content:encoded><![CDATA[
arXiv:2511.16535v1 Announce Type: new 
Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</title>
<link>https://arxiv.org/abs/2511.16541</link>
<guid>https://arxiv.org/abs/2511.16541</guid>
<content:encoded><![CDATA[
arXiv:2511.16541v1 Announce Type: new 
Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering</title>
<link>https://arxiv.org/abs/2511.16542</link>
<guid>https://arxiv.org/abs/2511.16542</guid>
<content:encoded><![CDATA[
arXiv:2511.16542v1 Announce Type: new 
Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Supernet Training for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2511.16546</link>
<guid>https://arxiv.org/abs/2511.16546</guid>
<content:encoded><![CDATA[
arXiv:2511.16546v1 Announce Type: new 
Abstract: Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.
  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.
  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.
  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lite Any Stereo: Efficient Zero-Shot Stereo Matching</title>
<link>https://arxiv.org/abs/2511.16555</link>
<guid>https://arxiv.org/abs/2511.16555</guid>
<content:encoded><![CDATA[
arXiv:2511.16555v1 Announce Type: new 
Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening</title>
<link>https://arxiv.org/abs/2511.16566</link>
<guid>https://arxiv.org/abs/2511.16566</guid>
<content:encoded><![CDATA[
arXiv:2511.16566v1 Announce Type: new 
Abstract: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POMA-3D: The Point Map Way to 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2511.16567</link>
<guid>https://arxiv.org/abs/2511.16567</guid>
<content:encoded><![CDATA[
arXiv:2511.16567v1 Announce Type: new 
Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks</title>
<link>https://arxiv.org/abs/2511.16574</link>
<guid>https://arxiv.org/abs/2511.16574</guid>
<content:encoded><![CDATA[
arXiv:2511.16574v1 Announce Type: new 
Abstract: The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.
  For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.
  These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.16595</link>
<guid>https://arxiv.org/abs/2511.16595</guid>
<content:encoded><![CDATA[
arXiv:2511.16595v1 Announce Type: new 
Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap</title>
<link>https://arxiv.org/abs/2511.16617</link>
<guid>https://arxiv.org/abs/2511.16617</guid>
<content:encoded><![CDATA[
arXiv:2511.16617v1 Announce Type: new 
Abstract: The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
<link>https://arxiv.org/abs/2511.16618</link>
<guid>https://arxiv.org/abs/2511.16618</guid>
<content:encoded><![CDATA[
arXiv:2511.16618v1 Announce Type: new 
Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning</title>
<link>https://arxiv.org/abs/2511.16619</link>
<guid>https://arxiv.org/abs/2511.16619</guid>
<content:encoded><![CDATA[
arXiv:2511.16619v1 Announce Type: new 
Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Guided Upsampling for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.16623</link>
<guid>https://arxiv.org/abs/2511.16623</guid>
<content:encoded><![CDATA[
arXiv:2511.16623v1 Announce Type: new 
Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM 3D: 3Dfy Anything in Images</title>
<link>https://arxiv.org/abs/2511.16624</link>
<guid>https://arxiv.org/abs/2511.16624</guid>
<content:encoded><![CDATA[
arXiv:2511.16624v1 Announce Type: new 
Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</title>
<link>https://arxiv.org/abs/2511.16635</link>
<guid>https://arxiv.org/abs/2511.16635</guid>
<content:encoded><![CDATA[
arXiv:2511.16635v1 Announce Type: new 
Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming</title>
<link>https://arxiv.org/abs/2511.16642</link>
<guid>https://arxiv.org/abs/2511.16642</guid>
<content:encoded><![CDATA[
arXiv:2511.16642v1 Announce Type: new 
Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision</title>
<link>https://arxiv.org/abs/2511.16650</link>
<guid>https://arxiv.org/abs/2511.16650</guid>
<content:encoded><![CDATA[
arXiv:2511.16650v1 Announce Type: new 
Abstract: 3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.16653</link>
<guid>https://arxiv.org/abs/2511.16653</guid>
<content:encoded><![CDATA[
arXiv:2511.16653v1 Announce Type: new 
Abstract: Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Spatial Supersensing Without Spatial Supersensing</title>
<link>https://arxiv.org/abs/2511.16655</link>
<guid>https://arxiv.org/abs/2511.16655</guid>
<content:encoded><![CDATA[
arXiv:2511.16655v1 Announce Type: new 
Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PartUV: Part-Based UV Unwrapping of 3D Meshes</title>
<link>https://arxiv.org/abs/2511.16659</link>
<guid>https://arxiv.org/abs/2511.16659</guid>
<content:encoded><![CDATA[
arXiv:2511.16659v1 Announce Type: new 
Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</title>
<link>https://arxiv.org/abs/2511.16662</link>
<guid>https://arxiv.org/abs/2511.16662</guid>
<content:encoded><![CDATA[
arXiv:2511.16662v1 Announce Type: new 
Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</title>
<link>https://arxiv.org/abs/2511.16666</link>
<guid>https://arxiv.org/abs/2511.16666</guid>
<content:encoded><![CDATA[
arXiv:2511.16666v1 Announce Type: new 
Abstract: Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
<link>https://arxiv.org/abs/2511.16668</link>
<guid>https://arxiv.org/abs/2511.16668</guid>
<content:encoded><![CDATA[
arXiv:2511.16668v1 Announce Type: new 
Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
<link>https://arxiv.org/abs/2511.16669</link>
<guid>https://arxiv.org/abs/2511.16669</guid>
<content:encoded><![CDATA[
arXiv:2511.16669v1 Announce Type: new 
Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Think Fast and Slow for Visual Language Models</title>
<link>https://arxiv.org/abs/2511.16670</link>
<guid>https://arxiv.org/abs/2511.16670</guid>
<content:encoded><![CDATA[
arXiv:2511.16670v1 Announce Type: new 
Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
<link>https://arxiv.org/abs/2511.16671</link>
<guid>https://arxiv.org/abs/2511.16671</guid>
<content:encoded><![CDATA[
arXiv:2511.16671v1 Announce Type: new 
Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</title>
<link>https://arxiv.org/abs/2511.16672</link>
<guid>https://arxiv.org/abs/2511.16672</guid>
<content:encoded><![CDATA[
arXiv:2511.16672v1 Announce Type: new 
Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</title>
<link>https://arxiv.org/abs/2511.16673</link>
<guid>https://arxiv.org/abs/2511.16673</guid>
<content:encoded><![CDATA[
arXiv:2511.16673v1 Announce Type: new 
Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation for Pre-Trained Self-Supervised Vision Models</title>
<link>https://arxiv.org/abs/2511.16674</link>
<guid>https://arxiv.org/abs/2511.16674</guid>
<content:encoded><![CDATA[
arXiv:2511.16674v1 Announce Type: new 
Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI</title>
<link>https://arxiv.org/abs/2511.15717</link>
<guid>https://arxiv.org/abs/2511.15717</guid>
<content:encoded><![CDATA[
arXiv:2511.15717v1 Announce Type: cross 
Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniUltra: Interactive Parameter-Efficient SAM2 for Universal Ultrasound Segmentation</title>
<link>https://arxiv.org/abs/2511.15771</link>
<guid>https://arxiv.org/abs/2511.15771</guid>
<content:encoded><![CDATA[
arXiv:2511.15771v1 Announce Type: cross 
Abstract: The Segment Anything Model 2 (SAM2) demonstrates remarkable universal segmentation capabilities on natural images. However, its performance on ultrasound images is significantly degraded due to domain disparities. This limitation raises two critical challenges: how to efficiently adapt SAM2 to ultrasound imaging while maintaining parameter efficiency, and how to deploy the adapted model effectively in resource-constrained clinical environments. To address these issues, we propose UniUltra for universal ultrasound segmentation. Specifically, we first introduce a novel context-edge hybrid adapter (CH-Adapter) that enhances fine-grained perception across diverse ultrasound imaging modalities while achieving parameter-efficient fine-tuning. To further improve clinical applicability, we develop a deep-supervised knowledge distillation (DSKD) technique that transfers knowledge from the large image encoder of the fine-tuned SAM2 to a super lightweight encoder, substantially reducing computational requirements without compromising performance. Extensive experiments demonstrate that UniUltra outperforms state-of-the-arts with superior generalization capabilities. Notably, our framework achieves competitive performance using only 8.91% of SAM2's parameters during fine-tuning, and the final compressed model reduces the parameter count by 94.08% compared to the original SAM2, making it highly suitable for practical clinical deployment. The source code is available at https://github.com/xq141839/UniUltra.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos</title>
<link>https://arxiv.org/abs/2511.16183</link>
<guid>https://arxiv.org/abs/2511.16183</guid>
<content:encoded><![CDATA[
arXiv:2511.16183v1 Announce Type: cross 
Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Robot Dogs See the Unseeable</title>
<link>https://arxiv.org/abs/2511.16262</link>
<guid>https://arxiv.org/abs/2511.16262</guid>
<content:encoded><![CDATA[
arXiv:2511.16262v1 Announce Type: cross 
Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</title>
<link>https://arxiv.org/abs/2511.16268</link>
<guid>https://arxiv.org/abs/2511.16268</guid>
<content:encoded><![CDATA[
arXiv:2511.16268v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arctic-Extract Technical Report</title>
<link>https://arxiv.org/abs/2511.16470</link>
<guid>https://arxiv.org/abs/2511.16470</guid>
<content:encoded><![CDATA[
arXiv:2511.16470v1 Announce Type: cross 
Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
<link>https://arxiv.org/abs/2511.16518</link>
<guid>https://arxiv.org/abs/2511.16518</guid>
<content:encoded><![CDATA[
arXiv:2511.16518v1 Announce Type: cross 
Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Green Resilience of Cyber-Physical Systems: Doctoral Dissertation</title>
<link>https://arxiv.org/abs/2511.16593</link>
<guid>https://arxiv.org/abs/2511.16593</guid>
<content:encoded><![CDATA[
arXiv:2511.16593v1 Announce Type: cross 
Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space</title>
<link>https://arxiv.org/abs/2209.12746</link>
<guid>https://arxiv.org/abs/2209.12746</guid>
<content:encoded><![CDATA[
arXiv:2209.12746v3 Announce Type: replace 
Abstract: As research on image inversion advances, the process is generally divided into two stages. The first step is Image Embedding, involves using an encoder or optimization procedure to embed an image and obtain its corresponding latent code. The second stage, referred to as Result Refinement, further improves the inversion and editing outcomes. Although this refinement stage substantially enhances reconstruction fidelity, perception and editability remain largely unchanged and are highly dependent on the latent codes derived from the first stage. Therefore, a key challenge lies in obtaining latent codes that preserve reconstruction fidelity while simultaneously improving perception and editability. In this work, we first reveal that these two properties are closely related to the degree of alignment (or disalignment) between the inverted latent codes and the synthetic distribution. Based on this insight, we propose the \textbf{ Latent Space Alignment Inversion Paradigm (LSAP)}, which integrates both an evaluation metric and a unified inversion solution. Specifically, we introduce the \textbf{Normalized Style Space ($\mathcal{S^N}$ space)} and \textbf{Normalized Style Space Cosine Distance (NSCD)} to quantify the disalignment of inversion methods. Moreover, our paradigm can be optimized for both encoder-based and optimization-based embeddings, providing a consistent alignment framework. Extensive experiments across various domains demonstrate that NSCD effectively captures perceptual and editable characteristics, and that our alignment paradigm achieves state-of-the-art performance in both stages of inversion.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models</title>
<link>https://arxiv.org/abs/2401.17629</link>
<guid>https://arxiv.org/abs/2401.17629</guid>
<content:encoded><![CDATA[
arXiv:2401.17629v2 Announce Type: replace 
Abstract: Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Translation via Token Warping</title>
<link>https://arxiv.org/abs/2402.12099</link>
<guid>https://arxiv.org/abs/2402.12099</guid>
<content:encoded><![CDATA[
arXiv:2402.12099v3 Announce Type: replace 
Abstract: With the revolution of generative AI, video-related tasks have been widely studied. However, current state-of-the-art video models still lag behind image models in visual quality and user control over generated content. In this paper, we introduce TokenWarping, a novel framework for temporally coherent video translation. Existing diffusion-based video editing approaches rely solely on key and value patches in self-attention to ensure temporal consistency, often sacrificing the preservation of local and structural regions. Critically, these methods overlook the significance of the query patches in achieving accurate feature aggregation and temporal coherence. In contrast, TokenWarping leverages complementary token priors by constructing temporal correlations across different frames. Our method begins by extracting optical flows from source videos. During the denoising process of the diffusion model, these optical flows are used to warp the previous frame's query, key, and value patches, aligning them with the current frame's patches. By directly warping the query patches, we enhance feature aggregation in self-attention, while warping the key and value patches ensures temporal consistency across frames. This token warping imposes explicit constraints on the self-attention layer outputs, effectively ensuring temporally coherent translation. Our framework does not require any additional training or fine-tuning and can be seamlessly integrated with existing text-to-image editing methods. We conduct extensive experiments on various video translation tasks, demonstrating that TokenWarping surpasses state-of-the-art methods both qualitatively and quantitatively. Video demonstrations are available in supplementary materials.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Query Prompting for Multi-Domain Landmark Detection</title>
<link>https://arxiv.org/abs/2404.01194</link>
<guid>https://arxiv.org/abs/2404.01194</guid>
<content:encoded><![CDATA[
arXiv:2404.01194v2 Announce Type: replace 
Abstract: Medical landmark detection is crucial in various medical imaging modalities and procedures. Although deep learning-based methods have achieve promising performance, they are mostly designed for specific anatomical regions or tasks. In this work, we propose a universal model for multi-domain landmark detection by leveraging transformer architecture and developing a prompting component, named as Adaptive Query Prompting (AQP). Instead of embedding additional modules in the backbone network, we design a separate module to generate prompts that can be effectively extended to any other transformer network. In our proposed AQP, prompts are learnable parameters maintained in a memory space called prompt pool. The central idea is to keep the backbone frozen and then optimize prompts to instruct the model inference process. Furthermore, we employ a lightweight decoder to decode landmarks from the extracted features, namely Light-MLD. Thanks to the lightweight nature of the decoder and AQP, we can handle multiple datasets by sharing the backbone encoder and then only perform partial parameter tuning without incurring much additional cost. It has the potential to be extended to more landmark detection tasks. We conduct experiments on three widely used X-ray datasets for different medical landmark detection tasks. Our proposed Light-MLD coupled with AQP achieves SOTA performance on many metrics even without the use of elaborate structural designs or complex frameworks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks</title>
<link>https://arxiv.org/abs/2406.04470</link>
<guid>https://arxiv.org/abs/2406.04470</guid>
<content:encoded><![CDATA[
arXiv:2406.04470v3 Announce Type: replace 
Abstract: This study assesses the ability of Large Vision-Language Models (LVLMs) to differentiate between AI-generated and human-generated images. It introduces a new automated benchmark construction method for this evaluation. The experiment compared common LVLMs with human participants using a mixed dataset of AI and human-created images. Results showed that LVLMs could distinguish between the image types to some extent but exhibited a rightward bias, and perform significantly worse compared to humans. To build on these findings, we developed an automated benchmark construction process using AI. This process involved topic retrieval, narrative script generation, error embedding, and image generation, creating a diverse set of text-image pairs with intentional errors. We validated our method through constructing two caparable benchmarks. This study highlights the strengths and weaknesses of LVLMs in real-world understanding and advances benchmark construction techniques, providing a scalable and automatic approach for AI model evaluation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IOR: Inversed Objects Replay for Incremental Object Detection</title>
<link>https://arxiv.org/abs/2406.04829</link>
<guid>https://arxiv.org/abs/2406.04829</guid>
<content:encoded><![CDATA[
arXiv:2406.04829v5 Announce Type: replace 
Abstract: Existing Incremental Object Detection (IOD) methods partially alleviate catastrophic forgetting when incrementally detecting new objects in real-world scenarios. However, many of these methods rely on the assumption that unlabeled old-class objects may co-occur with labeled new-class objects in the incremental data. When unlabeled old-class objects are absent, the performance of existing methods tends to degrade. The absence can be mitigated by generating old-class samples, but it incurs high costs. This paper argues that previous generation-based IOD suffers from redundancy, both in the use of generative models, which require additional training and storage, and in the overproduction of generated samples, many of which do not contribute significantly to performance improvements. To eliminate the redundancy, we propose Inversed Objects Replay (IOR). Specifically, we generate old-class samples by inversing the original detectors, thus eliminating the necessity of training and storing additional generative models. We propose augmented replay to reuse the objects in generated samples, reducing redundant generations. Moreover, we propose high-value knowledge distillation focusing on the positions of old-class objects overwhelmed by the background, which transfers the knowledge to the incremental detector. Extensive experiments conducted on MS COCO 2017 demonstrate that our method can efficiently improve detection performance in IOD scenarios with the absence of old-class objects. The code is available at https://github.com/JiaJia075/IOR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised learning of spatially varying regularization for diffeomorphic image registration</title>
<link>https://arxiv.org/abs/2412.17982</link>
<guid>https://arxiv.org/abs/2412.17982</guid>
<content:encoded><![CDATA[
arXiv:2412.17982v2 Announce Type: replace 
Abstract: Spatially varying regularization accommodates the deformation variations that may be necessary for different anatomical regions during deformable image registration. Historically, optimization-based registration models have harnessed spatially varying regularization to address anatomical subtleties. However, most modern deep learning-based models tend to gravitate towards spatially invariant regularization, wherein a homogenous regularization strength is applied across the entire image, potentially disregarding localized variations. In this paper, we propose a hierarchical probabilistic model that integrates a prior distribution on the deformation regularization strength, enabling the end-to-end learning of a spatially varying deformation regularizer directly from the data. The proposed method is straightforward to implement and easily integrates with various registration network architectures. Additionally, automatic tuning of hyperparameters is achieved through Bayesian optimization, allowing efficient identification of optimal hyperparameters for any given registration task. Comprehensive evaluations on publicly available datasets demonstrate that the proposed method significantly improves registration performance and enhances the interpretability of deep learning-based registration, all while maintaining smooth deformations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control</title>
<link>https://arxiv.org/abs/2501.02260</link>
<guid>https://arxiv.org/abs/2501.02260</guid>
<content:encoded><![CDATA[
arXiv:2501.02260v3 Announce Type: replace 
Abstract: We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Architectures for High Resolution Vision-Language Models</title>
<link>https://arxiv.org/abs/2501.02584</link>
<guid>https://arxiv.org/abs/2501.02584</guid>
<content:encoded><![CDATA[
arXiv:2501.02584v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Haze: Generative Nighttime Image Dehazing</title>
<link>https://arxiv.org/abs/2503.08073</link>
<guid>https://arxiv.org/abs/2503.08073</guid>
<content:encoded><![CDATA[
arXiv:2503.08073v2 Announce Type: replace 
Abstract: Nighttime image dehazing is particularly challenging when dense haze and intense glow severely degrade or entirely obscure background information. Existing methods often struggle due to insufficient background priors and limited generative capability, both of which are highly important under such conditions. In this paper, we introduce BeyondHaze, a generative nighttime dehazing method that not only reduces haze and glow effects but also reconstructs plausible background structures in regions where visual cues are heavily degraded. Our approach is built on two main ideas: obtaining strong background priors by adapting image diffusion models to nighttime dehazing, and enhancing generative ability in haze- and glow-obscured areas through guided training. Task-specific nighttime dehazing knowledge is distilled into an image diffusion model while preserving its capacity to generate clean images. The diffusion model is further trained on tailored image pairs to improve its ability to recover background details that are suppressed by haze effects. Since generative models may introduce hallucinated content, we design our framework to allow user control over the generative level, enabling a balance between visual realism and fidelity. Experiments on real-world nighttime images demonstrate that BeyondHaze substantially improves visibility and scene detail under dense haze.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation</title>
<link>https://arxiv.org/abs/2503.09878</link>
<guid>https://arxiv.org/abs/2503.09878</guid>
<content:encoded><![CDATA[
arXiv:2503.09878v3 Announce Type: replace 
Abstract: Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition</title>
<link>https://arxiv.org/abs/2503.14960</link>
<guid>https://arxiv.org/abs/2503.14960</guid>
<content:encoded><![CDATA[
arXiv:2503.14960v3 Announce Type: replace 
Abstract: Skeleton-based Human Action Recognition (HAR) is a vital technology in robotics and human-robot interaction. However, most existing methods concentrate primarily on full-body movements and often overlook subtle hand motions that are critical for distinguishing fine-grained actions. Recent work leverages a unified graph representation that combines body, hand, and foot keypoints to capture detailed body dynamics. Yet, these models often blur fine hand details due to the disparity between body and hand action characteristics and the loss of subtle features during the spatial-pooling. In this paper, we propose BHaRNet (Body-Hand action Recognition Network), a novel framework that augments a typical body-expert model with a hand-expert model. Our model jointly trains both streams with an ensemble loss that fosters cooperative specialization, functioning in a manner reminiscent of a Mixture-of-Experts (MoE). Moreover, cross-attention is employed via an expertized branch method and a pooling-attention module to enable feature-level interactions and selectively fuse complementary information. Inspired by MMNet, we also demonstrate the applicability of our approach to multi-modal tasks by leveraging RGB information, where body features guide RGB learning to capture richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive actions -- while maintaining fewer GFLOPs and parameters than the relevant unified methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Correspondence Learning for Relative Pose Estimation</title>
<link>https://arxiv.org/abs/2503.18671</link>
<guid>https://arxiv.org/abs/2503.18671</guid>
<content:encoded><![CDATA[
arXiv:2503.18671v2 Announce Type: replace 
Abstract: Relative pose estimation provides a promising way for achieving object-agnostic pose estimation. Despite the success of existing 3D correspondence-based methods, the reliance on explicit feature matching suffers from small overlaps in visible regions and unreliable feature estimation for invisible regions. Inspired by humans' ability to assemble two object parts that have small or no overlapping regions by considering object structure, we propose a novel Structure-Aware Correspondence Learning method for Relative Pose Estimation, which consists of two key modules. First, a structure-aware keypoint extraction module is designed to locate a set of kepoints that can represent the structure of objects with different shapes and appearance, under the guidance of a keypoint based image reconstruction loss. Second, a structure-aware correspondence estimation module is designed to model the intra-image and inter-image relationships between keypoints to extract structure-aware features for correspondence estimation. By jointly leveraging these two modules, the proposed method can naturally estimate 3D-3D correspondences for unseen objects without explicit feature matching for precise relative pose estimation. Experimental results on the CO3D, Objaverse and LineMOD datasets demonstrate that the proposed method significantly outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error on the CO3D dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Motion Unlearning</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
arXiv:2503.18674v2 Announce Type: replace 
Abstract: We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., "kicking" is "loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: https://www.pinlab.org/hmu.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation</title>
<link>https://arxiv.org/abs/2503.18944</link>
<guid>https://arxiv.org/abs/2503.18944</guid>
<content:encoded><![CDATA[
arXiv:2503.18944v2 Announce Type: replace 
Abstract: Vision foundation models (VFMs) trained on large-scale image datasets provide high-quality features that have significantly advanced 2D visual recognition. However, their potential in 3D scene segmentation remains largely untapped, despite the common availability of 2D images alongside 3D point cloud datasets. While significant research has been dedicated to 2D-3D fusion, recent state-of-the-art 3D methods predominantly focus on 3D data, leaving the integration of VFMs into 3D models underexplored. In this work, we challenge this trend by introducing DITR, a generally applicable approach that extracts 2D foundation model features, projects them to 3D, and finally injects them into a 3D point cloud segmentation model. DITR achieves state-of-the-art results on both indoor and outdoor 3D semantic segmentation benchmarks. To enable the use of VFMs even when images are unavailable during inference, we additionally propose to pretrain 3D models by distilling 2D foundation models. By initializing the 3D backbone with knowledge distilled from 2D VFMs, we create a strong basis for downstream 3D segmentation tasks, ultimately boosting performance across various datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape and Texture Recognition in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.23062</link>
<guid>https://arxiv.org/abs/2503.23062</guid>
<content:encoded><![CDATA[
arXiv:2503.23062v4 Announce Type: replace 
Abstract: Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shape and Textures dataset (LAS&amp;T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (VLM) recognize and represent shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape recognition capabilities of the LVLMs remain significantly below human performance. VLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler, more abstract 2D textures and shapes. These results are consistent across a wide range of leading LVLMs (GPT/Gemini/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of leading models to extract and represent low-level visual features. In contrast, humans and simple nets trained directly for these tasks achieve high accuracy. The LAS&amp;T dataset, featuring over 700,000 images for 2D/3D shape, texture, and material recognition and retrieval is freely available.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decade of You Only Look Once (YOLO) for Object Detection: A Review</title>
<link>https://arxiv.org/abs/2504.18586</link>
<guid>https://arxiv.org/abs/2504.18586</guid>
<content:encoded><![CDATA[
arXiv:2504.18586v3 Announce Type: replace 
Abstract: This review marks the tenth anniversary of You Only Look Once (YOLO), one of the most influential frameworks in real-time object detection. Over the past decade, YOLO has evolved from a streamlined detector into a diverse family of architectures characterized by efficient design, modular scalability, and cross-domain adaptability. The paper presents a technical overview of the main versions (from YOLOv1 to YOLOv13), highlights key architectural trends, and surveys the principal application areas in which YOLO has been adopted. It also addresses evaluation practices, ethical considerations, and potential future directions for the framework's continued development. The analysis aims to provide a comprehensive and critical perspective on YOLO's trajectory and ongoing transformation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.01406</link>
<guid>https://arxiv.org/abs/2505.01406</guid>
<content:encoded><![CDATA[
arXiv:2505.01406v2 Announce Type: replace 
Abstract: Video diffusion models can generate realistic and temporally consistent videos. This raises concerns about provenance, ownership, and integrity. Watermarking can help address these issues by embedding metadata directly into the content. To work well, a watermark needs enough capacity for meaningful metadata. It must also stay imperceptible and remain robust to common video manipulations. Existing methods struggle with limited capacity, extra inference cost, or reduced visual quality. We introduce VidStamp, a watermarking framework that embeds frame-level messages through the decoder of a latent video diffusion model. The decoder is fine-tuned in two stages. The first stage uses static image datasets to encourage spatial message separation. The second stage uses synthesized video sequences to restore temporal consistency. This approach enables high-capacity watermarks with minimal perceptual impact. VidStamp also supports dynamic watermarking through a control signal that selects message templates during inference. This adds flexibility and creates a second channel for communication. We evaluate VidStamp on Stable Video Diffusion (I2V), OpenSora, and Wan (T2V). The system embeds 48 bits per frame while preserving visual quality and staying robust to common distortions. Compared with VideoSeal, VideoShield, and RivaGAN, it achieves lower log P-values and stronger detectability. Its frame-wise watermarking design also enables precise temporal tamper localization, with an accuracy of 0.96, which exceeds the VideoShield baseline. Code: https://github.com/SPIN-UMass/VidStamp
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.03329</link>
<guid>https://arxiv.org/abs/2505.03329</guid>
<content:encoded><![CDATA[
arXiv:2505.03329v4 Announce Type: replace 
Abstract: Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97\%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at https://github.com/AMAP-ML/FluxText.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.04899</link>
<guid>https://arxiv.org/abs/2505.04899</guid>
<content:encoded><![CDATA[
arXiv:2505.04899v2 Announce Type: replace 
Abstract: Recent advances in representation learning often rely on holistic embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging, where downstream tasks depend on anatomically interpretable features. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while enabling fine-grained control for targeted clinical applications. Experiments on CT and MRI datasets demonstrate OWT's power: it not only achieves strong performance on standard tasks like image reconstruction and segmentation, but also unlocks novel, high-impact clinical capabilities including organ-specific tumor identification, organ-level retrieval and semantic-level generation, without requiring any additional training. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and a new perspective on how representations can be leveraged.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Reinforcement Learning for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2505.17534</link>
<guid>https://arxiv.org/abs/2505.17534</guid>
<content:encoded><![CDATA[
arXiv:2505.17534v3 Announce Type: replace 
Abstract: This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at https://github.com/mm-vl/ULM-R1.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation</title>
<link>https://arxiv.org/abs/2505.22444</link>
<guid>https://arxiv.org/abs/2505.22444</guid>
<content:encoded><![CDATA[
arXiv:2505.22444v2 Announce Type: replace 
Abstract: The emergence of large-scale pre-trained point cloud models has significantly advanced 3D scene understanding, but adapting these models to specific downstream tasks typically demands full fine-tuning, incurring high computational and storage costs. Parameter-efficient fine-tuning (PEFT) techniques, successful in natural language processing and 2D vision tasks, would underperform when naively applied to 3D point cloud models due to significant geometric and spatial distribution shifts. Existing PEFT methods commonly treat points as orderless tokens, neglecting important local spatial structures and global geometric contexts in 3D modeling. To bridge this gap, we introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT module specifically designed for 3D point cloud transformers. GEM explicitly integrates fine-grained local positional encodings with a lightweight latent attention mechanism to capture comprehensive global context, thereby effectively addressing the spatial and geometric distribution mismatch. Extensive experiments demonstrate that GEM achieves performance comparable to or sometimes even exceeding full fine-tuning, while only updating 1.6% of the model's parameters, fewer than other PEFT methods. With significantly reduced training time and memory requirements, our approach thus sets a new benchmark for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point cloud models. Code is available at https://github.com/LiyaoTang/GEM.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos</title>
<link>https://arxiv.org/abs/2506.05274</link>
<guid>https://arxiv.org/abs/2506.05274</guid>
<content:encoded><![CDATA[
arXiv:2506.05274v2 Announce Type: replace 
Abstract: Composed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving, and provides 180K triplets drawn from FineGym and FineDiving datasets. Previous CoVR benchmarks, focusing on temporal aspect, link each query to a single target segment taken from the same video, limiting practical usefulness. In TF-CoVR, we instead construct each  pair by prompting an LLM with the label differences between clips drawn from different videos; every pair is thus associated with multiple valid target videos (3.9 on average), reflecting real-world tasks such as sports-highlight generation. To model these temporal dynamics, we propose TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video encoder on fine-grained action classification to obtain temporally discriminative embeddings; (ii) align the composed query with candidate videos using contrastive learning. We conduct the first comprehensive study of image, video, and general multimodal embedding (GME) models on temporally fine-grained composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and after fine-tuning raises the state-of-the-art from 19.83 to 27.22.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation</title>
<link>https://arxiv.org/abs/2506.09109</link>
<guid>https://arxiv.org/abs/2506.09109</guid>
<content:encoded><![CDATA[
arXiv:2506.09109v2 Announce Type: replace 
Abstract: As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, an evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 22% F1 points. Additionally, we construct two datasets for culturally universal concepts, one comprising T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering</title>
<link>https://arxiv.org/abs/2506.09920</link>
<guid>https://arxiv.org/abs/2506.09920</guid>
<content:encoded><![CDATA[
arXiv:2506.09920v4 Announce Type: replace 
Abstract: Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer</title>
<link>https://arxiv.org/abs/2506.18904</link>
<guid>https://arxiv.org/abs/2506.18904</guid>
<content:encoded><![CDATA[
arXiv:2506.18904v3 Announce Type: replace 
Abstract: Illumination and texture editing are critical dimensions for world-to-world transfer, which is valuable for applications including sim2real and real2real visual data scaling up for embodied AI. Existing techniques generatively re-render the input video to realize the transfer, such as video relighting models and conditioned world generation models. Nevertheless, these models are predominantly limited to the domain of training data (e.g., portrait) or fall into the bottleneck of temporal consistency and computation efficiency, especially when the input video involves complex dynamics and long durations. In this paper, we propose TC-Light, a novel generative renderer to overcome these problems. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible re-rendering results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.19072</link>
<guid>https://arxiv.org/abs/2506.19072</guid>
<content:encoded><![CDATA[
arXiv:2506.19072v2 Announce Type: replace 
Abstract: Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII compared to popular open-source VLMs. The code is available at https://github.com/yimuwangcs/wise-hawaii.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Measurement: Efficient Estimation at Scale</title>
<link>https://arxiv.org/abs/2507.01372</link>
<guid>https://arxiv.org/abs/2507.01372</guid>
<content:encoded><![CDATA[
arXiv:2507.01372v2 Announce Type: replace 
Abstract: AI has the potential to transform scientific discovery by analyzing vast datasets with little human effort. However, current workflows often do not provide the accuracy or statistical guarantees that are needed. We introduce active measurement, a human-in-the-loop AI framework for scientific measurement. An AI model is used to predict measurements for individual units, which are then sampled for human labeling using importance sampling. With each new set of human labels, the AI model is improved and an unbiased Monte Carlo estimate of the total measurement is refined. Active measurement can provide precise estimates even with an imperfect AI model, and requires little human effort when the AI model is very accurate. We derive novel estimators, weighting schemes, and confidence intervals, and show that active measurement reduces estimation error compared to alternatives in several measurement tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset</title>
<link>https://arxiv.org/abs/2507.22152</link>
<guid>https://arxiv.org/abs/2507.22152</guid>
<content:encoded><![CDATA[
arXiv:2507.22152v2 Announce Type: replace 
Abstract: Background Brain tumours are the most common solid malignancies in children, encompassing diverse histological, molecular subtypes and imaging features and outcomes. Paediatric brain tumours (PBTs), including high- and low-grade gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation offers promising tools for tumour delineation, yet its performance across heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A retrospective single-centre cohort of 174 paediatric patients with HGG, LGG, medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual annotations were provided for four tumour subregions: whole tumour (WT), T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D nnU-Net model was trained and tested (121/53 split), with segmentation performance assessed using the Dice similarity coefficient (DSC) and compared against intra- and inter-rater variability. Results The model achieved robust performance for WT and T2H (mean DSC: 0.85), comparable to human annotator variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2 alone produced results nearly equivalent to the full protocol. Conclusions DL is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and CC segmentation, highlighting the need for further refinement. These findings support the potential for protocol simplification and automation to enhance volumetric assessment and streamline paediatric neuro-oncology workflows.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion</title>
<link>https://arxiv.org/abs/2508.04559</link>
<guid>https://arxiv.org/abs/2508.04559</guid>
<content:encoded><![CDATA[
arXiv:2508.04559v2 Announce Type: replace 
Abstract: Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios - for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce OMFA (One Model For All), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. OMFA is inspired by language modeling, where generation is guided by conditioning prompts. However, our framework differs fundamentally from LLMs in two key aspects. First, it employs a bidirectional modeling paradigm that symmetrically allows prompting either from the garment to generate try-on results or from the dressed person to recover the try-off garment. Second, it strictly adheres to Tweedie's formula, enabling faithful estimation of the underlying data distribution during the denoising process. Instead of imposing lower body constraints, OMFA is an entirely mask-free framework that requires only a single portrait and a target garment as input, and is designed to support flexible outfit combinations and cross-person garment transfer, making it better aligned with practical usage scenarios. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical solution for virtual garment synthesis.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring</title>
<link>https://arxiv.org/abs/2508.07369</link>
<guid>https://arxiv.org/abs/2508.07369</guid>
<content:encoded><![CDATA[
arXiv:2508.07369v2 Announce Type: replace 
Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models pretrained on data from a specific sensor often generalize poorly to data from other sensors. Existing methods to tackle such cross-sensor degradation include retraining model or zero-shot methods, but they are highly time-consuming or even need extra training data. To address these challenges, our method first performs modular decomposition on deep learning-based pansharpening models, revealing a general yet critical interface where high-dimensional fused features begin mapping to the channel space of the final image. % may need revisement A Feature Tailor is then integrated at this interface to address cross-sensor degradation at the feature level, and is trained efficiently with physics-aware unsupervised losses. Moreover, our method operates in a patch-wise manner, training on partial patches and performing parallel inference on all patches to boost efficiency. Our method offers two key advantages: (1) $\textit{Improved Generalization Ability}$: it significantly enhance performance in cross-sensor cases. (2) $\textit{Low Generalization Cost}$: it achieves sub-second training and inference, requiring only partial test inputs and no external data, whereas prior methods often take minutes or even hours. Experiments on the real-world data from multiple datasets demonstrate that our method achieves state-of-the-art quality and efficiency in tackling cross-sensor degradation. For example, training and inference of $512\times512\times8$ image within $\textit{0.2 seconds}$ and $4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest setting on a commonly used RTX 3090 GPU, which is over 100 times faster than zero-shot methods.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phased One-Step Adversarial Equilibrium for Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.21019</link>
<guid>https://arxiv.org/abs/2508.21019</guid>
<content:encoded><![CDATA[
arXiv:2508.21019v2 Announce Type: replace 
Abstract: Video diffusion generation suffers from critical sampling efficiency bottlenecks, particularly for large-scale models and long contexts. Existing video acceleration methods, adapted from image-based techniques, lack a single-step distillation ability for large-scale video models and task generalization for conditional downstream tasks. To bridge this gap, we propose the Video Phased Adversarial Equilibrium (V-PAE), a distillation framework that enables high-quality, single-step video generation from large-scale video models. Our approach employs a two-phase process. (i) Stability priming is a warm-up process to align the distributions of real and generated videos. It improves the stability of single-step adversarial distillation in the following process. (ii) Unified adversarial equilibrium is a flexible self-adversarial process that reuses generator parameters for the discriminator backbone. It achieves a co-evolutionary adversarial equilibrium in the Gaussian noise space. For the conditional tasks, we primarily preserve video-image subject consistency, which is caused by semantic degradation and conditional frame collapse during the distillation training in image-to-video (I2V) generation. Comprehensive experiments on VBench-I2V demonstrate that V-PAE outperforms existing acceleration methods by an average of 5.8% in the overall quality score, including semantic alignment, temporal coherence, and frame quality. In addition, our approach reduces the diffusion latency of the large-scale video model (e.g., Wan2.1-I2V-14B) by 100 times, while preserving competitive performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement</title>
<link>https://arxiv.org/abs/2509.09232</link>
<guid>https://arxiv.org/abs/2509.09232</guid>
<content:encoded><![CDATA[
arXiv:2509.09232v3 Announce Type: replace 
Abstract: In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at https://github.com/jiesihu/Medverse.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</title>
<link>https://arxiv.org/abs/2509.12090</link>
<guid>https://arxiv.org/abs/2509.12090</guid>
<content:encoded><![CDATA[
arXiv:2509.12090v2 Announce Type: replace 
Abstract: Reconstructing cardiac motion from CMR sequences is critical for diagnosis, prognosis, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their applicability during intervention when only sparse observations are available. We present TetHeart, the first end-to-end framework for unified 4D heart mesh recovery from both offline full-stack and intra-procedural sparse-slice observations. Our method leverages deformable tetrahedra to capture shape and motion in a coherent space shared across cardiac structures. Before a procedure, it initializes detailed, patient-specific heart meshes from high-quality full stacks, which can then be updated using whatever slices can be obtained in real-time, down to a single one during the procedure. TetHeart incorporates several key innovations: (i) an attentive slice-adaptive 2D-3D feature assembly mechanism that integrates information from arbitrary numbers of slices at any position; (ii) a distillation strategy to ensure accurate reconstruction under extreme sparsity; and (iii) a weakly supervised motion learning scheme requiring annotations only at keyframes, such as the end-diastolic and end-systolic phases. Trained and validated on three large public datasets and evaluated zero-shot on additional private interventional and public datasets without retraining, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localized Region Guidance for Class Activation Mapping in WSSS</title>
<link>https://arxiv.org/abs/2509.12496</link>
<guid>https://arxiv.org/abs/2509.12496</guid>
<content:encoded><![CDATA[
arXiv:2509.12496v2 Announce Type: replace 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations. Existing WSSS methods struggle with precise object boundary localization and focus only on the most discriminative regions. To address these challenges, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement using object proposals to guide CAM generation, ensuring complete object coverage; (2) Influence Function Integration that captures the relationship between training samples and model predictions; and (3) Multi-Scale Boundary Enhancement with progressive refinement strategies. IG-CAM achieves state-of-the-art performance on PASCAL VOC 2012 with 82.3% mIoU before post-processing, improving to 86.6% after CRF refinement, significantly outperforming previous WSSS methods. Extensive ablation studies validate each component's contribution, establishing IG-CAM as a new benchmark for weakly supervised semantic segmentation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning</title>
<link>https://arxiv.org/abs/2509.13161</link>
<guid>https://arxiv.org/abs/2509.13161</guid>
<content:encoded><![CDATA[
arXiv:2509.13161v2 Announce Type: replace 
Abstract: Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models. Code will be open-sourced at https://github.com/ziHoHe/SMV-CR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</title>
<link>https://arxiv.org/abs/2509.21223</link>
<guid>https://arxiv.org/abs/2509.21223</guid>
<content:encoded><![CDATA[
arXiv:2509.21223v2 Announce Type: replace 
Abstract: Pre-training has proven effective for learning transferable features in sign language understanding (SLU) tasks. Recently, skeleton-based methods have gained increasing attention because they can robustly handle variations in subjects and backgrounds without being affected by appearance or environmental factors. Current SLU methods continue to face three key limitations: 1) weak semantic grounding, as models often capture low-level motion patterns from skeletal data but struggle to relate them to linguistic meaning; 2) imbalance between local details and global context, with models either focusing too narrowly on fine-grained cues or overlooking them for broader context; and 3) inefficient cross-modal learning, as constructing semantically aligned representations across modalities remains difficult. To address these, we propose Sigma, a unified skeleton-based SLU framework featuring: 1) a sign-aware early fusion mechanism that facilitates deep interaction between visual and textual modalities, enriching visual features with linguistic context; 2) a hierarchical alignment learning strategy that jointly maximises agreements across different levels of paired features from different modalities, effectively capturing both fine-grained details and high-level semantic relationships; and 3) a unified pre-training framework that combines contrastive learning, text matching and language modelling to promote semantic consistency and generalisation. Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement</title>
<link>https://arxiv.org/abs/2509.23584</link>
<guid>https://arxiv.org/abs/2509.23584</guid>
<content:encoded><![CDATA[
arXiv:2509.23584v2 Announce Type: replace 
Abstract: Video Face Enhancement (VFE) aims to restore high-quality facial regions from degraded video sequences, enabling a wide range of practical applications. Despite substantial progress in the field, current methods that primarily rely on video super-resolution and generative frameworks continue to face three fundamental challenges: (1) computational inefficiency caused by iterative multi-step denoising in diffusion models; (2) faithfully modeling intricate facial textures while preserving temporal consistency; and (3) limited model generalization due to the lack of high-quality face video training data. To address these challenges, we propose VividFace, a novel and efficient one-step diffusion framework for VFE. Built upon the pretrained WANX video generation model, VividFace reformulates the traditional multi-step diffusion process as a single-step flow matching paradigm that directly maps degraded inputs to high-quality outputs with significantly reduced inference time. To enhance facial detail recovery, we introduce a Joint Latent-Pixel Face-Focused Training strategy that constructs spatiotemporally aligned facial masks to guide optimization toward critical facial regions in both latent and pixel spaces. Furthermore, we develop an MLLM-driven automated filtering pipeline that produces MLLM-Face90, a meticulously curated high-quality face video dataset, ensuring models learn from photorealistic facial textures. Extensive experiments demonstrate that VividFace achieves superior performance in perceptual quality, identity preservation, and temporal consistency across both synthetic and real-world benchmarks. We will publicly release our code, models, and dataset to support future research.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization</title>
<link>https://arxiv.org/abs/2510.16146</link>
<guid>https://arxiv.org/abs/2510.16146</guid>
<content:encoded><![CDATA[
arXiv:2510.16146v2 Announce Type: replace 
Abstract: The limited availability of annotated data in medical imaging makes semi-supervised learning increasingly appealing for its ability to learn from imperfect supervision. Recently, teacher-student frameworks have gained popularity for their training benefits and robust performance. However, jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios. To address this for medical image segmentation, we propose DuetMatch, a novel dual-branch semi-supervised framework with asynchronous optimization, where each branch optimizes either the encoder or decoder while keeping the other frozen. To improve consistency under noisy conditions, we introduce Decoupled Dropout Perturbation, enforcing regularization across branches. We also design Pair-wise CutMix Cross-Guidance to enhance model diversity by exchanging pseudo-labels through augmented input pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose Consistency Matching, refining labels using stable predictions from frozen teacher models. Extensive experiments on benchmark brain MRI segmentation datasets, including ISLES2022 and BraTS, show that DuetMatch consistently outperforms state-of-the-art methods, demonstrating its effectiveness and robustness across diverse semi-supervised segmentation scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
<link>https://arxiv.org/abs/2510.20470</link>
<guid>https://arxiv.org/abs/2510.20470</guid>
<content:encoded><![CDATA[
arXiv:2510.20470v2 Announce Type: replace 
Abstract: Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding, yet still struggle with inaccurate evidence localization. To address these limitations, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies context and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we 1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that include frame identification, evidence reasoning, and action decision, and 2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to progressively incentivize multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long video understanding tasks, validating its strong scalability and robustness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of Multi-scale Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2510.27237</link>
<guid>https://arxiv.org/abs/2510.27237</guid>
<content:encoded><![CDATA[
arXiv:2510.27237v2 Announce Type: replace 
Abstract: Whole slide image (WSI) analysis has emerged as an increasingly essential technique in computational pathology. Recent advances in the pathology foundation models (FMs) have demonstrated significant advantages in deriving meaningful patch-level or slide-level multi-scale features from WSIs. However, current pathology FMs have exhibited substantial heterogeneity caused by diverse private training datasets and different network architectures. This heterogeneity introduces performance variability when we utilize the features from different FMs in the downstream tasks. To fully explore the advantages of multiple FMs effectively, in this work, we propose a novel framework for the fusion of multi-scale heterogeneous pathology FMs, called FuseCPath, yielding a model with a superior ensemble performance. The main contributions of our framework can be summarized as follows: (i) To guarantee the representativeness of the training patches, we propose a multi-view clustering-based method to filter out the discriminative patches via multiple FMs' embeddings. (ii) To effectively fuse the patch-level FMs, we devise a cluster-level re-embedding strategy to online capture patch-level local features. (iii) To effectively fuse the slide-level FMs, we devise a collaborative distillation strategy to explore the connections between slide-level FMs. Extensive experiments demonstrate that the proposed FuseCPath achieves state-of-the-art performance across multiple tasks on diverse datasets.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks</title>
<link>https://arxiv.org/abs/2511.00396</link>
<guid>https://arxiv.org/abs/2511.00396</guid>
<content:encoded><![CDATA[
arXiv:2511.00396v2 Announce Type: replace 
Abstract: We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO's key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV</title>
<link>https://arxiv.org/abs/2511.06741</link>
<guid>https://arxiv.org/abs/2511.06741</guid>
<content:encoded><![CDATA[
arXiv:2511.06741v3 Announce Type: replace 
Abstract: Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</title>
<link>https://arxiv.org/abs/2511.09540</link>
<guid>https://arxiv.org/abs/2511.09540</guid>
<content:encoded><![CDATA[
arXiv:2511.09540v3 Announce Type: replace 
Abstract: Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</title>
<link>https://arxiv.org/abs/2511.10241</link>
<guid>https://arxiv.org/abs/2511.10241</guid>
<content:encoded><![CDATA[
arXiv:2511.10241v2 Announce Type: replace 
Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Discriminative Feature Learning for Deep Multi-View Clustering</title>
<link>https://arxiv.org/abs/2103.15069</link>
<guid>https://arxiv.org/abs/2103.15069</guid>
<content:encoded><![CDATA[
arXiv:2103.15069v3 Announce Type: replace-cross 
Abstract: Multi-view clustering is an important research topic due to its capability to utilize complementary information from multiple views. However, there are few methods to consider the negative impact caused by certain views with unclear clustering structures, resulting in poor multi-view clustering performance. To address this drawback, we propose self-supervised discriminative feature learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders are applied to learn embedded features for each view independently. To leverage the multi-view complementary information, we concatenate all views' embedded features to form the global features, which can overcome the negative impact of some views' unclear clustering structures. In a self-supervised manner, pseudo-labels are obtained to build a unified target distribution to perform multi-view discriminative feature learning. During this process, global discriminative information can be mined to supervise all views to learn more discriminative features, which in turn are used to update the target distribution. Besides, this unified target distribution can make SDMVC learn consistent cluster assignments, which accomplishes the clustering consistency of multiple views while preserving their features' diversity. Experiments on various types of multi-view datasets show that SDMVC outperforms 14 competitors including classic and state-of-the-art methods. The code is available at https://github.com/SubmissionsIn/SDMVC.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing DEFORMISE: A deep learning framework for dementia diagnosis in the elderly using optimized MRI slice selection</title>
<link>https://arxiv.org/abs/2407.17324</link>
<guid>https://arxiv.org/abs/2407.17324</guid>
<content:encoded><![CDATA[
arXiv:2407.17324v3 Announce Type: replace-cross 
Abstract: Dementia, a debilitating neurological condition affecting millions worldwide, presents significant diagnostic challenges. In this work, we introduce DEFORMISE, a novel DEep learning Framework for dementia diagnOsis of eldeRly patients using 3D brain Magnetic resonance Imaging (MRI) scans with Optimized Slice sElection. Our approach features a unique technique for selectively processing MRI slices, focusing on the most relevant brain regions and excluding less informative sections. This methodology is complemented by a confidence-based classification committee composed of three novel deep learning models. Tested on the Open OASIS datasets, our method achieved an impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore, validation on the ADNI dataset confirmed the robustness and generalizability of our approach. The use of explainable AI (XAI) techniques and comprehensive ablation studies further substantiate the effectiveness of our techniques, providing insights into the decision-making process and the importance of our methodology. This research offers a significant advancement in dementia diagnosis, providing a highly accurate and efficient tool for clinical applications.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEARNER: Contrastive Pretraining for Learning Fine-Grained Patient Progression from Coarse Inter-Patient Labels</title>
<link>https://arxiv.org/abs/2411.01144</link>
<guid>https://arxiv.org/abs/2411.01144</guid>
<content:encoded><![CDATA[
arXiv:2411.01144v2 Announce Type: replace-cross 
Abstract: Predicting whether a treatment leads to meaningful improvement is a central challenge in personalized medicine, particularly when disease progression manifests as subtle visual changes over time. While data-driven deep learning (DL) offers a promising route to automate such predictions, acquiring large-scale longitudinal data for each individual patient remains impractical. To address this limitation, we explore whether inter-patient variability can serve as a proxy for learning intra-patient progression. We propose LEARNER, a contrastive pretraining framework that leverages coarsely labeled inter-patient data to learn fine-grained, patient-specific representations. Using lung ultrasound (LUS) and brain MRI datasets, we demonstrate that contrastive objectives trained on coarse inter-patient differences enable models to capture subtle intra-patient changes associated with treatment response. Across both modalities, our approach improves downstream classification accuracy and F1-score compared to standard MSE pretraining, highlighting the potential of inter-patient contrastive learning for individualized outcome prediction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation</title>
<link>https://arxiv.org/abs/2502.02054</link>
<guid>https://arxiv.org/abs/2502.02054</guid>
<content:encoded><![CDATA[
arXiv:2502.02054v2 Announce Type: replace-cross 
Abstract: This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
arXiv:2503.16356v3 Announce Type: replace-cross 
Abstract: Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they often fail to generalize these updates to multi-hop reasoning tasks that rely on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we find that current layer-localized KE approaches (e.g., MEMIT, WISE), which edit only single or a few model layers, inadequately integrate updated knowledge into these reasoning pathways. To address this limitation, we present CaKE (Circuit-aware Knowledge Editing), a novel method that enhances the effective integration of updated knowledge in LLMs. By only leveraging a few curated data samples guided by our circuit-based analysis, CaKE stimulates the model to develop appropriate reasoning circuits for newly incorporated knowledge. Experiments show that CaKE enables more accurate and consistent use of edited knowledge across related reasoning tasks, achieving an average improvement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while requiring less memory than existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title>
<link>https://arxiv.org/abs/2504.02132</link>
<guid>https://arxiv.org/abs/2504.02132</guid>
<content:encoded><![CDATA[
arXiv:2504.02132v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) is instrumental for inhibiting hallucinations in large language models (LLMs) through the use of a factual knowledge base (KB). Although PDF documents are prominent sources of knowledge, text-based RAG pipelines are ineffective at capturing their rich multi-modal information. In contrast, visual document RAG (VD-RAG) uses screenshots of document pages as the KB, which has been shown to achieve state-of-the-art results. However, by introducing the image modality, VD-RAG introduces new attack vectors for adversaries to disrupt the system by injecting malicious documents into the KB. In this paper, we demonstrate the vulnerability of VD-RAG to poisoning attacks targeting both retrieval and generation. We define two attack objectives and demonstrate that both can be realized by injecting only a single adversarial image into the KB. Firstly, we introduce a targeted attack against one or a group of queries with the goal of spreading targeted disinformation. Secondly, we present a universal attack that, for any potential user query, influences the response to cause a denial-of-service in the VD-RAG system. We investigate the two attack objectives under both white-box and black-box assumptions, employing a multi-objective gradient-based optimization approach as well as prompting state-of-the-art generative models. Using two visual document datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (vision language models), we show VD-RAG is vulnerable to poisoning attacks in both the targeted and universal settings, yet demonstrating robustness to black-box attacks in the universal setting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Filter-Based Common Components Modeling for Cross-Subject EEG Decoding</title>
<link>https://arxiv.org/abs/2507.05268</link>
<guid>https://arxiv.org/abs/2507.05268</guid>
<content:encoded><![CDATA[
arXiv:2507.05268v2 Announce Type: replace-cross 
Abstract: Brain-computer interface (BCI) technology enables direct communication between the brain and external devices through electroencephalography (EEG) signals. However, existing decoding models often mix common and personalized components, leading to interference from individual variability that limits cross-subject decoding performance. To address this issue, this paper proposes a system filter that extends the concept of signal filtering to the system level. The method expands a system into its spectral representation, selectively removes unnecessary components, and reconstructs the system from the retained target components, thereby achieving explicit system-level decomposition and filtering. We further integrate the system filter into a Cross-Subject Decoding framework based on the System Filter (CSD-SF) and evaluate it on the four-class motor imagery (MI) task of the BCIC IV 2a dataset. Personalized models are transformed into relation spectrums, and statistical testing across subjects is used to remove personalized components. The remaining stable relations, representing common components across subjects, are then used to construct a common model for cross-subject decoding. Experimental results show an average improvement of 3.28% in decoding accuracy over baseline methods, demonstrating that the proposed system filter effectively isolates stable common components and enhances model robustness and generalizability in cross-subject EEG decoding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep-GLS: Report-Guided Generalized Label Smoothing for Robust Disease Detection</title>
<link>https://arxiv.org/abs/2508.02495</link>
<guid>https://arxiv.org/abs/2508.02495</guid>
<content:encoded><![CDATA[
arXiv:2508.02495v3 Announce Type: replace-cross 
Abstract: Unlike nature image classification where groundtruth label is explicit and of no doubt, physicians commonly interpret medical image conditioned on certainty like using phrase "probable" or "likely". Existing medical image datasets either simply overlooked the nuance and polarise into binary label. Here, we propose a novel framework that leverages a Large Language Model (LLM) to directly mine medical reports to utilise the uncertainty relevant expression for supervision signal. At first, we collect uncertainty keywords from medical reports. Then, we use Qwen-3 4B to identify the textual uncertainty and map them into an adaptive Generalized Label Smoothing (GLS) rate. This rate allows our model to treat uncertain labels not as errors, but as informative signals, effectively incorporating expert skepticism into the training process. We establish a new clinical expert uncertainty-aware benchmark to rigorously evaluate this problem. Experiments demonstrate that our approach significantly outperforms state-of-the-art methods in medical disease detection. The curated uncertainty words database, code, and benchmark will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.09201</link>
<guid>https://arxiv.org/abs/2508.09201</guid>
<content:encoded><![CDATA[
arXiv:2508.09201v3 Announce Type: replace-cross 
Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks, posing serious safety risks. To address this, existing detection methods either learn attack-specific parameters, which hinders generalization to unseen attacks, or rely on heuristically sound principles, which limit accuracy and efficiency. To overcome these limitations, we propose Learning to Detect (LoD), a general framework that accurately detects unknown jailbreak attacks by shifting the focus from attack-specific learning to task-specific learning. This framework includes a Multi-modal Safety Concept Activation Vector module for safety-oriented representation learning and a Safety Pattern Auto-Encoder module for unsupervised attack classification. Extensive experiments show that our method achieves consistently higher detection AUROC on diverse unknown attacks while improving efficiency. The code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection</title>
<link>https://arxiv.org/abs/2502.15488</link>
<guid>https://arxiv.org/abs/2502.15488</guid>
<content:encoded><![CDATA[
<div> Quantization, Multi-view 3D Detection, Position Embedding, Lookup Table, Autonomous Driving<br /><br />Summary:<br /><br />Camera-based multi-view 3D detection methods like PETR achieve strong benchmark performance but suffer from high computational cost and memory usage, limiting real-world deployment. Directly applying traditional quantization techniques to PETRs causes significant accuracy loss due to two main challenges: the large magnitude difference between image features and camera-ray positional embeddings (PE), and the complexity and approximation errors introduced when quantizing nonlinear operators that require hardware-unfriendly computations. This paper introduces FQ-PETR, a fully quantized framework for PETRs with three key innovations addressing these issues. First, the Quantization-Friendly LiDAR-ray Position Embedding (QFPE) replaces multi-point sampling with a LiDAR-prior-guided single-point sampling and anchor-based embedding, removing problematic nonlinearities such as inverse-sigmoid and aligning the PE scale with image features to maintain accuracy. Second, the Dual-Lookup Table (DULUT) algorithm approximates complex nonlinear functions through two cascaded linear lookup tables, providing high-fidelity results with minimal entries and no reliance on specialized hardware. Third, Quantization After Numerical Stabilization (QANS) applies quantization following softmax numerical stabilization to reduce attention distortion caused by large inputs. Experiments on PETRs and their variants demonstrate that FQ-PETR with 8-bit weights and activations (W8A8) achieves near-floating-point accuracy, with only about 1% degradation, while reducing latency by up to 75%, outperforming existing post-training and quantization-aware training baselines. <div>
arXiv:2502.15488v4 Announce Type: replace 
Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video</title>
<link>https://arxiv.org/abs/2511.14848</link>
<guid>https://arxiv.org/abs/2511.14848</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D Motion Transfer, Semantic Correspondence, Multiview Video, 4D Reconstruction<br /><br />Summary:  
The paper introduces Gaussian See, Gaussian Do, a novel technique for semantic 3D motion transfer using multiview video data, enabling rig-free and cross-category motion transfer between objects with meaningful semantic correspondences. The method builds on implicit motion transfer techniques by extracting motion embeddings from source videos through condition inversion, which are then applied to rendered frames of static target shapes. These generated videos serve as supervision inputs for dynamic 3D Gaussian Splatting reconstruction, enabling realistic motion synthesis. A key innovation is the anchor-based view-aware motion embedding mechanism that ensures consistency across different viewpoints and speeds up the model convergence process. The paper also proposes a robust 4D reconstruction pipeline designed to consolidate noisy supervision videos effectively to improve reconstruction quality. Additionally, the authors establish the first benchmark dataset for semantic 3D motion transfer to facilitate standardized evaluation. Experimental results demonstrate that their approach outperforms adapted baseline methods, achieving superior motion fidelity and structural consistency in the reconstructed dynamic 3D scenes. Code and dataset have been made publicly available at the project’s website to encourage further research in this domain. <div>
arXiv:2511.14848v1 Announce Type: new 
Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation</title>
<link>https://arxiv.org/abs/2511.14860</link>
<guid>https://arxiv.org/abs/2511.14860</guid>
<content:encoded><![CDATA[
<div> Dental caries segmentation, panoramic radiographs, convolutional neural networks, vision transformers, medical image analysis  

<br /><br />Summary:  
1. This study benchmarks twelve state-of-the-art neural network architectures, including convolutional neural networks (CNNs), vision transformers, and state-space mamba models, for automated dental caries segmentation in panoramic radiographs using the DC1000 dataset.  
2. The architectures evaluated include VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, all trained under identical conditions to ensure fair comparison.  
3. Results reveal that the CNN-based DoubleU-Net outperformed all other models, achieving the highest dice coefficient (0.7345), mean Intersection over Union (mIoU) of 0.5978, and precision of 0.8145.  
4. Transformer and Mamba-based architectures, despite their advantages in modeling global context, underperformed due to challenges such as limited annotated data and weaker spatial priors, which are critical for this specific medical imaging task.  
5. The findings highlight that aligning model architecture with domain-specific task requirements is more important than solely increasing model complexity for effective medical image segmentation.  
6. The study provides valuable insights for future research and practical deployment in automated dental caries diagnosis, and the implementation code is openly available on GitHub. <div>
arXiv:2511.14860v1 Announce Type: new 
Abstract: Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?</title>
<link>https://arxiv.org/abs/2511.14870</link>
<guid>https://arxiv.org/abs/2511.14870</guid>
<content:encoded><![CDATA[
<div> CAD, Boundary Representation, Signed Distance Function, Latent Diffusion, Marching Cubes<br /><br />Summary:<br /><br />1. The paper introduces a novel geometric representation for CAD Boundary Representation (B-Rep) models using volumetric distance functions, named B-Rep Distance Functions (BR-DF).<br /><br />2. BR-DF encodes the surface mesh geometry of CAD models as signed distance functions (SDF) and represents vertices, edges, faces, along with their topology as per-face unsigned distance functions (UDFs).<br /><br />3. An extended version of the Marching Cubes algorithm is employed to convert BR-DF directly into watertight, faceted CAD B-Rep models, ensuring the conversion process never fails.<br /><br />4. The approach exploits the volumetric nature of BR-DF by using a multi-branch latent diffusion model with a 3D U-Net backbone to simultaneously generate the SDF and per-face UDFs that define the BR-DF.<br /><br />5. Experimental results demonstrate that the proposed method achieves comparable performance to state-of-the-art CAD generation techniques while attaining an unprecedented 100% success rate in producing valid faceted B-Rep models. <div>
arXiv:2511.14870v1 Announce Type: new 
Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis</title>
<link>https://arxiv.org/abs/2511.14884</link>
<guid>https://arxiv.org/abs/2511.14884</guid>
<content:encoded><![CDATA[
<div> Keywords: GeoSceneGraph, 3D scene synthesis, equivariant graph neural networks, text conditioning, indoor scene graphs<br /><br />Summary:<br /><br />This paper introduces GeoSceneGraph, a novel method for synthesizing indoor 3D scenes directly from text prompts, addressing limitations in existing approaches. Traditional generative models either train from scratch, ignoring inherent graph structures and resulting in less coherent scenes, or rely on vision-language models (VLMs) which, while effective, are often too large for resource-limited devices like XR glasses or mobile phones. GeoSceneGraph leverages the graph structure and geometric symmetries of indoor scenes without requiring predefined relationship classes or ground-truth annotations, thereby capturing diverse object interactions naturally. The method is built on equivariant graph neural networks (EGNNs), which preserve scene symmetries and relational information, but existing EGNNs are typically constrained by low-dimensional inputs and lack support for complex modalities such as text. To overcome this, the authors propose a simple yet effective strategy to condition EGNNs on high-dimensional text features, enabling the integration of rich semantic information from text prompts. Ablation studies validate the effectiveness of their text conditioning approach. Overall, GeoSceneGraph achieves scene generation quality comparable to methods relying on annotated relationships while being more flexible and suitable for deployment on resource-constrained devices. <div>
arXiv:2511.14884v1 Announce Type: new 
Abstract: Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation</title>
<link>https://arxiv.org/abs/2511.14897</link>
<guid>https://arxiv.org/abs/2511.14897</guid>
<content:encoded><![CDATA[
<div> MRI synthesis, Ultra-Low Field, High-Field, Implicit Neural Representation, Tissue-type SNR<br /><br />Summary:  
1. The paper introduces an unsupervised bidirectional MRI synthesis method that converts High-Field (HF) magnitude images into Ultra-Low Field (ULF)-like images and vice versa.  
2. The approach is physics-inspired, simulating contrast changes between HF and ULF MRIs by estimating tissue-type Signal-to-Noise Ratio (SNR) values to model the HF-to-ULF transformation accurately.  
3. For the Super-Resolution task, an Implicit Neural Representation (INR) network is used to generate HF images from ULF data, predicting tissue-type segmentation and image intensity simultaneously without requiring observed HF images.  
4. The method was evaluated qualitatively using synthetic ULF-like images generated from standard 3T T₁-weighted images, and quantitatively validated with paired 3T and 64mT T₁-weighted images.  
5. Results show significant improvements in white matter–gray matter (WM-GM) contrast: a 52% increase in synthetic ULF-like images and 37% in 64mT images. Sensitivity analyses confirm the forward model’s robustness to variations in target contrast, noise levels, and initial seeding. <div>
arXiv:2511.14897v1 Announce Type: new 
Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization</title>
<link>https://arxiv.org/abs/2511.14899</link>
<guid>https://arxiv.org/abs/2511.14899</guid>
<content:encoded><![CDATA[
<div> multi-view image editing, diffusion model, cross-view consistency, Score Distillation Sampling, 3D prior  

<br /><br />Summary:  
This paper tackles the challenge of multi-view image editing from sparse input views that consist of images capturing a scene from different viewpoints. The objective is to edit the scene based on a textual instruction while ensuring consistency across all views. Existing approaches relying on per-scene neural fields or temporal attention mechanisms often result in artifacts and incoherent edits in this scenario. The authors introduce InstructMix2Mix (I-Mix2Mix), a novel framework that distills the editing abilities of a 2D diffusion model into a pretrained multi-view diffusion model. This approach leverages the multi-view model’s data-driven 3D prior to maintain cross-view consistency. A major innovation is replacing the traditional neural field consolidator used in Score Distillation Sampling (SDS) with a multi-view diffusion student model. This change requires several new adaptations: incremental updates of the student model across denoising timesteps, a tailored teacher noise scheduler designed to prevent training degeneration, and a specialized attention modification that improves cross-view coherence without increasing computational cost. Experimental results demonstrate that I-Mix2Mix significantly enhances multi-view consistency while preserving high-quality edits in each individual view, overcoming limitations of prior methods. <div>
arXiv:2511.14899v1 Announce Type: new 
Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2511.14900</link>
<guid>https://arxiv.org/abs/2511.14900</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, dermatological diagnosis, textbook-based reasoning, reinforcement learning, diagnostic accuracy  

<br /><br />Summary:  
The paper addresses the limitations of current vision-language models (VLMs) in dermatological diagnosis, focusing on data heterogeneity, lack of grounded diagnostic rationales, and poor scalability/generalization. To overcome these challenges, the authors propose SkinR1, a novel dermatological VLM that integrates deep, textbook-based reasoning with reinforcement learning (RL) for improved clinical reasoning and diagnosis. First, SkinR1 uses a textbook-based reasoning generator to create high-quality, hierarchy-aware, and differential-diagnosis-informed reasoning trajectories, providing expert-level supervision for training. Second, these trajectories are employed in supervised fine-tuning (SFT) to ground the model’s diagnostic reasoning ability effectively. Third, the model adopts a new RL training paradigm that incorporates the hierarchical disease structure, enabling the transfer of learned reasoning patterns from small, richly annotated datasets to larger, sparsely annotated ones. Extensive experiments across multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy compared to previous approaches. An ablation study confirms the crucial role of the reasoning foundation established during supervised fine-tuning. The work highlights the potential of combining hierarchical expert knowledge with reinforcement learning to enhance the trustworthiness and clinical utility of vision-language models in dermatology. <div>
arXiv:2511.14900v1 Announce Type: new 
Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding</title>
<link>https://arxiv.org/abs/2511.14901</link>
<guid>https://arxiv.org/abs/2511.14901</guid>
<content:encoded><![CDATA[
<div> Keywords: FarSLIP, remote sensing, multi-granularity dataset, fine-grained alignment, vision-language models<br /><br />Summary: This paper addresses limitations in existing CLIP-based models for remote sensing (RS) by enhancing fine-grained region-text alignment. It identifies two main problems: current RS image-text datasets rely mainly on global captions generated from object-level labels, thus underutilizing detailed supervision; and conventional region-text alignment methods effective in general domains degrade performance when directly applied to RS data. To overcome these challenges, the authors introduce MGRS-200k, the first multi-granularity RS image-text dataset with rich object-level textual labels aimed at improving region-category alignment. They analyze current fine-grained tuning methods for CLIP and find that explicit patch-level alignment harms semantic coherence and reduces model effectiveness. Building on this insight, the paper proposes FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework featuring a novel patch-to-patch distillation approach that aligns local and global visual features, enhancing discriminability without compromising coherence. Furthermore, FarSLIP replaces explicit patch alignment with CLS token-based region-category alignment to better leverage region-text supervision and improve spatial awareness. Experiments demonstrate that FarSLIP achieves state-of-the-art results on RS open-vocabulary semantic segmentation and excels in zero-shot classification and image-text retrieval tasks. The dataset, code, and models are publicly available for research advancement. <div>
arXiv:2511.14901v1 Announce Type: new 
Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnMIL: A generalizable multiple instance learning framework for computational pathology</title>
<link>https://arxiv.org/abs/2511.14907</link>
<guid>https://arxiv.org/abs/2511.14907</guid>
<content:encoded><![CDATA[
<div> Computational pathology, multiple-instance learning, foundation models, slide-level prediction, uncertainty estimation<br /><br />Summary:<br /><br />1. The study addresses the challenge of aggregating patch-level features extracted by pathology foundation models from whole-slide images (WSIs) into reliable and generalizable slide-level clinical predictions.  
2. The authors propose nnMIL, a simple yet flexible multiple-instance learning framework that incorporates random sampling at both patch and feature levels, facilitating large-batch optimization, task-specific sampling, and scalable training across diverse datasets and model types.  
3. nnMIL uses a lightweight aggregator with sliding-window inference, enabling ensemble predictions at the slide level and robust uncertainty estimation, which is crucial for clinical decision-making.  
4. Extensive validation was performed on 40,000 WSIs covering 35 clinical tasks and involving four different pathology foundation models, where nnMIL outperformed existing MIL methods in disease diagnosis, histologic subtyping, molecular biomarker detection, and pan-cancer prognosis prediction.  
5. The method also demonstrated strong generalization across models, reliable uncertainty quantification, and effective survival stratification in external cohorts, highlighting its potential for real-world clinical AI applications.  
6. Overall, nnMIL offers a practical, generalizable solution that bridges the gap from patch-level foundation model features to clinically meaningful slide-level inferences, advancing AI integration in computational pathology. <div>
arXiv:2511.14907v1 Announce Type: new 
Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-WIN: Building Chest Radiograph World Model via Predictive Sensing</title>
<link>https://arxiv.org/abs/2511.14918</link>
<guid>https://arxiv.org/abs/2511.14918</guid>
<content:encoded><![CDATA[
<div> Keywords: Chest X-ray, 3D anatomy, world model, contrastive alignment, domain adaptation<br /><br />Summary:<br /><br />The article introduces X-WIN, a novel world model designed to enhance Chest X-ray (CXR) analysis by integrating 3D anatomical knowledge derived from chest computed tomography (CT) scans. Recognizing the limitation of CXRs as 2D projection images that obscure 3D structures due to superposition, X-WIN learns to predict 2D projections in latent space, effectively internalizing 3D anatomical information. Critical to its training is the affinity-guided contrastive alignment loss, which leverages mutual similarities between different projections of the same volume to capture rich, correlated information. To ensure better generalization and adaptability, X-WIN incorporates real CXR images through masked image modeling and utilizes a domain classifier to align the statistical properties of real and simulated CXRs. Extensive experimental evaluations demonstrate that X-WIN surpasses existing foundation models in various downstream tasks, both in linear probing and few-shot fine-tuning settings. Additionally, the model exhibits the capability to generate 2D projections that can be employed for reconstructing 3D CT volumes, pointing to its potential in bridging the gap between 2D and 3D medical imaging modalities. Overall, X-WIN represents a significant step toward improved disease diagnosis and representation learning from CXRs by embedding volumetric knowledge within a learned world model. <div>
arXiv:2511.14918v1 Announce Type: new 
Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CPSL: Representing Volumetric Video via Content-Promoted Scene Layers</title>
<link>https://arxiv.org/abs/2511.14927</link>
<guid>https://arxiv.org/abs/2511.14927</guid>
<content:encoded><![CDATA[
<div> Volumetric video, Scene layers, Novel-view synthesis, Parallax, Real-time playback<br /><br />Summary: This paper introduces Content-Promoted Scene Layers (CPSL), a novel 2.5D video representation designed to deliver immersive volumetric video experiences with greater efficiency. Unlike traditional volumetric methods which rely on expensive 3D capture and rendering techniques, CPSL leverages per-frame depth and content saliency to decompose each frame into a small set of geometry-consistent 2D layers. These layers are equipped with soft alpha bands and an edge-depth cache, ensuring occlusion ordering and boundary continuity are preserved. By encoding the scene into lightweight 2D assets, CPSL enables parallax-corrected novel-view synthesis through depth-weighted warping and alpha compositing, circumventing costly 3D reconstruction. The approach also ensures temporal coherence via motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Evaluations across multiple benchmarks demonstrate that CPSL achieves superior perceptual quality and boundary fidelity compared to existing layer-based and neural-field baselines. Furthermore, this method significantly reduces storage and rendering costs by several folds. Overall, CPSL presents a practical and scalable solution to bridge conventional 2D video with immersive 2.5D media experiences, enhancing accessibility for on-demand and real-time interactive applications. <div>
arXiv:2511.14927v1 Announce Type: new 
Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities</title>
<link>https://arxiv.org/abs/2511.14945</link>
<guid>https://arxiv.org/abs/2511.14945</guid>
<content:encoded><![CDATA[
<div> Periodic workflows, multimodal sequences, unsupervised detection, anomaly detection, benchmark  

<br /><br />Summary: This paper addresses the challenge of modeling long-term periodic human activities, which are common in areas such as manufacturing, sports, and daily life but have low-contrast patterns and complex workflows compared to short-term periodic activities. To tackle this, the authors introduce the first comprehensive benchmark dataset consisting of 580 multimodal human activity sequences that capture long-term periodic workflows. The benchmark is designed to support three practical evaluation tasks: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. Additionally, the authors propose a lightweight, training-free baseline method capable of modeling diverse periodic workflow patterns without requiring annotated data. Experimental results demonstrate that the benchmark poses significant difficulties for existing unsupervised methods and zero-shot techniques leveraging large language models, highlighting the need for improved approaches in this domain. Notably, the proposed baseline outperforms competing methods by a large margin across all three evaluation tasks. Furthermore, in real-world deployment scenarios, the baseline exhibits advantages comparable to traditional supervised workflow detection systems, offering benefits such as eliminating the necessity for annotations and retraining. The project related to this work is publicly accessible at https://sites.google.com/view/periodicworkflow. <div>
arXiv:2511.14945v1 Announce Type: new 
Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems</title>
<link>https://arxiv.org/abs/2511.14948</link>
<guid>https://arxiv.org/abs/2511.14948</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view video synchronization, LED Clock, spatiotemporal alignment, RGB and IR cameras, pose estimation<br /><br />Summary:<br /><br />1. This paper addresses the challenge of accurately synchronizing multiple heterogeneous cameras, including professional and consumer devices, and both visible (RGB) and infrared (IR) sensors, particularly in uncontrolled, real-world settings.  
2. The authors introduce a novel, low-cost synchronization method based on a custom-built LED Clock that uses red and infrared LEDs to visually encode timing information within recorded video frames.  
3. Their method decodes the exposure window start and end times from the video data to achieve millisecond-level temporal alignment without requiring hardware synchronization capabilities.  
4. Benchmarking results show a low residual synchronization error of 1.34 ms RMSE, outperforming existing techniques based on light signals, audio, or timecodes.  
5. The synchronization improvements enhance downstream multi-view computer vision tasks such as pose estimation and 3D reconstruction.  
6. The system is validated in large-scale, complex scenarios involving over 25 heterogeneous cameras in both IR and RGB modalities, including surgical recording environments.  
7. This approach simplifies synchronization workflows and broadens access to advanced vision sensing in diverse, unconstrained industrial and clinical applications. <div>
arXiv:2511.14948v1 Announce Type: new 
Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial intelligence approaches for energy-efficient laser cutting machines</title>
<link>https://arxiv.org/abs/2511.14952</link>
<guid>https://arxiv.org/abs/2511.14952</guid>
<content:encoded><![CDATA[
<div> Keywords: laser cutting, energy reduction, deep learning, material classification, smoke level detection<br /><br />Summary:<br /><br />This research tackles the challenges of high energy consumption and environmental impact in laser cutting processes by proposing novel deep learning (DL) methods aimed at reducing energy use. The study identifies a common issue in CO2 laser suction pumps — their open-loop control systems — and introduces a closed-loop system that dynamically adjusts pump power according to the material being cut and the smoke generated. To enable this adaptivity, two material classification approaches are developed: one using lens-less speckle sensing combined with a customized Convolutional Neural Network (CNN), and another employing a USB camera with transfer learning based on the pre-trained VGG16 CNN model. In addition, a separate deep learning model is designed to detect smoke levels in real time, further optimizing the pump’s power output. This integrated system allows the exhaust suction pump to automatically switch off during downtime and adjust power dynamically during operation. Experimental results demonstrate significant energy savings, ranging from 20% to 50% reduction in the pump’s energy consumption. The approach offers a substantial contribution towards sustainable manufacturing practices by enhancing energy efficiency and reducing environmental impact in laser cutting operations. <div>
arXiv:2511.14952v1 Announce Type: new 
Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects</title>
<link>https://arxiv.org/abs/2511.14970</link>
<guid>https://arxiv.org/abs/2511.14970</guid>
<content:encoded><![CDATA[
<div> Transparent objects, depth estimation, semantic segmentation, edge-guided fusion, multi-modal training<br /><br />Summary:<br /><br />This paper addresses the challenge of perceiving transparent objects in computer vision, which is difficult due to transparency affecting both depth estimation and semantic segmentation accuracy. The authors propose Edge-Guided Spatial Attention (EGSA), a novel fusion mechanism that leverages boundary information to better integrate semantic and geometric features, reducing negative interactions between these tasks. EGSA demonstrates consistent improvements over the current state-of-the-art method, MODEST, particularly in transparent regions, while maintaining competitive segmentation performance on Syn-TODD and ClearPose benchmarks. Additionally, the paper presents a multi-modal progressive training strategy that starts by learning from edges derived from RGB images and transitions to edges from predicted depth images. This approach enables the model to utilize the rich texture information in RGB images initially and later focus on more relevant geometric features in depth maps, importantly eliminating the need for ground-truth depth data during training. These contributions emphasize the effectiveness of edge-guided fusion and multi-stage training in enhancing the perception of transparent objects, offering a robust framework that improves depth accuracy without sacrificing segmentation quality. <div>
arXiv:2511.14970v1 Announce Type: new 
Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.14981</link>
<guid>https://arxiv.org/abs/2511.14981</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, feature-based losses, teacher-student models, latent representations, image classification<br /><br />Summary:<br /><br />This paper proposes a novel knowledge distillation (KD) framework that focuses exclusively on feature-based losses to train the student model’s backbone, completely omitting traditional logit-based losses like cross entropy. Unlike existing methods that leverage both logits and intermediate layer features, this approach solely utilizes latent representations during distillation. The authors introduce a knowledge quality metric grounded in recent insights into the geometry of latent representations, which helps identify the most effective teacher layers for transferring knowledge. Extensive experiments were conducted on three diverse image classification datasets using four different student-teacher combinations, including convolutional neural networks and vision transformers. These experiments demonstrate that the proposed method achieves state-of-the-art results, significantly outperforming standard KD approaches with top-1 accuracy improvements of up to 15%. The approach not only simplifies the distillation process by eliminating the need for logit-based losses but also highlights the importance of selective feature guidance from the teacher model. To promote further research, the authors have made their code publicly available at the provided GitHub repository. This work offers a new perspective on optimizing knowledge transfer in model compression and efficiency enhancement. <div>
arXiv:2511.14981v1 Announce Type: new 
Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</title>
<link>https://arxiv.org/abs/2511.14993</link>
<guid>https://arxiv.org/abs/2511.14993</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.14993v1 Keywords: Kandinsky 5.0, image synthesis, video synthesis, foundation models, training pipeline<br /><br />Summary:<br /><br />This report introduces Kandinsky 5.0, a cutting-edge family of foundation models designed for high-resolution image generation and up to 10-second video synthesis. The framework consists of three main model lines: Kandinsky 5.0 Image Lite, featuring 6-billion parameter image generation models; Kandinsky 5.0 Video Lite, lightweight 2-billion parameter models for rapid text-to-video and image-to-video synthesis; and Kandinsky 5.0 Video Pro, a 19-billion parameter model line achieving superior video generation quality. The authors provide an in-depth overview of the data curation lifecycle including collection, processing, filtering, and clustering that supports the multi-stage training pipeline. This extensive pipeline involves comprehensive pre-training complemented by quality-enhancing techniques like self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. Additionally, the report details novel architectural, training, and inference optimizations enabling Kandinsky 5.0 models to generate outputs at high speed while maintaining state-of-the-art performance, as validated through human evaluation. As an open-source, large-scale generative framework, Kandinsky 5.0 is designed for adaptability across various generative tasks. The release of both training checkpoints and code aims to foster accessibility and accelerate progress in high-quality generative model research. <div>
arXiv:2511.14993v1 Announce Type: new 
Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation</title>
<link>https://arxiv.org/abs/2511.14998</link>
<guid>https://arxiv.org/abs/2511.14998</guid>
<content:encoded><![CDATA[
<div> Financial documents, OCR evaluation, fact-level benchmark, financial facts, LLM-as-Judge<br /><br />Summary:<br /><br />1. The paper introduces FinCriticalED, a visual benchmark designed specifically for evaluating OCR and vision-language models on financial documents at the fact level, addressing challenges in visually dense and table-heavy layouts common in these documents.<br />2. Traditional OCR metrics such as ROUGE and edit distance focus only on surface-level text similarity, which can miss critical errors like sign inversions or shifted dates that materially alter financial interpretations; FinCriticalED shifts evaluation towards domain-critical factual correctness.<br />3. The dataset contains 500 image-HTML pairs annotated by financial experts, covering over 700 numerical and temporal facts, ensuring high-quality annotations verified for accuracy in signs, magnitudes, and time expressions.<br />4. The authors develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for complex financial documents, enabling more precise and domain-aware assessment.<br />5. Benchmarking results show that although the strongest proprietary models achieve the highest factual accuracy, significant errors remain in complex numerical and temporal contexts, indicating room for improvement; through rigorous quantitative evaluation and expert case studies, FinCriticalED provides a solid foundation for advancing visual factual precision in finance and other precision-critical fields. <div>
arXiv:2511.14998v1 Announce Type: new 
Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification</title>
<link>https://arxiv.org/abs/2511.15016</link>
<guid>https://arxiv.org/abs/2511.15016</guid>
<content:encoded><![CDATA[
<div> Keywords: Lifelong person Re-Identification, Visible-Infrared modalities, Knowledge disentanglement, Cross-modal alignment, Catastrophic forgetting  

<br /><br />Summary:  
This paper addresses Lifelong person Re-Identification (LReID), particularly the Visible-Infrared Lifelong person Re-IDentification (VI-LReID) task, which involves continuously training models on visible and infrared data collected sequentially to achieve consistent matching across day and night. Existing approaches rely on cross-modal knowledge distillation to prevent catastrophic forgetting but neglect the conflict between modality-specific knowledge acquisition and modality-common knowledge preservation, causing collaborative forgetting. To overcome these challenges, the authors propose CKDA (Cross-modality Knowledge Disentanglement and Alignment), a novel framework that explicitly separates and preserves modality-specific and common knowledge in a balanced fashion. CKDA introduces two prompting modules: Modality-Common Prompting (MCP) and Modality-Specific Prompting (MSP), designed to disentangle and purify discriminative features unique or shared across modalities, thereby reducing mutual interference. Additionally, the Cross-modal Knowledge Alignment (CKA) module aligns the newly learned knowledge with previously acquired knowledge using dual-modality prototypes, independently considering both inter- and intra-modality feature spaces to ensure balanced retention. Extensive experiments on four benchmark datasets demonstrate CKDA’s effectiveness and superiority over state-of-the-art methods in lifelong VI-ReID scenarios. The source code is publicly available, promoting reproducibility and further research. <div>
arXiv:2511.15016v1 Announce Type: new 
Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex-Valued 2D Gaussian Representation for Computer-Generated Holography</title>
<link>https://arxiv.org/abs/2511.15022</link>
<guid>https://arxiv.org/abs/2511.15022</guid>
<content:encoded><![CDATA[
<div> Keywords: hologram representation, Gaussian primitives, differentiable rasterizer, VRAM optimization, phase-only holograms<br /><br />Summary:<br /><br />This paper introduces a novel hologram representation that utilizes structured complex-valued 2D Gaussian primitives instead of traditional per-pixel storage, significantly reducing the parameter search space by up to a factor of 10. To facilitate end-to-end learning, the authors develop a differentiable rasterizer compatible with their representation, which is combined with a GPU-optimized kernel designed for simulating free-space light propagation. Experimental results demonstrate that this approach reduces VRAM consumption by up to 2.5 times and speeds up the optimization process by 50%, all while delivering higher-quality holographic reconstructions compared to existing methods. Furthermore, the authors propose a conversion technique that adapts their representation to various practical hologram formats, including both smooth and random phase-only holograms. This conversion effectively suppresses noise artifacts commonly found in previous approaches. Overall, by minimizing the parameter search space, the proposed representation offers a more scalable solution for hologram estimation, paving the way for advancements in next-generation computer-generated holography systems. <div>
arXiv:2511.15022v1 Announce Type: new 
Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans</title>
<link>https://arxiv.org/abs/2511.15029</link>
<guid>https://arxiv.org/abs/2511.15029</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, developmental alignment, mathematical cognition, ResNet-50, numerical representation<br /><br />Summary:<br />1. The study explores how computer vision (CV) models, specifically ResNet-50, exhibit developmental alignment with human mathematical cognition, meaning their learning progression mirrors that of children.<br />2. Previous research had found that CV models trained for image classification develop latent geometric and numerical representations similar to adults, and this study extends that work by focusing on developmental trajectories.<br />3. For geometry and topology concepts, developmental alignment was observed in certain areas such as Euclidean Geometry, Geometrical Figures, Metric Properties, and Topology but not in others including Chiral Figures, Geometric Transformations, and Symmetrical Figures.<br />4. Regarding numerical cognition, the model demonstrated developmental alignment by forming a human-like "mental number line," a key aspect of numerical understanding, as training experience increased.<br />5. The findings highlight the potential of CV models as tools to understand mathematical development in humans and suggest future research avenues including different model architectures and expanding benchmark datasets. <div>
arXiv:2511.15029v1 Announce Type: new 
Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space</title>
<link>https://arxiv.org/abs/2511.15046</link>
<guid>https://arxiv.org/abs/2511.15046</guid>
<content:encoded><![CDATA[
<div> human-object interaction, HOI detection, generation, unified token space, semi-supervised learning  

<br /><br />Summary:  
The paper addresses human-object interaction (HOI) by unifying detection and generation tasks, which have been conventionally treated separately. This separation has limited the comprehensive understanding of interactions. To overcome this, the authors propose UniHOI, a framework that models HOI detection and generation together using a unified token space, facilitating effective knowledge sharing and better generalization across tasks. UniHOI introduces a symmetric interaction-aware attention module that enhances the joint modeling process. Additionally, a unified semi-supervised learning paradigm is designed to support bidirectional mapping between images and interaction semantics, even when annotation data is limited. Extensive experiments validate that UniHOI achieves state-of-the-art performance in both HOI detection and generation domains. It notably improves accuracy by 4.9% on long-tailed HOI detection benchmarks, demonstrating better handling of rare interaction categories. Furthermore, UniHOI substantially boosts interaction metrics by 42.0% in open-vocabulary generation tasks, indicating superior generalization to unseen or novel interactions. Overall, the approach advances integrated HOI understanding by effectively bridging visual and semantic interaction representations under limited supervision. <div>
arXiv:2511.15046v1 Announce Type: new 
Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method</title>
<link>https://arxiv.org/abs/2511.15052</link>
<guid>https://arxiv.org/abs/2511.15052</guid>
<content:encoded><![CDATA[
<div> Hyperspectral imaging, multispectral imaging, image fusion, spectral variability, low-rank decomposition<br /><br />Summary:<br /><br />This paper addresses the problem of fusing hyperspectral images (HSI) with multispectral images (MSI) to enhance the spatial resolution of HSI, focusing on challenges caused by inter-image variability—spectral variability and spatially localized changes arising from differing acquisition conditions. Unlike existing approaches that directly transform the images and potentially worsen the fusion problem's ill-posedness, the authors propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. This model first interprets spectral variability as changes in the spectral degradation operator. To recover spatial details lost due to localized changes, the target HSI is decomposed into low-rank and residual components, with the residual capturing fine details. Dimensionality reduction is performed on both components to exploit spectral correlations effectively. An implicit regularizer is introduced to incorporate spatial prior information, enhancing reconstruction quality. The optimization problem is solved via a Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where an external denoiser addresses the implicit regularizer subproblem. The authors provide a thorough convergence analysis and demonstrate through extensive experiments that DLRRF outperforms existing methods in handling inter-image variability during HSI and MSI fusion. <div>
arXiv:2511.15052v1 Announce Type: new 
Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues</title>
<link>https://arxiv.org/abs/2511.15054</link>
<guid>https://arxiv.org/abs/2511.15054</guid>
<content:encoded><![CDATA[
<div> Keywords: nuclei segmentation, knowledge distillation, student-teacher architecture, cross-tissue, semi-supervised learning<br /><br />Summary:  
Accurate nuclei segmentation in whole slide images (WSIs) of microscopy remains challenging due to variability in staining, imaging conditions, and tissue morphology across samples. To address these issues, the paper proposes CellGenNet, a knowledge distillation framework designed for robust cross-tissue cell segmentation under limited supervision. CellGenNet employs a student-teacher architecture where a high-capacity teacher network is first trained on sparse annotations and subsequently produces soft pseudo-labels for unlabeled image regions. The student network is then optimized using a joint objective combining ground-truth labels, teacher-generated probabilistic targets, and a hybrid loss function that integrates binary cross-entropy with Tversky loss. This hybrid loss allows asymmetric penalties to better tackle class imbalance and preserve minority nuclear structures effectively. Additionally, the framework incorporates consistency regularization and layerwise dropout to stabilize feature representations and encourage dependable feature transfer between teacher and student models. Extensive experiments conducted on diverse cancer tissue WSIs demonstrate that CellGenNet surpasses both fully supervised and semi-supervised baseline methods in segmentation accuracy and generalization. Overall, the approach facilitates scalable and reproducible histopathology analysis by efficiently leveraging limited labeled data alongside unlabeled regions through knowledge distillation. <div>
arXiv:2511.15054v1 Announce Type: new 
Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2511.15057</link>
<guid>https://arxiv.org/abs/2511.15057</guid>
<content:encoded><![CDATA[
<div> Keywords: universal ultrasound segmentation, semi-supervised learning, prompt-guided decoding, uncertainty-driven calibration, multi-organ dataset  

<br /><br />Summary:  
1. The paper addresses the limitation of existing ultrasound image segmentation methods, which are typically specialized for particular anatomical structures or tasks, hindering their widespread clinical application.  
2. It introduces the task of universal semi-supervised ultrasound image segmentation, aiming to develop a framework capable of segmenting multiple organs and handling diverse segmentation tasks simultaneously.  
3. The proposed method, ProPL, integrates a shared vision encoder with prompt-guided dual decoders that utilize a prompting-upon-decoding mechanism to flexibly adapt to different segmentation tasks.  
4. ProPL incorporates an uncertainty-driven pseudo-label calibration (UPLC) module to enhance self-training performance by refining pseudo-labels generated from unlabeled data.  
5. A novel, comprehensive ultrasound dataset covering 5 organs and 8 segmentation tasks is introduced to support research and benchmarking in universal ultrasound segmentation.  
6. Extensive experiments validate that ProPL consistently outperforms state-of-the-art methods across multiple metrics and tasks, setting a new benchmark in the domain of universal ultrasound image segmentation. <div>
arXiv:2511.15057v1 Announce Type: new 
Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Multimodal Large Language Models on Vertically Written Japanese Text</title>
<link>https://arxiv.org/abs/2511.15059</link>
<guid>https://arxiv.org/abs/2511.15059</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, Japanese OCR, Vertical writing, Document understanding, Synthetic dataset<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) have recently advanced and are increasingly applied to tasks involving visual document understanding across multiple languages, including Japanese. <br /><br />2. A key challenge in processing Japanese documents is the vertical writing style, common in many Japanese documents, which requires specialized model support. <br /><br />3. Existing research on handling vertically written Japanese text by MLLMs is limited, motivating the need for targeted evaluation and improvement. <br /><br />4. The study generates a synthetic OCR dataset containing both horizontally and vertically written Japanese text by rendering text into images, which is utilized for fine-tuning and evaluating MLLMs. Additionally, a real-world evaluation dataset with vertically written Japanese documents is created. <br /><br />5. Evaluation results show that current MLLMs underperform on vertically written text compared to horizontal text. However, fine-tuning models with the synthesized Japanese OCR dataset significantly improves their ability to read vertical writing, addressing a previously unmet capability. The datasets and code have been made publicly available to support ongoing research. <div>
arXiv:2511.15059v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks</title>
<link>https://arxiv.org/abs/2511.15065</link>
<guid>https://arxiv.org/abs/2511.15065</guid>
<content:encoded><![CDATA[
<div> Video Models, Spatial Reasoning, VR-Bench, Maze-Solving, Test-Time Scaling<br /><br />Summary:<br /><br />This work introduces VR-Bench, a comprehensive benchmark aimed at evaluating video models' reasoning capabilities specifically in spatial reasoning tasks. The benchmark consists of 7,920 procedurally generated videos across five distinct maze types with various visual styles, designed to require spatial planning and multi-step reasoning. Unlike discrete text corpora, videos provide continuous spatial and temporal information that is ideal for spatial reasoning, motivating the exploration of reasoning through video generation. The authors demonstrate that supervised fine-tuning (SFT) effectively enhances the reasoning abilities of video models. Experiments show that video models outperform leading vision-language models (VLMs) in spatial perception and generalize well across a range of scenarios, tasks, and complexity levels. Additionally, a notable test-time scaling effect is discovered, where applying diverse sampling during inference boosts the reliability of reasoning performance by 10–20%. Overall, the results highlight the unique potential and scalability of the reasoning-via-video paradigm, suggesting it as a promising direction for advancing spatial reasoning tasks using video-based models. <div>
arXiv:2511.15065v1 Announce Type: new 
Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching</title>
<link>https://arxiv.org/abs/2511.15066</link>
<guid>https://arxiv.org/abs/2511.15066</guid>
<content:encoded><![CDATA[
<div> Keywords: Bokeh rendering, depth-free, flow matching, cross-attention, controllable synthesis<br /><br />Summary:<br /><br />This paper introduces BokehFlow, a novel framework for rendering controllable bokeh effects in images without relying on depth information. Traditional and neural methods for bokeh rendering usually depend on accurate depth maps, while generative models often face limitations in controllability and computational efficiency. BokehFlow addresses these challenges by leveraging flow matching techniques to generate photorealistic shallow depth-of-field effects directly from all-in-focus images. A key innovation is the use of a cross-attention mechanism that allows users to exert semantic control over focus regions and blur intensity through text prompts, enhancing user interaction and customization. To facilitate model training and objective evaluation, the authors collect and synthesize four diverse datasets specifically tailored for bokeh effect learning. Extensive experiments reveal that BokehFlow not only produces visually compelling and realistic bokeh but also delivers more precise control compared to existing depth-dependent and generative approaches. Furthermore, the method surpasses others in efficiency, making it practical for real-world applications. Overall, BokehFlow represents a significant advance by removing the dependency on depth inputs while maintaining high-quality, user-controllable bokeh rendering. <div>
arXiv:2511.15066v1 Announce Type: new 
Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation</title>
<link>https://arxiv.org/abs/2511.15077</link>
<guid>https://arxiv.org/abs/2511.15077</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D single object tracking, LiDAR point clouds, high temporal variation, MambaTrack3D, computational efficiency

<br /><br />Summary: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds due to temporal redundancy and complex computation. Existing memory-based trackers commonly suffer from quadratic computational complexity and insufficient use of geometric priors, limiting their efficiency and performance. To address these issues, the paper proposes MambaTrack3D, a novel HTV-oriented tracking framework based on the Mamba state space model. A key innovation is the Mamba-based Inter-frame Propagation (MIP) module, which replaces traditional single-frame feature extraction with an inter-frame propagation strategy, achieving near-linear computational complexity while explicitly modeling spatial relationships across historical frames. Additionally, the Grouped Feature Enhancement Module (GFEM) separates foreground and background semantics at the channel level, reducing temporal redundancy in the memory bank to boost tracking accuracy and efficiency. Extensive experiments conducted on challenging HTV benchmarks such as KITTI-HTV and nuScenes-HTV show that MambaTrack3D consistently outperforms both HTV-specific and conventional trackers, achieving up to 6.5% improvement in success and 9.5% in precision compared to HVTrack under moderate temporal gaps. Furthermore, the method remains competitive on the standard KITTI dataset, demonstrating strong generalization ability across diverse tracking scenarios. Overall, MambaTrack3D delivers a superior accuracy-efficiency trade-off with robust performance in both specialized HTV and conventional environments. <div>
arXiv:2511.15077v1 Announce Type: new 
Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2511.15085</link>
<guid>https://arxiv.org/abs/2511.15085</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Emotion Recognition, inter-modal emotion conflicts, TiCAL, hyperbolic space, consistency estimation<br /><br />Summary:<br /><br />1. This article addresses the challenge of inter-modal emotion conflicts in Multimodal Emotion Recognition (MER), where different modalities (visual, auditory, textual) within the same sample may exhibit conflicting emotional cues.  
2. The authors propose a novel framework called Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise process of human emotion perception.  
3. TiCAL dynamically evaluates the consistency of training samples by using pseudo unimodal emotion labels combined with a typicality estimation, allowing the model to handle inconsistent emotional signals across modalities better.  
4. To enhance emotion representation, features are embedded into a hyperbolic space, which helps capture subtle and fine-grained distinctions between emotion categories more effectively than traditional embedding spaces.  
5. Experimental results on benchmark datasets like CMU-MOSEI and MER2023 demonstrate that TiCAL significantly improves recognition accuracy, particularly for samples with high modality inconsistency, achieving around a 2.6% performance boost over the previous state-of-the-art method DMD. <div>
arXiv:2511.15085v1 Announce Type: new 
Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis</title>
<link>https://arxiv.org/abs/2511.15092</link>
<guid>https://arxiv.org/abs/2511.15092</guid>
<content:encoded><![CDATA[
<div> Keywords: pose-guided human image generation, diffusion model, multi-view priors, appearance prior module, joint conditional injection<br /><br />Summary:<br /><br />1. Pose-guided human image generation faces significant challenges due to incomplete textures derived from single reference views and a lack of explicit cross-view interaction, leading to inconsistencies in generated images.  
2. The paper introduces the Jointly Conditioned Diffusion Model (JCDM), a novel diffusion-based framework designed to leverage multi-view priors for improved image synthesis.  
3. The Appearance Prior Module (APM) within JCDM creates a holistic identity-preserving prior by intelligently inferring missing texture and appearance details from incomplete input references, addressing the main limitation of single view inputs.  
4. The Joint Conditional Injection (JCI) mechanism enables effective fusion of cues from multiple views and injects a shared conditioning signal into the backbone denoising process, which helps align important visual attributes such as identity, color, and texture across different poses.  
5. JCDM is flexible, supporting variable numbers of reference views, and integrates seamlessly with standard diffusion backbones by introducing minimal and targeted architectural modifications. Experimental results demonstrate that JCDM achieves state-of-the-art fidelity and cross-view consistency, significantly advancing the quality and coherence of pose-guided human image generation. <div>
arXiv:2511.15092v1 Announce Type: new 
Abstract: Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.15098</link>
<guid>https://arxiv.org/abs/2511.15098</guid>
<content:encoded><![CDATA[
<div> Discrete diffusion-based multimodal large language models, visual token redundancy, pruning strategies, inference efficiency, multimodal understanding tasks  

<br /><br />Summary:  
This paper investigates efficiency challenges in discrete diffusion-based multimodal large language models (dMLLMs), which provide advantages like parallel decoding and bidirectional context modeling compared to autoregressive MLLMs but suffer from high computational costs due to full-sequence attention at each denoising step. The authors focus on visual token redundancy, finding it occurs predominantly in from-scratch dMLLMs tackling long-answer tasks. They analyze how pruning visual tokens to reduce redundancy impacts model performance and efficiency, showing that non-negligible information loss results from pruning, but only from-scratch dMLLMs can recover this information progressively during later denoising steps. The study further compares acceleration methods, revealing that layer-skipping benefits AR-to-diffusion dMLLMs, while progressive or late-step pruning is a more effective approach for from-scratch dMLLMs. Overall, the work sheds light on modality-specific redundancies and pruning effects, providing novel insights for optimizing inference speed without severely compromising accuracy, thus advancing the practical deployment of dMLLMs in various multimodal understanding applications. <div>
arXiv:2511.15098v1 Announce Type: new 
Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.15102</link>
<guid>https://arxiv.org/abs/2511.15102</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, Gaussian Blending, alpha blending, rendering artifacts

<br /><br />Summary: The paper addresses limitations in 3D Gaussian Splatting (3DGS), a recent technique that greatly improved novel view synthesis. Existing 3DGS methods produce noticeable visual artifacts when rendering views at sampling rates different from those used in training, specifically erosion-induced blurring when zooming in and dilation-induced staircase artifacts when zooming out. The authors identify the root cause as the conventional alpha blending approach that treats alpha and transmittance as scalar values over pixels. To overcome this, they introduce Gaussian Blending, a novel method that models alpha and transmittance as spatially varying distributions instead of scalars. This approach allows transmittance to be updated by considering the spatial distribution of alpha values within a pixel area, enabling contributions from nearby background splats to better preserve image details. Gaussian Blending integrates seamlessly with existing 3DGS and other novel view synthesis frameworks, requires no additional memory, and preserves real-time rendering speeds. Extensive experiments demonstrate that this method effectively captures fine details at both seen and unseen sampling rates, consistently outperforming state-of-the-art novel view synthesis models. This advancement addresses previous visual discrepancies and improves generalization across varying sampling resolutions. <div>
arXiv:2511.15102v1 Announce Type: new 
Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring</title>
<link>https://arxiv.org/abs/2511.15117</link>
<guid>https://arxiv.org/abs/2511.15117</guid>
<content:encoded><![CDATA[
<div> Keywords: elder care, event-triggered system, GMM background modeling, SVM machine learning, social media communication

<br /><br />Summary:  
This study focuses on monitoring both the physical and mental states of elderly individuals through an event-triggered system designed to detect specific events such as watch dog alerts, danger notices, and photo link communications. The system uses Gaussian Mixture Model (GMM) background modeling to identify motion behaviors of visitors and elders during watch dog and danger notice events. Experiments were conducted in home scenarios involving five families to detect and record these three event types based on real-life activities. Captured images from these events were further analyzed using Support Vector Machine (SVM) machine learning techniques to enhance event recognition. To address the elders' lack of technical experience, the system incorporates an intuitive operation method aligned with normal daily activities. This design facilitates communication between elders and their relatives through social media platforms, providing a seamless and natural interaction process. The approach aims to improve elder safety and emotional connections by combining automated event detection with user-friendly communication tools adapted to the needs of elderly users. <div>
arXiv:2511.15117v1 Announce Type: new 
Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation</title>
<link>https://arxiv.org/abs/2511.15118</link>
<guid>https://arxiv.org/abs/2511.15118</guid>
<content:encoded><![CDATA[
<div> Few-shot segmentation, Segment Anything Model, Unbiased Semantic Decoding, Contrastive Language-Image Pre-training, Visual-text prompt generator

<br /><br />Summary:  
This work addresses the challenges in few-shot segmentation by integrating the Segment Anything Model (SAM) with an Unbiased Semantic Decoding (USD) strategy. 1) Although SAM exhibits strong generalization and object-specific extraction capabilities, its decoding process depends heavily on accurate prompts, which previously focused mainly on the support set, leading to biased decoding on unknown classes. 2) The proposed USD strategy overcomes this limitation by extracting target information simultaneously from both the support and query images, facilitating more consistent predictions using semantic guidance from the Contrastive Language-Image Pre-training (CLIP) model. 3) Two feature enhancement strategies are designed to improve SAM’s semantic discrimination: a global supplement at the image level provides generalized category information from the support image, while a local guidance at the pixel level offers precise target location information from the query image. 4) To generate target-focused prompt embeddings, a learnable visual-text target prompt generator is introduced, which interacts between text embeddings and CLIP visual features. 5) Importantly, this framework enhances target attention without requiring re-training of the foundational vision models, exploiting semantic features to guide prompts enriched with target information, thus improving generalization and accuracy in few-shot segmentation tasks. <div>
arXiv:2511.15118v1 Announce Type: new 
Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images</title>
<link>https://arxiv.org/abs/2511.15132</link>
<guid>https://arxiv.org/abs/2511.15132</guid>
<content:encoded><![CDATA[
<div> Active learning, medical imaging, acquisition strategies, performance adaptation, annotation efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of reducing annotation costs in medical imaging by improving active learning methods. Traditional single acquisition strategies often show inconsistent performance throughout the active learning cycle. To overcome this, the authors introduce WaveFuse-AL, a novel framework that adaptively fuses multiple well-known acquisition strategies: BALD, BADGE, Entropy, and CoreSet. WaveFuse-AL employs cyclical (sinusoidal) temporal priors combined with performance-driven adaptation, allowing the dynamic adjustment of the importance of each strategy over time. This adaptive fusion approach balances exploration and exploitation to select the most informative samples more effectively. The framework is validated on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results reveal that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines. The improvements are statistically significant in ten out of twelve metric evaluations, demonstrating greater annotation efficiency under limited budget conditions. Overall, WaveFuse-AL offers a more robust and effective active learning framework adaptable to various stages of learning and diverse medical imaging tasks. <div>
arXiv:2511.15132v1 Announce Type: new 
Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging</title>
<link>https://arxiv.org/abs/2511.15151</link>
<guid>https://arxiv.org/abs/2511.15151</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Curriculum Learning, Spatiotemporal Encoding, Approximate Rank Pooling, Neuroimaging, Brain Disease Classification<br /><br />Summary: The paper introduces Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an innovative end-to-end framework designed to enhance high-dimensional neuroimaging analyses for clinical diagnosis. The central component of this approach is data-driven spatiotemporal encoding (DaSE), which uses Approximate Rank Pooling (ARP) to transform complex three-dimensional brain volumetric data into compact and informative two-dimensional dynamic representations. This dimensionality reduction facilitates efficient processing while preserving critical spatial and temporal information. DCL-SE incorporates a dynamic curriculum learning strategy governed by a Dynamic Group Mechanism (DGM) that progressively trains the model's decoder. This curriculum guides the model from capturing broad, global anatomical structures to identifying detailed and fine pathological features, improving feature extraction quality. The framework is extensively evaluated on six publicly available neuroimaging datasets, spanning tasks such as Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction. Across these diverse applications, DCL-SE consistently outperforms existing state-of-the-art methods in terms of accuracy, robustness, and interpretability of the results. The study highlights the value of tailored, compact architectures that are optimized for specific tasks, providing an effective alternative to large-scale general-purpose pretrained networks in neuroimaging contexts. <div>
arXiv:2511.15151v1 Announce Type: new 
Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection</title>
<link>https://arxiv.org/abs/2511.15153</link>
<guid>https://arxiv.org/abs/2511.15153</guid>
<content:encoded><![CDATA[
<div> Keywords: HD maps, 3D point cloud updating, urban change detection, SceneEdited dataset, autonomous navigation<br /><br />Summary:  
1. The paper addresses the issue of maintaining accurate, up-to-date High-Definition (HD) maps, which are essential for urban planning, infrastructure monitoring, and autonomous navigation.  
2. It highlights the challenge that current change detection methods face when it comes to updating 3D maps, especially those relying on 2D image-based change detection, creating a gap between detection and map updating.  
3. To bridge this gap, the authors introduce SceneEdited, the first large-scale dataset specifically designed for city-wide HD map maintenance through 3D point cloud updates.  
4. SceneEdited encompasses over 800 scenes covering 73 km of driving and about 3 km² of urban area, featuring more than 23,000 synthesized object changes including missing roadside infrastructure, buildings, overpasses, and utility poles, generated both manually and automatically with over 2000 outdated scene versions.  
5. Each scene provides calibrated RGB images, LiDAR scans, and detailed change masks, making it suitable for training and evaluating change detection and map updating methods.  
6. The authors also provide baseline methods based on image-based structure-from-motion pipelines and a comprehensive toolkit designed to support scalability, trackability, and portability for expanding the dataset and unifying annotations.  
7. Both the dataset and toolkit are publicly released on GitHub, establishing SceneEdited as a standardized benchmark for 3D HD map updating research. <div>
arXiv:2511.15153v1 Announce Type: new 
Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2511.15159</link>
<guid>https://arxiv.org/abs/2511.15159</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical training, feedback generation, Instrument-Action-Target triplets, video recognition, GPT-4o<br /><br />Summary:<br /><br />1. The paper addresses the critical need for high-quality intraoperative feedback in surgical training to improve trainee skills and long-term acquisition. 2. It proposes a structure-aware pipeline that learns a surgical action ontology derived from real trainer-to-trainee feedback transcripts covering 33 surgeries, used to inform feedback generation. 3. The authors contribute by mining Instrument-Action-Target (IAT) triplets from feedback text and normalizing these into clustered categories, fine-tuning a video-to-IAT recognition model that incorporates surgical context and temporal instrument motions. 4. The study demonstrates how conditioning GPT-4o on IAT triplets enhances the generation of clinically relevant, trainer-style feedback. 5. Experimental results show that context and temporal tracking improve video-to-IAT recognition AUC scores (Instrument from 0.67 to 0.74, Action from 0.60 to 0.63, Tissue from 0.74 to 0.79). 6. For feedback text generation, GPT-4o conditioned on IAT triplets improved average fidelity scores from 2.17 to 2.44 (+12.4%) with admissible feedback doubling from 21% to 42%. 7. Traditional text metrics also improved, with word error rates decreasing by 15-31% and ROUGE scores rising by 9-64%. 8. Grounding feedback generation in explicit IAT structures not only improves fidelity but also produces clinician-verifiable rationales, supporting transparency and auditability in surgical training feedback. <div>
arXiv:2511.15159v1 Announce Type: new 
Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance</title>
<link>https://arxiv.org/abs/2511.15164</link>
<guid>https://arxiv.org/abs/2511.15164</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal continual learning, catastrophic forgetting, gradient approximation, parameter space geometry, replay buffer<br /><br />Summary:<br /><br />This paper addresses the challenge of catastrophic forgetting in multimodal continual instruction tuning, which occurs when models lose performance on previously learned tasks while adapting to new ones. The authors present a novel perspective that treats catastrophic forgetting as a consequence of missing gradients from old tasks during the learning of new tasks. To counter this, they propose approximating these missing gradients by exploiting the geometric relationship between current model parameters and parameters optimized for previous tasks, specifically using the directional vector connecting them as a surrogate gradient. Furthermore, their method integrates this approximated gradient with real gradients obtained from a limited replay buffer containing samples from past tasks. A Bernoulli sampling strategy is introduced to dynamically balance the trade-off between maintaining model stability (retaining old knowledge) and plasticity (learning new knowledge). Extensive experiments on multimodal continual instruction tuning benchmarks validate that their approach achieves state-of-the-art performance, mitigating forgetting effectively without increasing the model size, thus preserving a compact and efficient architecture. This work provides a new direction for continual learning by framing missing gradient recovery through the lens of parameter space geometry, enabling robust continual adaptation in large multimodal language models. <div>
arXiv:2511.15164v1 Announce Type: new 
Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation</title>
<link>https://arxiv.org/abs/2511.15167</link>
<guid>https://arxiv.org/abs/2511.15167</guid>
<content:encoded><![CDATA[
<div> Depth estimation, self-supervised learning, contrastive learning, adverse weather, robustness<br /><br />Summary:<br /><br />1. This paper addresses the significant challenge of self-supervised depth estimation performance degradation in adverse weather conditions like rain and fog, where visibility reduction critically impairs depth prediction accuracy. <br />2. The authors propose a novel framework named SEC-Depth, which introduces a self-evolution contrastive learning strategy aimed at robust depth estimation under challenging environmental conditions. <br />3. SEC-Depth constructs temporally evolving latency models by leveraging intermediate parameters generated during the training process to capture optimization states across different training stages through a dynamic update strategy. <br />4. The core innovation is the self-evolution contrastive loss (SECL), which uses outputs from historical latency models as negative samples, enabling adaptive adjustment of learning objectives and implicitly sensing weather degradation severity without the need for manual intervention. <br />5. Experiments demonstrate that SEC-Depth integrates well with a variety of existing baseline models and substantially improves the robustness of depth estimation in zero-shot evaluation settings, confirming its effectiveness for autonomous driving and robotics applications in adverse weather. <div>
arXiv:2511.15167v1 Announce Type: new 
Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction</title>
<link>https://arxiv.org/abs/2511.15179</link>
<guid>https://arxiv.org/abs/2511.15179</guid>
<content:encoded><![CDATA[
<div> Human Motion Prediction, Multimodality, Metric, Clustering, Validity<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating Human Motion Prediction (HMP) methods that produce multiple possible future motions from a single past motion sequence. Existing evaluation metrics often fail to properly assess probabilistic predictions, as they do not sufficiently consider the distribution of predicted motions across diverse motion modes or their kinematic validity. To overcome these shortcomings, the authors introduce a novel metric called Multimodality-aware Metric using Clustering-based Modes (MMCM). MMCM assesses two key criteria: (a) coverage, by clustering motion space into distinct modes to check if predicted motions spread among multiple plausible futures, and (b) validity, by verifying that predicted motions align with kinematically valid future motions observed in a real dataset. The clustering approach ensures sensible grouping of motion modes, which allows MMCM to explicitly evaluate the diversity and realism of motion predictions. Experimental results demonstrate that MMCM provides a more accurate and insightful evaluation of multimodal human motion predictions compared to previous methods. The proposed metric thereby facilitates better development and assessment of stochastic HMP models. The code for MMCM is publicly available, enabling broader use and validation in the research community. <div>
arXiv:2511.15179v1 Announce Type: new 
Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</title>
<link>https://arxiv.org/abs/2511.15186</link>
<guid>https://arxiv.org/abs/2511.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: lesion segmentation, chest X-rays, instruction-guided segmentation, MIMIC-ILS dataset, vision-language model<br /><br />Summary:<br /><br />1. The paper addresses limitations in current chest X-ray (CXR) lesion segmentation models caused by a small set of target labels and dependence on complex expert-level text inputs, which hinder practical applications.<br /><br />2. The authors propose a new approach called instruction-guided lesion segmentation (ILS), which allows segmenting diverse lesion types using simple, user-friendly instructions.<br /><br />3. To support this paradigm, they develop MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, created through an automated multimodal pipeline that extracts annotations from chest X-ray images and corresponding medical reports.<br /><br />4. MIMIC-ILS comprises 1.1 million instruction-answer pairs from 192,000 images and 91,000 unique segmentation masks, covering seven major lesion categories.<br /><br />5. To validate the dataset's utility, the authors introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS that can accurately segment different lesion types and generate textual explanations based on user instructions.<br /><br />6. Experimental results demonstrate high performance in segmentation accuracy and textual output quality, underscoring the effectiveness of the proposed pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level lesion grounding in CXR images. <div>
arXiv:2511.15186v1 Announce Type: new 
Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</title>
<link>https://arxiv.org/abs/2511.15188</link>
<guid>https://arxiv.org/abs/2511.15188</guid>
<content:encoded><![CDATA[
<div> Brain age estimation, Vision Transformer, Residual CNN, MRI, Neurodegeneration<br /><br />Summary:<br /><br />1. This paper introduces BrainRotViT, a hybrid model combining Vision Transformer (ViT) global context encoding and residual CNN local refinement for brain age estimation from structural MRI.  
2. A ViT encoder is first trained on age and sex classification at the slice level, then frozen to generate embeddings from sagittal slices, which are input to a residual CNN regressor incorporating subject sex to predict continuous brain age.  
3. The model achieves strong performance with a mean absolute error (MAE) of 3.34 years, Pearson r=0.98, Spearman ρ=0.97, and R²=0.95 on validation using 11 MRI datasets from over 130 acquisition sites, outperforming baseline and state-of-the-art methods.  
4. It generalizes well to four independent cohorts, with MAEs between 3.77 and 5.04 years.  
5. Analysis of brain age gap reveals links to Alzheimer's disease, cognitive impairment, and autism spectrum disorder.  
6. Attention maps highlight brain regions associated with aging, including the cerebellar vermis, precentral/postcentral gyri, temporal lobes, and medial superior frontal gyrus.  
7. The study presents an interpretable, efficient, and generalizable approach filling the gap between CNN and transformer methods, advancing research in aging and neurodegeneration. <div>
arXiv:2511.15188v1 Announce Type: new 
Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $\rho=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition</title>
<link>https://arxiv.org/abs/2511.15197</link>
<guid>https://arxiv.org/abs/2511.15197</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot generation, object composition, style disentanglement, masked-attention, dataset curation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of inserting real-world objects into stylized domains, a task where existing reference-based object composition methods fail. 2. It introduces "Insert In Style," the first zero-shot generative framework that combines practical usability with high-fidelity results without needing per-subject finetuning or text prompts. 3. The key innovations include a multi-stage training protocol that disentangles identity, style, and composition representations, and a specialized masked-attention architecture that enforces this disentanglement during the generation process, preventing concept interference common in unified-attention models. 4. The framework is trained on a newly created 100k-sample dataset, obtained via a large-scale generation pipeline coupled with a rigorous two-stage filtering process to ensure both semantic identity and style coherence. 5. A new public benchmark for stylized composition is introduced, and the method demonstrates state-of-the-art performance, significantly outperforming prior techniques on identity and style metrics, supported by user studies. <div>
arXiv:2511.15197v1 Announce Type: new 
Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval</title>
<link>https://arxiv.org/abs/2511.15201</link>
<guid>https://arxiv.org/abs/2511.15201</guid>
<content:encoded><![CDATA[
<div> keywords: cross-modal retrieval, causal bias, food image, recipe representation, ingredient confounder  

<br /><br />Summary:  
1. The paper tackles the problem of learning effective representations for recipes and food images in cross-modal retrieval tasks.  
2. It points out that existing methods treating recipes purely as textual descriptions of dish appearance introduce bias, as the visual representation of a dish may not fully correspond to all recipe details due to factors like cooking methods and presentation.  
3. The authors introduce a causal theory perspective, identifying ingredients as confounders that create bias in similarity judgments between images and recipes.  
4. They propose a causal intervention approach with backdoor adjustment to remove this bias, reformulating the food-to-recipe retrieval model to include an additional corrective term.  
5. Empirical results on the Recipe1M dataset demonstrate that their approach achieves near-oracle performance (MedR=1) at various test sizes (1K, 10K, and 50K), surpassing previous state-of-the-art methods.  
6. Additionally, they develop a plug-and-play neural module, a multi-label ingredient classifier, to facilitate debiasing in retrieval models.  
7. Overall, their method improves the robustness and accuracy of cross-modal recipe-image retrieval by explicitly accounting for the causal relationship and bias inherent in the task. <div>
arXiv:2511.15201v1 Announce Type: new 
Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Based Benchmarking Metrics for Multimodal Synthetic Images</title>
<link>https://arxiv.org/abs/2511.15204</link>
<guid>https://arxiv.org/abs/2511.15204</guid>
<content:encoded><![CDATA[
<div> Physics-Constrained Multimodal Data Evaluation, large language models, vision-language models, semantic accuracy, physics-guided reasoning<br /><br />Summary:<br /><br />This paper addresses the limitations of current evaluation metrics such as BLEU, CIDEr, VQA score, SigLIP-2, and CLIPScore, which often fail to capture semantic or structural accuracy, especially in domain-specific or context-dependent applications. The authors propose a novel Physics-Constrained Multimodal Data Evaluation (PCMDE) metric designed to overcome these deficiencies by integrating large language models (LLMs), knowledge-based mapping, and vision-language models (VLMs). The PCMDE architecture consists of three key stages: first, feature extraction through object detection and VLMs to gather spatial and semantic multimodal information; second, a Confidence-Weighted Component Fusion mechanism that adaptively validates each component using confidence scores, ensuring robust fusion of extracted features; and third, physics-guided reasoning leveraging LLMs to enforce structural and relational constraints such as alignment, positional consistency, and coherence. This combination aims to enhance the accuracy and reliability of multimodal data evaluation by incorporating domain knowledge and reasoning capabilities beyond superficial matching. Ultimately, PCMDE represents a significant step toward semantically and structurally aware evaluation metrics that are better suited to nuanced, domain-specific scenarios, enabling more meaningful assessments of generated data in multimodal contexts. <div>
arXiv:2511.15204v1 Announce Type: new 
Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning</title>
<link>https://arxiv.org/abs/2511.15242</link>
<guid>https://arxiv.org/abs/2511.15242</guid>
<content:encoded><![CDATA[
<div> SkinGPT-R1, dermatology, chain of thought, DermCoT, DermBench<br /><br />Summary:<br /><br />1. The paper introduces SkinGPT-R1, a vision language model specialized for dermatology that explicitly performs diagnostic chain of thought reasoning step-by-step, making the reasoning process verifiable.<br />2. To facilitate skin-specific reasoning, the authors create DermCoT, a dataset combining 10,000 filtered training cases from DermEval with 3,000 certified cases scored by dermatologists, ensuring high-quality standardized dermatologic narratives.<br />3. DermEval is defined as a physician-aligned six-dimensional evaluator, and DermBench is introduced as the corresponding benchmark designed to assess the quality of dermatologic chain of thought reasoning.<br />4. On DermBench, SkinGPT-R1 achieves a leading average score of 4.031 out of 5 across the six clinician-defined dimensions, outperforming 14 other vision and medical vision language models, and improving average performance over the previous Vision-R1 model by approximately 41%.<br />5. SkinGPT-R1 also demonstrates stable accuracy improvements on three separate dermatology classification benchmarks, maintaining strong competitiveness with other advanced vision language models.<br />6. Ablation studies confirm that chain of thought supervision based on DermCoT significantly enhances model performance, and incorporating dermatology-aware visual distillation yields consistent additional benefits in both narrative explanation quality and classification accuracy. <div>
arXiv:2511.15242v1 Announce Type: new 
Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplitFlux: Learning to Decouple Content and Style from a Single Image</title>
<link>https://arxiv.org/abs/2511.15258</link>
<guid>https://arxiv.org/abs/2511.15258</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, content-style disentanglement, SplitFlux, LoRA fine-tuning, SDXL

<br /><br />Summary:  
This paper addresses the challenge of disentangling image content and style for customized image generation, noting limitations in current SDXL-based methods and the Flux model. Through systematic analysis of Flux, two main observations are made: Single Dream Blocks are crucial for generation, with early blocks focusing on content control and later blocks governing style. Building on these insights, the authors propose SplitFlux, which fine-tunes Single Dream Blocks using LoRA to effectively separate content and style, allowing content to be re-embedded into new contexts. SplitFlux introduces two key components: Rank-Constrained Adaptation compresses update rank and amplifies magnitudes within certain blocks to maintain content identity and prevent content leakage into style blocks; Visual-Gated LoRA splits content LoRA into two branches guided by image saliency— a high-rank branch retains main subject details, and a low-rank branch encodes residual information, reducing overfitting and supporting smooth re-embedding. Extensive experimental results demonstrate SplitFlux’s superiority over state-of-the-art methods, showing better content preservation and stylization quality across various scenarios. The approach offers a robust solution for high-quality, disentangled image generation by effectively separating and controlling content and style components. <div>
arXiv:2511.15258v1 Announce Type: new 
Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Query Networks for Object Detection with Automotive Radar</title>
<link>https://arxiv.org/abs/2511.15271</link>
<guid>https://arxiv.org/abs/2511.15271</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D radar, object detection, Graph Query Networks, relational reasoning, NuScenes dataset<br /><br />Summary: This paper addresses the challenge of object detection using 3D radar for automotive perception, focusing on the difficulties caused by radar's sparse and irregular reflections due to long wavelengths. It introduces Graph Query Networks (GQN), an innovative attention-based framework that represents radar-detected objects as graphs to better extract relational and contextual features. A key innovation is the concept of graph queries, which dynamically attend to the bird's-eye view (BEV) space to build tailored object-specific graphs. The framework includes two novel modules: EdgeFocus, which enhances relational reasoning between graph nodes, and DeepContext Pooling, which aggregates contextual information effectively. Experimental results on the NuScenes dataset demonstrate that GQN significantly boosts relative mean Average Precision (mAP) by up to 53%, achieving an 8.2% improvement over the strongest previous radar-based detection methods. Additionally, the approach reduces the peak graph construction overhead by 80%, making it computationally efficient while maintaining moderate FLOPs. Overall, this work advances 3D radar object detection by combining graph-based attention mechanisms and efficient contextual modeling, leading to more accurate and faster radar perception in autonomous driving scenarios. <div>
arXiv:2511.15271v1 Announce Type: new 
Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Centric Relational Reasoning for 3D Scene Graph Prediction</title>
<link>https://arxiv.org/abs/2511.15288</link>
<guid>https://arxiv.org/abs/2511.15288</guid>
<content:encoded><![CDATA[
arXiv:2511.15288v1 Announce Type: new 
Abstract: 3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Generative Synthetic Data for X-ray Prohibited Item Detection</title>
<link>https://arxiv.org/abs/2511.15299</link>
<guid>https://arxiv.org/abs/2511.15299</guid>
<content:encoded><![CDATA[
arXiv:2511.15299v1 Announce Type: new 
Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language</title>
<link>https://arxiv.org/abs/2511.15308</link>
<guid>https://arxiv.org/abs/2511.15308</guid>
<content:encoded><![CDATA[
arXiv:2511.15308v1 Announce Type: new 
Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models</title>
<link>https://arxiv.org/abs/2511.15311</link>
<guid>https://arxiv.org/abs/2511.15311</guid>
<content:encoded><![CDATA[
arXiv:2511.15311v1 Announce Type: new 
Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data</title>
<link>https://arxiv.org/abs/2511.15312</link>
<guid>https://arxiv.org/abs/2511.15312</guid>
<content:encoded><![CDATA[
arXiv:2511.15312v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs</title>
<link>https://arxiv.org/abs/2511.15316</link>
<guid>https://arxiv.org/abs/2511.15316</guid>
<content:encoded><![CDATA[
arXiv:2511.15316v1 Announce Type: new 
Abstract: Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive thresholding pattern for fingerprint forgery detection</title>
<link>https://arxiv.org/abs/2511.15322</link>
<guid>https://arxiv.org/abs/2511.15322</guid>
<content:encoded><![CDATA[
arXiv:2511.15322v1 Announce Type: new 
Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.15343</link>
<guid>https://arxiv.org/abs/2511.15343</guid>
<content:encoded><![CDATA[
arXiv:2511.15343v1 Announce Type: new 
Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers</title>
<link>https://arxiv.org/abs/2511.15369</link>
<guid>https://arxiv.org/abs/2511.15369</guid>
<content:encoded><![CDATA[
arXiv:2511.15369v1 Announce Type: new 
Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training</title>
<link>https://arxiv.org/abs/2511.15379</link>
<guid>https://arxiv.org/abs/2511.15379</guid>
<content:encoded><![CDATA[
arXiv:2511.15379v1 Announce Type: new 
Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2511.15390</link>
<guid>https://arxiv.org/abs/2511.15390</guid>
<content:encoded><![CDATA[
arXiv:2511.15390v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation</title>
<link>https://arxiv.org/abs/2511.15396</link>
<guid>https://arxiv.org/abs/2511.15396</guid>
<content:encoded><![CDATA[
arXiv:2511.15396v1 Announce Type: new 
Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling False Positives in Image Segmentation via Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.15406</link>
<guid>https://arxiv.org/abs/2511.15406</guid>
<content:encoded><![CDATA[
arXiv:2511.15406v1 Announce Type: new 
Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models</title>
<link>https://arxiv.org/abs/2511.15411</link>
<guid>https://arxiv.org/abs/2511.15411</guid>
<content:encoded><![CDATA[
arXiv:2511.15411v1 Announce Type: new 
Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes</title>
<link>https://arxiv.org/abs/2511.15429</link>
<guid>https://arxiv.org/abs/2511.15429</guid>
<content:encoded><![CDATA[
arXiv:2511.15429v1 Announce Type: new 
Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection</title>
<link>https://arxiv.org/abs/2511.15433</link>
<guid>https://arxiv.org/abs/2511.15433</guid>
<content:encoded><![CDATA[
arXiv:2511.15433v1 Announce Type: new 
Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.15435</link>
<guid>https://arxiv.org/abs/2511.15435</guid>
<content:encoded><![CDATA[
arXiv:2511.15435v1 Announce Type: new 
Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing</title>
<link>https://arxiv.org/abs/2511.15440</link>
<guid>https://arxiv.org/abs/2511.15440</guid>
<content:encoded><![CDATA[
arXiv:2511.15440v1 Announce Type: new 
Abstract: Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras</title>
<link>https://arxiv.org/abs/2511.15459</link>
<guid>https://arxiv.org/abs/2511.15459</guid>
<content:encoded><![CDATA[
arXiv:2511.15459v1 Announce Type: new 
Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v1 Announce Type: new 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners</title>
<link>https://arxiv.org/abs/2511.15468</link>
<guid>https://arxiv.org/abs/2511.15468</guid>
<content:encoded><![CDATA[
arXiv:2511.15468v1 Announce Type: new 
Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection</title>
<link>https://arxiv.org/abs/2511.15476</link>
<guid>https://arxiv.org/abs/2511.15476</guid>
<content:encoded><![CDATA[
arXiv:2511.15476v1 Announce Type: new 
Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI</title>
<link>https://arxiv.org/abs/2511.15481</link>
<guid>https://arxiv.org/abs/2511.15481</guid>
<content:encoded><![CDATA[
arXiv:2511.15481v1 Announce Type: new 
Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels</title>
<link>https://arxiv.org/abs/2511.15496</link>
<guid>https://arxiv.org/abs/2511.15496</guid>
<content:encoded><![CDATA[
arXiv:2511.15496v1 Announce Type: new 
Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Expand Images for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2511.15499</link>
<guid>https://arxiv.org/abs/2511.15499</guid>
<content:encoded><![CDATA[
arXiv:2511.15499v1 Announce Type: new 
Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Text Guided Few-Shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.15515</link>
<guid>https://arxiv.org/abs/2511.15515</guid>
<content:encoded><![CDATA[
arXiv:2511.15515v1 Announce Type: new 
Abstract: Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture</title>
<link>https://arxiv.org/abs/2511.15535</link>
<guid>https://arxiv.org/abs/2511.15535</guid>
<content:encoded><![CDATA[
arXiv:2511.15535v1 Announce Type: new 
Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scriboora: Rethinking Human Pose Forecasting</title>
<link>https://arxiv.org/abs/2511.15565</link>
<guid>https://arxiv.org/abs/2511.15565</guid>
<content:encoded><![CDATA[
arXiv:2511.15565v1 Announce Type: new 
Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer-Use Agents as Judges for Generative User Interface</title>
<link>https://arxiv.org/abs/2511.15567</link>
<guid>https://arxiv.org/abs/2511.15567</guid>
<content:encoded><![CDATA[
arXiv:2511.15567v1 Announce Type: new 
Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector</title>
<link>https://arxiv.org/abs/2511.15571</link>
<guid>https://arxiv.org/abs/2511.15571</guid>
<content:encoded><![CDATA[
arXiv:2511.15571v1 Announce Type: new 
Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers</title>
<link>https://arxiv.org/abs/2511.15572</link>
<guid>https://arxiv.org/abs/2511.15572</guid>
<content:encoded><![CDATA[
arXiv:2511.15572v1 Announce Type: new 
Abstract: Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning</title>
<link>https://arxiv.org/abs/2511.15578</link>
<guid>https://arxiv.org/abs/2511.15578</guid>
<content:encoded><![CDATA[
arXiv:2511.15578v1 Announce Type: new 
Abstract: With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking</title>
<link>https://arxiv.org/abs/2511.15580</link>
<guid>https://arxiv.org/abs/2511.15580</guid>
<content:encoded><![CDATA[
arXiv:2511.15580v1 Announce Type: new 
Abstract: 3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition</title>
<link>https://arxiv.org/abs/2511.15597</link>
<guid>https://arxiv.org/abs/2511.15597</guid>
<content:encoded><![CDATA[
arXiv:2511.15597v1 Announce Type: new 
Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery</title>
<link>https://arxiv.org/abs/2511.15600</link>
<guid>https://arxiv.org/abs/2511.15600</guid>
<content:encoded><![CDATA[
arXiv:2511.15600v1 Announce Type: new 
Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.15603</link>
<guid>https://arxiv.org/abs/2511.15603</guid>
<content:encoded><![CDATA[
arXiv:2511.15603v1 Announce Type: new 
Abstract: Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Think and When to Look: Uncertainty-Guided Lookback</title>
<link>https://arxiv.org/abs/2511.15613</link>
<guid>https://arxiv.org/abs/2511.15613</guid>
<content:encoded><![CDATA[
arXiv:2511.15613v1 Announce Type: new 
Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</title>
<link>https://arxiv.org/abs/2511.15618</link>
<guid>https://arxiv.org/abs/2511.15618</guid>
<content:encoded><![CDATA[
arXiv:2511.15618v1 Announce Type: new 
Abstract: Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</title>
<link>https://arxiv.org/abs/2511.15622</link>
<guid>https://arxiv.org/abs/2511.15622</guid>
<content:encoded><![CDATA[
arXiv:2511.15622v1 Announce Type: new 
Abstract: Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\href{https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2511.15633</link>
<guid>https://arxiv.org/abs/2511.15633</guid>
<content:encoded><![CDATA[
arXiv:2511.15633v1 Announce Type: new 
Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography</title>
<link>https://arxiv.org/abs/2511.15640</link>
<guid>https://arxiv.org/abs/2511.15640</guid>
<content:encoded><![CDATA[
arXiv:2511.15640v1 Announce Type: new 
Abstract: Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling</title>
<link>https://arxiv.org/abs/2511.15645</link>
<guid>https://arxiv.org/abs/2511.15645</guid>
<content:encoded><![CDATA[
arXiv:2511.15645v1 Announce Type: new 
Abstract: Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases</title>
<link>https://arxiv.org/abs/2511.15656</link>
<guid>https://arxiv.org/abs/2511.15656</guid>
<content:encoded><![CDATA[
arXiv:2511.15656v1 Announce Type: new 
Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI</title>
<link>https://arxiv.org/abs/2511.15658</link>
<guid>https://arxiv.org/abs/2511.15658</guid>
<content:encoded><![CDATA[
arXiv:2511.15658v1 Announce Type: new 
Abstract: Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.
  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisPlay: Self-Evolving Vision-Language Models from Images</title>
<link>https://arxiv.org/abs/2511.15661</link>
<guid>https://arxiv.org/abs/2511.15661</guid>
<content:encoded><![CDATA[
arXiv:2511.15661v1 Announce Type: new 
Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features</title>
<link>https://arxiv.org/abs/2511.15675</link>
<guid>https://arxiv.org/abs/2511.15675</guid>
<content:encoded><![CDATA[
arXiv:2511.15675v1 Announce Type: new 
Abstract: Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</title>
<link>https://arxiv.org/abs/2511.15690</link>
<guid>https://arxiv.org/abs/2511.15690</guid>
<content:encoded><![CDATA[
arXiv:2511.15690v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Image Classification using Spectral-Spatial Mixer Network</title>
<link>https://arxiv.org/abs/2511.15692</link>
<guid>https://arxiv.org/abs/2511.15692</guid>
<content:encoded><![CDATA[
arXiv:2511.15692v1 Announce Type: new 
Abstract: This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First Frame Is the Place to Go for Video Content Customization</title>
<link>https://arxiv.org/abs/2511.15700</link>
<guid>https://arxiv.org/abs/2511.15700</guid>
<content:encoded><![CDATA[
arXiv:2511.15700v1 Announce Type: new 
Abstract: What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Visually, Reason Textually: Vision-Language Synergy in ARC</title>
<link>https://arxiv.org/abs/2511.15703</link>
<guid>https://arxiv.org/abs/2511.15703</guid>
<content:encoded><![CDATA[
arXiv:2511.15703v1 Announce Type: new 
Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</title>
<link>https://arxiv.org/abs/2511.15705</link>
<guid>https://arxiv.org/abs/2511.15705</guid>
<content:encoded><![CDATA[
arXiv:2511.15705v1 Announce Type: new 
Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoMa v2: Harder Better Faster Denser Feature Matching</title>
<link>https://arxiv.org/abs/2511.15706</link>
<guid>https://arxiv.org/abs/2511.15706</guid>
<content:encoded><![CDATA[
arXiv:2511.15706v1 Announce Type: new 
Abstract: Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of Graph Based Vision Transformers Architectures for Accurate Temperature Prediction in Fiber Specklegram Sensors</title>
<link>https://arxiv.org/abs/2511.14792</link>
<guid>https://arxiv.org/abs/2511.14792</guid>
<content:encoded><![CDATA[
arXiv:2511.14792v1 Announce Type: cross 
Abstract: Fiber Specklegram Sensors (FSS) are highly effective for environmental monitoring, particularly for detecting temperature variations. However, the nonlinear nature of specklegram data presents significant challenges for accurate temperature prediction. This study investigates the use of transformer-based architectures, including Vision Transformers (ViTs), Swin Transformers, and emerging models such as Learnable Importance Non-Symmetric Attention Vision Transformers (LINA-ViT) and Multi-Adaptive Proximity Vision Graph Attention Transformers (MAP-ViGAT), to predict temperature from specklegram data over a range of 0 to 120 Celsius. The results show that ViTs achieved a Mean Absolute Error (MAE) of 1.15, outperforming traditional models such as CNNs. GAT-ViT and MAP-ViGAT variants also demonstrated competitive accuracy, highlighting the importance of adaptive attention mechanisms and graph-based structures in capturing complex modal interactions and phase shifts in specklegram data. Additionally, this study incorporates Explainable AI (XAI) techniques, including attention maps and saliency maps, to provide insights into the decision-making processes of the transformer models, improving interpretability and transparency. These findings establish transformer architectures as strong benchmarks for optical fiber-based temperature sensing and offer promising directions for industrial monitoring and structural health assessment applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence</title>
<link>https://arxiv.org/abs/2511.14823</link>
<guid>https://arxiv.org/abs/2511.14823</guid>
<content:encoded><![CDATA[
arXiv:2511.14823v1 Announce Type: cross 
Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard</title>
<link>https://arxiv.org/abs/2511.14876</link>
<guid>https://arxiv.org/abs/2511.14876</guid>
<content:encoded><![CDATA[
arXiv:2511.14876v1 Announce Type: cross 
Abstract: To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference</title>
<link>https://arxiv.org/abs/2511.14961</link>
<guid>https://arxiv.org/abs/2511.14961</guid>
<content:encoded><![CDATA[
arXiv:2511.14961v1 Announce Type: cross 
Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Denoising Using Transformed L1 (TL1) Regularization via ADMM</title>
<link>https://arxiv.org/abs/2511.15060</link>
<guid>https://arxiv.org/abs/2511.15060</guid>
<content:encoded><![CDATA[
arXiv:2511.15060v1 Announce Type: cross 
Abstract: Total variation (TV) regularization is a classical tool for image denoising, but its convex $\ell_1$ formulation often leads to staircase artifacts and loss of contrast. To address these issues, we introduce the Transformed $\ell_1$ (TL1) regularizer applied to image gradients. In particular, we develop a TL1-regularized denoising model and solve it using the Alternating Direction Method of Multipliers (ADMM), featuring a closed-form TL1 proximal operator and an FFT-based image update under periodic boundary conditions. Experimental results demonstrate that our approach achieves superior denoising performance, effectively suppressing noise while preserving edges and enhancing image contrast.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer</title>
<link>https://arxiv.org/abs/2511.15067</link>
<guid>https://arxiv.org/abs/2511.15067</guid>
<content:encoded><![CDATA[
arXiv:2511.15067v1 Announce Type: cross 
Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer</title>
<link>https://arxiv.org/abs/2511.15090</link>
<guid>https://arxiv.org/abs/2511.15090</guid>
<content:encoded><![CDATA[
arXiv:2511.15090v1 Announce Type: cross 
Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits</title>
<link>https://arxiv.org/abs/2511.15173</link>
<guid>https://arxiv.org/abs/2511.15173</guid>
<content:encoded><![CDATA[
arXiv:2511.15173v1 Announce Type: cross 
Abstract: The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
arXiv:2511.15244v1 Announce Type: cross 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>