<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>Reconstruction and Reenactment Separated Method for Realistic Gaussian Head</title>
<link>https://arxiv.org/abs/2509.05582</link>
<guid>https://arxiv.org/abs/2509.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: reconstruction, reenactment, 3D Gaussians, avatar, high frame-rate rendering

Summary: 
This paper introduces a reconstruction and reenactment framework for 3D Gaussian heads, utilizing a single portrait image to generate a controllable avatar. The framework includes a large-scale one-shot Gaussian head generator trained in two stages for improved generalization and texture reconstruction. The system achieves high frame-rate rendering of 90 FPS at 512x512 resolution through an ultra-lightweight Gaussian avatar driven by control signals. Scaling up the reconstruction module enhances performance without impacting driving efficiency. Extensive experiments demonstrate the superiority of the proposed approach over existing state-of-the-art methods in terms of both quantitative and qualitative results. <br /><br />Summary: <div>
arXiv:2509.05582v2 Announce Type: replace 
Abstract: In this paper, we explore a reconstruction and reenactment separated framework for 3D Gaussians head, which requires only a single portrait image as input to generate controllable avatar. Specifically, we developed a large-scale one-shot gaussian head generator built upon WebSSL and employed a two-stage training approach that significantly enhances the capabilities of generalization and high-frequency texture reconstruction. During inference, an ultra-lightweight gaussian avatar driven by control signals enables high frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further demonstrate that the proposed framework follows the scaling law, whereby increasing the parameter scale of the reconstruction module leads to improved performance. Moreover, thanks to the separation design, driving efficiency remains unaffected. Finally, extensive quantitative and qualitative experiments validate that our approach outperforms current state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks</title>
<link>https://arxiv.org/abs/2509.13338</link>
<guid>https://arxiv.org/abs/2509.13338</guid>
<content:encoded><![CDATA[
<div> Keywords: evidence-retrieval, uncertainty-aware decision-making, Dempster-Shafer theory, CIFAR-10/100, interpretable

Summary:
This work introduces an evidence-retrieval mechanism for uncertainty-aware decision-making, utilizing proximal exemplars in an embedding space to create a per-instance thresholding criterion. By fusing predictive distributions through Dempster-Shafer theory, a transparent and auditable decision-making process is achieved. Experimentation on CIFAR-10/100 datasets with BiT and ViT backbones demonstrates improved uncertainty-aware performance with fewer confidently incorrect outcomes and manageable review loads compared to traditional entropy-based thresholds. Interestingly, the study finds that only a small set of evidences is necessary to realize these benefits, with minimal improvements gained from increasing the evidence set. The evidence-conditioned tagging approach emerges as a reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making.<br /><br />Summary: <div>
arXiv:2509.13338v1 Announce Type: new 
Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware decision-making that replaces a single global cutoff with an evidence-conditioned, instance-adaptive criterion. For each test instance, proximal exemplars are retrieved in an embedding space; their predictive distributions are fused via Dempster-Shafer theory. The resulting fused belief acts as a per-instance thresholding mechanism. Because the supporting evidences are explicit, decisions are transparent and auditable. Experiments on CIFAR-10/100 with BiT and ViT backbones show higher or comparable uncertainty-aware performance with materially fewer confidently incorrect outcomes and a sustainable review load compared with applying threshold on prediction entropy. Notably, only a few evidences are sufficient to realize these gains; increasing the evidence set yields only modest changes. These results indicate that evidence-conditioned tagging provides a more reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Model for Image Classification</title>
<link>https://arxiv.org/abs/2509.13353</link>
<guid>https://arxiv.org/abs/2509.13353</guid>
<content:encoded><![CDATA[
<div> Quantum-classical neural networks, hybrid models, benchmark datasets, performance evaluation, resource efficiency
Summary:
Hybrid quantum-classical neural networks were compared to classical models on three datasets (MNIST, CIFAR100, STL10), demonstrating higher accuracy (99.38% vs 98.21%, 41.69% vs 32.25%, 74.05% vs 63.76%), faster training (5-12 times), and fewer parameters (6-32% less). Hybrid models showed superior generalization and better adversarial robustness on simpler datasets. Resource efficiency analysis revealed lower memory consumption (4-5GB vs 5-6GB) and lower CPU utilization (9.5% vs 23.2%). The study suggests that hybrid quantum-classical architectures offer significant advantages in accuracy, training efficiency, and scalability for complex vision tasks.
<br /><br />Summary: <div>
arXiv:2509.13353v1 Announce Type: new 
Abstract: This study presents a systematic comparison between hybrid quantum-classical neural networks and purely classical models across three benchmark datasets (MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and robustness. The hybrid models integrate parameterized quantum circuits with classical deep learning architectures, while the classical counterparts use conventional convolutional neural networks (CNNs). Experiments were conducted over 50 training epochs for each dataset, with evaluations on validation accuracy, test accuracy, training time, computational resource usage, and adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings demonstrate that hybrid models consistently outperform classical models in final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\% (STL10) validation accuracy, compared to classical benchmarks of 98.21\%, 32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%) and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g., 21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while maintaining superior generalization to unseen test data.Adversarial robustness tests reveal that hybrid models are significantly more resilient on simpler datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but show comparable fragility on complex datasets like CIFAR100 ($\sim$1\% robustness for both). Resource efficiency analyses indicate that hybrid models consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization (9.5\% vs. 23.2\% on average).These results suggest that hybrid quantum-classical architectures offer compelling advantages in accuracy, training efficiency, and parameter scalability, particularly for complex vision tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention</title>
<link>https://arxiv.org/abs/2509.13361</link>
<guid>https://arxiv.org/abs/2509.13361</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic congestion, YOLOv11-DIoU, DeepSort, GRU-Attention model, expressway control <br />
<br />
Summary: 
This study introduces an integrated technical framework to improve traffic flow perception and congestion forecasting on expressways. By optimizing baseline algorithms such as YOLOv11-DIoU and DeepSort, the study achieved higher accuracy in vehicle perception and tracking under occlusion. The Greenberg model was used to analyze traffic flow, showing a strong negative correlation between speed and density. A GRU-Attention model was developed for congestion warning, achieving high test accuracy and precise time error predictions. In validation tests, the framework demonstrated significant improvements in warning accuracy and spatial overlap of congestion points, even in high-flow scenarios. Overall, this framework provides valuable support for expressway congestion control and shows great potential for intelligent transportation applications. <br /> <div>
arXiv:2509.13361v1 Announce Type: new 
Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders regional connectivity. Existing "detection-prediction" systems have critical flaws: low vehicle perception accuracy under occlusion and loss of long-sequence dependencies in congestion forecasting. This study proposes an integrated technical framework to resolve these issues.For traffic flow perception, two baseline algorithms were optimized. Traditional YOLOv11 was upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort was improved by fusing Mahalanobis (motion) and cosine (appearance) distances. Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\% mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT) with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km high-density scenarios), speed and density showed a strong negative correlation (r=-0.97), conforming to traffic flow theory. For congestion warning, a GRU-Attention model was built to capture congestion precursors. Trained 300 epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9 percentage points higher than traditional GRU). In 10-minute advance warnings for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an independent video showed 95\% warning accuracy, over 90\% spatial overlap of congestion points, and stable performance in high-flow ($>$5 vehicles/second) scenarios.This framework provides quantitative support for expressway congestion control, with promising intelligent transportation applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2509.13366</link>
<guid>https://arxiv.org/abs/2509.13366</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time parking service, crowd-sourced data, machine learning, convolutional neural networks, automation

Summary: 
This research focuses on improving a real-time on-street parking service by using crowd-sourced in-vehicle fleet data and machine learning techniques. The study aims to automate the existing test process for ground truth tests, reducing the need for human intervention significantly. By applying convolutional neural networks and image pattern recognition, the researchers were able to enhance the database and streamline the analysis process. The findings indicate a substantial reduction in human resources required, up to 99.58%. The paper outlines the methods and implementations used to achieve high levels of automation, as well as the performance metrics that demonstrate the effectiveness of the approach. The overall improvements in the parking service quality are discussed, along with potential future developments and applications of the analysis automation tool.<br /><br />Summary: <div>
arXiv:2509.13366v1 Announce Type: new 
Abstract: This research is part of a study of a real-time, cloud-based on-street parking service using crowd-sourced in-vehicle fleet data. The service provides real-time information about available parking spots by classifying crowd-sourced detections observed via ultrasonic sensors. The goal of this research is to optimize the current parking service quality by analyzing the automation of the existing test process for ground truth tests. Therefore, methods from the field of machine learning, especially image pattern recognition, are applied to enrich the database and substitute human engineering work in major areas of the analysis process. After an introduction into the related areas of machine learning, this paper explains the methods and implementations made to achieve a high level of automation, applying convolutional neural networks. Finally, predefined metrics present the performance level achieved, showing a time reduction of human resources up to 99.58 %. The overall improvements are discussed, summarized, and followed by an outlook for future development and potential application of the analysis automation tool.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity</title>
<link>https://arxiv.org/abs/2509.13375</link>
<guid>https://arxiv.org/abs/2509.13375</guid>
<content:encoded><![CDATA[
<div> Characterize, formalize, operational properties, VLM, embedding space
<br />
Summary: 
This paper presents a systematic empirical analysis of Vision-Language Models (VLMs) for out-of-distribution (OOD) detection. The study investigates the mechanisms, advantages, and sensitivity of VLM-based OOD detection. Mechanisms are characterized and formalized, highlighting key operational properties in the VLM embedding space that enable effective zero-shot OOD detection. Empirical results demonstrate the superiority of VLMs over single-modal approaches, attributed to their ability to leverage rich semantic novelty. However, the study reveals a sensitivity in VLM-based methods to prompt phrasing, despite their resilience to common image noise. This asymmetry in robustness profile indicates a critical vulnerability in VLM-based OOD detection. The findings contribute to a more structured understanding of VLM strengths and vulnerabilities, providing essential guidance for enhancing the robustness and reliability of future designs.
<br /><br /> <div>
arXiv:2509.13375v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable AI systems. Despite this promising capability, a comprehensive understanding of (1) why they work so effectively, (2) what advantages do they have over single-modal methods, and (3) how is their behavioral robustness -- remains notably incomplete within the research community. This paper presents a systematic empirical analysis of VLM-based OOD detection using in-distribution (ID) and OOD prompts. (1) Mechanisms: We systematically characterize and formalize key operational properties within the VLM embedding space that facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the superiority of these models over established single-modal approaches, attributing this distinct advantage to the VLM's capacity to leverage rich semantic novelty. (3) Sensitivity: We uncovers a significant and previously under-explored asymmetry in their robustness profile: while exhibiting resilience to common image noise, these VLM-based methods are highly sensitive to prompt phrasing. Our findings contribute a more structured understanding of the strengths and critical vulnerabilities inherent in VLM-based OOD detection, offering crucial, empirically-grounded guidance for developing more robust and reliable future designs.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension</title>
<link>https://arxiv.org/abs/2509.13385</link>
<guid>https://arxiv.org/abs/2509.13385</guid>
<content:encoded><![CDATA[
<div> curvature, geometric profile, metric spaces, dimensionality reduction, intrinsic dimensionality <br />
Summary: <br />
The article introduces a method for constructing a curvature-based geometric profile of discrete metric spaces using abstract notions of sectional curvature. It offers a quantitative measure to assess data representations' effectiveness, including those produced by dimensionality reduction techniques. The curvature concept captures metric relations between triples of points and other points, enabling estimation of intrinsic dimensionality of datasets. Experiments show that the curvature-based analysis can estimate dataset dimensionality accurately. This approach is utilized to explore the large-scale geometry of empirical networks. Furthermore, it evaluates the effectiveness of dimensionality reduction techniques in data representation. <div>
arXiv:2509.13385v1 Announce Type: new 
Abstract: Utilizing recently developed abstract notions of sectional curvature, we introduce a method for constructing a curvature-based geometric profile of discrete metric spaces. The curvature concept that we use here captures the metric relations between triples of points and other points. More significantly, based on this curvature profile, we introduce a quantitative measure to evaluate the effectiveness of data representations, such as those produced by dimensionality reduction techniques. Furthermore, Our experiments demonstrate that this curvature-based analysis can be employed to estimate the intrinsic dimensionality of datasets. We use this to explore the large-scale geometry of empirical networks and to evaluate the effectiveness of dimensionality reduction techniques.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji</title>
<link>https://arxiv.org/abs/2509.13388</link>
<guid>https://arxiv.org/abs/2509.13388</guid>
<content:encoded><![CDATA[
<div> Framework, Land use, Land cover change, Machine learning, Remote sensing

Summary:<br />
The study focuses on using machine learning and remote sensing frameworks to analyze land use and land cover changes in Nadi, Fiji, from 2013 to 2024. The goal is to provide technical support for land cover/land use modeling and change detection. Landsat-8 satellite images were utilized, along with a training dataset for supervised machine learning. Google Earth Engine and unsupervised machine learning via k-means clustering were employed to create a land cover map. Convolutional neural networks were used for land cover classification in selected regions. A visual representation of change detection was presented, emphasizing urban area changes over time for monitoring purposes. <div>
arXiv:2509.13388v1 Announce Type: new 
Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible in the massive development projects that include housing, roads, and civil works. In this study, we present machine learning and remote sensing frameworks to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The ultimate goal of this study is to provide technical support in land cover/land use modelling and change detection. We used Landsat-8 satellite image for the study region and created our training dataset with labels for supervised machine learning. We used Google Earth Engine and unsupervised machine learning via k-means clustering to generate the land cover map. We used convolutional neural networks to classify the selected regions' land cover types. We present a visualisation of change detection, highlighting urban area changes over time to monitor changes in the map.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence</title>
<link>https://arxiv.org/abs/2509.13396</link>
<guid>https://arxiv.org/abs/2509.13396</guid>
<content:encoded><![CDATA[
<div> YOLOv7, ConvNeXt-based feature extractor, triplet loss, feature-assisted IoU tracker, mixed-precision inference <br />
<br />
Summary: <br />
This paper introduces a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. It combines a YOLOv7 segmentation model for object localization, a ConvNeXt-based feature extractor trained with triplet loss for generating discriminative embeddings, and a feature-assisted IoU tracker for resilient multi-object tracking. The framework is optimized for deployment on low-cost edge hardware using mixed-precision inference and supports incremental updates by adding embeddings from new objects into a reference database without retraining. Experiments on real-world datasets demonstrate high accuracy and robustness across diverse FOI scenarios. Hardware benchmarks on NVIDIA Jetson devices confirm the practicality and scalability of the framework for real-world edge applications. <div>
arXiv:2509.13396v1 Announce Type: new 
Abstract: This paper presents a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. The framework integrates: (1) a YOLOv7 segmentation model for fast and robust object localization, (2) a ConvNeXt-based feature extractor trained with triplet loss to generate discriminative embeddings, and (3) a feature-assisted IoU tracker that ensures resilient multi-object tracking under occlusion and motion. To enable scalable field deployment, the pipeline is optimized for deployment on low-cost edge hardware using mixed-precision inference. The system supports incremental updates by adding embeddings from previously unseen objects into a reference database without requiring model retraining. Extensive experiments on real-world surveillance and drone video datasets demonstrate the framework's high accuracy and robustness across diverse FOI scenarios. In addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's practicality and scalability for real-world edge applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing</title>
<link>https://arxiv.org/abs/2509.13399</link>
<guid>https://arxiv.org/abs/2509.13399</guid>
<content:encoded><![CDATA[
<div> Keywords: Instruction-based image editing, Evaluation framework, Object-centric perspective, Multi-turn editing benchmark, State-of-the-art editing models

Summary: 
The article introduces EdiVal-Agent, an evaluation framework for instruction-based image editing that focuses on an object-centric perspective. It decomposes images into semantically meaningful objects and synthesizes diverse editing instructions. The framework combines vision-language models with object detectors for instruction-following evaluation and uses semantic-level feature extractors for content consistency assessment. Human preference models are leveraged for judging visual quality. EdiVal-Agent shows improved agreement with human judgments compared to existing methods and allows for seamless integration of future tools. The framework is instantiated in EdiVal-Bench, a benchmark covering multiple editing models and instruction types. It aids in identifying failure modes of current editing models and informing the development of next-generation models. The project page for EdiVal-Agent can be found at https://tianyucodings.github.io/EdiVAL-page/. 

<br /><br />Summary:  <div>
arXiv:2509.13399v1 Announce Type: new 
Abstract: Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images -- resulting in limited coverage and inheriting biases from prior generative models -- or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: https://tianyucodings.github.io/EdiVAL-page/.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapAnything: Universal Feed-Forward Metric 3D Reconstruction</title>
<link>https://arxiv.org/abs/2509.13414</link>
<guid>https://arxiv.org/abs/2509.13414</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, MapAnything, 3D scene geometry, feed-forward model, multi-view scene geometry<br />
<br />
Summary: <br />
MapAnything is a transformer-based feed-forward model that can process one or more images and additional geometric inputs to directly estimate metric 3D scene geometry and cameras. It utilizes a factored representation of multi-view scene geometry, including depth maps, local ray maps, camera poses, and a metric scale factor to ensure global consistency. By standardizing supervision and training protocols across datasets and allowing for flexible input augmentation, MapAnything can perform various 3D vision tasks in a single pass. These tasks include uncalibrated structure-from-motion, multi-view stereo, monocular depth estimation, camera localization, and depth completion. Experimental results show that MapAnything outperforms or matches specialized models while offering more efficient joint training behavior. This model could potentially serve as a universal backbone for 3D reconstruction tasks. <div>
arXiv:2509.13414v1 Announce Type: new 
Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</title>
<link>https://arxiv.org/abs/2509.13474</link>
<guid>https://arxiv.org/abs/2509.13474</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Place Recognition, Robotics, Localization, LiDAR, Semantic-enhanced

Summary:<br /><br />Ensuring accurate localization of robots in GPS-denied environments is challenging. Current RGB-based Visual Place Recognition (VPR) methods are sensitive to environmental changes. This study introduces a Semantic-Enhanced Cross-Modal Place Recognition (SCM-PR) framework for robust localization in LiDAR maps. The proposed framework includes a VMamba backbone for feature extraction, Semantic-Aware Feature Fusion (SAFF) module, LiDAR descriptors with semantic information, and a cross-modal semantic attention mechanism in NetVLAD. Additionally, a Multi-View Semantic-Geometric Matching and Semantic Consistency Loss were implemented in a contrastive learning framework. Experimental results on KITTI and KITTI-360 datasets demonstrate that SCM-PR outperforms existing cross-modal place recognition methods, showcasing state-of-the-art performance. <div>
arXiv:2509.13474v1 Announce Type: new 
Abstract: Ensuring accurate localization of robots in environments without GPS capability is a challenging task. Visual Place Recognition (VPR) techniques can potentially achieve this goal, but existing RGB-based methods are sensitive to changes in illumination, weather, and other seasonal changes. Existing cross-modal localization methods leverage the geometric properties of RGB images and 3D LiDAR maps to reduce the sensitivity issues highlighted above. Currently, state-of-the-art methods struggle in complex scenes, fine-grained or high-resolution matching, and situations where changes can occur in viewpoint. In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB images for robust localization in LiDAR maps. Our proposed method introduces: a VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature Fusion (SAFF) module for using both place descriptors and segmentation masks; LiDAR descriptors that incorporate both semantics and geometry; and a cross-modal semantic attention mechanism in NetVLAD to improve matching. Incorporating the semantic information also was instrumental in designing a Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in a contrastive learning framework. Our experimental work on the KITTI and KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance compared to other cross-modal place recognition methods.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization</title>
<link>https://arxiv.org/abs/2509.13482</link>
<guid>https://arxiv.org/abs/2509.13482</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, data compression, neural networks, lattice vector quantization, scene-adaptive optimization

Summary:<br />
3D Gaussian Splatting (3DGS) is a popular rendering method with high quality but generates large amounts of data. Compressing this data is crucial for cost effectiveness. Previous compression methods relied on uniform scalar quantization (USQ). This study proposes replacing USQ with lattice vector quantization (LVQ) to improve compression performance. Scene-adaptive LVQ (SALVQ) optimizes the lattice basis for each scene, enhancing adaptability and rate-distortion (R-D) efficiency. SALVQ integrates seamlessly into existing 3DGS compression architectures, improving R-D performance with minimal modifications. It can dynamically adjust lattice density by scaling basis vectors, accommodating multiple bit rate targets within a single model. SALVQ eliminates the need for separate models for different compression levels, reducing training time and memory usage significantly. <div>
arXiv:2509.13482v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its photorealistic rendering quality and real-time performance, but it generates massive amounts of data. Hence compressing 3DGS data is necessary for the cost effectiveness of 3DGS models. Recently, several anchor-based neural compression methods have been proposed, achieving good 3DGS compression performance. However, they all rely on uniform scalar quantization (USQ) due to its simplicity. A tantalizing question is whether more sophisticated quantizers can improve the current 3DGS compression methods with very little extra overhead and minimal change to the system. The answer is yes by replacing USQ with lattice vector quantization (LVQ). To better capture scene-specific characteristics, we optimize the lattice basis for each scene, improving LVQ's adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a balance between the R-D efficiency of vector quantization and the low complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS compression architectures, enhancing their R-D performance with minimal modifications and computational overhead. Moreover, by scaling the lattice basis vectors, SALVQ can dynamically adjust lattice density, enabling a single model to accommodate multiple bit rate targets. This flexibility eliminates the need to train separate models for different compression levels, significantly reducing training time and memory consumption.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes</title>
<link>https://arxiv.org/abs/2509.13484</link>
<guid>https://arxiv.org/abs/2509.13484</guid>
<content:encoded><![CDATA[
<div> Keywords: social group detection, urban planning, image analysis, human detection, VLM-based reasoning<br />
Summary: <br />
Understanding group-level social interactions in public spaces is essential for urban planning and designing inclusive environments. The task of social group region detection involves interpreting complex visual cues related to interpersonal relationships. A three-stage pipeline called MINGLE (Modeling INterpersonal Group-Level Engagement) is proposed, combining human detection, depth estimation, VLM-based reasoning for social affiliation classification, and a spatial aggregation algorithm for group localization. A new dataset of 100k urban street-view images with annotations for individuals and groups is introduced, combining human-created labels and MINGLE pipeline outputs for comprehensive coverage of real-world scenarios. This dataset aims to support the social group region detection task and facilitate future research in the field. <br /> <div>
arXiv:2509.13484v1 Announce Type: new 
Abstract: Understanding group-level social interactions in public spaces is crucial for urban planning, informing the design of socially vibrant and inclusive environments. Detecting such interactions from images involves interpreting subtle visual cues such as relations, proximity, and co-movement - semantically complex signals that go beyond traditional object detection. To address this challenge, we introduce a social group region detection task, which requires inferring and spatially grounding visual regions defined by abstract interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf human detection and depth estimation, (2) VLM-based reasoning to classify pairwise social affiliation, and (3) a lightweight spatial aggregation algorithm to localize socially connected groups. To support this task and encourage future research, we present a new dataset of 100K urban street-view images annotated with bounding boxes and labels for both individuals and socially interacting groups. The annotations combine human-created labels and outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage of real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.13496</link>
<guid>https://arxiv.org/abs/2509.13496</guid>
<content:encoded><![CDATA[
<div> framework, BiasMap, latent concept-level representational biases, cross-attention attribution maps, spatial demographics-semantics concept entanglement <br />
Summary: 
BiasMap is proposed as a model-agnostic framework for uncovering latent concept-level representational biases in stable diffusion models, particularly in text-to-image models. It leverages cross-attention attribution maps to reveal the entanglements between demographics and semantics in image generation. The framework quantifies the spatial demographics-semantics concept entanglement using Intersection over Union (IoU), offering insights into hidden biases. Furthermore, BiasMap is utilized for bias mitigation by modifying the latent noise space through energy-guided diffusion sampling, minimizing concept-level coupling during the denoising process. The study demonstrates that existing fairness interventions may address distributional biases but fail to disentangle concept-level coupling, while the proposed mitigation method effectively mitigates concept entanglement in image generation. <div>
arXiv:2509.13496v1 Announce Type: new 
Abstract: Bias discovery is critical for black-box generative models, especiall text-to-image (TTI) models. Existing works predominantly focus on output-level demographic distributions, which do not necessarily guarantee concept representations to be disentangled post-mitigation. We propose BiasMap, a model-agnostic framework for uncovering latent concept-level representational biases in stable diffusion models. BiasMap leverages cross-attention attribution maps to reveal structural entanglements between demographics (e.g., gender, race) and semantics (e.g., professions), going deeper into representational bias during the image generation. Using attribution maps of these concepts, we quantify the spatial demographics-semantics concept entanglement via Intersection over Union (IoU), offering a lens into bias that remains hidden in existing fairness discovery approaches. In addition, we further utilize BiasMap for bias mitigation through energy-guided diffusion sampling that directly modifies latent noise space and minimizes the expected SoftIoU during the denoising process. Our findings show that existing fairness interventions may reduce the output distributional gap but often fail to disentangle concept-level coupling, whereas our mitigation method can mitigate concept entanglement in image generation while complementing distributional bias mitigation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming</title>
<link>https://arxiv.org/abs/2509.13504</link>
<guid>https://arxiv.org/abs/2509.13504</guid>
<content:encoded><![CDATA[
<div> Python, image annotation, real-time, machine learning, LivePyxel

Summary:<br />
The article introduces LivePyxel, a Python-based image annotation tool that supports real-time annotation for datasets collected from imaging systems like webcams and microscopes. Unlike existing tools, LivePyxel eliminates the need to pre-upload datasets, making it ideal for on-demand pipelines and real-time data acquisition in laboratory settings. The software offers a user-friendly interface with tools commonly found in commercial graphics editing software, including BÃ©zier splines and binary masks. It also supports non-destructive layers for high-performance editing. LivePyxel is compatible with various video devices and is optimized for object detection tasks using OpenCV and Numpy libraries for efficient matrix operations. By enabling seamless data collection and labeling, LivePyxel accelerates the development of AI models in experimental workflows. Read more at https://github.com/UGarCil/LivePyxel <br /><br /> <div>
arXiv:2509.13504v1 Announce Type: new 
Abstract: The lack of flexible annotation tools has hindered the deployment of AI models in some scientific areas. Most existing image annotation software requires users to upload a precollected dataset, which limits support for on-demand pipelines and introduces unnecessary steps to acquire images. This constraint is particularly problematic in laboratory environments, where real-time data acquisition from instruments such as microscopes is increasingly common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical user interface that integrates with imaging systems, such as webcams, microscopes, and others, to enable real-time image annotation. LivePyxel is designed to be easy to use through a simple interface that allows users to precisely delimit areas for annotation using tools commonly found in commercial graphics editing software. Of particular interest is the availability of B\'ezier splines and binary masks, and the software's capacity to work with non-destructive layers that enable high-performance editing. LivePyxel also integrates a wide compatibility across video devices, and it's optimized for object detection operations via the use of OpenCV in combination with high-performance libraries designed to handle matrix and linear algebra operations via Numpy effectively. LivePyxel facilitates seamless data collection and labeling, accelerating the development of AI models in experimental workflows. LivePyxel freely available at https://github.com/UGarCil/LivePyxel
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform</title>
<link>https://arxiv.org/abs/2509.13506</link>
<guid>https://arxiv.org/abs/2509.13506</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Virtual try-on, Fine-tuning, H-transform, Consistency loss

Summary:
DEFT-VTON introduces efficient fine-tuning using Doob's h-transform for adapting pre-trained models for virtual try-on (VTO) applications. By freezing most parameters and training a small h-transform network for conditional learning, DEFT significantly reduces the number of parameters that need training compared to traditional methods. Additionally, an adaptive consistency loss is proposed to further enhance performance and reduce inference time. This loss function combines consistency training with denoising score matching, resulting in a low-cost fine-tuning process for existing VTO models. Empirical results demonstrate that DEFT-VTON achieves state-of-the-art VTO performance with a minimal number of denoising steps, while maintaining competitive results. <div>
arXiv:2509.13506v1 Announce Type: new 
Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their established image synthesis abilities. Despite the extensive end-to-end training of large pre-trained models involved in current VTO methods, real-world applications often prioritize limited training and inference, serving, and deployment budgets for VTO. To solve this obstacle, we apply Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained unconditional models for downstream image-conditioned VTO abilities. DEFT freezes the pre-trained model's parameters and trains a small h-transform network to learn a conditional h-transform. The h-transform network allows training only 1.42 percent of the frozen parameters, compared to a baseline of 5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference time, we additionally propose an adaptive consistency loss. Consistency training distills slow but high-performing diffusion models into a fast one while retaining performance by enforcing consistencies along the inference path. Inspired by constrained optimization, instead of distillation, we combine the consistency loss and the denoising score matching loss in a data-adaptive manner for fine-tuning existing VTO models at a low cost. Empirical results show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO tasks, with as few as 15 denoising steps, while maintaining competitive results.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.13507</link>
<guid>https://arxiv.org/abs/2509.13507</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, synthetic data, data augmentation, pedestrian recognition, generative network architecture

Summary: 
This paper discusses the importance of synthetic data in autonomous driving, particularly for creating specific traffic scenarios that autonomous vehicles need to navigate. The use of synthetic data often leads to a gap between synthetic and real domains, which can impact the accuracy of pedestrian recognition. The authors propose a data augmentation pipeline to generate custom traffic scenarios with virtual pedestrians to enhance pedestrian recognition. They also introduce a novel generative network architecture for adversarial learning to improve the realism of the augmented data-set lighting conditions. The approach is evaluated on tasks of semantic and instance segmentation, demonstrating its effectiveness in improving pedestrian recognition in autonomous driving scenarios.<br /><br />Summary: <div>
arXiv:2509.13507v1 Announce Type: new 
Abstract: In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation</title>
<link>https://arxiv.org/abs/2509.13508</link>
<guid>https://arxiv.org/abs/2509.13508</guid>
<content:encoded><![CDATA[
<div> medical image enhancement, segmentation, deep learning, interpretable, FunKAN

Summary:
FunKAN is a novel neural framework designed for medical image processing, offering interpretable solutions by generalizing the Kolmogorov-Arnold representation theorem onto functional spaces. It utilizes Fourier decomposition over Hermite functions to learn inner functions and overcome the limitations of traditional deep learning approaches. The framework is applied to tasks such as Gibbs ringing suppression in magnetic resonance images and binary medical segmentation on datasets like BUSI, GlaS, and CVC-ClinicDB for breast cancer, glands, and polyps detection. FunKAN demonstrates superior performance in both image enhancement (PSNR, TV) and segmentation (IoU, F1) compared to other Kolmogorov-Arnold network-based backbones. By bridging the gap between theoretical function approximation and medical image analysis, FunKAN presents a robust and interpretable solution for clinical applications. <div>
arXiv:2509.13508v1 Announce Type: new 
Abstract: Medical image enhancement and segmentation are critical yet challenging tasks in modern clinical practice, constrained by artifacts and complex anatomical variations. Traditional deep learning approaches often rely on complex architectures with limited interpretability. While Kolmogorov-Arnold networks offer interpretable solutions, their reliance on flattened feature representations fundamentally disrupts the intrinsic spatial structure of imaging data. To address this issue we propose a Functional Kolmogorov-Arnold Network (FunKAN) -- a novel interpretable neural framework, designed specifically for image processing, that formally generalizes the Kolmogorov-Arnold representation theorem onto functional spaces and learns inner functions using Fourier decomposition over the basis Hermite functions. We explore FunKAN on several medical image processing tasks, including Gibbs ringing suppression in magnetic resonance images, benchmarking on IXI dataset. We also propose U-FunKAN as state-of-the-art binary medical segmentation model with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS (histological structures) and CVC-ClinicDB (colonoscopy videos), detecting breast cancer, glands and polyps, respectively. Experiments on those diverse datasets demonstrate that our approach outperforms other KAN-based backbones in both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work bridges the gap between theoretical function approximation and medical image analysis, offering a robust, interpretable solution for clinical applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Hate Detection Using Dual-Stream Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.13515</link>
<guid>https://arxiv.org/abs/2509.13515</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Hateful Video Classification, Multimodal Fusion, Structured Information, Explainability
Summary:
- Hateful videos pose significant risks to online safety and real-world well-being, necessitating effective detection methods.
- Existing multimodal classification approaches often overlook the importance of identifying hateful components within videos.
- The proposed dual-stream graph neural network model introduces an instance graph and a weight graph to extract instance-level features and highlight hateful instances.
- By systematically capturing structured relationships within and across modalities, the model demonstrates state-of-the-art performance in hateful video classification.
- The model also offers strong explainability, making it a valuable tool for understanding the classification process. 

Summary: <div>
arXiv:2509.13515v1 Announce Type: new 
Abstract: Hateful videos present serious risks to online safety and real-world well-being, necessitating effective detection methods. Although multimodal classification approaches integrating information from several modalities outperform unimodal ones, they typically neglect that even minimal hateful content defines a video's category. Specifically, they generally treat all content uniformly, instead of emphasizing the hateful components. Additionally, existing multimodal methods cannot systematically capture structured information in videos, limiting the effectiveness of multimodal fusion. To address these limitations, we propose a novel multimodal dual-stream graph neural network model. It constructs an instance graph by separating the given video into several instances to extract instance-level features. Then, a complementary weight graph assigns importance weights to these features, highlighting hateful instances. Importance weights and instance features are combined to generate video labels. Our model employs a graph-based framework to systematically model structured relationships within and across modalities. Extensive experiments on public datasets show that our model is state-of-the-art in hateful video classification and has strong explainability. Code is available: https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors</title>
<link>https://arxiv.org/abs/2509.13525</link>
<guid>https://arxiv.org/abs/2509.13525</guid>
<content:encoded><![CDATA[
<div> Depth estimation, Colonoscopy, 3D reconstruction, Temporal consistency, Synthetic training <br />
Summary:
ColonCrafter is a new model designed for 3D scene understanding in colonoscopy, focusing on depth estimation. By utilizing diffusion-based techniques and learning from synthetic colonoscopy sequences, ColonCrafter is able to generate accurate and temporally consistent depth maps from monocular videos. The model also incorporates a style transfer method to adapt real clinical videos to the synthetic training domain while preserving geometric structure. ColonCrafter outperforms existing approaches and achieves state-of-the-art zero-shot performance on the C3VD dataset. While full trajectory 3D reconstruction remains a challenge, the model demonstrates clinically relevant applications such as 3D point cloud generation and surface coverage assessment. The research highlights the importance of automated methods for accurate depth estimation in colonoscopy and presents a promising solution with potential clinical impact. <br /><br />Summary: <div>
arXiv:2509.13525v1 Announce Type: new 
Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents significant challenges that necessitate automated methods for accurate depth estimation. However, existing depth estimation models for endoscopy struggle with temporal consistency across video sequences, limiting their applicability for 3D reconstruction. We present ColonCrafter, a diffusion-based depth estimation model that generates temporally consistent depth maps from monocular colonoscopy videos. Our approach learns robust geometric priors from synthetic colonoscopy sequences to generate temporally consistent depth maps. We also introduce a style transfer technique that preserves geometric structure while adapting real clinical videos to match our synthetic training domain. ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD dataset, outperforming both general-purpose and endoscopy-specific approaches. Although full trajectory 3D reconstruction remains a challenge, we demonstrate clinically relevant applications of ColonCrafter, including 3D point cloud generation and surface coverage assessment.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM</title>
<link>https://arxiv.org/abs/2509.13536</link>
<guid>https://arxiv.org/abs/2509.13536</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, rendering, reconstruction, embedded platforms, micro air vehicles<br />
Summary:<br />
This paper introduces enhancements to 3D Gaussian Splatting (3DGS) for rendering and reconstruction techniques, with a focus on applications for embedded platforms like micro air vehicles (MAVs). The improvements aim to reduce GPU memory usage and enhance rendering quality. By merging redundant 3D Gaussian primitives in voxel space based on geometric similarity, the method decreases GPU memory usage without affecting system performance. Additionally, rendering quality is enhanced by initializing 3D Gaussian primitives through Patch-Grid (PG) point sampling, improving the accuracy of scene modeling. Evaluations on public datasets show the effectiveness of these enhancements in improving both memory utilization and rendering quality. <br /><br />Summary: <div>
arXiv:2509.13536v1 Announce Type: new 
Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.13577</link>
<guid>https://arxiv.org/abs/2509.13577</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory prediction, autonomous vehicles, out-of-distribution detection, quickest change detection, adaptive mechanisms

Summary:
In the field of autonomous vehicles, accurate trajectory prediction is crucial for safe operation. However, models may encounter shifts in data distribution when faced with rare or underrepresented traffic scenarios, leading to out-of-distribution (OOD) cases. While previous research has focused on OOD detection in computer vision tasks, trajectory-level OOD detection has been overlooked. A new framework is proposed to address this issue, incorporating adaptive mechanisms to enhance detection in complex driving environments. The framework accounts for evolving error distributions on in-distribution samples, resulting in improved detection delay and false alarm rates. Through empirical analysis on real-world datasets, the method outperforms existing approaches in both accuracy and computational efficiency. This advancement offers a practical solution for reliable and driving-aware autonomy.<br /><br />Summary: <div>
arXiv:2509.13577v1 Announce Type: new 
Abstract: Trajectory prediction is central to the safe and seamless operation of autonomous vehicles (AVs). In deployment, however, prediction models inevitably face distribution shifts between training data and real-world conditions, where rare or underrepresented traffic scenarios induce out-of-distribution (OOD) cases. While most prior OOD detection research in AVs has concentrated on computer vision tasks such as object detection and segmentation, trajectory-level OOD detection remains largely underexplored. A recent study formulated this problem as a quickest change detection (QCD) task, providing formal guarantees on the trade-off between detection delay and false alarms [1]. Building on this foundation, we propose a new framework that introduces adaptive mechanisms to achieve robust detection in complex driving environments. Empirical analysis across multiple real-world datasets reveals that prediction errors -- even on in-distribution samples -- exhibit mode-dependent distributions that evolve over time with dataset-specific dynamics. By explicitly modeling these error modes, our method achieves substantial improvements in both detection delay and false alarm rates. Comprehensive experiments on established trajectory prediction benchmarks show that our framework significantly outperforms prior UQ- and vision-based OOD approaches in both accuracy and computational efficiency, offering a practical path toward reliable, driving-aware autonomy.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection</title>
<link>https://arxiv.org/abs/2509.13586</link>
<guid>https://arxiv.org/abs/2509.13586</guid>
<content:encoded><![CDATA[
<div> rain forest, deforestation, Amazon, Earth observation satellites, deep learning

Summary:
The paper presents a method for detecting deforestation in the Amazon rain forest using image pairs from Earth observation satellites. Deep learning techniques are utilized to compare images at different dates and identify changes in forest cover. A visual semantic model automatically annotates detected changes with relevant keywords extracted from scientific documents related to the Amazon region. The approach is evaluated on a dataset of Amazon image pairs, demonstrating its effectiveness in detecting deforestation and generating relevant annotations. The method serves as a valuable tool for monitoring and studying the impact of deforestation in the Amazon, and its applicability extends beyond environmental applications to other domains. <div>
arXiv:2509.13586v1 Announce Type: new 
Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in regulating the Earth's climate and providing habitat for countless species. Deforestation in the Amazon is a major concern as it has a significant impact on global carbon emissions and biodiversity. In this paper, we present a method for detecting deforestation in the Amazon using image pairs from Earth observation satellites. Our method leverages deep learning techniques to compare the images of the same area at different dates and identify changes in the forest cover. We also propose a visual semantic model that automatically annotates the detected changes with relevant keywords. The candidate annotation for images are extracted from scientific documents related to the Amazon region. We evaluate our approach on a dataset of Amazon image pairs and demonstrate its effectiveness in detecting deforestation and generating relevant annotations. Our method provides a useful tool for monitoring and studying the impact of deforestation in the Amazon. While we focus on environment applications of our work by using images of deforestation in the Amazon rain forest to demonstrate the effectiveness of our proposed approach, it is generic enough to be applied to other domains.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation</title>
<link>https://arxiv.org/abs/2509.13590</link>
<guid>https://arxiv.org/abs/2509.13590</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, healthcare imaging, multimodal framework, Vision-Language Models, tumor detection

Summary:
The article introduces an intelligent multimodal framework for medical image analysis that incorporates Vision-Language Models (VLMs) to enhance diagnostic processes in healthcare imaging. The framework utilizes Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across various imaging modalities. By combining visual feature extraction with natural language processing, the system enables contextual image interpretation and anomaly detection with high accuracy. The framework also integrates coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution, resulting in precise anomaly detection with 80 pixels average deviation. Additionally, the system includes user-friendly interfaces for clinical workflow integration and exhibits zero-shot learning capabilities to reduce reliance on large datasets. While the framework shows promising results in improving diagnostic support and radiological workflow efficiency, further clinical validation and multi-center evaluation are necessary before widespread adoption. 

<br /><br />Summary: <div>
arXiv:2509.13590v1 Announce Type: new 
Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging has revolutionized diagnostic medicine and clinical decision-making processes. This work presents an intelligent multimodal framework for medical image analysis that leverages Vision-Language Models (VLMs) in healthcare diagnostics. The framework integrates Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across multiple imaging modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual feature extraction with natural language processing to enable contextual image interpretation, incorporating coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution. Multi-layered visualization techniques generate detailed medical illustrations, overlay comparisons, and statistical representations to enhance clinical confidence, with location measurement achieving 80 pixels average deviation. Result processing utilizes precise prompt engineering and textual analysis to extract structured clinical information while maintaining interpretability. Experimental evaluations demonstrated high performance in anomaly detection across multiple modalities. The system features a user-friendly Gradio interface for clinical workflow integration and demonstrates zero-shot learning capabilities to reduce dependence on large datasets. This framework represents a significant advancement in automated diagnostic support and radiological workflow efficiency, though clinical validation and multi-center evaluation are necessary prior to widespread adoption.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC &amp; Hough Transforms</title>
<link>https://arxiv.org/abs/2509.13605</link>
<guid>https://arxiv.org/abs/2509.13605</guid>
<content:encoded><![CDATA[
<div> Localization, CLAP, 2D, 3D, Image Stitching <br />
Summary:<br /> 
The article introduces an extension of the CLAP algorithm, originally designed for 2D localization, to 3D localization and image stitching. CLAP, known for its robustness against outliers, uses clustering to suppress noise and improve accuracy. This approach differs from traditional methods like RANSAC, which rely on reprojection error for outlier rejection. The study also explores the relationship between CLAP, RANSAC, and Hough transforms. The generalized CLAP framework offers a versatile solution for handling noise and uncertainty in various fields. The algorithm's performance was highlighted during a championship win in the RoboCup 2024 competition, showcasing its effectiveness in autonomous humanoid soccer scenarios. <div>
arXiv:2509.13605v1 Announce Type: new 
Abstract: In previous work, we introduced a 2D localization algorithm called CLAP, Clustering to Localize Across $n$ Possibilities, which was used during our championship win in RoboCup 2024, an international autonomous humanoid soccer competition. CLAP is particularly recognized for its robustness against outliers, where clustering is employed to suppress noise and mitigate against erroneous feature matches. This clustering-based strategy provides an alternative to traditional outlier rejection schemes such as RANSAC, in which candidates are validated by reprojection error across all data points. In this paper, CLAP is extended to a more general framework beyond 2D localization, specifically to 3D localization and image stitching. We also show how CLAP, RANSAC, and Hough transforms are related. The generalization of CLAP is widely applicable to many different fields and can be a useful tool to deal with noise and uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMIR, an efficient registration framework via robust feature learning from SAM</title>
<link>https://arxiv.org/abs/2509.13629</link>
<guid>https://arxiv.org/abs/2509.13629</guid>
<content:encoded><![CDATA[
<div> image registration, medical image analysis, weakly supervised methods, visual foundation models, feature extraction

Summary:<br />
This paper introduces SAMIR, a medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on natural image datasets and can learn robust visual representations. SAMIR uses SAM's image encoder to extract structure-aware feature embeddings, improving modeling of anatomical consistency and deformation patterns. A 3D head refines features within the embedding space to adapt to local deformations. A Hierarchical Feature Consistency Loss guides coarse-to-fine feature matching for better anatomical alignment. SAMIR outperforms state-of-the-art methods on intra-subject cardiac image registration and inter-subject abdomen CT image registration datasets, achieving significant performance improvements. The source code will be made publicly available on GitHub after acceptance. <br /> <div>
arXiv:2509.13629v1 Announce Type: new 
Abstract: Image registration is a fundamental task in medical image analysis. Deformations are often closely related to the morphological characteristics of tissues, making accurate feature extraction crucial. Recent weakly supervised methods improve registration by incorporating anatomical priors such as segmentation masks or landmarks, either as inputs or in the loss function. However, such weak labels are often not readily available, limiting their practical use. Motivated by the strong representation learning ability of visual foundation models, this paper introduces SAMIR, an efficient medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on large-scale natural image datasets and can learn robust, general-purpose visual representations. Rather than using raw input images, we design a task-specific adaptation pipeline using SAM's image encoder to extract structure-aware feature embeddings, enabling more accurate modeling of anatomical consistency and deformation patterns. We further design a lightweight 3D head to refine features within the embedding space, adapting to local deformations in medical images. Additionally, we introduce a Hierarchical Feature Consistency Loss to guide coarse-to-fine feature matching and improve anatomical alignment. Extensive experiments demonstrate that SAMIR significantly outperforms state-of-the-art methods on benchmark datasets for both intra-subject cardiac image registration and inter-subject abdomen CT image registration, achieving performance improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code will be publicly available on GitHub following the acceptance of this paper.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery</title>
<link>https://arxiv.org/abs/2509.13631</link>
<guid>https://arxiv.org/abs/2509.13631</guid>
<content:encoded><![CDATA[
<div> Keywords: deforestation, satellite images, Federated Learning, distributed approach, image segmentation

Summary:
This paper presents a novel distributed approach using Federated Learning (FL) to accurately identify and locate deforestation in satellite images across different clients. FL allows network clients to collaborate in training a model while ensuring data privacy and security. Each client represents an edge satellite center responsible for local data processing. By leveraging the FLOWER and RAY frameworks, an efficient distributed learning workload execution is achieved. The RAY framework enables precise client spawning by selecting a specific number of users to create an emulation environment. The FL framework utilizes models like YOLOS-small, Faster R-CNN with a ResNet50 backbone, and Faster R-CNN with a MobileNetV3 backbone that are trained and tested on publicly available datasets. This approach provides a new perspective on image segmentation tasks for satellite imagery. <div>
arXiv:2509.13631v1 Announce Type: new 
Abstract: Accurate identification of deforestation from satellite images is essential in order to understand the geographical situation of an area. This paper introduces a new distributed approach to identify as well as locate deforestation across different clients using Federated Learning (FL). Federated Learning enables distributed network clients to collaboratively train a model while maintaining data privacy and security of the active users. In our framework, a client corresponds to an edge satellite center responsible for local data processing. Moreover, FL provides an advantage over centralized training method which requires combining data, thereby compromising with data security of the clients. Our framework leverages the FLOWER framework with RAY framework to execute the distributed learning workload. Furthermore, efficient client spawning is ensured by RAY as it can select definite amount of users to create an emulation environment. Our FL framework uses YOLOS-small (a Vision Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN with a MobileNetV3 backbone models trained and tested on publicly available datasets. Our approach provides us a different view for image segmentation-based tasks on satellite imagery.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction</title>
<link>https://arxiv.org/abs/2509.13652</link>
<guid>https://arxiv.org/abs/2509.13652</guid>
<content:encoded><![CDATA[
<div> Estimating metric relative camera pose, two-view pose estimation, GARPS framework, metric monocular depth estimator, Gaussian scene reconstructor<br />
Summary:<br />
Estimating metric relative camera pose is crucial for 3D reconstruction and localisation. Conventional methods struggle with wide baselines and textureless or reflective surfaces as they are not metric. The GARPS framework addresses this by aligning two independently reconstructed 3D scenes. By leveraging a metric monocular depth estimator and Gaussian scene reconstructor, GARPS obtains a metric 3D Gaussian Mixture Model for each image and refines the initial pose using a differentiable GMM alignment objective. This objective considers geometric structure, view-independent colour, anisotropic covariance, and semantic feature consistency, making it robust to occlusions and texture-poor regions. GARPS outperforms classical and state-of-the-art methods like MASt3R on the Real-Estate10K dataset, showcasing the potential of integrating single-view perception with multi-view geometry for robust and metric relative pose estimation.<br /> <div>
arXiv:2509.13652v1 Announce Type: new 
Abstract: Estimating metric relative camera pose from a pair of images is of great importance for 3D reconstruction and localisation. However, conventional two-view pose estimation methods are not metric, with camera translation known only up to a scale, and struggle with wide baselines and textureless or reflective surfaces. This paper introduces GARPS, a training-free framework that casts this problem as the direct alignment of two independently reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model (GMM) for each image. It then refines an initial pose from a feed-forward two-view pose estimator by optimising a differentiable GMM alignment objective. This objective jointly considers geometric structure, view-independent colour, anisotropic covariance, and semantic feature consistency, and is robust to occlusions and texture-poor regions without requiring explicit 2D correspondences. Extensive experiments on the Real\-Estate10K dataset demonstrate that GARPS outperforms both classical and state-of-the-art learning-based methods, including MASt3R. These results highlight the potential of bridging single-view perception with multi-view geometry to achieve robust and metric relative pose estimation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Lookup Network</title>
<link>https://arxiv.org/abs/2509.13662</link>
<guid>https://arxiv.org/abs/2509.13662</guid>
<content:encoded><![CDATA[
<div> convolutional neural networks, lookup operation, energy consumption, inference speed, image classification <br />
Summary:<br />
This paper introduces a new efficient lookup operation as a basic operation for constructing neural networks. By using lookup tables instead of traditional multiplication operations, the proposed lookup networks are able to achieve higher efficiency in terms of energy consumption and inference speed. The lookup tables are constructed in a differentiable manner, allowing for end-to-end optimization. The research presents several training strategies to enhance the convergence of the lookup operation. Experimental results demonstrate that the lookup networks outperform traditional convolutional networks in tasks such as image classification, image super-resolution, and point cloud classification. The proposed method shows state-of-the-art performance on various tasks and data types, showcasing its effectiveness in improving computational efficiency without compromising performance.<br /> <div>
arXiv:2509.13662v1 Announce Type: new 
Abstract: Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires {more} energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2509.13676</link>
<guid>https://arxiv.org/abs/2509.13676</guid>
<content:encoded><![CDATA[
<div> semantic visual projector, Referring Image Segmentation, Multimodal Large Language Model, Segment Anything Model, superpixels

Summary:
- The article introduces a novel semantic visual projector for Referring Image Segmentation (RIS) frameworks that combines the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM).
- Current patch-wise visual projectors struggle to balance reducing the number of visual tokens and preserving semantic clarity in RIS due to visual token redundancy.
- The proposed semantic visual projector uses semantic superpixels generated by SAM to identify "visual words" in an image, compressing and projecting them as visual tokens.
- This approach significantly reduces visual tokens by 93% without compromising performance, speeding up MLLM training and inference.
- The method includes a semantic superpixel positional embedding to enhance MLLM's awareness of superpixel geometry and position, and a semantic superpixel aggregator to maintain fine-grained details within superpixels and global context outside. 
<br /><br />Summary: <div>
arXiv:2509.13676v1 Announce Type: new 
Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify "visual words" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM's awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras</title>
<link>https://arxiv.org/abs/2509.13681</link>
<guid>https://arxiv.org/abs/2509.13681</guid>
<content:encoded><![CDATA[
<div> autonomous driving, Bird's Eye View segmentation, fisheye cameras, distortion resilience, uncertainty estimation<br />
Summary:<br />
FishBEV is a novel Bird's Eye View segmentation framework designed specifically for fisheye cameras, addressing challenges such as geometric distortion, multi-view correspondences, and temporal dynamics. It introduces a Distortion-Resilient Multi-scale Extraction backbone for robust feature learning, an Uncertainty-aware Spatial Cross-Attention mechanism for reliable alignment, and a Distance-aware Temporal Self-Attention module for balancing near and far field context. Experimentation on the Synwoodscapes dataset showcases FishBEV's superior performance compared to state-of-the-art baselines for fisheye BEV segmentation tasks. <div>
arXiv:2509.13681v1 Announce Type: new 
Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV) segmentation has recently achieved remarkable progress with pinhole cameras. However, it is non-trivial to extend the existing methods to fisheye cameras with severe geometric distortion, ambiguous multi-view correspondences and unstable temporal dynamics, all of which significantly degrade BEV performance. To address these challenges, we propose FishBEV, a novel BEV segmentation framework specifically tailored for fisheye cameras. This framework introduces three complementary innovations, including a Distortion-Resilient Multi-scale Extraction (DRME) backbone that learns robust features under distortion while preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention (U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that adaptively balances near field details and far field context to ensure temporal coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that FishBEV consistently outperforms SOTA baselines, regarding the performance evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification</title>
<link>https://arxiv.org/abs/2509.13687</link>
<guid>https://arxiv.org/abs/2509.13687</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image classification, spline-based Kolmogorov-Arnold Networks, interpretable, resource-limited, gradient-weighted Class Activation Mapping 

Summary: 
Spline-based Kolmogorov-Arnold Networks (KANs) are introduced for medical image classification using limited and diverse datasets in clinical settings. Three models are developed: SBTAYLOR-KAN, SBRBF-KAN, and SBWAVELET-KAN, leveraging spline-based function approximation to capture local and global nonlinearities. The models were evaluated on various medical image datasets without preprocessing, showing strong generalization and stability, with SBTAYLOR-KAN achieving up to 98.93% accuracy. These models have a low parameter count, making them suitable for resource-constrained environments. Gradient-weighted Class Activation Mapping (Grad-CAM) is used for interpretability, highlighting relevant image regions. This framework provides an interpretable, lightweight, and generalizable solution for medical image classification, addressing challenges in clinical AI applications with limited data availability. 

Summary: <div>
arXiv:2509.13687v1 Announce Type: new 
Abstract: Effective and interpretable classification of medical images is a challenge in computer-aided diagnosis, especially in resource-limited clinical settings. This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for accurate medical image classification with limited, diverse datasets. The models include SBTAYLOR-KAN, integrating B-splines with Taylor series; SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN, embedding B-splines in Morlet wavelet transforms. These approaches leverage spline-based function approximation to capture both local and global nonlinearities. The models were evaluated on brain MRI, chest X-rays, tuberculosis X-rays, and skin lesion images without preprocessing, demonstrating the ability to learn directly from raw data. Extensive experiments, including cross-dataset validation and data reduction analysis, showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93% accuracy, with a balanced F1-score, maintaining over 86% accuracy using only 30% of the training data across three datasets. Despite class imbalance in the skin cancer dataset, experiments on both imbalanced and balanced versions showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy. Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50 with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872 trainable parameters, making it more suitable for constrained medical environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used for interpretability, highlighting relevant regions in medical images. This framework provides a lightweight, interpretable, and generalizable solution for medical image classification, addressing the challenges of limited datasets and data-scarce scenarios in clinical AI applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models</title>
<link>https://arxiv.org/abs/2509.13711</link>
<guid>https://arxiv.org/abs/2509.13711</guid>
<content:encoded><![CDATA[
<div> sensitivity, artistic style, protection strategy, diffusion models, style mimicry
Summary:<br /><br />The article discusses the misuse of generative models, specifically diffusion-based approaches, in replicating artistic styles, posing a threat to artists' creative labor. The study explores the sensitivity of cross-attention layers to artistic styles, proposing a protection strategy called StyleProtect to defend against fine-tuned diffusion models. By updating selected cross-attention layers, StyleProtect effectively safeguards unique styles of artworks and anime from malicious customization while maintaining imperceptibility. Experiments conducted on a curated dataset of artworks and anime demonstrate the method's promising performance in preventing style mimicry. This research aims to address the growing concern of protecting artists' original styles in the face of advancing generative models. <br /><br /> <div>
arXiv:2509.13711v1 Announce Type: new 
Abstract: The rapid advancement of generative models, particularly diffusion-based approaches, has inadvertently facilitated their potential for misuse. Such models enable malicious exploiters to replicate artistic styles that capture an artist's creative labor, personal vision, and years of dedication in an inexpensive manner. This has led to a rise in the need and exploration of methods for protecting artworks against style mimicry. Although generic diffusion models can easily mimic an artistic style, finetuning amplifies this capability, enabling the model to internalize and reproduce the style with higher fidelity and control. We hypothesize that certain cross-attention layers exhibit heightened sensitivity to artistic styles. Sensitivity is measured through activation strengths of attention layers in response to style and content representations, and assessing their correlations with features extracted from external models. Based on our findings, we introduce an efficient and lightweight protection strategy, StyleProtect, that achieves effective style defense against fine-tuned diffusion models by updating only selected cross-attention layers. Our experiments utilize a carefully curated artwork dataset based on WikiArt, comprising representative works from 30 artists known for their distinctive and influential styles and cartoon animations from the Anita dataset. The proposed method demonstrates promising performance in safeguarding unique styles of artworks and anime from malicious diffusion customization, while maintaining competitive imperceptibility.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry</title>
<link>https://arxiv.org/abs/2509.13713</link>
<guid>https://arxiv.org/abs/2509.13713</guid>
<content:encoded><![CDATA[
<div> motion-aware, uncertainty-aware, depth estimation, robotics, self-supervised <br />
<br />
Summary: 
The paper introduces UM-Depth, a framework for monocular depth estimation that enhances accuracy in dynamic object boundaries and textureless regions. The framework combines motion- and uncertainty-aware refinement to address challenges caused by input data uncertainty. UM-Depth employs a teacher-student training strategy that embeds uncertainty estimation in the training pipeline and network architecture to improve supervision where photometric signals are weak. Unlike previous methods, UM-Depth utilizes optical flow exclusively within the teacher network during training, eliminating the need for additional labeling demands and any runtime cost. Experimental results on KITTI and Cityscapes datasets demonstrate UM-Depth's efficacy, achieving state-of-the-art results in self-supervised depth and pose estimation on KITTI datasets. <div>
arXiv:2509.13713v1 Announce Type: new 
Abstract: Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera. In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels. However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy. To address this, we introduce UM-Depth, a framework that combines motion- and uncertainty-aware refinement to enhance depth accuracy at dynamic object boundaries and in textureless regions. Specifically, we develop a teacherstudent training strategy that embeds uncertainty estimation into both the training pipeline and network architecture, thereby strengthening supervision where photometric signals are weak. Unlike prior motion-aware approaches that incur inference-time overhead and rely on additional labels or auxiliary networks for real-time generation, our method uses optical flow exclusively within the teacher network during training, which eliminating extra labeling demands and any runtime cost. Extensive experiments on the KITTI and Cityscapes datasets demonstrate the effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves state-of-the-art results in both self-supervised depth and pose estimation on the KITTI datasets.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Query Selection Bias in Referring Video Object Segmentation</title>
<link>https://arxiv.org/abs/2509.13722</link>
<guid>https://arxiv.org/abs/2509.13722</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Video Object Segmentation, Query-based methods, Triple Query Former, Motion-aware aggregation modules, Cross-modal alignment

Summary: 
Triple Query Former (TQF) addresses the issue of query selection bias in Referring Video Object Segmentation (RVOS) by factorizing the referring query into three specialized components: appearance query, intra-frame interaction query, and inter-frame motion query. By dynamically constructing queries through linguistic cues and visual guidance, TQF improves the accuracy of object segmentation. The motion-aware aggregation modules, Intra-frame Interaction Aggregation, and Inter-frame Motion Aggregation enhance object token representations by incorporating spatial relations and temporal association. Experimental results on RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of the structured query design and motion-aware aggregation modules.<br /><br />Summary: <div>
arXiv:2509.13722v1 Announce Type: new 
Abstract: Recently, query-based methods have achieved remarkable performance in Referring Video Object Segmentation (RVOS) by using textual static object queries to drive cross-modal alignment. However, these static queries are easily misled by distractors with similar appearance or motion, resulting in \emph{query selection bias}. To address this issue, we propose Triple Query Former (TQF), which factorizes the referring query into three specialized components: an appearance query for static attributes, an intra-frame interaction query for spatial relations, and an inter-frame motion query for temporal association. Instead of relying solely on textual embeddings, our queries are dynamically constructed by integrating both linguistic cues and visual guidance. Furthermore, we introduce two motion-aware aggregation modules that enhance object token representations: Intra-frame Interaction Aggregation incorporates position-aware interactions among objects within a single frame, while Inter-frame Motion Aggregation leverages trajectory-guided alignment across frames to ensure temporal coherence. Extensive experiments on multiple RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our structured query design and motion-aware aggregation modules.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalized Visual Grounding with Instance-aware Joint Learning</title>
<link>https://arxiv.org/abs/2509.13747</link>
<guid>https://arxiv.org/abs/2509.13747</guid>
<content:encoded><![CDATA[
<div> InstanceVG, generalized visual grounding, multi-task learning, instance-aware capabilities, joint predictions, consistency predictions.<br />
<br />
Summary:
The paper introduces InstanceVG, a framework for generalized visual grounding tasks that combines Generalized Referring Expression Comprehension (GREC) and Generalized Segmentation (GRES) while incorporating instance-aware capabilities. By jointly training GREC and GRES, InstanceVG ensures consistent multi-granularity predictions and streamlines the process. It leverages instance queries to unify the predictions of instance-level boxes and masks and assigns each query a prior reference point for target matching, enhancing prediction consistency. InstanceVG outperforms existing methods on various datasets across four tasks, achieving state-of-the-art performance in evaluation metrics. The code and model will be available on GitHub, contributing to advancements in generalized visual grounding research. <br />
 <div>
arXiv:2509.13747v1 Announce Type: new 
Abstract: Generalized visual grounding tasks, including Generalized Referring Expression Comprehension (GREC) and Segmentation (GRES), extend the classical visual grounding paradigm by accommodating multi-target and non-target scenarios. Specifically, GREC focuses on accurately identifying all referential objects at the coarse bounding box level, while GRES aims for achieve fine-grained pixel-level perception. However, existing approaches typically treat these tasks independently, overlooking the benefits of jointly training GREC and GRES to ensure consistent multi-granularity predictions and streamline the overall process. Moreover, current methods often treat GRES as a semantic segmentation task, neglecting the crucial role of instance-aware capabilities and the necessity of ensuring consistent predictions between instance-level boxes and masks. To address these limitations, we propose InstanceVG, a multi-task generalized visual grounding framework equipped with instance-aware capabilities, which leverages instance queries to unify the joint and consistency predictions of instance-level boxes and masks. To the best of our knowledge, InstanceVG is the first framework to simultaneously tackle both GREC and GRES while incorporating instance-aware capabilities into generalized visual grounding. To instantiate the framework, we assign each instance query a prior reference point, which also serves as an additional basis for target matching. This design facilitates consistent predictions of points, boxes, and masks for the same instance. Extensive experiments obtained on ten datasets across four tasks demonstrate that InstanceVG achieves state-of-the-art performance, significantly surpassing the existing methods in various evaluation metrics. The code and model will be publicly available at https://github.com/Dmmm1997/InstanceVG.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval</title>
<link>https://arxiv.org/abs/2509.13754</link>
<guid>https://arxiv.org/abs/2509.13754</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image Person Retrieval, Full-Mode Fine-grained Alignment, Adaptive Similarity Distribution Matching, Explicit Fine-grained Alignment, Cross-modal matching

Summary:
The article introduces FMFA, a novel framework for Text-to-Image Person Retrieval that enhances global matching through explicit fine-grained alignment and implicit relational reasoning. By incorporating Adaptive Similarity Distribution Matching (A-SDM) and Explicit Fine-grained Alignment (EFA) modules, FMFA is able to address the challenge of achieving effective alignment between textual and visual modalities. A-SDM rectifies unmatched positive sample pairs by pulling them closer in the joint embedding space, while EFA strengthens explicit cross-modal interactions for precise alignment. The proposed method outperforms existing approaches on public datasets and does not require additional supervision. The code for FMFA is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2509.13754v1 Announce Type: new 
Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that aims to retrieve the most relevant person images based on a given text query. The key challenge in TIPR lies in achieving effective alignment between textual and visual modalities within a common latent space. To address this challenge, prior approaches incorporate attention mechanisms for implicit cross-modal local alignment. However, they lack the ability to verify whether all local features are correctly aligned. Moreover, existing methods primarily focus on hard negative samples during model updates, with the goal of refining distinctions between positive and negative pairs, often neglecting incorrectly matched positive pairs. To alleviate these issues, we propose FMFA, a cross-modal Full-Mode Fine-grained Alignment framework, which enhances global matching through explicit fine-grained alignment and existing implicit relational reasoning -- hence the term ``full-mode" -- without requiring additional supervision. Specifically, we design an Adaptive Similarity Distribution Matching (A-SDM) module to rectify unmatched positive sample pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint embedding space, thereby achieving more precise global alignment. Additionally, we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up for the lack of verification capability of implicit relational reasoning. EFA strengthens explicit cross-modal fine-grained interactions by sparsifying the similarity matrix and employs a hard coding method for local alignment. Our proposed method is evaluated on three public datasets, achieving state-of-the-art performance among all global matching methods. Our code is available at https://github.com/yinhao1102/FMFA.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable-Continuous Color Editing in Diffusion Model via Color Mapping</title>
<link>https://arxiv.org/abs/2509.13756</link>
<guid>https://arxiv.org/abs/2509.13756</guid>
<content:encoded><![CDATA[
<div> Keywords: text-driven image editing, color editing, precision, continuous control, color mapping module

Summary: 
The article discusses the challenges faced in color editing in text-driven image editing due to the ambiguity and discreteness of natural language. It highlights the limitations of linearly interpolating embedding vectors for color changes and the lack of control over color range in output images. To address these issues, a color mapping module is introduced, which models the correspondence between text embedding space and image RGB values. This module predicts the embedding vector based on a given RGB value, allowing precise color control while maintaining semantic consistency. Users can specify a target RGB range for generating images with continuous color variations within the desired range. Experimental results show improved color continuity and controllability, enhancing the overall performance of the method in color editing. 

<br /><br />Summary: <div>
arXiv:2509.13756v1 Announce Type: new 
Abstract: In recent years, text-driven image editing has made significant progress. However, due to the inherent ambiguity and discreteness of natural language, color editing still faces challenges such as insufficient precision and difficulty in achieving continuous control. Although linearly interpolating the embedding vectors of different textual descriptions can guide the model to generate a sequence of images with varying colors, this approach lacks precise control over the range of color changes in the output images. Moreover, the relationship between the interpolation coefficient and the resulting image color is unknown and uncontrollable. To address these issues, we introduce a color mapping module that explicitly models the correspondence between the text embedding space and image RGB values. This module predicts the corresponding embedding vector based on a given RGB value, enabling precise color control of the generated images while maintaining semantic consistency. Users can specify a target RGB range to generate images with continuous color variations within the desired range, thereby achieving finer-grained, continuous, and controllable color editing. Experimental results demonstrate that our method performs well in terms of color continuity and controllability.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Prompt Refinement for Safer Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.13760</link>
<guid>https://arxiv.org/abs/2509.13760</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image, safety, prompt refinement, Vision Language Models, dataset

Summary:
Text-to-Image (T2I) models have shown progress in generating images from text but still rely heavily on prompt phrasing for output quality and safety. Existing safety methods often overlook generated images, leading to unsafe outputs or unnecessary prompt changes. This study introduces an iterative prompt refinement algorithm that leverages Vision Language Models (VLMs) to analyze input prompts and generated images, enhancing safety and aligning with user intent. The algorithm refines prompts effectively by incorporating visual feedback, offering a practical solution for generating safer T2I content. The authors also introduce a new dataset labeled with textual and visual safety signals and demonstrate the effectiveness of their approach through experimental results. This work provides a method to enhance T2I safety without compromising user intent, bridging the gap between text prompts and image generation. The code is available for further exploration. 

Summary: <br /><br />Text-to-Image models have made progress in generating images from text prompts, but their quality and safety heavily rely on prompt phrasing. Safety methods often overlook images, leading to unsafe outputs or unnecessary prompt changes. An iterative prompt refinement algorithm using Vision Language Models is proposed to enhance safety and align with user intent. A new dataset labeled with safety signals is introduced for supervised fine-tuning. Experimental results demonstrate enhanced safety without compromising user intent, providing a practical solution for safer T2I content generation. The code is available for exploration. <div>
arXiv:2509.13760v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images from text prompts, but their output quality and safety still depend heavily on how prompts are phrased. Existing safety methods typically refine prompts using large language models (LLMs), but they overlook the images produced, which can result in unsafe outputs or unnecessary changes to already safe prompts. To address this, we propose an iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both the input prompts and the generated images. By leveraging visual feedback, our method refines prompts more effectively, improving safety while maintaining user intent and reliability comparable to existing LLM-based approaches. Additionally, we introduce a new dataset labeled with both textual and visual safety signals using off-the-shelf multi-modal LLM, enabling supervised fine-tuning. Experimental results demonstrate that our approach produces safer outputs without compromising alignment with user intent, offering a practical solution for generating safer T2I content. Our code is available at https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper contains examples of harmful or inappropriate images generated by models.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Aware Image Signal Processor for Advanced Visual Perception</title>
<link>https://arxiv.org/abs/2509.13762</link>
<guid>https://arxiv.org/abs/2509.13762</guid>
<content:encoded><![CDATA[
<div> RAW sensor data, computer vision, image signal processing, object detection, segmentation<br />
<br />
Summary: 
The article introduces a new approach called Task-Aware Image Signal Processing (TA-ISP) for processing RAW sensor data in computer vision applications. The traditional methods of enhancing visual quality or using dense convolutional pipelines for processing RAW data have limitations in terms of computational overhead and representational capacity. TA-ISP addresses these issues by generating lightweight modulation operators that can control image statistics at different scales, allowing for a wider range of spatial transformations while keeping memory and computation requirements low. The proposed framework improves object detection and segmentation accuracy on various benchmarks, particularly in challenging conditions like nighttime. It also reduces the parameter count and inference time, making it suitable for deployment on devices with limited resources.<br /><br />Summary: <div>
arXiv:2509.13762v1 Announce Type: new 
Abstract: In recent years, there has been a growing trend in computer vision towards exploiting RAW sensor data, which preserves richer information compared to conventional low-bit RGB images. Early studies mainly focused on enhancing visual quality, while more recent efforts aim to leverage the abundant information in RAW data to improve the performance of visual perception tasks such as object detection and segmentation. However, existing approaches still face two key limitations: large-scale ISP networks impose heavy computational overhead, while methods based on tuning traditional ISP pipelines are restricted by limited representational capacity.To address these issues, we propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB framework that produces task-oriented representations for pretrained vision models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small set of lightweight, multi-scale modulation operators that act at global, regional, and pixel scales to reshape image statistics across different spatial extents. This factorized control significantly expands the range of spatially varying transforms that can be represented while keeping memory usage, computation, and latency tightly constrained. Evaluated on several RAW-domain detection and segmentation benchmarks under both daytime and nighttime conditions, TA-ISP consistently improves downstream accuracy while markedly reducing parameter count and inference time, making it well suited for deployment on resource-constrained devices.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset</title>
<link>https://arxiv.org/abs/2509.13766</link>
<guid>https://arxiv.org/abs/2509.13766</guid>
<content:encoded><![CDATA[
<div> rain streak artifacts, low-light conditions, nighttime deraining, spatial contextual information, NSR dataset <br />
Summary: <br />
The paper introduces a novel Nighttime Deraining Location-enhanced Perceptual Network (NDLPNet) designed to address visual degradation caused by rain streak artifacts in low-light conditions, which can impact nighttime surveillance and autonomous navigation. The NDLPNet includes a Position Perception Module (PPM) to capture spatial contextual information and density distribution of rain streaks, improving the model's ability to identify important feature channels. A night scene rainy (NSR) dataset comprising 900 image pairs from real-world nighttime scenes is created as a benchmark for research on nighttime deraining. Experimental evaluations show that NDLPNet outperforms existing methods in nighttime deraining tasks, effectively removing rain streaks while preserving background information. The source code and dataset are available for further research. <div>
arXiv:2509.13766v1 Announce Type: new 
Abstract: Visual degradation caused by rain streak artifacts in low-light conditions significantly hampers the performance of nighttime surveillance and autonomous navigation. Existing image deraining techniques are primarily designed for daytime conditions and perform poorly under nighttime illumination due to the spatial heterogeneity of rain distribution and the impact of light-dependent stripe visibility. In this paper, we propose a novel Nighttime Deraining Location-enhanced Perceptual Network(NDLPNet) that effectively captures the spatial positional information and density distribution of rain streaks in low-light environments. Specifically, we introduce a Position Perception Module (PPM) to capture and leverage spatial contextual information from input data, enhancing the model's capability to identify and recalibrate the importance of different feature channels. The proposed nighttime deraining network can effectively remove the rain streaks as well as preserve the crucial background information. Furthermore, We construct a night scene rainy (NSR) dataset comprising 900 image pairs, all based on real-world nighttime scenes, providing a new benchmark for nighttime deraining task research. Extensive qualitative and quantitative experimental evaluations on both existing datasets and the NSR dataset consistently demonstrate our method outperform the state-of-the-art (SOTA) methods in nighttime deraining tasks. The source code and dataset is available at https://github.com/Feecuin/NDLPNet.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI</title>
<link>https://arxiv.org/abs/2509.13767</link>
<guid>https://arxiv.org/abs/2509.13767</guid>
<content:encoded><![CDATA[
<div> framework, rtMRI, multimodal, segmentation, Vocal<br />
Summary:<br />
- VocSegMRI is introduced, a framework for segmenting articulatory structures in real-time magnetic resonance imaging (rtMRI) that integrates video, audio, and phonological inputs through cross-attention fusion for dynamic feature alignment.
- A contrastive learning objective is incorporated to improve segmentation performance even in the absence of audio modality during inference.
- The approach achieves state-of-the-art performance on a sub-set of USC-75 rtMRI dataset, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance (HD_95) of 4.20 mm, surpassing unimodal and multimodal baselines.
- Ablation studies demonstrate the importance of cross-attention and contrastive learning in enhancing segmentation precision and robustness.
- The results emphasize the significance of integrative multimodal modeling for accurate analysis of the vocal tract. 
<br /><br />Summary: <div>
arXiv:2509.13767v1 Announce Type: new 
Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance imaging (rtMRI) remains challenging, as most existing methods rely almost entirely on visual cues. Yet synchronized acoustic and phonological signals provide complementary context that can enrich visual information and improve precision. In this paper, we introduce VocSegMRI, a multimodal framework that integrates video, audio, and phonological inputs through cross-attention fusion for dynamic feature alignment. To further enhance cross-modal representation, we incorporate a contrastive learning objective that improves segmentation performance even when the audio modality is unavailable at inference. Evaluated on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance (HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines. Ablation studies confirm the contributions of cross-attention and contrastive learning to segmentation precision and robustness. These results highlight the value of integrative multimodal modeling for accurate vocal tract analysis.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Image Coding with Diffusion Prior</title>
<link>https://arxiv.org/abs/2509.13768</link>
<guid>https://arxiv.org/abs/2509.13768</guid>
<content:encoded><![CDATA[
<div> compression, generative coding, diffusion priors, pretrained models, visual fidelity
Summary: 
 1. The article introduces a novel generative coding framework that utilizes diffusion priors to improve compression performance at low bitrates. 
 2. The framework incorporates a pre-optimized encoder, a lightweight adapter, and an attentive fusion module to integrate with pretrained models' internal features effectively.
 3. The proposed method surpasses existing techniques in visual fidelity at low compression ratios.
 4. It demonstrates superior compression performance, exhibiting up to a 79% improvement over H.266/VVC.
 5. The framework is not only suitable for AI-generated content but can also be adapted for a wide range of content types efficiently.<br /><br />Summary: <div>
arXiv:2509.13768v1 Announce Type: new 
Abstract: As generative technologies advance, visual content has evolved into a complex mix of natural and AI-generated images, driving the need for more efficient coding techniques that prioritize perceptual quality. Traditional codecs and learned methods struggle to maintain subjective quality at high compression ratios, while existing generative approaches face challenges in visual fidelity and generalization. To this end, we propose a novel generative coding framework leveraging diffusion priors to enhance compression performance at low bitrates. Our approach employs a pre-optimized encoder to generate generalized compressed-domain representations, integrated with the pretrained model's internal features via a lightweight adapter and an attentive fusion module. This framework effectively leverages existing pretrained diffusion models and enables efficient adaptation to different pretrained models for new requirements with minimal retraining costs. We also introduce a distribution renormalization method to further enhance reconstruction fidelity. Extensive experiments show that our method (1) outperforms existing methods in visual fidelity across low bitrates, (2) improves compression performance by up to 79% over H.266/VVC, and (3) offers an efficient solution for AI-generated content while being adaptable to broader content types.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.13769</link>
<guid>https://arxiv.org/abs/2509.13769</guid>
<content:encoded><![CDATA[
<div> keywords: AdaThinkDrive, VLA framework, autonomous driving, reasoning mechanism, adaptive reasoning

Summary:
AdaThinkDrive is a new Vision Language Action framework designed for autonomous driving. It incorporates a dual-mode reasoning mechanism inspired by fast and slow thinking, allowing the model to selectively apply reasoning based on scenario complexity. The framework is pretrained on large-scale autonomous driving datasets and fine-tuned using a two-mode dataset for supervised training. An Adaptive Think Reward strategy is implemented to encourage the model to use reasoning effectively. Experimental results on the Navsim benchmark demonstrate that AdaThinkDrive outperforms vision-only baselines in terms of decision-making accuracy, achieving a high PDMS score. Furthermore, the framework shows improvements in both efficiency and accuracy compared to baseline models, highlighting its ability to balance the two factors through adaptive reasoning.<br /><br />Summary: <div>
arXiv:2509.13769v1 Announce Type: new 
Abstract: While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization</title>
<link>https://arxiv.org/abs/2509.13776</link>
<guid>https://arxiv.org/abs/2509.13776</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, manipulated regions, localization, morphological operations, forgery localization

Summary: 
In the pursuit of improving deepfake detection accuracy, precise localization of manipulated regions is becoming increasingly important. Current classification-based detection methods often struggle with accurately pinpointing forged areas. By incorporating both local detail and global semantic context, a novel approach has been proposed to independently predict manipulated regions. This approach utilizes morphological operations to fuse outputs, effectively reducing noise and improving spatial coherence. Through extensive experiments, the effectiveness of each module in enhancing forgery localization accuracy and robustness has been demonstrated. This innovative method addresses the challenges of accurately localizing forged areas and highlights the significance of collaborating both local and global perspectives in deepfake detection. <br /><br />Summary: <div>
arXiv:2509.13776v1 Announce Type: new 
Abstract: While the pursuit of higher accuracy in deepfake detection remains a central goal, there is an increasing demand for precise localization of manipulated regions. Despite the remarkable progress made in classification-based detection, accurately localizing forged areas remains a significant challenge. A common strategy is to incorporate forged region annotations during model training alongside manipulated images. However, such approaches often neglect the complementary nature of local detail and global semantic context, resulting in suboptimal localization performance. Moreover, an often-overlooked aspect is the fusion strategy between local and global predictions. Naively combining the outputs from both branches can amplify noise and errors, thereby undermining the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently predicts manipulated regions using both local and global perspectives. We employ morphological operations to fuse the outputs, effectively suppressing noise while enhancing spatial coherence. Extensive experiments reveal the effectiveness of each module in improving the accuracy and robustness of forgery localization.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling</title>
<link>https://arxiv.org/abs/2509.13784</link>
<guid>https://arxiv.org/abs/2509.13784</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, high-speed vision tasks, Variable-Rate Spatial Event Mamba, temporal modeling, adaptive processing speed<br />
Summary: <br />
Event cameras offer high temporal resolution for vision tasks. Existing methods convert event streams into intermediate representations, leading to window latency. Pointwise detection methods are computationally expensive. The Variable-Rate Spatial Event Mamba architecture processes raw event streams directly without intermediate representations. It includes a causal spatial neighborhood encoder and Mamba-based state space models for efficient local geometric relation capturing and linear complexity temporal modeling. An adaptive controller adjusts processing speed based on event rate, balancing window latency and inference latency effectively. <div>
arXiv:2509.13784v1 Announce Type: new 
Abstract: Event cameras capture asynchronous pixel-level brightness changes with microsecond temporal resolution, offering unique advantages for high-speed vision tasks. Existing methods often convert event streams into intermediate representations such as frames, voxel grids, or point clouds, which inevitably require predefined time windows and thus introduce window latency. Meanwhile, pointwise detection methods face computational challenges that prevent real-time efficiency due to their high computational cost. To overcome these limitations, we propose the Variable-Rate Spatial Event Mamba, a novel architecture that directly processes raw event streams without intermediate representations. Our method introduces a lightweight causal spatial neighborhood encoder to efficiently capture local geometric relations, followed by Mamba-based state space models for scalable temporal modeling with linear complexity. During inference, a controller adaptively adjusts the processing speed according to the event rate, achieving an optimal balance between window latency and inference latency.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching</title>
<link>https://arxiv.org/abs/2509.13789</link>
<guid>https://arxiv.org/abs/2509.13789</guid>
<content:encoded><![CDATA[
<div> Block-Wise Caching, Diffusion Transformers, Video Generation, Latency Reduction, Computational Efficiency
Summary:
Block-Wise Caching (BWCache) is proposed as a training-free method to accelerate video generation using Diffusion Transformers (DiTs). By dynamically caching and reusing features from DiT blocks across diffusion timesteps, BWCache addresses the latency issues associated with sequential denoising processes. An indicator is introduced to trigger feature reuse only when differences between block features at adjacent timesteps fall below a threshold, minimizing redundant computations while maintaining visual fidelity. Experimental results show that BWCache can achieve up to 2.24x speedup in video diffusion models while preserving visual quality. This approach effectively reduces computational redundancy in DiT blocks, resulting in faster video generation without compromising on visual fidelity. <br /><br />Summary: <div>
arXiv:2509.13789v1 Announce Type: new 
Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\times$ speedup with comparable visual quality.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation</title>
<link>https://arxiv.org/abs/2509.13792</link>
<guid>https://arxiv.org/abs/2509.13792</guid>
<content:encoded><![CDATA[
<div> Spacecraft Pose Estimation, Autonomous Space Operations, Supervised Domain Adaptation, Keypoint Regression, Rendezvous<br />
Summary:<br />
The article introduces a novel Supervised Domain Adaptation (SDA) framework for Spacecraft Pose Estimation (SPE). SPE is crucial for tasks like rendezvous and docking in space. While current pipelines perform well on synthetic data, they struggle with real-world imagery due to domain gaps. The SDA framework optimizes domain-invariant representations and task-specific risk using both labeled synthetic and limited real data to enhance generalization under domain shift. Experiment results on the SPEED+ benchmark show that the proposed method consistently outperforms other baselines with only 5% labeled target data. The framework is lightweight, adaptable to different backbones, and computationally efficient, promising robust spacecraft pose estimation in real-world space scenarios.<br />  
Summary: <div>
arXiv:2509.13792v1 Announce Type: new 
Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous space operations such as rendezvous, docking, and in-orbit servicing. Hybrid pipelines that combine object detection, keypoint regression, and Perspective-n-Point (PnP) solvers have recently achieved strong results on synthetic datasets, yet their performance deteriorates sharply on real or lab-generated imagery due to the persistent synthetic-to-real domain gap. Existing unsupervised domain adaptation approaches aim to mitigate this issue but often underperform when a modest number of labeled target samples are available. In this work, we propose the first Supervised Domain Adaptation (SDA) framework tailored for SPE keypoint regression. Building on the Learning Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes domain-invariant representations and task-specific risk using both labeled synthetic and limited labeled real data, thereby reducing generalization error under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate that our approach consistently outperforms source-only, fine-tuning, and oracle baselines. Notably, with only 5% labeled target data, our method matches or surpasses oracle performance trained on larger fractions of labeled data. The framework is lightweight, backbone-agnostic, and computationally efficient, offering a practical pathway toward robust and deployable spacecraft pose estimation in real-world space environments.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments</title>
<link>https://arxiv.org/abs/2509.13795</link>
<guid>https://arxiv.org/abs/2509.13795</guid>
<content:encoded><![CDATA[
<div> dataset, UAV localization, semantic features, particle filtering, satellite imagery
<br />
The article introduces a new dataset called Multi-Altitude Flight Segments (MAFS) for UAV localization in variable altitude scenarios. It proposes a Semantic-Weighted Adaptive Particle Filter (SWA-PF) method that combines robust semantic features from UAV-captured images and satellite imagery. The SWA-PF method includes a semantic weighting mechanism and optimized particle filtering architecture to improve performance in dynamic environments. The approach achieves a 10x efficiency gain over traditional methods, maintains positioning errors below 10 meters, and enables rapid 4-DoF pose estimation using low-resolution satellite maps. The code and dataset are available on GitHub. 
<br /><br />Summary: <div>
arXiv:2509.13795v1 Announce Type: new 
Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been extensively investigated for Global Navigation Satellite System (GNSS)-denied environments. However, existing retrieval-based approaches face limitations in dataset availability and persistent challenges including suboptimal real-time performance, environmental sensitivity, and limited generalization capability, particularly in dynamic or temporally varying environments. To overcome these limitations, we present a large-scale Multi-Altitude Flight Segments dataset (MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted Adaptive Particle Filter (SWA-PF) method. This approach integrates robust semantic features from both UAV-captured images and satellite imagery through two key innovations: a semantic weighting mechanism and an optimized particle filtering architecture. Evaluated using our dataset, the proposed method achieves 10x computational efficiency gain over feature extraction methods, maintains global positioning errors below 10 meters, and enables rapid 4 degree of freedom (4-DoF) pose estimation within seconds using accessible low-resolution satellite maps. Code and dataset will be available at https://github.com/YuanJiayuuu/SWA-PF.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Feature Modeling Enhances Adaptive Segmentation</title>
<link>https://arxiv.org/abs/2509.13801</link>
<guid>https://arxiv.org/abs/2509.13801</guid>
<content:encoded><![CDATA[
<div> Unsupervised domain adaptation, semantic segmentation, Masked Feature Modeling, Rebuilder, auxiliary task <br />
Summary: <br />
The article introduces a novel approach called Masked Feature Modeling (MFM) for unsupervised domain adaptation (UDA) in semantic segmentation. MFM performs feature masking and reconstruction in the feature space, aligning its learning target with the main segmentation task. A lightweight auxiliary module, Rebuilder, facilitates effective reconstruction without adding computational overhead at test time. MFM leverages the segmentation decoder to classify the reconstructed features, tightly coupling the auxiliary objective with the pixel-wise prediction task. Extensive experiments across various architectures and UDA benchmarks show that MFM consistently improves segmentation performance. This approach is simple, efficient, and can be generalized for unsupervised domain-adaptive semantic segmentation. <br /> <div>
arXiv:2509.13801v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer models from a labeled source domain to an unlabeled target domain. While auxiliary self-supervised tasks-particularly contrastive learning-have improved feature discriminability, masked modeling approaches remain underexplored in this setting, largely due to architectural incompatibility and misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a novel auxiliary task that performs feature masking and reconstruction directly in the feature space. Unlike existing masked modeling methods that reconstruct low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM aligns its learning target with the main segmentation task, ensuring compatibility with standard architectures like DeepLab and DAFormer without modifying the inference pipeline. To facilitate effective reconstruction, we introduce a lightweight auxiliary module, Rebuilder, which is trained jointly but discarded during inference, adding zero computational overhead at test time. Crucially, MFM leverages the segmentation decoder to classify the reconstructed features, tightly coupling the auxiliary objective with the pixel-wise prediction task to avoid interference with the primary task. Extensive experiments across various architectures and UDA benchmarks demonstrate that MFM consistently enhances segmentation performance, offering a simple, efficient, and generalizable strategy for unsupervised domain-adaptive semantic segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET</title>
<link>https://arxiv.org/abs/2509.13809</link>
<guid>https://arxiv.org/abs/2509.13809</guid>
<content:encoded><![CDATA[
arXiv:2509.13809v1 Announce Type: new 
Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral classification, is used in many fields ranging from agricultural, over medical to remote sensing applications and is currently also expanding to areas such as autonomous driving. Even though for full hyperspectral images the best-performing methods exploit spatial-spectral information, performing classification solely on spectral information has its own advantages, e.g. smaller model size and thus less data required for training. Moreover, spectral information is complementary to spatial information and improvements on either part can be used to improve spatial-spectral approaches in the future. Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with very few parameters, which currently defines the state of the art in spectral classification. However, we show that with limited training data the model performance deteriorates. Therefore, we investigate MiniROCKET and HDC-MiniROCKET for spectral classification to mitigate that problem. The model extracts well-engineered features without trainable parameters in the feature extraction part and is therefore less vulnerable to limited training data. We show that even though MiniROCKET has more parameters it outperforms 1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the general case
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation</title>
<link>https://arxiv.org/abs/2509.13834</link>
<guid>https://arxiv.org/abs/2509.13834</guid>
<content:encoded><![CDATA[
arXiv:2509.13834v1 Announce Type: new 
Abstract: Semi-supervised learning has been employed to alleviate the need for extensive labeled data for histopathology image segmentation, but existing methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and morphological misclassification. This paper introduces Semi-MOE, to the best of our knowledge, the first multi-task Mixture-of-Experts framework for semi-supervised histopathology image segmentation. Our approach leverages three specialized expert networks: A main segmentation expert, a signed distance field regression expert, and a boundary prediction expert, each dedicated to capturing distinct morphological features. Subsequently, the Multi-Gating Pseudo-labeling module dynamically aggregates expert features, enabling a robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate manual tuning while dynamically balancing multiple learning objectives, we propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and CRAG benchmarks show that our method outperforms state-of-the-art approaches in low-label settings, highlighting the potential of MoE-based architectures in advancing semi-supervised segmentation. Our code is available at https://github.com/vnlvi2k3/Semi-MoE.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.13836</link>
<guid>https://arxiv.org/abs/2509.13836</guid>
<content:encoded><![CDATA[
arXiv:2509.13836v1 Announce Type: new 
Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly impedes their real-world applicability. As the primary component for accurately interpreting visual information, the choice of visual encoder is pivotal. We hypothesize that the diverse training paradigms employed by different visual encoders instill them with distinct inductive biases, which leads to their diverse hallucination performances. Existing benchmarks typically focus on coarse-grained hallucination detection and fail to capture the diverse hallucinations elaborated in our hypothesis. To systematically analyze these effects, we introduce VHBench-10, a comprehensive benchmark with approximately 10,000 samples for evaluating LVLMs across ten fine-grained hallucination categories. Our evaluations confirm encoders exhibit unique hallucination characteristics. Building on these insights and the suboptimality of simple feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network. It employs global visual features to generate routing signals, dynamically aggregating visual features from multiple specialized experts. Comprehensive experiments confirm the effectiveness of VisionWeaver in significantly reducing hallucinations and improving overall model performance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.13846</link>
<guid>https://arxiv.org/abs/2509.13846</guid>
<content:encoded><![CDATA[
arXiv:2509.13846v1 Announce Type: new 
Abstract: Many recent approaches in representation learning implicitly assume that uncorrelated views of a data point are sufficient to learn meaningful representations for various downstream tasks. In this work, we challenge this assumption and demonstrate that meaningful structure in the latent space does not emerge naturally. Instead, it must be explicitly induced. We propose a method that aligns representations from different views of the data to align complementary information without inducing false positives. Our experiments show that our proposed self-supervised learning method, Consistent View Alignment, improves performance for downstream tasks, highlighting the critical role of structured view alignment in learning effective representations. Our method achieved first and second place in the MICCAI 2025 SSL3D challenge when using a Primus vision transformer and ResEnc convolutional neural network, respectively. The code and pretrained model weights are released at https://github.com/Tenbatsu24/LatentCampus.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation</title>
<link>https://arxiv.org/abs/2509.13848</link>
<guid>https://arxiv.org/abs/2509.13848</guid>
<content:encoded><![CDATA[
arXiv:2509.13848v1 Announce Type: new 
Abstract: Feature caching has recently emerged as a promising method for diffusion model acceleration. It effectively alleviates the inefficiency problem caused by high computational requirements by caching similar features in the inference process of the diffusion model. In this paper, we analyze existing feature caching methods from the perspective of information utilization, and point out that relying solely on historical information will lead to constrained accuracy and speed performance. And we propose a novel paradigm that introduces future information via self-speculation based on the information similarity at the same time step across different iteration times. Based on this paradigm, we present \textit{SpecDiff}, a training-free multi-level feature caching strategy including a cached feature selection algorithm and a multi-level feature classification algorithm. (1) Feature selection algorithm based on self-speculative information. \textit{SpecDiff} determines a dynamic importance score for each token based on self-speculative information and historical information, and performs cached feature selection through the importance score. (2) Multi-level feature classification algorithm based on feature importance scores. \textit{SpecDiff} classifies tokens by leveraging the differences in feature importance scores and introduces a multi-level feature calculation strategy. Extensive experiments show that \textit{SpecDiff} achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow on NVIDIA A800-80GB GPU. By merging speculative and historical information, \textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing the Pareto frontier of speedup and accuracy in the efficient diffusion model inference.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</title>
<link>https://arxiv.org/abs/2509.13858</link>
<guid>https://arxiv.org/abs/2509.13858</guid>
<content:encoded><![CDATA[
arXiv:2509.13858v1 Announce Type: new 
Abstract: Dataset distillation aims to synthesize a compact dataset from the original large-scale one, enabling highly efficient learning while preserving competitive model performance. However, traditional techniques primarily capture low-level visual features, neglecting the high-level semantic and structural information inherent in images. In this paper, we propose EDITS, a novel framework that exploits the implicit textual semantics within the image data to achieve enhanced distillation. First, external texts generated by a Vision Language Model (VLM) are fused with image features through a Global Semantic Query module, forming the prior clustered buffer. Local Semantic Awareness then selects representative samples from the buffer to construct image and text prototypes, with the latter produced by guiding a Large Language Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype Guidance strategy generates the final synthetic dataset through a diffusion model. Extensive experiments confirm the effectiveness of our method.Source code is available in: https://github.com/einsteinxia/EDITS.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction</title>
<link>https://arxiv.org/abs/2509.13863</link>
<guid>https://arxiv.org/abs/2509.13863</guid>
<content:encoded><![CDATA[
arXiv:2509.13863v1 Announce Type: new 
Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$\%$ of full views to achieve superior performance over the iterative method optimized on a full dataset.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distractor-Aware Memory-Based Visual Object Tracking</title>
<link>https://arxiv.org/abs/2509.13864</link>
<guid>https://arxiv.org/abs/2509.13864</guid>
<content:encoded><![CDATA[
arXiv:2509.13864v1 Announce Type: new 
Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has led to models with excellent performance in segmentation tasks, achieving leading results on numerous benchmarks. However, these modes are not fully adjusted for visual object tracking, where distractors (i.e., objects visually similar to the target) pose a key challenge. In this paper we propose a distractor-aware drop-in memory module and introspection-based management method for SAM2, leading to DAM4SAM. Our design effectively reduces the tracking drift toward distractors and improves redetection capability after object occlusion. To facilitate the analysis of tracking in the presence of distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results on ten. Furthermore, integrating the proposed distractor-aware memory into a real-time tracker EfficientTAM leads to 11% improvement and matches tracking quality of the non-real-time SAM2.1-L on multiple tracking and segmentation benchmarks, while integration with edge-based tracker EdgeTAM delivers 4% performance boost, demonstrating a very good generalization across architectures.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis</title>
<link>https://arxiv.org/abs/2509.13873</link>
<guid>https://arxiv.org/abs/2509.13873</guid>
<content:encoded><![CDATA[
arXiv:2509.13873v1 Announce Type: new 
Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in cases where fracture signs are subtle or invisible on standard radiographs. To address this, we introduce PelFANet, a dual-stream attention network that fuses raw pelvic X-rays with segmented bone images to improve fracture classification. The network em-ploys Fused Attention Blocks (FABlocks) to iteratively exchange and refine fea-tures from both inputs, capturing global context and localized anatomical detail. Trained in a two-stage pipeline with a segmentation-guided approach, PelFANet demonstrates superior performance over conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and 0.9334 AUC on visible fractures, while generalizing effectively to invisible fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained on them. These results highlight the clini-cal potential of anatomy-aware dual-input architectures for robust fracture detec-tion, especially in scenarios with subtle radiographic presentations.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View</title>
<link>https://arxiv.org/abs/2509.13883</link>
<guid>https://arxiv.org/abs/2509.13883</guid>
<content:encoded><![CDATA[
arXiv:2509.13883v1 Announce Type: new 
Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but frame-based methods often struggle to meet the requirements of accuracy, low latency, and energy efficiency, especially in resource-constrained settings such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level temporal resolution at mW-level power by asynchronously sensing brightness changes. In this work, we present EvHand-FPV, a lightweight framework for egocentric First-Person-View 3D hand tracking from a single event camera. We construct an event-based FPV dataset that couples synthetic training data with 3D labels and real event data with 2D labels for evaluation to address the scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based region of interest (ROI) that localizes the hand region via geometric cues, combined with an end-to-end mapping strategy that embeds ROI offsets into the network to reduce computation without explicit reconstruction, and a multi-task learning strategy with an auxiliary geometric feature head that improves representations without test-time overhead. On our real FPV test set, EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from 11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results demonstrate accurate and efficient egocentric event-based hand tracking suitable for on-device XR applications. The dataset and code are available at https://github.com/zen5x5/EvHand-FPV.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.13907</link>
<guid>https://arxiv.org/abs/2509.13907</guid>
<content:encoded><![CDATA[
arXiv:2509.13907v1 Announce Type: new 
Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point labels for an unlabeled point cloud, given only a few labeled examples. To extract discriminative representations from the limited support set, existing methods have constructed prototypes using conventional algorithms such as farthest point sampling. However, we point out that its initial randomness significantly affects FS-PCS performance and that the prototype generation process remains underexplored despite its prevalence. This motivates us to investigate an advanced prototype generation method based on attention mechanism. Despite its potential, we found that vanilla module suffers from the distributional gap between learnable prototypical tokens and support features. To overcome this, we propose White Aggregation and Restoration Module (WARM), which resolves the misalignment by sandwiching cross-attention between whitening and coloring transformations. Specifically, whitening aligns the support features to prototypical tokens before attention process, and subsequently coloring restores the original distribution to the attended tokens. This simple yet effective design enables robust attention, thereby generating representative prototypes by capturing the semantic relationships among support features. Our method achieves state-of-the-art performance with a significant margin on multiple FS-PCS benchmarks, demonstrating its effectiveness through extensive experiments.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration</title>
<link>https://arxiv.org/abs/2509.13919</link>
<guid>https://arxiv.org/abs/2509.13919</guid>
<content:encoded><![CDATA[
arXiv:2509.13919v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question answering capability. However, they still struggle with aligning the rationale and the generated answer, leading to inconsistent reasoning and incorrect responses. To this end, this paper introduces the Self-Rationale Calibration (SRC) framework to iteratively calibrate the alignment between rationales and answers. SRC begins by employing a lightweight "rationale fine-tuning" approach, which modifies the model's response format to require a rationale before deriving an answer without explicit prompts. Next, SRC searches for a diverse set of candidate responses from the fine-tuned LVLMs for each sample, followed by a proposed pairwise scoring strategy using a tailored scoring model, R-Scorer, to evaluate both rationale quality and factual consistency of candidates. Based on a confidence-weighted preference curation process, SRC decouples the alignment calibration into a preference fine-tuning manner, leading to significant improvements of LVLMs in perception, reasoning, and generalization across multiple benchmarks. Our results emphasize the rationale-oriented alignment in exploring the potential of LVLMs.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification</title>
<link>https://arxiv.org/abs/2509.13922</link>
<guid>https://arxiv.org/abs/2509.13922</guid>
<content:encoded><![CDATA[
arXiv:2509.13922v1 Announce Type: new 
Abstract: Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the "purification-customization" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Level Diffusion Guidance: Well Begun is Half Done</title>
<link>https://arxiv.org/abs/2509.13936</link>
<guid>https://arxiv.org/abs/2509.13936</guid>
<content:encoded><![CDATA[
arXiv:2509.13936v1 Announce Type: new 
Abstract: Diffusion models have achieved state-of-the-art image generation. However, the random Gaussian noise used to start the diffusion process influences the final output, causing variations in image quality and prompt adherence. Existing noise-level optimization approaches generally rely on extra dataset construction, additional networks, or backpropagation-based optimization, limiting their practicality. In this paper, we propose Noise Level Guidance (NLG), a simple, efficient, and general noise-level optimization approach that refines initial noise by increasing the likelihood of its alignment with general guidance - requiring no additional training data, auxiliary networks, or backpropagation. The proposed NLG approach provides a unified framework generalizable to both conditional and unconditional diffusion models, accommodating various forms of diffusion-level guidance. Extensive experiments on five standard benchmarks demonstrate that our approach enhances output generation quality and input condition adherence. By seamlessly integrating with existing guidance methods while maintaining computational efficiency, our method establishes NLG as a practical and scalable enhancement to diffusion models. Code can be found at https://github.com/harveymannering/NoiseLevelGuidance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation</title>
<link>https://arxiv.org/abs/2509.13939</link>
<guid>https://arxiv.org/abs/2509.13939</guid>
<content:encoded><![CDATA[
arXiv:2509.13939v1 Announce Type: new 
Abstract: Visual counting is a fundamental yet challenging task, especially when users need to count objects of a specific type in complex scenes. While recent models, including class-agnostic counting models and large vision-language models (VLMs), show promise in counting tasks, their ability to perform fine-grained, intent-driven counting remains unclear. In this paper, we introduce PairTally, a benchmark dataset specifically designed to evaluate fine-grained visual counting. Each of the 681 high-resolution images in PairTally contains two object categories, requiring models to distinguish and count based on subtle differences in shape, size, color, or semantics. The dataset includes both inter-category (distinct categories) and intra-category (closely related subcategories) settings, making it suitable for rigorous evaluation of selective counting capabilities. We benchmark a variety of state-of-the-art models, including exemplar-based methods, language-prompted models, and large VLMs. Our results show that despite recent advances, current models struggle to reliably count what users intend, especially in fine-grained and visually ambiguous cases. PairTally provides a new foundation for diagnosing and improving fine-grained visual counting systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment</title>
<link>https://arxiv.org/abs/2509.14001</link>
<guid>https://arxiv.org/abs/2509.14001</guid>
<content:encoded><![CDATA[
arXiv:2509.14001v1 Announce Type: new 
Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a knowledge distillation approach that transfers region-level multimodal semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight vision-only object detector student (e.g., YOLO). A translation module maps student features into a joint space, where the training of the student and translator is guided by a dual-objective loss that enforces both local alignment and global relational consistency. Unlike prior approaches focused on dense or global alignment, MOCHA operates at the object level, enabling efficient transfer of semantics without modifying the teacher or requiring textual input at inference. We validate our method across four personalized detection benchmarks under few-shot regimes. Results show consistent gains over baselines, with a +10.1 average score improvement. Despite its compact architecture, MOCHA reaches performance on par with larger multimodal models, proving its suitability for real-world deployment.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments</title>
<link>https://arxiv.org/abs/2509.14012</link>
<guid>https://arxiv.org/abs/2509.14012</guid>
<content:encoded><![CDATA[
arXiv:2509.14012v1 Announce Type: new 
Abstract: Drone detection in visually complex environments remains challenging due to background clutter, small object scale, and camouflage effects. While generic object detectors like YOLO exhibit strong performance in low-texture scenes, their effectiveness degrades in cluttered environments with low object-background separability. To address these limitations, this work presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework that integrates generic object detection with camouflage object detection techniques. Building upon the original architecture, the proposed iteration introduces systematic advancements in training data composition, feature fusion strategies, and backbone design. Specifically, the training process leverages large-scale, photo-realistic synthetic data, complemented by a small set of real-world samples, to enhance robustness under visually complex conditions. The contribution of intermediate multi-scale FEDER features is systematically evaluated, and detection performance is comprehensively benchmarked across multiple YOLO-based backbone configurations. Empirical results indicate that integrating intermediate FEDER features, in combination with backbone upgrades, contributes to notable performance improvements. In the most promising configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER features derived from the DWD module -- these enhancements lead to a FNR reduction of up to 39.1 percentage points and a mAP increase of up to 62.8 percentage points at an IoU threshold of 0.5, compared to the initial baseline.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAIL-VL2 Technical Report</title>
<link>https://arxiv.org/abs/2509.14033</link>
<guid>https://arxiv.org/abs/2509.14033</guid>
<content:encoded><![CDATA[
arXiv:2509.14033v1 Announce Type: new 
Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings</title>
<link>https://arxiv.org/abs/2509.14051</link>
<guid>https://arxiv.org/abs/2509.14051</guid>
<content:encoded><![CDATA[
arXiv:2509.14051v1 Announce Type: new 
Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy (RP) experience biochemical recurrence (BCR), characterized by increased prostate specific antigen (PSA) and associated with increased mortality. Accurate early prediction of BCR, at the time of RP, would contribute to prompt adaptive clinical decision-making and improved patient outcomes. In this work, we propose prostate cancer BCR prediction via fused multi-modal embeddings (PROFUSEme), which learns cross-modal interactions of clinical, radiology, and pathology data, following an intermediate fusion configuration in combination with Cox Proportional Hazard regressors. Quantitative evaluation of our proposed approach reveals superior performance, when compared with late fusion configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on the hold out data of CHIMERA 2025 challenge validation leaderboard.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</title>
<link>https://arxiv.org/abs/2509.14055</link>
<guid>https://arxiv.org/abs/2509.14055</guid>
<content:encoded><![CDATA[
arXiv:2509.14055v1 Announce Type: new 
Abstract: We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement</title>
<link>https://arxiv.org/abs/2509.14060</link>
<guid>https://arxiv.org/abs/2509.14060</guid>
<content:encoded><![CDATA[
arXiv:2509.14060v1 Announce Type: new 
Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues inherent in low-quality videos, leading to significant degradation in tracking performance when confronted with real-world image deterioration. Therefore, advancing the application of MOT algorithms in real-world low-quality video scenarios represents a critical and meaningful endeavor. To address the challenges posed by low-quality scenarios, inspired by vision-language models, this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking framework (VSE-MOT). Specifically, we first design a tri-branch architecture that leverages a vision-language model to extract global visual semantic information from images and fuse it with query vectors. Subsequently, to further enhance the utilization of visual semantic information, we introduce the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic information to suit multi-object tracking tasks, while the VSFM improves the efficacy of feature fusion. Through extensive experiments, we validate the effectiveness and superiority of the proposed method in real-world low-quality video scenarios. Its tracking performance metrics outperform those of existing methods by approximately 8% to 20%, while maintaining robust performance in conventional scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
<link>https://arxiv.org/abs/2509.14084</link>
<guid>https://arxiv.org/abs/2509.14084</guid>
<content:encoded><![CDATA[
arXiv:2509.14084v1 Announce Type: new 
Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary novel categories, offering a scalable and annotation-efficient solution. Traditionally, most ZSAD works have been based on the CLIP model, which performs anomaly detection by calculating the similarity between visual and text embeddings. Recently, vision foundation models such as DINOv3 have demonstrated strong transferable representation capabilities. In this work, we are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two key challenges: (i) the domain bias between large-scale pretraining data and anomaly detection tasks leads to feature misalignment; and (ii) the inherent bias toward global semantics in pretrained representations often leads to subtle anomalies being misinterpreted as part of the normal foreground objects, rather than being distinguished as abnormal regions. To overcome these challenges, we introduce AD-DINOv3, a novel vision-language multimodal framework designed for ZSAD. Specifically, we formulate anomaly detection as a multimodal contrastive learning problem, where DINOv3 is employed as the visual backbone to extract patch tokens and a CLS token, and the CLIP text encoder provides embeddings for both normal and abnormal prompts. To bridge the domain gap, lightweight adapters are introduced in both modalities, enabling their representations to be recalibrated for the anomaly detection task. Beyond this baseline alignment, we further design an Anomaly-Aware Calibration Module (AACM), which explicitly guides the CLS token to attend to anomalous regions rather than generic foreground semantics, thereby enhancing discriminability. Extensive experiments on eight industrial and medical benchmarks demonstrate that AD-DINOv3 consistently matches or surpasses state-of-the-art methods, verifying its superiority as a general zero-shot anomaly detection framework.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing</title>
<link>https://arxiv.org/abs/2509.14097</link>
<guid>https://arxiv.org/abs/2509.14097</guid>
<content:encoded><![CDATA[
arXiv:2509.14097v1 Announce Type: new 
Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible, visible, and audio-visual events without temporal annotations. Previous work has emphasized refining global predictions through contrastive or collaborative learning, but neglected stable segment-level supervision and class-aware cross-modal alignment. To address this, we propose two strategies: (1) an exponential moving average (EMA)-guided pseudo supervision framework that generates reliable segment-level masks via adaptive thresholds or top-k selection, offering stable temporal guidance beyond video-level labels; and (2) a class-aware cross-modal agreement (CMA) loss that aligns audio and visual embeddings at reliable segment-class pairs, ensuring consistency across modalities while preserving temporal structure. Evaluations on LLP and UnAV-100 datasets shows that our method achieves state-of-the-art (SOTA) performance across multiple metrics.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2509.14104</link>
<guid>https://arxiv.org/abs/2509.14104</guid>
<content:encoded><![CDATA[
arXiv:2509.14104v1 Announce Type: new 
Abstract: Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks. However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity. These issues restrict their practical applicability in RS. To address this limitation, we propose an adaptation for enhancing the efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism into the FM. The integration of Soft MoEs into the FM allows modality-specific expert specialization alongside shared cross-sensor representation learning. To demonstrate the effectiveness of our adaptation, we apply it on the Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic descriptor-driven sampling strategy for the construction of a representative and diverse training set to train our CSMoE model. Extensive experiments on scene classification, semantic segmentation, and content-based image retrieval demonstrate that our adaptation yields a reduction in computational requirements while maintaining or improving representational performance. Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off between representational capacity, accuracy, and computational efficiency. On average, CSMoE achieves more than twice the computational efficiency of existing RS FMs, while maintaining competitive performance across all experiments. These results show the effectiveness of the proposed adaptation for creating computationally efficient RS FMs. The code for the model, the training set creation, and the model weights will be available at https://git.tu-berlin.de/rsim/csmoe.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows</title>
<link>https://arxiv.org/abs/2509.14119</link>
<guid>https://arxiv.org/abs/2509.14119</guid>
<content:encoded><![CDATA[
arXiv:2509.14119v1 Announce Type: new 
Abstract: Accurate histopathological diagnosis often requires multiple differently stained tissue sections, a process that is time-consuming, labor-intensive, and environmentally taxing due to the use of multiple chemical stains. Recently, virtual staining has emerged as a promising alternative that is faster, tissue-conserving, and environmentally friendly. However, existing virtual staining methods face significant challenges in clinical applications, primarily due to their reliance on well-aligned paired data. Obtaining such data is inherently difficult because chemical staining processes can distort tissue structures, and a single tissue section cannot undergo multiple staining procedures without damage or loss of information. As a result, most available virtual staining datasets are either unpaired or roughly paired, making it difficult for existing methods to achieve accurate pixel-level supervision. To address this challenge, we propose a robust virtual staining framework featuring cascaded registration mechanisms to resolve spatial mismatches between generated outputs and their corresponding ground truth. Experimental results demonstrate that our method significantly outperforms state-of-the-art models across five datasets, achieving an average improvement of 3.2% on internal datasets and 10.1% on external datasets. Moreover, in datasets with substantial misalignment, our approach achieves a remarkable 23.8% improvement in peak signal-to-noise ratio compared to baseline models. The exceptional robustness of the proposed method across diverse datasets simplifies the data acquisition process for virtual staining and offers new insights for advancing its development.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection</title>
<link>https://arxiv.org/abs/2509.14120</link>
<guid>https://arxiv.org/abs/2509.14120</guid>
<content:encoded><![CDATA[
arXiv:2509.14120v1 Announce Type: new 
Abstract: Digital beautification through social media filters has become increasingly popular, raising concerns about the reliability of facial images and videos and the effectiveness of automated face analysis. This issue is particularly critical for digital manipulation detectors, systems aiming at distinguishing between genuine and manipulated data, especially in cases involving deepfakes and morphing attacks designed to deceive humans and automated facial recognition. This study examines whether beauty filters impact the performance of deepfake and morphing attack detectors. We perform a comprehensive analysis, evaluating multiple state-of-the-art detectors on benchmark datasets before and after applying various smoothing filters. Our findings reveal performance degradation, highlighting vulnerabilities introduced by facial enhancements and underscoring the need for robust detection models resilient to such alterations.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</title>
<link>https://arxiv.org/abs/2509.14142</link>
<guid>https://arxiv.org/abs/2509.14142</guid>
<content:encoded><![CDATA[
arXiv:2509.14142v1 Announce Type: new 
Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Exploratory Study on Abstract Images and Visual Representations Learned from Them</title>
<link>https://arxiv.org/abs/2509.14149</link>
<guid>https://arxiv.org/abs/2509.14149</guid>
<content:encoded><![CDATA[
arXiv:2509.14149v1 Announce Type: new 
Abstract: Imagine living in a world composed solely of primitive shapes, could you still recognise familiar objects? Recent studies have shown that abstract images-constructed by primitive shapes-can indeed convey visual semantic information to deep learning models. However, representations obtained from such images often fall short compared to those derived from traditional raster images. In this paper, we study the reasons behind this performance gap and investigate how much high-level semantic content can be captured at different abstraction levels. To this end, we introduce the Hierarchical Abstraction Image Dataset (HAID), a novel data collection that comprises abstract images generated from normal raster images at multiple levels of abstraction. We then train and evaluate conventional vision systems on HAID across various tasks including classification, segmentation, and object detection, providing a comprehensive study between rasterised and abstract image representations. We also discuss if the abstract image can be considered as a potentially effective format for conveying visual semantic information and contributing to vision tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.14151</link>
<guid>https://arxiv.org/abs/2509.14151</guid>
<content:encoded><![CDATA[
arXiv:2509.14151v1 Announce Type: new 
Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise for autonomous driving. Recent studies have prioritized efficiency or accuracy enhancements, yet the issue of domain shift has been overlooked, leading to substantial performance degradation upon transfer. We identify major domain gaps in real-world cross-domain scenarios and initiate the first effort to address the Domain Adaptation (DA) challenge in multi-view 3D object detection for BEV perception. Given the complexity of BEV perception approaches with their multiple components, domain shift accumulation across multi-geometric spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain adaptation. In this paper, we introduce an innovative geometric-aware teacher-student framework, BEVUDA++, to diminish this issue, comprising a Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model. Specifically, RDT effectively blends target LiDAR with dependable depth predictions to generate depth-aware information based on uncertainty estimation, enhancing the extraction of Voxel and BEV features that are essential for understanding the target domain. To collaboratively reduce the domain shift, GCS maps features from multiple spaces into a unified geometric embedding space, thereby narrowing the gap in data distribution between the two domains. Additionally, we introduce a novel Uncertainty-guided Exponential Moving Average (UEMA) to further reduce error accumulation due to domain shifts informed by previously obtained uncertainty guidance. To demonstrate the superiority of our proposed method, we execute comprehensive experiments in four cross-domain scenarios, securing state-of-the-art performance in BEV 3D object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night adaptation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions</title>
<link>https://arxiv.org/abs/2509.14165</link>
<guid>https://arxiv.org/abs/2509.14165</guid>
<content:encoded><![CDATA[
arXiv:2509.14165v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic segmentation but are hindered by high computational and memory costs. To address this, we propose STEP (SuperToken and Early-Pruning), a hybrid token-reduction framework that combines dynamic patch merging and token pruning to enhance efficiency without significantly compromising accuracy. At the core of STEP is dCTS, a lightweight CNN-based policy network that enables flexible merging into superpatches. Encoder blocks integrate also early-exits to remove high-confident supertokens, lowering computational load. We evaluate our method on high-resolution semantic segmentation benchmarks, including images up to 1024 x 1024, and show that when dCTS is applied alone, the token count can be reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase in throughput when using ViT-Large as the backbone. Applying the full STEP framework further improves efficiency, reaching up to a 4x reduction in computational complexity and a 1.7x gain in inference speed, with a maximum accuracy drop of no more than 2.0%. With the proposed STEP configurations, up to 40% of tokens can be confidently predicted and halted before reaching the final encoder layer.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Video Understanding with Gated Residual Tokenization</title>
<link>https://arxiv.org/abs/2509.14199</link>
<guid>https://arxiv.org/abs/2509.14199</guid>
<content:encoded><![CDATA[
arXiv:2509.14199v1 Announce Type: new 
Abstract: High temporal resolution is essential for capturing fine-grained details in video understanding. However, current video large language models (VLLMs) and benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or keyframe selection, discarding dense temporal information. This compromise avoids the high cost of tokenizing every frame, which otherwise leads to redundant computation and linear token growth as video length increases. While this trade-off works for slowly changing content, it fails for tasks like lecture comprehension, where information appears in nearly every frame and requires precise temporal alignment. To address this gap, we introduce Dense Video Understanding (DVU), which enables high-FPS video comprehension by reducing both tokenization time and token overhead. Existing benchmarks are also limited, as their QA pairs focus on coarse content changes. We therefore propose DIVE (Dense Information Video Evaluation), the first benchmark designed for dense temporal reasoning. To make DVU practical, we present Gated Residual Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated Tokenization uses pixel-level motion estimation to skip static regions during tokenization, achieving sub-linear growth in token count and compute. (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions within a scene, further reducing redundancy while preserving dynamic semantics. Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales positively with FPS. These results highlight the importance of dense temporal information and demonstrate that GRT enables efficient, scalable high-FPS video understanding.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cin\'{e}aste: A Fine-grained Contextual Movie Question Answering Benchmark</title>
<link>https://arxiv.org/abs/2509.14227</link>
<guid>https://arxiv.org/abs/2509.14227</guid>
<content:encoded><![CDATA[
arXiv:2509.14227v1 Announce Type: new 
Abstract: While recent advancements in vision-language models have improved video understanding, diagnosing their capacity for deep, narrative comprehension remains a challenge. Existing benchmarks often test short-clip recognition or use template-based questions, leaving a critical gap in evaluating fine-grained reasoning over long-form narrative content. To address these gaps, we introduce $\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie understanding. Our dataset comprises 3,119 multiple-choice question-answer pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel fine-grained contextual reasoning categories. We use GPT-4o to generate diverse, context-rich questions by integrating visual descriptions, captions, scene titles, and summaries, which require deep narrative understanding. To ensure high-quality evaluation, our pipeline incorporates a two-stage filtering process: Context-Independence filtering ensures questions require video context, while Contextual Veracity filtering validates factual consistency against the movie content, mitigating hallucinations. Experiments show that existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals that long-range temporal reasoning is a primary bottleneck, with the top open-source model achieving only 63.15\% accuracy. This underscores significant challenges in fine-grained contextual understanding and the need for advancements in long-form movie comprehension.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenExam: A Multidisciplinary Text-to-Image Exam</title>
<link>https://arxiv.org/abs/2509.14232</link>
<guid>https://arxiv.org/abs/2509.14232</guid>
<content:encoded><![CDATA[
arXiv:2509.14232v1 Announce Type: new 
Abstract: Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Reconstruction of Coronary Vessel Trees from Biplanar X-Ray Images Using a Geometric Approach</title>
<link>https://arxiv.org/abs/2509.13358</link>
<guid>https://arxiv.org/abs/2509.13358</guid>
<content:encoded><![CDATA[
arXiv:2509.13358v1 Announce Type: cross 
Abstract: X-ray angiography is widely used in cardiac interventions to visualize coronary vessels, assess integrity, detect stenoses and guide treatment. We propose a framework for reconstructing 3D vessel trees from biplanar X-ray images which are extracted from two X-ray videos captured at different C-arm angles. The proposed framework consists of three main components: image segmentation, motion phase matching, and 3D reconstruction. An automatic video segmentation method for X-ray angiography to enable semantic segmentation for image segmentation and motion phase matching. The goal of the motion phase matching is to identify a pair of X-ray images that correspond to a similar respiratory and cardiac motion phase to reduce errors in 3D reconstruction. This is achieved by tracking a stationary object such as a catheter or lead within the X-ray video. The semantic segmentation approach assigns different labels to different object classes enabling accurate differentiation between blood vessels, balloons, and catheters. Once a suitable image pair is selected, key anatomical landmarks (vessel branching points and endpoints) are matched between the two views using a heuristic method that minimizes reconstruction errors. This is followed by a novel geometric reconstruction algorithm to generate the 3D vessel tree. The algorithm computes the 3D vessel centrelines by determining the intersection of two 3D surfaces. Compared to traditional methods based on epipolar constraints, the proposed approach simplifies there construction workflow and improves overall accuracy. We trained and validated our segmentation method on 62 X-ray angiography video sequences. On the test set, our method achieved a segmentation accuracy of 0.703. The 3D reconstruction framework was validated by measuring the reconstruction error of key anatomical landmarks, achieving a reprojection errors of 0.62mm +/- 0.38mm.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma</title>
<link>https://arxiv.org/abs/2509.13360</link>
<guid>https://arxiv.org/abs/2509.13360</guid>
<content:encoded><![CDATA[
arXiv:2509.13360v1 Announce Type: cross 
Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by its highly invasive behavior and exceptionally high rates of recurrence. Conventional radiation therapy, which employs uniform treatment margins, fails to account for patient-specific anatomical and biological factors that critically influence tumor cell migration. To address this limitation, numerous computational models of glioblastoma growth have been developed, enabling generation of tumor cell distribution maps extending beyond radiographically visible regions and thus informing more precise treatment strategies. However, despite encouraging preliminary findings, the clinical adoption of these growth models remains limited. To bridge this translational gap and accelerate both model development and clinical validation, we introduce PREDICT-GBM, a comprehensive integrated pipeline and dataset for modeling and evaluation. This platform enables systematic benchmarking of state-of-the-art tumor growth models using an expert-curated clinical dataset comprising 255 subjects with complete tumor segmentations and tissue characterization maps. Our analysis demonstrates that personalized radiation treatment plans derived from tumor growth predictions achieved superior recurrence coverage compared to conventional uniform margin approaches for two of the evaluated models. This work establishes a robust platform for advancing and systematically evaluating cutting-edge tumor growth modeling approaches, with the ultimate goal of facilitating clinical translation and improving patient outcomes.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging</title>
<link>https://arxiv.org/abs/2509.13372</link>
<guid>https://arxiv.org/abs/2509.13372</guid>
<content:encoded><![CDATA[
arXiv:2509.13372v1 Announce Type: cross 
Abstract: Fontan palliation for univentricular congenital heart disease progresses to hemodynamic failure with complex flow patterns poorly characterized by conventional 2D imaging. Current assessment relies on fluoroscopic angiography, providing limited 3D geometric information essential for computational fluid dynamics (CFD) analysis and surgical planning.
  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash (2.5B parameters) for systematic, iterative processing of fluoroscopic angiograms through transformer-based neural architecture. The pipeline encompasses medical image preprocessing, vascular segmentation, contrast enhancement, artifact removal, and virtual hemodynamic flow visualization within 2D projections. Final views were processed through Tencent's Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
  The pipeline successfully generated geometrically optimized 2D projections from single-view angiograms after 16 processing steps using a custom web interface. Initial iterations contained hallucinated vascular features requiring iterative refinement to achieve anatomically faithful representations. Final projections demonstrated accurate preservation of complex Fontan geometry with enhanced contrast suitable for 3D conversion. AI-generated virtual flow visualization identified stagnation zones in central connections and flow patterns in branch arteries. Complete processing required under 15 minutes with second-level API response times.
  This approach demonstrates clinical feasibility of generating CFD-suitable geometries from routine angiographic data, enabling 3D generation and rapid virtual flow visualization for cursory insights prior to full CFD simulation. While requiring refinement cycles for accuracy, this establishes foundation for democratizing advanced geometric and hemodynamic analysis using readily available imaging data.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs</title>
<link>https://arxiv.org/abs/2509.13379</link>
<guid>https://arxiv.org/abs/2509.13379</guid>
<content:encoded><![CDATA[
arXiv:2509.13379v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex visual understanding across scientific and reasoning tasks. While performance benchmarking has advanced our understanding of these capabilities, the critical dimension of uncertainty quantification has received insufficient attention. Therefore, unlike prior conformal prediction studies that focused on limited settings, we conduct a comprehensive uncertainty benchmarking study, evaluating 16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets with 3 distinct scoring functions. Our findings demonstrate that larger models consistently exhibit better uncertainty quantification; models that know more also know better what they don't know. More certain models achieve higher accuracy, while mathematical and reasoning tasks elicit poorer uncertainty performance across all models compared to other domains. This work establishes a foundation for reliable uncertainty evaluation in multimodal systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds</title>
<link>https://arxiv.org/abs/2509.13390</link>
<guid>https://arxiv.org/abs/2509.13390</guid>
<content:encoded><![CDATA[
arXiv:2509.13390v1 Announce Type: cross 
Abstract: The detection of anomalies in automotive cabin sounds is critical for ensuring vehicle quality and maintaining passenger comfort. In many real-world settings, this task is more appropriately framed as an unsupervised learning problem rather than the supervised case due to the scarcity or complete absence of labeled faulty data. In such an unsupervised setting, the model is trained exclusively on healthy samples and detects anomalies as deviations from normal behavior. However, in the absence of labeled faulty samples for validation and the limited reliability of commonly used metrics, such as validation reconstruction error, effective model selection remains a significant challenge. To overcome these limitations, a domain-knowledge-informed approach for model selection is proposed, in which proxy-anomalies engineered through structured perturbations of healthy spectrograms are used in the validation set to support model selection. The proposed methodology is evaluated on a high-fidelity electric vehicle dataset comprising healthy and faulty cabin sounds across five representative fault types viz., Imbalance, Modulation, Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced sound synthesis techniques, and validated via expert jury assessments, has been made publicly available to facilitate further research. Experimental evaluations on the five fault cases demonstrate the selection of optimal models using proxy-anomalies, significantly outperform conventional model selection strategies.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?</title>
<link>https://arxiv.org/abs/2509.13428</link>
<guid>https://arxiv.org/abs/2509.13428</guid>
<content:encoded><![CDATA[
arXiv:2509.13428v1 Announce Type: cross 
Abstract: Chest X-rays (CXRs) are the most commonly performed imaging investigation. In the UK, many centres experience reporting delays due to radiologist workforce shortages. Artificial intelligence (AI) tools capable of distinguishing normal from abnormal CXRs have emerged as a potential solution. If normal CXRs could be safely identified and reported without human input, a substantial portion of radiology workload could be reduced.
  This article examines the feasibility and implications of autonomous AI reporting of normal CXRs. Key issues include defining normal, ensuring generalisability across populations, and managing the sensitivity-specificity trade-off. It also addresses legal and regulatory challenges, such as compliance with IR(ME)R and GDPR, and the lack accountability frameworks for errors. Further considerations include the impact on radiologists practice, the need for robust post-market surveillance, and incorporation of patient perspectives. While the benefits are clear, adoption must be cautious.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</title>
<link>https://arxiv.org/abs/2509.13541</link>
<guid>https://arxiv.org/abs/2509.13541</guid>
<content:encoded><![CDATA[
arXiv:2509.13541v1 Announce Type: cross 
Abstract: Central airway obstruction (CAO) is a life-threatening condition with increasing incidence, caused by tumors in and outside of the airway. Traditional treatment methods such as bronchoscopy and electrocautery can be used to remove the tumor completely; however, these methods carry a high risk of complications. Recent advances allow robotic interventions with lesser risk. The combination of robot interventions with scene understanding and mapping also opens up the possibilities for automation. We present a novel pipeline that enables real-time, semantically informed 3D reconstructions of the central airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to identify obstructive tissues. The SLAM module reconstructs the 3D geometry of the airway in real time, while the segmentation masks guide the annotation of obstruction regions within the reconstructed point cloud. To validate our pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By integrating segmentation directly into the SLAM workflow, our system produces annotated 3D maps that highlight clinically relevant regions in real time. High-speed capabilities of the pipeline allows quicker reconstructions compared to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our framework is modular and can generalize to other anatomies or procedures with minimal changes, offering a promising step toward autonomous robotic interventions.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT</title>
<link>https://arxiv.org/abs/2509.13576</link>
<guid>https://arxiv.org/abs/2509.13576</guid>
<content:encoded><![CDATA[
arXiv:2509.13576v1 Announce Type: cross 
Abstract: Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces radiation dose, yet its clinical use is hindered by artifacts due to view reduction and domain shifts from scanner, protocol, or anatomical variations, leading to performance degradation in out-of-distribution (OOD) scenarios. In this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR integrates cross-distribution diffusion priors, derived from a Scalable Interpolant Transformer (SiT), with model-based iterative reconstruction methods. Specifically, we train a SiT backbone, an extension of the Diffusion Transformer (DiT) architecture, to establish a unified stochastic interpolant framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets. By randomly dropping the conditioning with a null embedding during training, the model learns both domain-specific and domain-invariant priors, enhancing generalizability. During sampling, the globally sensitive transformer-based diffusion model exploits the cross-distribution prior within the unified stochastic interpolant framework, enabling flexible and stable control over multi-distribution-to-noise interpolation paths and decoupled sampling strategies, thereby improving adaptation to OOD reconstruction. By alternating between data fidelity and sampling updates, our model achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Extensive experiments demonstrate that CDPIR significantly outperforms existing approaches, particularly under OOD conditions, highlighting its robustness and potential clinical value in challenging imaging scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Pose Estimation through Dexterous Touch</title>
<link>https://arxiv.org/abs/2509.13591</link>
<guid>https://arxiv.org/abs/2509.13591</guid>
<content:encoded><![CDATA[
arXiv:2509.13591v1 Announce Type: cross 
Abstract: Robust object pose estimation is essential for manipulation and interaction tasks in robotics, particularly in scenarios where visual data is limited or sensitive to lighting, occlusions, and appearances. Tactile sensors often offer limited and local contact information, making it challenging to reconstruct the pose from partial data. Our approach uses sensorimotor exploration to actively control a robot hand to interact with the object. We train with Reinforcement Learning (RL) to explore and collect tactile data. The collected 3D point clouds are used to iteratively refine the object's shape and pose. In our setup, one hand holds the object steady while the other performs active exploration. We show that our method can actively explore an object's surface to identify critical pose features without prior knowledge of the object's geometry. Supplementary material and more demonstrations will be provided at https://amirshahid.github.io/BimanualTactilePose .
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans</title>
<link>https://arxiv.org/abs/2509.13612</link>
<guid>https://arxiv.org/abs/2509.13612</guid>
<content:encoded><![CDATA[
arXiv:2509.13612v1 Announce Type: cross 
Abstract: Understanding how spontaneous brain activity relates to stimulus-driven neural responses is a fundamental challenge in cognitive neuroscience. While task-based functional magnetic resonance imaging (fMRI) captures localized stimulus-evoked brain activation, its acquisition is costly, time-consuming, and difficult to scale across populations. In contrast, resting-state fMRI (rs-fMRI) is task-free and abundant, but lacks direct interpretability. We introduce Rest2Visual, a conditional generative model that predicts visually evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It follows a volumetric encoder--decoder design, where multiscale 3D features from rs-fMRI are modulated by image embeddings via adaptive normalization, enabling spatially accurate, stimulus-specific activation synthesis. To enable model training, we construct a large-scale triplet dataset from the Natural Scenes Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their corresponding ve-fMRI activation maps. Quantitative evaluation shows that the predicted activations closely match ground truth across standard similarity and representational metrics, and support successful image reconstruction in downstream decoding. Notably, the predicted maps preserve subject-specific structure, demonstrating the model's capacity to generate individualized functional surrogates. Our results provide compelling evidence that individualized spontaneous neural activity can be transformed into stimulus-aligned representations, opening new avenues for scalable, task-free functional brain modeling.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-I: LLMs are Naturally Interleaved Multimodal Creators</title>
<link>https://arxiv.org/abs/2509.13642</link>
<guid>https://arxiv.org/abs/2509.13642</guid>
<content:encoded><![CDATA[
arXiv:2509.13642v1 Announce Type: cross 
Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the "one-tool" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: https://github.com/ByteDance-BandAI/LLM-I.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap</title>
<link>https://arxiv.org/abs/2509.13857</link>
<guid>https://arxiv.org/abs/2509.13857</guid>
<content:encoded><![CDATA[
arXiv:2509.13857v1 Announce Type: cross 
Abstract: Reliable global localization is critical for autonomous vehicles, especially in environments where GNSS is degraded or unavailable, such as urban canyons and tunnels. Although high-definition (HD) maps provide accurate priors, the cost of data collection, map construction, and maintenance limits scalability. OpenStreetMap (OSM) offers a free and globally available alternative, but its coarse abstraction poses challenges for matching with sensor data. We propose InterKey, a cross-modal framework that leverages road intersections as distinctive landmarks for global localization. Our method constructs compact binary descriptors by jointly encoding road and building imprints from point clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation, orientation determination, and area-equalized sampling strategies, enabling robust cross-modal matching. Experiments on the KITTI dataset demonstrate that InterKey achieves state-of-the-art accuracy, outperforming recent baselines by a large margin. The framework generalizes to sensors that can produce dense structural point clouds, offering a scalable and cost-effective solution for robust vehicle localization.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAP: End-to-End Autonomous Driving with Map-Assisted Planning</title>
<link>https://arxiv.org/abs/2509.13926</link>
<guid>https://arxiv.org/abs/2509.13926</guid>
<content:encoded><![CDATA[
arXiv:2509.13926v1 Announce Type: cross 
Abstract: In recent years, end-to-end autonomous driving has attracted increasing attention for its ability to jointly model perception, prediction, and planning within a unified framework. However, most existing approaches underutilize the online mapping module, leaving its potential to enhance trajectory planning largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel map-assisted end-to-end trajectory planning framework. MAP explicitly integrates segmentation-based map features and the current ego status through a Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and a Weight Adapter based on current ego status. Experiments conducted on the DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6% reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a 44.5% improvement in overall score compared to the UniV2X baseline, even without post-processing. Furthermore, it achieves top ranking in Track 2 of the End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of overall score. These results highlight the effectiveness of explicitly leveraging semantic map features in planning and suggest new directions for improving structure design in end-to-end autonomous driving systems. Our code is available at https://gitee.com/kymkym/map.git
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetricNet: Recovering Metric Scale in Generative Navigation Policies</title>
<link>https://arxiv.org/abs/2509.13965</link>
<guid>https://arxiv.org/abs/2509.13965</guid>
<content:encoded><![CDATA[
arXiv:2509.13965v1 Announce Type: cross 
Abstract: Generative navigation policies have made rapid progress in improving end-to-end learned navigation. Despite their promising results, this paradigm has two structural problems. First, the sampled trajectories exist in an abstract, unscaled space without metric grounding. Second, the control strategy discards the full path, instead moving directly towards a single waypoint. This leads to short-sighted and unsafe actions, moving the robot towards obstacles that a complete and correctly scaled path would circumvent. To address these issues, we propose MetricNet, an effective add-on for generative navigation that predicts the metric distance between waypoints, grounding policy outputs in real-world coordinates. We evaluate our method in simulation with a new benchmarking framework and show that executing MetricNet-scaled waypoints significantly improves both navigation and exploration performance. Beyond simulation, we further validate our approach in real-world experiments. Finally, we propose MetricNav, which integrates MetricNet into a navigation policy to guide the robot away from obstacles while still moving towards the goal.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping</title>
<link>https://arxiv.org/abs/2509.14191</link>
<guid>https://arxiv.org/abs/2509.14191</guid>
<content:encoded><![CDATA[
arXiv:2509.14191v1 Announce Type: cross 
Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture-Aware Superpixel Segmentation</title>
<link>https://arxiv.org/abs/1901.11111</link>
<guid>https://arxiv.org/abs/1901.11111</guid>
<content:encoded><![CDATA[
arXiv:1901.11111v4 Announce Type: replace 
Abstract: Most superpixel algorithms compute a trade-off between spatial and color features at the pixel level. Hence, they may need fine parameter tuning to balance the two measures, and highly fail to group pixels with similar local texture properties. In this paper, we address these issues with a new Texture-Aware SuperPixel (TASP) method. To accurately segment textured and smooth areas, TASP automatically adjusts its spatial constraint according to the local feature variance. Then, to ensure texture homogeneity within superpixels, a new pixel to superpixel patch-based distance is proposed. TASP outperforms the segmentation accuracy of the state-of-the-art methods on texture and also natural color image datasets.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superpixel-based Color Transfer</title>
<link>https://arxiv.org/abs/1903.06010</link>
<guid>https://arxiv.org/abs/1903.06010</guid>
<content:encoded><![CDATA[
arXiv:1903.06010v2 Announce Type: replace 
Abstract: In this work, we propose a fast superpixel-based color transfer method (SCT) between two images. Superpixels enable to decrease the image dimension and to extract a reduced set of color candidates. We propose to use a fast approximate nearest neighbor matching algorithm in which we enforce the match diversity by limiting the selection of the same superpixels. A fusion framework is designed to transfer the matched colors, and we demonstrate the improvement obtained over exact matching results. Finally, we show that SCT is visually competitive compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Shape Regularity Criteria for Superpixel Evaluation</title>
<link>https://arxiv.org/abs/1903.07146</link>
<guid>https://arxiv.org/abs/1903.07146</guid>
<content:encoded><![CDATA[
arXiv:1903.07146v2 Announce Type: replace 
Abstract: Regular decompositions are necessary for most superpixel-based object recognition or tracking applications. So far in the literature, the regularity or compactness of a superpixel shape is mainly measured by its circularity. In this work, we first demonstrate that such measure is not adapted for superpixel evaluation, since it does not directly express regularity but circular appearance. Then, we propose a new metric that considers several shape regularity aspects: convexity, balanced repartition, and contour smoothness. Finally, we demonstrate that our measure is robust to scale and noise and enables to more relevantly compare superpixel methods.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCALP: Superpixels with Contour Adherence using Linear Path</title>
<link>https://arxiv.org/abs/1903.07149</link>
<guid>https://arxiv.org/abs/1903.07149</guid>
<content:encoded><![CDATA[
arXiv:1903.07149v2 Announce Type: replace 
Abstract: Superpixel decomposition methods are generally used as a pre-processing step to speed up image processing tasks. They group the pixels of an image into homogeneous regions while trying to respect existing contours. For all state-of-the-art superpixel decomposition methods, a trade-off is made between 1) computational time, 2) adherence to image contours and 3) regularity and compactness of the decomposition. In this paper, we propose a fast method to compute Superpixels with Contour Adherence using Linear Path (SCALP) in an iterative clustering framework. The distance computed when trying to associate a pixel to a superpixel during the clustering is enhanced by considering the linear path to the superpixel barycenter. The proposed framework produces regular and compact superpixels that adhere to the image contours. We provide a detailed evaluation of SCALP on the standard Berkeley Segmentation Dataset. The obtained results outperform state-of-the-art methods in terms of standard superpixel and contour detection metrics.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlaneRecTR++: Unified Query Learning for Joint 3D Planar Reconstruction and Pose Estimation</title>
<link>https://arxiv.org/abs/2307.13756</link>
<guid>https://arxiv.org/abs/2307.13756</guid>
<content:encoded><![CDATA[
arXiv:2307.13756v4 Announce Type: replace 
Abstract: The challenging task of 3D planar reconstruction from images involves several sub-tasks including frame-wise plane detection, segmentation, parameter regression and possibly depth prediction, along with cross-frame plane correspondence and relative camera pose estimation. Previous works adopt a divide and conquer strategy, addressing above sub-tasks with distinct network modules in a two-stage paradigm. Specifically, given an initial camera pose and per-frame plane predictions from the first stage, further exclusively designed modules relying on external plane correspondence labeling are applied to merge multi-view plane entities and produce refined camera pose. Notably, existing work fails to integrate these closely related sub-tasks into a unified framework, and instead addresses them separately and sequentially, which we identify as a primary source of performance limitations. Motivated by this finding and the success of query-based learning in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR++, a Transformer-based architecture, which for the first time unifies all tasks of multi-view planar reconstruction and pose estimation within a compact single-stage framework, eliminating the need for the initial pose estimation and supervision of plane correspondence. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across sub-tasks, achieving a new state-of-the-art performance on the public ScanNetv1, ScanNetv2, NYUv2-Plane, and MatterPort3D datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR-PP.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GROOD: GRadient-Aware Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2312.14427</link>
<guid>https://arxiv.org/abs/2312.14427</guid>
<content:encoded><![CDATA[
arXiv:2312.14427v3 Announce Type: replace 
Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability of deep learning models in real-world applications. Existing methods typically focus on feature representations or output-space analysis, often assuming a distribution over these spaces or leveraging gradient norms with respect to model parameters. However, these approaches struggle to distinguish near-OOD samples and often require extensive hyper-parameter tuning, limiting their practicality.In this work, we propose GRadient-aware Out-Of-Distribution detection (GROOD), a method that derives an OOD prototype from synthetic samples and computes class prototypes directly from In-distribution (ID) training data. By analyzing the gradients of a nearest-class-prototype loss function concerning an artificial OOD prototype, our approach achieves a clear separation between in-distribution and OOD samples. Experimental evaluations demonstrate that gradients computed from the OOD prototype enhance the distinction between ID and OOD data, surpassing established baselines in robustness, particularly on ImageNet-1k. These findings highlight the potential of gradient-based methods and prototype-driven approaches in advancing OOD detection within deep neural networks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Perceptual Scores for Dataset Pruning in Computer Vision Tasks</title>
<link>https://arxiv.org/abs/2408.07243</link>
<guid>https://arxiv.org/abs/2408.07243</guid>
<content:encoded><![CDATA[
arXiv:2408.07243v2 Announce Type: replace 
Abstract: In this paper we propose a score of an image to use for coreset selection in image classification and semantic segmentation tasks. The score is the entropy of an image as approximated by the bits-per-pixel of its compressed version. Thus the score is intrinsic to an image and does not require supervision or training. It is very simple to compute and readily available as all images are stored in a compressed format. The motivation behind our choice of score is that most other scores proposed in literature are expensive to compute. More importantly, we want a score that captures the perceptual complexity of an image. Entropy is one such measure, images with clutter tend to have a higher entropy. However sampling only low entropy iconic images, for example, leads to biased learning and an overall decrease in test performance with current deep learning models. To mitigate the bias we use a graph based method that increases the spatial diversity of the selected samples. We show that this simple score yields good results, particularly for semantic segmentation tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</title>
<link>https://arxiv.org/abs/2408.08872</link>
<guid>https://arxiv.org/abs/2408.08872</guid>
<content:encoded><![CDATA[
arXiv:2408.08872v4 Announce Type: replace 
Abstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing</title>
<link>https://arxiv.org/abs/2409.01086</link>
<guid>https://arxiv.org/abs/2409.01086</guid>
<content:encoded><![CDATA[
arXiv:2409.01086v3 Announce Type: replace 
Abstract: Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user's textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks for Image Analysis</title>
<link>https://arxiv.org/abs/2410.19794</link>
<guid>https://arxiv.org/abs/2410.19794</guid>
<content:encoded><![CDATA[
arXiv:2410.19794v4 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models' outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain age identification from diffusion MRI synergistically predicts neurodegenerative disease</title>
<link>https://arxiv.org/abs/2410.22454</link>
<guid>https://arxiv.org/abs/2410.22454</guid>
<content:encoded><![CDATA[
arXiv:2410.22454v3 Announce Type: replace 
Abstract: Estimated brain age from magnetic resonance image (MRI) and its deviation from chronological age can provide early insights into potential neurodegenerative diseases, supporting early detection and implementation of prevention strategies. Diffusion MRI (dMRI) presents an opportunity to build an earlier biomarker for neurodegenerative disease prediction because it captures subtle microstructural changes that precede more perceptible macrostructural changes. However, the coexistence of macro- and micro-structural information in dMRI raises the question of whether current dMRI-based brain age estimation models are leveraging the intended microstructural information or if they inadvertently rely on the macrostructural information. To develop a microstructure-specific brain age, we propose a method for brain age identification from dMRI that mitigates the model's use of macrostructural information by non-rigidly registering all images to a standard template. Imaging data from 13,398 participants across 12 datasets were used for the training and evaluation. We compare our brain age models, trained with and without macrostructural information mitigated, with an architecturally similar T1-weighted (T1w) MRI-based brain age model and two recent, popular, openly available T1w MRI-based brain age models that primarily use macrostructural information. We observe difference between our dMRI-based brain age and T1w MRI-based brain age across stages of neurodegeneration, with dMRI-based brain age being older than T1w MRI-based brain age in participants transitioning from cognitively normal (CN) to mild cognitive impairment (MCI), but younger in participants already diagnosed with Alzheimer's disease (AD). Furthermore, dMRI-based brain age may offer advantages over T1w MRI-based brain age in predicting the transition from CN to MCI up to five years before diagnosis.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo Anything: Unifying Zero-shot Stereo Matching with Large-Scale Mixed Data</title>
<link>https://arxiv.org/abs/2411.14053</link>
<guid>https://arxiv.org/abs/2411.14053</guid>
<content:encoded><![CDATA[
arXiv:2411.14053v3 Announce Type: replace 
Abstract: Stereo matching serves as a cornerstone in 3D vision, aiming to establish pixel-wise correspondences between stereo image pairs for depth recovery. Despite remarkable progress driven by deep neural architectures, current models often exhibit severe performance degradation when deployed in unseen domains, primarily due to the limited diversity of training data. In this work, we introduce StereoAnything, a data-centric framework that substantially enhances the zero-shot generalization capability of existing stereo models. Rather than devising yet another specialized architecture, we scale stereo training to an unprecedented level by systematically unifying heterogeneous stereo sources: (1) curated labeled datasets covering diverse environments, and (2) large-scale synthetic stereo pairs generated from unlabeled monocular images. Our mixed-data strategy delivers consistent and robust learning signals across domains, effectively mitigating dataset bias. Extensive zero-shot evaluations on four public benchmarks demonstrate that Stereo Anything achieves state-of-the-art generalization. This work paves the way towards truly universal stereo matching, offering a scalable data paradigm applicable to any stereo image pair. We extensively evaluate the zero-shot capabilities of our model on four public datasets, showcasing its impressive ability to generalize to any stereo image pair. Code is available at https://github.com/XiandaGuo/OpenStereo.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniPLV: Towards Label-Efficient Open-World 3D Scene Understanding by Regional Visual Language Supervision</title>
<link>https://arxiv.org/abs/2412.18131</link>
<guid>https://arxiv.org/abs/2412.18131</guid>
<content:encoded><![CDATA[
arXiv:2412.18131v2 Announce Type: replace 
Abstract: Open-world 3D scene understanding is a critical challenge that involves recognizing and distinguishing diverse objects and categories from 3D data, such as point clouds, without relying on manual annotations. Traditional methods struggle with this open-world task, especially due to the limitations of constructing extensive point cloud-text pairs and handling multimodal data effectively. In response to these challenges, we present UniPLV, a robust framework that unifies point clouds, images, and text within a single learning paradigm for comprehensive 3D scene understanding. UniPLV leverages images as a bridge to co-embed 3D points with pre-aligned images and text in a shared feature space, eliminating the need for labor-intensive point cloud-text pair crafting. Our framework achieves precise multimodal alignment through two innovative strategies: (i) Logit and feature distillation modules between images and point clouds to enhance feature coherence; (ii) A vision-point matching module that implicitly corrects 3D semantic predictions affected by projection inaccuracies from points to pixels. To further boost performance, we implement four task-specific losses alongside a two-stage training strategy. Extensive experiments demonstrate that UniPLV significantly surpasses state-of-the-art methods, with average improvements of 15.6% and 14.8% in semantic segmentation for Base-Annotated and Annotation-Free tasks, respectively. These results underscore UniPLV's efficacy in pushing the boundaries of open-world 3D scene understanding. We will release the code to support future research and development.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Pipeline for Solid Waste Detection in Remote Sensing Images</title>
<link>https://arxiv.org/abs/2502.06607</link>
<guid>https://arxiv.org/abs/2502.06607</guid>
<content:encoded><![CDATA[
arXiv:2502.06607v4 Announce Type: replace 
Abstract: Improper solid waste management represents both a serious threat to ecosystem health and a significant source of revenues for criminal organizations perpetrating environmental crimes. This issue can be mitigated thanks to the increasing availability of Very-High-Resolution Remote Sensing (VHR RS) images. Modern image-analysis tools support automated photo-interpretation and large territory scanning in search of illegal waste disposal sites. This paper illustrates a semi-automatic waste detection pipeline, developed in collaboration with a regional environmental protection agency, for detecting candidate illegal dumping sites in VHR RS images. To optimize the effectiveness of the waste detector at the core of the pipeline, extensive experiments evaluate such design choices as the network architecture, the ground resolution and geographic span of the input images, as well as the pretraining procedures. The best model attains remarkable performance, achieving 92.02 % F1-Score and 94.56 % Accuracy. A generalization study assesses the performance variation when the detector processes images from various territories substantially different from the one used during training, incurring only a moderate performance loss, namely an average 5.1 % decrease in the F1-Score. Finally, an exercise in which expert photo-interpreters compare the effort required to scan large territories with and without support from the waste detector assesses the practical benefit of introducing a computer-aided image analysis tool in a professional environmental protection agency. Results show that a reduction of up to 30 % of the time spent for waste site detection can be attained.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Preference Optimization for Vision-Language Long-Horizon Task Planning</title>
<link>https://arxiv.org/abs/2502.20742</link>
<guid>https://arxiv.org/abs/2502.20742</guid>
<content:encoded><![CDATA[
arXiv:2502.20742v4 Announce Type: replace 
Abstract: Existing methods for vision-language task planning excel in short-horizon tasks but often fall short in complex, long-horizon planning within dynamic environments. These challenges primarily arise from the difficulty of effectively training models to produce high-quality reasoning processes for long-horizon tasks. To address this, we propose Structured Preference Optimization (SPO), which aims to enhance reasoning and action selection in long-horizon task planning through structured preference evaluation and optimized training strategies. Specifically, SPO introduces: 1) Preference-Based Scoring and Optimization, which systematically evaluates reasoning chains based on task relevance, visual grounding, and historical consistency; and 2) Curriculum-Guided Training, where the model progressively adapts from simple to complex tasks, improving its generalization ability in long-horizon scenarios and enhancing reasoning robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images</title>
<link>https://arxiv.org/abs/2503.14171</link>
<guid>https://arxiv.org/abs/2503.14171</guid>
<content:encoded><![CDATA[
arXiv:2503.14171v2 Announce Type: replace 
Abstract: We introduce an image upscaling technique tailored for 3D Gaussian Splatting (3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher rendering speeds and reduces artifacts commonly observed in 3DGS reconstructions. Our technique upscales low-resolution 3DGS renderings with a marginal increase in cost by directly leveraging the analytical image gradients of Gaussians for gradient-based bicubic spline interpolation. The technique is agnostic to the specific 3DGS implementation, achieving novel view synthesis at rates 3x-4x higher than the baseline implementation. Through extensive experiments on multiple datasets, we showcase the performance improvements and high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS images. We further demonstrate the integration of gradient-aware upscaling into the gradient-based optimization of a 3DGS model and analyze its effects on reconstruction quality and performance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</title>
<link>https://arxiv.org/abs/2504.08531</link>
<guid>https://arxiv.org/abs/2504.08531</guid>
<content:encoded><![CDATA[
arXiv:2504.08531v2 Announce Type: replace 
Abstract: We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning/
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging</title>
<link>https://arxiv.org/abs/2504.10288</link>
<guid>https://arxiv.org/abs/2504.10288</guid>
<content:encoded><![CDATA[
arXiv:2504.10288v2 Announce Type: replace 
Abstract: We present a new self-supervised deep-learning-based Ghost Imaging (GI) reconstruction method, which provides unparalleled reconstruction performance for noisy acquisitions among unsupervised methods. We present the supporting mathematical framework and results from theoretical and real data use cases. Self-supervision removes the need for clean reference data while offering strong noise reduction. This provides the necessary tools for addressing signal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge low-light GI scenarios. Notable examples include micro- and nano-scale x-ray emission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples. Their applications include in-vivo and in-operando case studies for biological samples and batteries.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Video-Based Spatiotemporal Deep Learning for Cattle Lameness Detection</title>
<link>https://arxiv.org/abs/2504.16404</link>
<guid>https://arxiv.org/abs/2504.16404</guid>
<content:encoded><![CDATA[
arXiv:2504.16404v3 Announce Type: replace 
Abstract: Cattle lameness is a prevalent health problem in livestock farming, often resulting from hoof injuries or infections, and severely impacts animal welfare and productivity. Early and accurate detection is critical for minimizing economic losses and ensuring proper treatment. This study proposes a spatiotemporal deep learning framework for automated cattle lameness detection using publicly available video data. We curate and publicly release a balanced set of 50 online video clips featuring 42 individual cattle, recorded from multiple viewpoints in both indoor and outdoor environments. The videos were categorized into lame and non-lame classes based on visual gait characteristics and metadata descriptions. After applying data augmentation techniques to enhance generalization, two deep learning architectures were trained and evaluated: 3D Convolutional Neural Networks (3D CNN) and Convolutional Long-Short-Term Memory (ConvLSTM2D). The 3D CNN achieved a video-level classification accuracy of 90%, with a precision, recall, and F1 score of 90.9% each, outperforming the ConvLSTM2D model, which achieved 85% accuracy. Unlike conventional approaches that rely on multistage pipelines involving object detection and pose estimation, this study demonstrates the effectiveness of a direct end-to-end video classification approach. Compared with the best end-to-end prior method (C3D-ConvLSTM, 90.3%), our model achieves comparable accuracy while eliminating pose estimation pre-processing.The results indicate that deep learning models can successfully extract and learn spatio-temporal features from various video sources, enabling scalable and efficient cattle lameness detection in real-world farm settings.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROP: Contextual Region-Oriented Visual Token Pruning</title>
<link>https://arxiv.org/abs/2505.21233</link>
<guid>https://arxiv.org/abs/2505.21233</guid>
<content:encoded><![CDATA[
arXiv:2505.21233v2 Announce Type: replace 
Abstract: Current VLM-based VQA methods often process entire images, leading to excessive visual tokens that include redundant information irrelevant to the posed question. This abundance of unnecessary image details creates numerous visual tokens, drastically increasing memory and computational requirements in VLMs. To address this, we propose Contextual Region-Oriented Visual Token Pruning (CROP), a novel framework to compress visual tokens through a two-step process: Localization and Pruning. Specifically, CROP first employs an efficient model to identify the contextual region relevant to the input query. Subsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM Compression (PLC), which adaptively compresses different image regions with varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that prunes tokens within early LLM layers guided by the identified contextual region. Extensive experiments on a wide range of VQA tasks demonstrate that CROP significantly outperforms existing visual token pruning methods and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations</title>
<link>https://arxiv.org/abs/2507.04705</link>
<guid>https://arxiv.org/abs/2507.04705</guid>
<content:encoded><![CDATA[
arXiv:2507.04705v2 Announce Type: replace 
Abstract: Identity-preserving text-to-video (IPT2V) generation, which aims to create high-fidelity videos with consistent human identity, has become crucial for downstream applications. However, current end-to-end frameworks suffer a critical spatial-temporal trade-off: optimizing for spatially coherent layouts of key elements (e.g., character identity preservation) often compromises instruction-compliant temporal smoothness, while prioritizing dynamic realism risks disrupting the spatial coherence of visual structures. To tackle this issue, we propose a simple yet effective spatial-temporal decoupled framework that decomposes representations into spatial features for layouts and temporal features for motion dynamics. Specifically, our paper proposes a semantic prompt optimization mechanism and stage-wise decoupled generation paradigm. The former module decouples the prompt into spatial and temporal components. Aligned with the subsequent stage-wise decoupled approach, the spatial prompts guide the text-to-image (T2I) stage to generate coherent spatial features, while the temporal prompts direct the sequential image-to-video (I2V) stage to ensure motion consistency. Experimental results validate that our approach achieves excellent spatiotemporal consistency, demonstrating outstanding performance in identity preservation, text relevance, and video quality. By leveraging this simple yet robust mechanism, our algorithm secures the runner-up position in 2025 ACM MultiMedia Challenge.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effort-Optimized, Accuracy-Driven Labelling and Validation of Test Inputs for DL Systems: A Mixed-Integer Linear Programming Approach</title>
<link>https://arxiv.org/abs/2507.04990</link>
<guid>https://arxiv.org/abs/2507.04990</guid>
<content:encoded><![CDATA[
arXiv:2507.04990v2 Announce Type: replace 
Abstract: Software systems increasingly include AI components based on deep learning (DL). Reliable testing of such systems requires near-perfect test-input validity and label accuracy, with minimal human effort. Yet, the DL community has largely overlooked the need to build highly accurate datasets with minimal effort, since DL training is generally tolerant of labelling errors. This challenge, instead, reflects concerns more familiar to software engineering, where a central goal is to construct high-accuracy test inputs, with accuracy as close to 100% as possible, while keeping associated costs in check. In this article we introduce OPAL, a human-assisted labelling method that can be configured to target a desired accuracy level while minimizing the manual effort required for labelling. The main contribution of OPAL is a mixed-integer linear programming (MILP) formulation that minimizes labelling effort subject to a specified accuracy target. To evaluate OPAL we instantiate it for two tasks in the context of testing vision systems: automatic labelling of test inputs and automated validation of test inputs. Our evaluation, based on more than 2500 experiments performed on seven datasets, comparing OPAL with eight baseline methods, shows that OPAL, relying on its MILP formulation, achieves an average accuracy of 98.8%, while cutting manual labelling by more than half. OPAL significantly outperforms automated labelling baselines in labelling accuracy across all seven datasets, when all methods are provided with the same manual-labelling budget. For automated test-input validation, on average, OPAL reduces manual effort by 28.8% while achieving 4.5% higher accuracy than the SOTA test-input validation baselines. Finally, we show that augmenting OPAL with an active-learning loop leads to an additional 4.5% reduction in required manual labelling, without compromising accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification</title>
<link>https://arxiv.org/abs/2508.00552</link>
<guid>https://arxiv.org/abs/2508.00552</guid>
<content:encoded><![CDATA[
arXiv:2508.00552v2 Announce Type: replace 
Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</title>
<link>https://arxiv.org/abs/2508.05606</link>
<guid>https://arxiv.org/abs/2508.05606</guid>
<content:encoded><![CDATA[
arXiv:2508.05606v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.
  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment</title>
<link>https://arxiv.org/abs/2508.06082</link>
<guid>https://arxiv.org/abs/2508.06082</guid>
<content:encoded><![CDATA[
arXiv:2508.06082v2 Announce Type: replace 
Abstract: Diffusion-based or flow-based models have achieved significant progress in video synthesis but require multiple iterative sampling steps, which incurs substantial computational overhead. While many distillation methods that are solely based on trajectory-preserving or distribution-matching have been developed to accelerate video generation models, these approaches often suffer from performance breakdown or increased artifacts under few-step settings. To address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and stable distillation framework that combines the advantages of trajectory-preserving and distribution-matching strategies. Our approach introduces continuous-time consistency distillation to ensure precise preservation of ODE trajectories. Subsequently, we propose a dual-perspective alignment that includes distribution alignment between synthetic and real data along with trajectory alignment across different inference steps. Our method maintains high-quality video generation while substantially reducing the number of inference steps. Quantitative evaluations on the OpenVid-1M benchmark demonstrate that our method significantly outperforms existing approaches in few-step video generation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety</title>
<link>https://arxiv.org/abs/2508.09397</link>
<guid>https://arxiv.org/abs/2508.09397</guid>
<content:encoded><![CDATA[
arXiv:2508.09397v2 Announce Type: replace 
Abstract: Drones operating in complex environments face a significant threat from thin obstacles, such as steel wires and kite strings at the submillimeter level, which are notoriously difficult for conventional sensors like RGB cameras, LiDAR, and depth cameras to detect. This paper introduces SkyShield, an event-driven, end-to-end framework designed for the perception of submillimeter scale obstacles. Drawing upon the unique features that thin obstacles present in the event stream, our method employs a lightweight U-Net architecture and an innovative Dice-Contour Regularization Loss to ensure precise detection. Experimental results demonstrate that our event-based approach achieves mean F1 Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment on edge and mobile platforms.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</title>
<link>https://arxiv.org/abs/2508.10256</link>
<guid>https://arxiv.org/abs/2508.10256</guid>
<content:encoded><![CDATA[
arXiv:2508.10256v2 Announce Type: replace 
Abstract: Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset acquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new annotated dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: https://github.com/nantonzhang/Awesome-Crack-Detection
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singular Value Few-shot Adaptation of Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.03740</link>
<guid>https://arxiv.org/abs/2509.03740</guid>
<content:encoded><![CDATA[
arXiv:2509.03740v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics</title>
<link>https://arxiv.org/abs/2405.19988</link>
<guid>https://arxiv.org/abs/2405.19988</guid>
<content:encoded><![CDATA[
arXiv:2405.19988v3 Announce Type: replace-cross 
Abstract: Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event Condition For Foley Sound</title>
<link>https://arxiv.org/abs/2408.11915</link>
<guid>https://arxiv.org/abs/2408.11915</guid>
<content:encoded><![CDATA[
arXiv:2408.11915v3 Announce Type: replace-cross 
Abstract: Foley sound synthesis is crucial for multimedia production, enhancing user experience by synchronizing audio and video both temporally and semantically. Recent studies on automating this labor-intensive process through video-to-sound generation face significant challenges. Systems lacking explicit temporal features suffer from poor alignment and controllability, while timestamp-based models require costly and subjective human annotation. We propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an intuitive condition with semantic timbre prompts (audio or text). RMS, a frame-level intensity envelope closely related to audio semantics, acts as a temporal event feature to guide audio generation from video. The annotation-free self-supervised learning framework consists of two stages, Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization and RMS-ControlNet with a pretrained text-to-audio model. Our extensive evaluation shows that Video-Foley achieves state-of-the-art performance in audio-visual alignment and controllability for sound timing, intensity, timbre, and nuance. Source code, model weights and demos are available on our companion website. (https://jnwnlee.github.io/video-foley-demo)
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis and Perceptual Scaling of High Resolution Naturalistic Images Using Stable Diffusion</title>
<link>https://arxiv.org/abs/2410.13034</link>
<guid>https://arxiv.org/abs/2410.13034</guid>
<content:encoded><![CDATA[
arXiv:2410.13034v2 Announce Type: replace-cross 
Abstract: Naturalistic scenes are of key interest for visual perception, but controlling their perceptual and semantic properties is challenging. Previous work on naturalistic scenes has frequently focused on collections of discrete images with considerable physical differences between stimuli. However, it is often desirable to assess representations of naturalistic images that vary along a continuum. Traditionally, perceptually continuous variations of naturalistic stimuli have been obtained by morphing a source image into a target image. This produces transitions driven mainly by low-level physical features and can result in semantically ambiguous outcomes. More recently, generative adversarial networks (GANs) have been used to generate continuous perceptual variations within a stimulus category. Here we extend and generalize this approach using a different machine learning approach, a text-to-image diffusion model (Stable Diffusion XL), to generate a freely customizable stimulus set of photorealistic images that are characterized by gradual transitions, with each image representing a unique exemplar within a prompted category. We demonstrate the approach by generating a set of 108 object scenes from 6 categories. For each object scene, we generate 10 variants that are ordered along a perceptual continuum. This ordering was first estimated using a machine learning model of perceptual similarity (LPIPS) and then subsequently validated with a large online sample of human participants. In a subsequent experiment we show that this ordering is also predictive of confusability of stimuli in a working memory experiment. Our image set is suited for studies investigating the graded encoding of naturalistic stimuli in visual perception, attention, and memory.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scattering approach to diffusion quantifies axonal damage in brain injury</title>
<link>https://arxiv.org/abs/2501.18167</link>
<guid>https://arxiv.org/abs/2501.18167</guid>
<content:encoded><![CDATA[
arXiv:2501.18167v2 Announce Type: replace-cross 
Abstract: Early diagnosis and noninvasive monitoring of neurological disorders require sensitivity to elusive cellular-level alterations that occur much earlier than volumetric changes observable with the millimeter-resolution of medical imaging modalities. Morphological changes in axons, such as axonal varicosities or beadings, are observed in neurological disorders, as well as in development and aging. Here, we reveal the sensitivity of time-dependent diffusion MRI (dMRI) to the structurally disordered axonal morphology at the micrometer scale. Scattering theory uncovers the two parameters that determine the diffusive dynamics of water along axons: the average reciprocal cross-section and the variance of long-range cross-sectional fluctuations. This theoretical development allows us to predict dMRI metrics sensitive to axonal alterations over tens of thousands of axons in seconds rather than months of simulations in a rat model of traumatic brain injury, and is corroborated with ex vivo dMRI. Our approach bridges the gap between micrometers and millimeters in resolution, offering quantitative and objective biomarkers applicable to a broad spectrum of neurological disorders.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients</title>
<link>https://arxiv.org/abs/2503.05424</link>
<guid>https://arxiv.org/abs/2503.05424</guid>
<content:encoded><![CDATA[
arXiv:2503.05424v2 Announce Type: replace-cross 
Abstract: Deep learning models achieve high predictive performance but lack intrinsic interpretability, hindering our understanding of the learned prediction behavior. Existing local explainability methods focus on associations, neglecting the causal drivers of model predictions. Other approaches adopt a causal perspective but primarily provide global, model-level explanations. However, for specific inputs, it's unclear whether globally identified factors apply locally. To address this limitation, we introduce a novel framework for local interventional explanations by leveraging recent advances in image-to-image editing models. Our approach performs gradual interventions on semantic properties to quantify the corresponding impact on a model's predictions using a novel score, the expected property gradient magnitude. We demonstrate the effectiveness of our approach through an extensive empirical evaluation on a wide range of architectures and tasks. First, we validate it in a synthetic scenario and demonstrate its ability to locally identify biases. Afterward, we apply our approach to investigate medical skin lesion classifiers, analyze network training dynamics, and study a pre-trained CLIP model with real-life interventional data. Our results highlight the potential of interventional explanations on the property level to reveal new insights into the behavior of deep models.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes</title>
<link>https://arxiv.org/abs/2505.05798</link>
<guid>https://arxiv.org/abs/2505.05798</guid>
<content:encoded><![CDATA[
arXiv:2505.05798v2 Announce Type: replace-cross 
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming distance decoding. Our proposed KAN with ECOC framework outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy across diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first work of ECOC with KAN for enhancing multi-class medical image classification performance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
arXiv:2505.23759v2 Announce Type: replace-cross 
Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Culturally-diverse Multilingual Multimodal Video Benchmark &amp; Model</title>
<link>https://arxiv.org/abs/2506.07032</link>
<guid>https://arxiv.org/abs/2506.07032</guid>
<content:encoded><![CDATA[
arXiv:2506.07032v2 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have recently gained attention due to their effectiveness to understand and generate descriptions of visual content. Most existing LMMs are in English language. While few recent works explore multilingual image LMMs, to the best of our knowledge, moving beyond the English language for cultural and linguistic inclusivity is yet to be investigated in the context of video LMMs. In pursuit of more inclusive video LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to evaluate Video LMMs across 14 languages, including both low- and high-resource languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is designed to rigorously test video LMMs across 15 categories including eight culturally diverse categories, ranging from lifestyles and festivals to foods and rituals and from local landmarks to prominent cultural personalities. ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice questions spanning various video durations (short, medium, and long) with 8k samples that are manually verified by native language speakers. In addition, we also introduce a machine translated multilingual video training set comprising 1.2 million samples and develop a simple multilingual video LMM, named ViMUL, that is shown to provide a better tradeoff between high-and low-resource languages for video understanding. We hope our ViMUL-Bench and multilingual video LMM along with a large-scale multilingual video training set will help ease future research in developing cultural and linguistic inclusive multilingual video LMMs. Our proposed benchmark, video LMM and training data will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection</title>
<link>https://arxiv.org/abs/2507.02668</link>
<guid>https://arxiv.org/abs/2507.02668</guid>
<content:encoded><![CDATA[
arXiv:2507.02668v4 Announce Type: replace-cross 
Abstract: Colorectal polyp segmentation is critical for early detection of colorectal cancer, yet weak and low contrast boundaries significantly limit automated accuracy. Existing deep models either blur fine edge details or rely on handcrafted filters that perform poorly under variable imaging conditions. We propose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects directional, parameter free Haar wavelet edge maps into each decoder stage to recalibrate semantic features. The key novelties of MEGANet-W include a two-level Haar wavelet head for multi-orientation edge extraction; and Wavelet Edge Guided Attention (W-EGA) modules that fuse wavelet cues with boundary and input branches. On five public polyp datasets, MEGANet-W consistently outperforms existing methods, improving mIoU by up to 2.3% and mDice by 1.2%, while introducing no additional learnable parameters. This approach improves reliability in difficult cases and offers a robust solution for medical image segmentation tasks requiring precise boundary detection.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2508.17600</link>
<guid>https://arxiv.org/abs/2508.17600</guid>
<content:encoded><![CDATA[
arXiv:2508.17600v2 Announce Type: replace-cross 
Abstract: Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR: A DETR-based Framework for Defect Detection in Transmission Lines from UAV Imagery</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
<div> Keywords: automated defect detection, UAV imagery, transmission lines, TinyDef-DETR, DETR-based framework

Summary: 
TinyDef-DETR is a framework designed for accurate and efficient detection of transmission line defects from UAV-acquired images. It integrates edge-enhanced ResNet backbone, a space-to-depth module for downsampling, a cross-stage dual-domain multi-scale attention mechanism, and a Focaler-Wise-SIoU regression loss. These components work together to overcome the challenges of detecting small and ambiguous defects in complex backgrounds. The framework demonstrates superior detection performance, strong generalization capability, and modest computational overhead through extensive experiments on various datasets. TinyDef-DETR's accuracy and efficiency make it suitable for UAV-based defect detection on transmission lines, especially in challenging scenarios with small and ambiguous targets.

<br /><br />Summary: <div>
arXiv:2509.06035v3 Announce Type: replace 
Abstract: Automated defect detection from UAV imagery of transmission lines is a challenging task due to the small size, ambiguity, and complex backgrounds of defects. This paper proposes TinyDef-DETR, a DETR-based framework designed to achieve accurate and efficient detection of transmission line defects from UAV-acquired images. The model integrates four major components: an edge-enhanced ResNet backbone to strengthen boundary-sensitive representations, a stride-free space-to-depth module to enable detail-preserving downsampling, a cross-stage dual-domain multi-scale attention mechanism to jointly model global context and local cues, and a Focaler-Wise-SIoU regression loss to improve the localization of small and difficult targets. Together, these designs effectively mitigate the limitations of conventional detectors. Extensive experiments on both public and real-world datasets demonstrate that TinyDef-DETR achieves superior detection performance and strong generalization capability, while maintaining modest computational overhead. The accuracy and efficiency of TinyDef-DETR make it a suitable method for UAV-based transmission line defect detection, particularly in scenarios involving small and ambiguous targets.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06040</link>
<guid>https://arxiv.org/abs/2509.06040</guid>
<content:encoded><![CDATA[
<div> branchGRPO, generative models, alignment, reinforcement learning, image denoising 

Summary: BranchGRPO introduces a novel method for improving human preference alignment in image and video generative models. It addresses the inefficiency of existing variants by restructuring the rollout process into a branching tree, where shared prefixes amortize computation and pruning removes low-value paths. The method includes a branching scheme for cost amortization, a reward fusion and advantage estimator for transforming sparse rewards into dense signals, and pruning strategies for efficient gradient computation. BranchGRPO outperforms DanceGRPO in image alignment scores by up to 16% while reducing training time by 55%. The hybrid variant, BranchGRPO-Mix, achieves even faster training without compromising alignment quality. On video generation tasks, BranchGRPO produces sharper and temporally consistent frames compared to DanceGRPO. This method shows promising results for improving alignment in generative models. 

<br /><br />Summary: <div>
arXiv:2509.06040v4 Announce Type: replace 
Abstract: Recent progress in aligning image and video generative models with Group Relative Policy Optimization (GRPO) has improved human preference alignment, but existing variants remain inefficient due to sequential rollouts and large numbers of sampling steps, unreliable credit assignment: sparse terminal rewards are uniformly propagated across timesteps, failing to capture the varying criticality of decisions during denoising. In this paper, we present BranchGRPO, a method that restructures the rollout process into a branching tree, where shared prefixes amortize computation and pruning removes low-value paths and redundant depths. BranchGRPO introduces three contributions: (1) a branching scheme that amortizes rollout cost through shared prefixes while preserving exploration diversity; (2) a reward fusion and depth-wise advantage estimator that transforms sparse terminal rewards into dense step-level signals; and (3) pruning strategies that cut gradient computation but leave forward rollouts and exploration unaffected. On HPDv2.1 image alignment, BranchGRPO improves alignment scores by up to \textbf{16\%} over DanceGRPO, while reducing per-iteration training time by nearly \textbf{55\%}. A hybrid variant, BranchGRPO-Mix, further accelerates training to 4.7x faster than DanceGRPO without degrading alignment. On WanX video generation, it further achieves higher Video-Align scores with sharper and temporally consistent frames compared to DanceGRPO. Codes are available at \href{https://fredreic1849.github.io/BranchGRPO-Webpage/}{BranchGRPO}.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction</title>
<link>https://arxiv.org/abs/2509.12242</link>
<guid>https://arxiv.org/abs/2509.12242</guid>
<content:encoded><![CDATA[
<div> Machine learning, 3D reconstruction, anatomical segmentation, breast MRI, preoperative planning

Summary:
U-Mamba, a novel machine learning methodology, was developed to improve algorithm generalization for 3D anatomical reconstruction using 120 breast MRI datasets. The three-phase process included anonymization, manual segmentation, co-registration, and 3D visualization. U-Mamba demonstrated high performance with Dice similarity coefficient values of 0.97 for whole organs, 0.96 for fibroglandular tissue, and 0.82 for tumors on T1-weighted images. Clinician interviews revealed enhanced planning, navigation, and decision support, while patient interviews highlighted improved education, communication, and understanding. The integration of 3D visualization facilitated shared decision-making and empowered informed patient choices across medical applications.<br /><br />Summary: <div>
arXiv:2509.12242v1 Announce Type: new 
Abstract: Effective preoperative planning requires accurate algorithms for segmenting anatomical structures across diverse datasets, but traditional models struggle with generalization. This study presents a novel machine learning methodology to improve algorithm generalization for 3D anatomical reconstruction beyond breast cancer applications. We processed 120 retrospective breast MRIs (January 2018-June 2023) through three phases: anonymization and manual segmentation of T1-weighted and dynamic contrast-enhanced sequences; co-registration and segmentation of whole breast, fibroglandular tissue, and tumors; and 3D visualization using ITK-SNAP. A human-in-the-loop approach refined segmentations using U-Mamba, designed to generalize across imaging scenarios. Dice similarity coefficient assessed overlap between automated segmentation and ground truth. Clinical relevance was evaluated through clinician and patient interviews. U-Mamba showed strong performance with DSC values of 0.97 ($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and 0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate 3D reconstructions enabling visualization of complex anatomical features. Clinician interviews indicated improved planning, intraoperative navigation, and decision support. Integration of 3D visualization enhanced patient education, communication, and understanding. This human-in-the-loop machine learning approach successfully generalizes algorithms for 3D reconstruction and anatomical segmentation across patient datasets, offering enhanced visualization for clinicians, improved preoperative planning, and more effective patient education, facilitating shared decision-making and empowering informed patient choices across medical applications.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RU-Net for Automatic Characterization of TRISO Fuel Cross Sections</title>
<link>https://arxiv.org/abs/2509.12244</link>
<guid>https://arxiv.org/abs/2509.12244</guid>
<content:encoded><![CDATA[
<div> Keywords: TRISO fuel, convolutional neural networks, image segmentation, irradiated particles, machine learning

Summary: 
Convolutional neural networks (CNNs) were utilized to automatically segment cross-sectional images of irradiated TRISO fuel particles. With a dataset of over 2,000 microscopic images, including annotated data, four different CNN architectures were tested â RU-Net, U-Net, ResNet, and Attention U-Net. The RU-Net model demonstrated the highest performance based on Intersection over Union (IoU). This automated segmentation approach using CNNs allows for expedited analysis of TRISO particle cross sections, minimizing manual labor and enhancing objectivity in results. The study aims to address the challenges of identifying irradiation-induced morphological changes in TRISO fuel, such as kernel swelling and buffer densification, by leveraging machine learning techniques for efficient data analysis in the field of nuclear fuel research.<br /><br />Summary: <div>
arXiv:2509.12244v1 Announce Type: new 
Abstract: During irradiation, phenomena such as kernel swelling and buffer densification may impact the performance of tristructural isotropic (TRISO) particle fuel. Post-irradiation microscopy is often used to identify these irradiation-induced morphologic changes. However, each fuel compact generally contains thousands of TRISO particles. Manually performing the work to get statistical information on these phenomena is cumbersome and subjective. To reduce the subjectivity inherent in that process and to accelerate data analysis, we used convolutional neural networks (CNNs) to automatically segment cross-sectional images of microscopic TRISO layers. CNNs are a class of machine-learning algorithms specifically designed for processing structured grid data. They have gained popularity in recent years due to their remarkable performance in various computer vision tasks, including image classification, object detection, and image segmentation. In this research, we generated a large irradiated TRISO layer dataset with more than 2,000 microscopic images of cross-sectional TRISO particles and the corresponding annotated images. Based on these annotated images, we used different CNNs to automatically segment different TRISO layers. These CNNs include RU-Net (developed in this study), as well as three existing architectures: U-Net, Residual Network (ResNet), and Attention U-Net. The preliminary results show that the model based on RU-Net performs best in terms of Intersection over Union (IoU). Using CNN models, we can expedite the analysis of TRISO particle cross sections, significantly reducing the manual labor involved and improving the objectivity of the segmentation results.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture</title>
<link>https://arxiv.org/abs/2509.12247</link>
<guid>https://arxiv.org/abs/2509.12247</guid>
<content:encoded><![CDATA[
<div> autoencoder, anomaly detection, multispectral imaging, nutrient management, sustainable agriculture

Summary:
This study proposes a tiered pipeline for efficient nutrient management in agriculture, addressing the need for real-time optimization and sustainable resource consumption. It utilizes multispectral imaging (MSI) and machine learning techniques for anomaly detection and status estimation of crop nutrients. The pipeline includes an autoencoder for early warning of anomalies and two status estimation modules â vegetation index (VI) features with Random Forest (RF) and raw whole-image deep learning with Vision Transformer (ViT). Results showed high-efficiency anomaly detection and detailed nutrient status estimation, with ViT outperforming RF in certain nutrient estimations. The study highlights the trade-offs between complexity and energy cost in status estimation modules, offering practical opportunities for agricultural sustainability and edge diagnostics. <div>
arXiv:2509.12247v1 Announce Type: new 
Abstract: Efficient nutrient management is critical for crop growth and sustainable resource consumption (e.g., nitrogen, energy). Current approaches require lengthy analyses, preventing real-time optimization; similarly, imaging facilitates rapid phenotyping but can be computationally intensive, preventing deployment under resource constraints. This study proposes a flexible, tiered pipeline for anomaly detection and status estimation (fresh weight, dry mass, and tissue nutrients), including a comprehensive energy analysis of approaches that span the efficiency-accuracy spectrum. Using a nutrient depletion experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer strength) and multispectral imaging (MSI), we developed a hierarchical pipeline using an autoencoder (AE) for early warning. Further, we compared two status estimation modules of different complexity for more detailed analysis: vegetation index (VI) features with machine learning (Random Forest, RF) and raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated high-efficiency anomaly detection (73% net detection of T3 samples 9 days after transplanting) at substantially lower energy than embodied energy in wasted nitrogen. The state estimation modules show trade-offs, with ViT outperforming RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at higher energy cost. With our modular pipeline, this work opens opportunities for edge diagnostics and practical opportunities for agricultural sustainability.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics</title>
<link>https://arxiv.org/abs/2509.12248</link>
<guid>https://arxiv.org/abs/2509.12248</guid>
<content:encoded><![CDATA[
<div> benchmark, dataset, multimodal humor, narrative sequences, large multimodal models <br />
<br />
PixelHumor introduces a benchmark dataset of 2,800 annotated multi-panel comics to evaluate Large Multimodal Models' ability to interpret multimodal humor and recognize narrative sequences. State-of-the-art models achieve only 61% accuracy in panel sequencing, highlighting the substantial gaps in current models' integration of visual and textual cues for coherent narrative and humor understanding. The dataset aims to drive the development of models that better engage in natural, socially aware interactions by providing a framework for evaluating multimodal contextual and narrative reasoning. <br /><br />Summary: PixelHumor introduces a dataset of multi-panel comics to evaluate Large Multimodal Models' ability to understand humor and narrative sequences. Current models perform poorly in panel sequencing, emphasizing the need for better integration of visual and textual cues for coherent narrative and humor understanding. The dataset aims to improve models' engagement in socially aware interactions by providing a framework for evaluating multimodal contextual and narrative reasoning. <div>
arXiv:2509.12248v1 Announce Type: new 
Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a significant challenge for Large Multimodal Models (LMMs). We introduce PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed to evaluate LMMs' ability to interpret multimodal humor and recognize narrative sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for instance, top models achieve only 61% accuracy in panel sequencing, far below human performance. This underscores critical limitations in current models' integration of visual and textual cues for coherent narrative and humor understanding. By providing a rigorous framework for evaluating multimodal contextual and narrative reasoning, PixelHumor aims to drive the development of LMMs that better engage in natural, socially aware interactions.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OnlineHOI: Towards Online Human-Object Interaction Generation and Perception</title>
<link>https://arxiv.org/abs/2509.12250</link>
<guid>https://arxiv.org/abs/2509.12250</guid>
<content:encoded><![CDATA[
<div> perception, generation, Human-Object Interaction, online setting, memory mechanism
<br />
Summary: 
The study focuses on Human-Object Interaction (HOI) perception and generation and highlights the limitations of current offline methods in handling online scenarios where information is limited to the current moment and historical data. To tackle this challenge, the researchers propose two new tasks: Online HOI Generation and Perception. They introduce the OnlineHOI framework, a network architecture based on the Mamba framework with a memory mechanism. This framework effectively integrates historical information and streaming data, yielding state-of-the-art results on online generation tasks such as Core4D and OAKINK2, as well as the online HOI4D perception task. By addressing the need for real-time processing of HOI tasks, the study demonstrates the importance of adapting existing methods to online settings for improved performance in fields like robotics, AR/VR, and human behavior understanding.
<br /><br />Summary: <div>
arXiv:2509.12250v1 Announce Type: new 
Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial for fields such as robotics, AR/VR, and human behavior understanding. However, current approaches model this task in an offline setting, where information at each time step can be drawn from the entire interaction sequence. In contrast, in real-world scenarios, the information available at each time step comes only from the current moment and historical data, i.e., an online setting. We find that offline methods perform poorly in an online context. Based on this observation, we propose two new tasks: Online HOI Generation and Perception. To address this task, we introduce the OnlineHOI framework, a network architecture based on the Mamba framework that employs a memory mechanism. By leveraging Mamba's powerful modeling capabilities for streaming data and the Memory mechanism's efficient integration of historical information, we achieve state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as well as the online HOI4D perception task.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces</title>
<link>https://arxiv.org/abs/2509.12258</link>
<guid>https://arxiv.org/abs/2509.12258</guid>
<content:encoded><![CDATA[
<div> deep learning, computer vision, deepfake technology, privacy, national security<br />
<br />
Summary:<br />
Deep learning technology has made significant advancements in various domains, including computer vision, leading to the emergence of deepfake technology. While deepfake technology has the potential to revolutionize social interactions, it also poses several risks to society. Misuse of deepfake technology, such as face-swapping programs, can deceive individuals and manipulate public perception, affecting privacy and reputation. Additionally, the creation of counterfeit images and videos can threaten national security by compromising the functionality of facial recognition systems. As a result, the improper application of deepfake technology can have detrimental effects on political and economic structures, with the potential to influence election campaigns and undermine the credibility of prominent figures. Thus, careful consideration and regulation of deepfake technology are necessary to mitigate its negative impacts on human society. <br /> <div>
arXiv:2509.12258v1 Announce Type: new 
Abstract: Currently, deep learning has been utilised to tackle several difficulties in our everyday lives. It not only exhibits progress in computer vision but also constitutes the foundation for several revolutionary technologies. Nonetheless, similar to all phenomena, the use of deep learning in diverse domains has produced a multifaceted interaction of advantages and disadvantages for human society. Deepfake technology has advanced, significantly impacting social life. However, developments in this technology can affect privacy, the reputations of prominent personalities, and national security via software development. It can produce indistinguishable counterfeit photographs and films, potentially impairing the functionality of facial recognition systems, so presenting a significant risk.
  The improper application of deepfake technology produces several detrimental effects on society. Face-swapping programs mislead users by altering persons' appearances or expressions to fulfil particular aims or to appropriate personal information. Deepfake technology permeates daily life through such techniques. Certain individuals endeavour to sabotage election campaigns or subvert prominent political figures by creating deceptive pictures to influence public perception, causing significant harm to a nation's political and economic structure.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modern Look at Simplicity Bias in Image Classification Tasks</title>
<link>https://arxiv.org/abs/2509.12265</link>
<guid>https://arxiv.org/abs/2509.12265</guid>
<content:encoded><![CDATA[
<div> Keywords: Simplicity Bias, Neural Networks, Image Classification Tasks, CLIP Models, Performance

Summary:
In this study, the relationship between the Simplicity Bias (SB) in CLIP models and their performance in various image classification tasks is explored. The researchers first analyze the limitations of existing complexity measures and propose a more refined frequency-aware measure to capture SB differences in large models. By applying this new measure to CLIP models, they demonstrate its effectiveness in understanding SB variations. The study then investigates how the SB of models relates to their performance in different types of image classification tasks, including zero-shot and fine-tuning scenarios. The results show that a stronger SB in models can lead to better performance in out-of-distribution generalization but not necessarily in terms of adversarial robustness. This suggests that aligning a model's inductive biases with the characteristics of the target task is crucial for optimal performance. <br /><br />Summary: <div>
arXiv:2509.12265v1 Announce Type: new 
Abstract: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to represent simple functions, is a key factor in their generalization capabilities. Recent studies show that an excessive SB may harm performance on complex tasks, and the need for this bias varies across tasks. Many of these studies focus on simple models or synthetic tasks. It remains challenging to measure the SB in large models and little is known about the relevance of the SB to various image classification tasks.
  In this paper, we investigate the relationship between the SB in CLIP models and their performance across image classification tasks. First, we theoretically analyze the potential limitation of existing measures of complexity that have been used to characterize small models. To address this, we propose a frequency-aware measure capturing finer-grained SB differences. We validate this measure on CLIP models subjected to two recent SB-modulation methods, demonstrating that it is more informative and consistent than previous measures. Second, we examine the relation between the SB of those models and their performance across a range of image classification tasks, including zero-shot and fine-tuning settings. These experiments reveal a range of behaviors. For example, a stronger SB correlates with a better performance on OOD generalization than on adversarial robustness. These results highlight the benefits of aligning a model's inductive biases with the characteristics of the target task.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions</title>
<link>https://arxiv.org/abs/2509.12277</link>
<guid>https://arxiv.org/abs/2509.12277</guid>
<content:encoded><![CDATA[
<div> Keywords: Dermoscopy, GraphDerm, GNNs, ISIC, AI

Summary:
GraphDerm is a framework that incorporates patient metadata, physical scale, and imaging data for dermoscopic classification, using Graph Neural Networks (GNNs). The study utilizes ISIC 2018/2019 data, synthesizing ruler-embedded images and training U-Nets for lesion and ruler segmentation. Scale calibration is achieved through regression of pixels-per-millimeter using a 1D-CNN. Real-scale descriptors are computed from lesion masks, and node features are extracted using EfficientNet-B3. A spectral GNN is employed for semi-supervised node classification, outperforming an image-only ANN baseline. The results demonstrate high accuracy with AUC of 0.9812, and a sparser graph with 25% of edges preserves a high AUC of 0.9788. The study highlights the benefits of incorporating scale, geometry, and metadata in dermoscopic decision support systems, offering a promising direction for future AI research in dermatology. Future work will focus on refining edge semantics and evaluating on broader datasets. 

<br /><br />Summary: <div>
arXiv:2509.12277v1 Announce Type: new 
Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often ignores patient metadata (age, sex, site) and the physical scale needed for geometric analysis. We present GraphDerm, a population-graph framework that fuses imaging, millimeter-scale calibration, and metadata for multiclass dermoscopic classification, to the best of our knowledge the first ISIC-scale application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019, synthesize ruler-embedded images with exact masks, and train U-Nets (SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN. From lesion masks we compute real-scale descriptors (area, perimeter, radius of gyration). Node features use EfficientNet-B3; edges encode metadata/geometry similarity (fully weighted or thresholded). A spectral GNN performs semi-supervised node classification; an image-only ANN is the baseline. Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440 for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99 range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in a population graph yields substantial gains over image-only pipelines on ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient deployment. Scale-aware, graph-based AI is a promising direction for dermoscopic decision support; future work will refine learned edge semantics and evaluate on broader curated benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12278</link>
<guid>https://arxiv.org/abs/2509.12278</guid>
<content:encoded><![CDATA[
<div> Keywords: Text Image Machine Translation, position-aware translation, benchmark, Adaptive Image OCR Refinement Pipeline, Large Vision-Language Models

Summary:
In this work, Text Image Machine Translation (TIMT) is extended to position-aware TIMT (PATIMT) to support fine-grained and layout-preserving translation. This includes region-specific translation and full-image translation with grounding, addressing the limitations of existing TIMT models. The PATIMT benchmark (PATIMTBench) is introduced, consisting of 10 diverse real-world scenarios. An Adaptive Image OCR Refinement Pipeline is developed to refine results from text-rich images. A test set with 1,200 high-quality instances manually annotated is created for reliable evaluation. Large Vision-Language Models (LVLMs) achieve state-of-the-art performance after fine-tuning on the data. The experimental results demonstrate the scalability and generalizability of the training data.

Summary: <br /><br />Keywords: Text Image Machine Translation, position-aware translation, benchmark, Adaptive Image OCR Refinement Pipeline, Large Vision-Language Models. This work introduces position-aware TIMT to support fine-grained and layout-preserving translation, addressing the limitations of existing TIMT models. The PATIMTBench benchmark and Adaptive Image OCR Refinement Pipeline are developed for evaluation and refinement. State-of-the-art performance is achieved by LVLMs after fine-tuning on the data, highlighting the scalability and generalizability of the training data. <div>
arXiv:2509.12278v1 Announce Type: new 
Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within an image into another language. Current TIMT studies primarily focus on providing translations for all the text within an image, while neglecting to provide bounding boxes and covering limited scenarios. In this work, we extend traditional TIMT into position-aware TIMT (PATIMT), aiming to support fine-grained and layoutpreserving translation, which holds great practical value but remains largely unexplored. This task comprises two key sub-tasks: regionspecific translation and full-image translation with grounding. To support existing models on PATIMT and conduct fair evaluation, we construct the PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world scenarios. Specifically, we introduce an Adaptive Image OCR Refinement Pipeline, which adaptively selects appropriate OCR tools based on scenario and refines the results of text-rich images. To ensure evaluation reliability, we further construct a test set, which contains 1,200 high-quality instances manually annotated and reviewed by human experts. After fine-tuning on our data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art performance on both sub-tasks. Experimental results also highlight the scalability and generalizability of our training data
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance</title>
<link>https://arxiv.org/abs/2509.12279</link>
<guid>https://arxiv.org/abs/2509.12279</guid>
<content:encoded><![CDATA[
<div> wake detection, synthetic aperture radar, domain adaptation, feature similarity filtering, memory guidance<br />
Summary:<br />
The article introduces a new approach, SimMemDA, for unsupervised domain adaptive ship wake detection using synthetic aperture radar (SAR) images. The complex imaging mechanism of SAR images makes wake features challenging to annotate accurately. To address the domain shift issue between optical and SAR images, the SimMemDA framework incorporates WakeGAN for style transfer and utilizes instance-level feature similarity filtering to prioritize source samples with target-like distributions. A Feature-Confidence Memory Bank and a K-nearest neighbor confidence-weighted fusion strategy are introduced to improve the reliability of pseudo-labels in the target domain. Furthermore, region-mixed training is implemented by combining source annotations with calibrated target pseudo-labels to enhance generalization. Experimental results demonstrate the effectiveness and robustness of the SimMemDA method in improving the accuracy of cross-modal ship wake detection tasks. <div>
arXiv:2509.12279v1 Announce Type: new 
Abstract: Synthetic Aperture Radar (SAR), with its all- weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adap- tation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like dis- tributions, minimizing negative transfer. Meanwhile, a Feature- Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated tar- get pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning</title>
<link>https://arxiv.org/abs/2509.12329</link>
<guid>https://arxiv.org/abs/2509.12329</guid>
<content:encoded><![CDATA[
<div> deep learning, air temperature, data-driven, spatiotemporal, weather stations 

Summary: 
The article introduces a data-driven, physics-guided deep learning method, called Amplifier Air-Transformer, to generate hourly air temperature data at 2 km resolution over the contiguous United States. The approach first reconstructs GOES-16 surface temperature data obscured by clouds using neural networks. It then transforms the reconstructed surface temperature into air temperature by leveraging its relationship with Earth surface properties. Predictive uncertainty estimation through deep ensemble learning is used to improve reliability. The approach achieves an accuracy of 1.93Â°C in station-based validation. The method streamlines surface temperature reconstruction and air temperature prediction and can be extended to other satellite sources for high-resolution air temperature monitoring. The generated data is available for download and more information can be found on the project webpage. 

Summary: <div>
arXiv:2509.12329v1 Announce Type: new 
Abstract: Near-surface air temperature is a key physical property of the Earth's surface. Although weather stations offer continuous monitoring and satellites provide broad spatial coverage, no single data source offers seamless data in a spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep learning approach to generate hourly air temperature data at 2 km resolution over the contiguous United States. The approach, called Amplifier Air-Transformer, first reconstructs GOES-16 surface temperature data obscured by clouds. It does so through a neural network encoded with the annual temperature cycle, incorporating a linear term to amplify ERA5 temperature values at finer scales and convolutional layers to capture spatiotemporal variations. Then, another neural network transforms the reconstructed surface temperature into air temperature by leveraging its latent relationship with key Earth surface properties. The approach is further enhanced with predictive uncertainty estimation through deep ensemble learning to improve reliability. The proposed approach is built and tested on 77.7 billion surface temperature pixels and 155 million air temperature records from weather stations across the contiguous United States (2018-2024), achieving hourly air temperature mapping accuracy of 1.93 C in station-based validation. The proposed approach streamlines surface temperature reconstruction and air temperature prediction, and it can be extended to other satellite sources for seamless air temperature monitoring at high spatiotemporal resolution. The generated data of this study can be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project webpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification</title>
<link>https://arxiv.org/abs/2509.12353</link>
<guid>https://arxiv.org/abs/2509.12353</guid>
<content:encoded><![CDATA[
<div> Keywords: AnimalCLEF 2025, re-identification challenge, post-hoc metric learning, domain-specific model, general-purpose model

Summary: 
The DS@GT team participated in the AnimalCLEF 2025 re-identification challenge, focusing on the effectiveness of post-hoc metric learning. They compared a general-purpose model (DINOv2) with a domain-specific model (MegaDescriptor) as backbone embeddings. Their findings showed that the quality and domain-specificity of the backbone embeddings significantly impact the effectiveness of metric learning. The specialized MegaDescriptor model benefited from a triplet-learning projection head, while the general-purpose DINOv2 model had minimal gains. The study revealed the challenges of reshaping general-purpose features for fine-grained tasks, highlighting the importance of domain-specific pre-training for limited-data re-ID tasks. The team's implementation is publicly available on GitHub at github.com/dsgt-arc/animalclef-2025. 

<br /><br />Summary: <div>
arXiv:2509.12353v1 Announce Type: new 
Abstract: This paper details the DS@GT team's entry for the AnimalCLEF 2025 re-identification challenge. Our key finding is that the effectiveness of post-hoc metric learning is highly contingent on the initial quality and domain-specificity of the backbone embeddings. We compare a general-purpose model (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A K-Nearest Neighbor classifier with robust thresholding then identifies known individuals or flags new ones. While a triplet-learning projection head improved the performance of the specialized MegaDescriptor model by 0.13 points, it yielded minimal gains (0.03) for the general-purpose DINOv2 on averaged BAKS and BAUS. We demonstrate that the general-purpose manifold is more difficult to reshape for fine-grained tasks, as evidenced by stagnant validation loss and qualitative visualizations. This work highlights the critical limitations of refining general-purpose features for specialized, limited-data re-ID tasks and underscores the importance of domain-specific pre-training. The implementation for this work is publicly available at github.com/dsgt-arc/animalclef-2025.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images</title>
<link>https://arxiv.org/abs/2509.12380</link>
<guid>https://arxiv.org/abs/2509.12380</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, resource-constrained edge devices, GhostNetV3-Small, knowledge distillation, low-resolution domains

Summary: 
This paper explores strategies for compressing and adapting deep neural networks for efficient deployment on resource-constrained edge devices. The focus is on GhostNetV3, a mobile-friendly architecture, with the introduction of GhostNetV3-Small optimized for low-resolution inputs like those in CIFAR-10. Experimental results demonstrate the superior performance of GhostNetV3-Small, achieving an accuracy of 93.94% on CIFAR-10. Surprisingly, various knowledge distillation techniques did not enhance accuracy compared to baseline training, indicating the significance of architectural adaptation in small-scale image classification tasks. The study emphasizes the importance of further research on effective model design and advanced distillation methods for improving performance in low-resolution domains.<br /><br />Summary: <div>
arXiv:2509.12380v1 Announce Type: new 
Abstract: Deep neural networks have achieved remarkable success across a range of tasks, however their computational demands often make them unsuitable for deployment on resource-constrained edge devices. This paper explores strategies for compressing and adapting models to enable efficient inference in such environments. We focus on GhostNetV3, a state-of-the-art architecture for mobile applications, and propose GhostNetV3-Small, a modified variant designed to perform better on low-resolution inputs such as those in the CIFAR-10 dataset. In addition to architectural adaptation, we provide a comparative evaluation of knowledge distillation techniques, including traditional knowledge distillation, teacher assistants, and teacher ensembles. Experimental results show that GhostNetV3-Small significantly outperforms the original GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to expectations, all examined distillation strategies led to reduced accuracy compared to baseline training. These findings indicate that architectural adaptation can be more impactful than distillation in small-scale image classification tasks, highlighting the need for further research on effective model design and advanced distillation techniques for low-resolution domains.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization</title>
<link>https://arxiv.org/abs/2509.12400</link>
<guid>https://arxiv.org/abs/2509.12400</guid>
<content:encoded><![CDATA[
<div> detection, imagery, UAV, palm, localization

Summary: 
This study investigates the use of raw imagery from UAVs for palm detection and crown-center localization in tropical forests. It compares the performance of detection in raw and orthomosaic imagery, examining within-domain and cross-domain transfer. The research also explores the impact of crown-center annotations on localization accuracy. The results show that raw imagery outperforms orthomosaics in deployment-relevant scenarios, while orthomosaics are valuable for cross-domain generalization. Additionally, incorporating crown-center annotations in training improves localization accuracy and provides precise tree positions for ecological analyses. This study offers practical guidance for using UAVs in biodiversity and conservation monitoring. 

<br /><br />Summary: <div>
arXiv:2509.12400v1 Announce Type: new 
Abstract: Accurate mapping of individual trees is essential for ecological monitoring and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs) is widely used, but stitching artifacts and heavy preprocessing limit its suitability for field deployment. This study explores the use of raw UAV imagery for palm detection and crown-center localization in tropical forests. Two research questions are addressed: (1) how detection performance varies across orthomosaic and raw imagery, including within-domain and cross-domain transfer, and (2) to what extent crown-center annotations improve localization accuracy beyond bounding-box centroids. Using state-of-the-art detectors and keypoint models, we show that raw imagery yields superior performance in deployment-relevant scenarios, while orthomosaics retain value for robust cross-domain generalization. Incorporating crown-center annotations in training further improves localization and provides precise tree positions for downstream ecological analyses. These findings offer practical guidance for UAV-based biodiversity and conservation monitoring.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</title>
<link>https://arxiv.org/abs/2509.12430</link>
<guid>https://arxiv.org/abs/2509.12430</guid>
<content:encoded><![CDATA[
<div> gear assemblies, motion trajectories, CAD point clouds, DYNAMO, MechBench 

Summary: 
The paper introduces MechBench, a benchmark dataset comprising 693 synthetic gear assemblies with ground-truth motion trajectories. These assemblies simulate realistic mechanical structures where motion is generated through geometric coupling rather than predefined joints. The dataset serves as a platform to study coupled motion in mechanical assemblies. The authors propose DYNAMO, a neural model that predicts SE(3) motion trajectories of individual parts directly from segmented CAD point clouds. Experimental results demonstrate that DYNAMO performs better than existing methods, producing accurate and consistent predictions for diverse gear configurations. MechBench and DYNAMO combined create a systematic framework for data-driven learning of coupled mechanical motion in CAD assemblies. <div>
arXiv:2509.12430v1 Announce Type: new 
Abstract: Understanding the motion of articulated mechanical assemblies from static geometry remains a core challenge in 3D perception and design automation. Prior work on everyday articulated objects such as doors and laptops typically assumes simplified kinematic structures or relies on joint annotations. However, in mechanical assemblies like gears, motion arises from geometric coupling, through meshing teeth or aligned axes, making it difficult for existing methods to reason about relational motion from geometry alone. To address this gap, we introduce MechBench, a benchmark dataset of 693 diverse synthetic gear assemblies with part-wise ground-truth motion trajectories. MechBench provides a structured setting to study coupled motion, where part dynamics are induced by contact and transmission rather than predefined joints. Building on this, we propose DYNAMO, a dependency-aware neural model that predicts per-part SE(3) motion trajectories directly from segmented CAD point clouds. Experiments show that DYNAMO outperforms strong baselines, achieving accurate and temporally consistent predictions across varied gear configurations. Together, MechBench and DYNAMO establish a novel systematic framework for data-driven learning of coupled mechanical motion in CAD assemblies.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions</title>
<link>https://arxiv.org/abs/2509.12442</link>
<guid>https://arxiv.org/abs/2509.12442</guid>
<content:encoded><![CDATA[
<div> Dataset, Cotton boll recognition, Real-time detector, Automation, Phenotypic analysis 
Summary: 
Cott-ADNet is introduced as a lightweight real-time detector for accurate recognition of cotton bolls and flowers in field conditions. It incorporates a NeLU-enhanced Global Attention Mechanism and a Dilated Receptive Field SPPF to improve spatial representation and multi-scale context modeling. With a labeled dataset of 4,966 images and a validation set of 1,216 field images, Cott-ADNet achieves high precision, recall, mAP, and F1-Score with low computational cost. The model maintains stability under various variations, making it suitable for in-field deployment. The results demonstrate Cott-ADNet as an efficient solution for automated cotton harvesting and high-throughput phenotypic analysis. The code and dataset are available for further research and development. 
Summary:  <div>
arXiv:2509.12442v1 Announce Type: new 
Abstract: Cotton is one of the most important natural fiber crops worldwide, yet harvesting remains limited by labor-intensive manual picking, low efficiency, and yield losses from missing the optimal harvest window. Accurate recognition of cotton bolls and their maturity is therefore essential for automation, yield estimation, and breeding research. We propose Cott-ADNet, a lightweight real-time detector tailored to cotton boll and flower recognition under complex field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial representation and robustness through improved convolutional designs, while introducing two new modules: a NeLU-enhanced Global Attention Mechanism to better capture weak and low-contrast features, and a Dilated Receptive Field SPPF to expand receptive fields for more effective multi-scale context modeling at low computational cost. We curate a labeled dataset of 4,966 images, and release an external validation set of 1,216 field images to support future research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8% Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs, maintaining stable performance under multi-scale and rotational variations. These results demonstrate Cott-ADNet as an accurate and efficient solution for in-field deployment, and thus provide a reliable basis for automated cotton harvesting and high-throughput phenotypic analysis. Code and dataset is available at https://github.com/SweefongWong/Cott-ADNet.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</title>
<link>https://arxiv.org/abs/2509.12452</link>
<guid>https://arxiv.org/abs/2509.12452</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud processing, deep learning, geomatics, computer vision, practical applications

Summary:
Point cloud processing is a crucial task in geomatics and computer vision, with applications ranging from mapping to disaster response. Deep learning has revolutionized point cloud processing, but many algorithms have not yet been implemented in real-world scenarios. This paper provides a meta review of deep learning approaches for tasks such as scene completion, registration, semantic segmentation, and modeling. The review includes a wide range of urban and environmental applications, highlighting gaps that need to be addressed for these methods to be effectively applied. The survey evaluates the algorithmic and practical aspects of the reviewed methods to guide future research and development in point cloud processing. <div>
arXiv:2509.12452v1 Announce Type: new 
Abstract: Point cloud processing as a fundamental task in the field of geomatics and computer vision, has been supporting tasks and applications at different scales from air to ground, including mapping, environmental monitoring, urban/tree structure modeling, automated driving, robotics, disaster responses etc. Due to the rapid development of deep learning, point cloud processing algorithms have nowadays been almost explicitly dominated by learning-based approaches, most of which are yet transitioned into real-world practices. Existing surveys primarily focus on the ever-updating network architecture to accommodate unordered point clouds, largely ignoring their practical values in typical point cloud processing applications, in which extra-large volume of data, diverse scene contents, varying point density, data modality need to be considered. In this paper, we provide a meta review on deep learning approaches and datasets that cover a selection of critical tasks of point cloud processing in use such as scene completion, registration, semantic segmentation, and modeling. By reviewing a broad range of urban and environmental applications these tasks can support, we identify gaps to be closed as these methods transformed into applications and draw concluding remarks in both the algorithmic and practical aspects of the surveyed methods.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis</title>
<link>https://arxiv.org/abs/2509.12453</link>
<guid>https://arxiv.org/abs/2509.12453</guid>
<content:encoded><![CDATA[
<div> prognosis, glaucoma, Two-Stage Decoupling Framework, feature representation module, self-supervised learning

Summary: 
The study introduces a Two-Stage Decoupling Framework (TSDF) for variable-length glaucoma prognosis, addressing limitations of fixed-length inputs and inadequate dataset sizes. The first stage utilizes a feature representation module with self-supervised learning to combine multiple glaucoma datasets for improved feature representations despite differences in supervision. In the second stage, a temporal aggregation module with an attention-based mechanism is introduced to process sequences of varying lengths efficiently. This framework enhances model performance while maintaining parameter size efficacy. Experiments on Ocular Hypertension Treatment Study (OHTS) and Glaucoma Real-world Appraisal Progression Ensemble (GRAPE) datasets demonstrate the robustness and effectiveness of the proposed approach. <br /><br />Summary: <div>
arXiv:2509.12453v1 Announce Type: new 
Abstract: Glaucoma is one of the leading causes of irreversible blindness worldwide. Glaucoma prognosis is essential for identifying at-risk patients and enabling timely intervention to prevent blindness. Many existing approaches rely on historical sequential data but are constrained by fixed-length inputs, limiting their flexibility. Additionally, traditional glaucoma prognosis methods often employ end-to-end models, which struggle with the limited size of glaucoma datasets. To address these challenges, we propose a Two-Stage Decoupling Framework (TSDF) for variable-length glaucoma prognosis. In the first stage, we employ a feature representation module that leverages self-supervised learning to aggregate multiple glaucoma datasets for training, disregarding differences in their supervisory information. This approach enables datasets of varying sizes to learn better feature representations. In the second stage, we introduce a temporal aggregation module that incorporates an attention-based mechanism to process sequential inputs of varying lengths, ensuring flexible and efficient utilization of all available data. This design significantly enhances model performance while maintaining a compact parameter size. Extensive experiments on two benchmark glaucoma datasets:the Ocular Hypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal Progression Ensemble (GRAPE),which differ significantly in scale and clinical settings,demonstrate the effectiveness and robustness of our approach.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Tokenizer Needs Post-Training</title>
<link>https://arxiv.org/abs/2509.12474</link>
<guid>https://arxiv.org/abs/2509.12474</guid>
<content:encoded><![CDATA[
<div> tokenizer, generative models, latent space, training scheme, reconstruction

Summary:
The paper discusses the limitations of current image generative models in capturing the distribution of images in a pre-constructed latent space. It proposes a novel tokenizer training scheme that includes main training and post-training to address the discrepancy between reconstruction and generation distribution. During main training, a latent perturbation strategy is introduced to simulate sampling noises and improve the robustness of the tokenizer. The paper also introduces a new evaluation metric, pFID, to assess the tokenizer's performance in relation to generation quality. Post-training optimizes the tokenizer decoder to mitigate the distribution difference between generated and reconstructed tokens. Through experiments, the proposed training scheme significantly enhances generation quality and convergence speed. The effectiveness of the post-training strategy is validated on various tokenizers and generators, demonstrating improved performance in generating high-quality images. 

<br /><br />Summary: <div>
arXiv:2509.12474v1 Announce Type: new 
Abstract: Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundational Models for Single-Chip Radar</title>
<link>https://arxiv.org/abs/2509.12482</link>
<guid>https://arxiv.org/abs/2509.12482</guid>
<content:encoded><![CDATA[
<div> mmWave, radar, dataset, Generalizable Radar Transformer, data scaling
<br />
Summary:<br />
This paper introduces a large raw radar dataset comprising 1 million samples (29 hours) and presents a foundational model for 4D single-chip radar. The model, called Generalizable Radar Transformer (GRT), can accurately predict 3D occupancy and semantic segmentation with high quality, even with the poor angular resolution characteristic of mmWave radars. The GRT demonstrates generalizability across various environments, can be fine-tuned for different tasks, and exhibits logarithmic data scaling of 20% per 10x increase in data. The use of raw radar data significantly outperforms lossy representations, equivalent to a 10x increase in training data. The study suggests that approximately 100 million samples (3000 hours) of data are needed to fully leverage the capabilities of the GRT. <div>
arXiv:2509.12482v1 Announce Type: new 
Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.
  In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20\% per $10\times$ data. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a $10\times$ increase in training data. Finally, we roughly estimate that $\approx$100M samples (3000 hours) of data are required to fully exploit the potential of GRT.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Robustness of Vision-Language Models Under Noisy Conditions</title>
<link>https://arxiv.org/abs/2509.12492</link>
<guid>https://arxiv.org/abs/2509.12492</guid>
<content:encoded><![CDATA[
<div> evaluation, Vision-Language Models (VLMs), noise resilience, multimodal learning, robustness

Summary:<br />
1. The study evaluates the performance of Vision-Language Models (VLMs) under controlled perturbations, such as lighting variation, motion blur, and compression artifacts.
2. The descriptiveness of ground-truth captions significantly impacts model performance, showing a nuanced trade-off between dataset characteristics and noise resilience.
3. Larger models like LLaVA excel in semantic understanding but do not universally outperform smaller models, indicating that model size is not the sole determinant of performance.
4. Certain noise types, such as JPEG compression and motion blur, can significantly degrade model performance across datasets, highlighting the importance of noise resilience in multimodal learning.
5. The study presents a comprehensive evaluation framework using both lexical-based metrics and neural-based similarity measures, offering a standardized benchmark for future robust multimodal learning. 

<br /><br />Summary: <div>
arXiv:2509.12492v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have attained exceptional success across multimodal tasks such as image captioning and visual question answering. However, their robustness under noisy conditions remains unfamiliar. In this study, we present a comprehensive evaluation framework to evaluate the performance of several state-of-the-art VLMs under controlled perturbations, including lighting variation, motion blur, and compression artifacts. We used both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based similarity measures using sentence embeddings to quantify semantic alignment. Our experiments span diverse datasets, revealing key insights: (1) descriptiveness of ground-truth captions significantly influences model performance; (2) larger models like LLaVA excel in semantic understanding but do not universally outperform smaller models; and (3) certain noise types, such as JPEG compression and motion blur, dramatically degrade performance across models. Our findings highlight the nuanced trade-offs between model size, dataset characteristics, and noise resilience, offering a standardized benchmark for future robust multimodal learning.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.12496</link>
<guid>https://arxiv.org/abs/2509.12496</guid>
<content:encoded><![CDATA[
<div> Instance-Guided Refinement, Influence Function Integration, Multi-Scale Boundary Enhancement, Weakly Supervised Semantic Segmentation, IG-CAM <br />
<br />
Summary: IG-CAM, a novel weakly supervised semantic segmentation approach, improves object boundary localization by utilizing instance-level cues and influence functions. It incorporates Instance-Guided Refinement for complete object coverage, Influence Function Integration for robust representations, and Multi-Scale Boundary Enhancement for sharp boundaries. IG-CAM achieves state-of-the-art performance on PASCAL VOC 2012 dataset, with an mIoU of 82.3% pre and 86.6% post Conditional Random Field refinement. It outperforms existing methods in terms of localization accuracy, object coverage, and computational efficiency. Extensive ablation studies validate the efficacy of each component, and qualitative comparisons across 600 images demonstrate robustness and generalization. IG-CAM sets a new benchmark for weakly supervised semantic segmentation, offering a practical solution when pixel-level annotations are limited or costly. <br /> <div>
arXiv:2509.12496v1 Announce Type: new 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artist-Created Mesh Generation from Raw Observation</title>
<link>https://arxiv.org/abs/2509.12501</link>
<guid>https://arxiv.org/abs/2509.12501</guid>
<content:encoded><![CDATA[
<div> Keywords: artist-style meshes, point cloud refinement, 2D inpainting task, generative models, ShapeNet dataset

Summary:
This paper introduces a novel end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds commonly captured by real-world sensors like LiDAR or RGB-D cameras. The traditional methods for creating artist-style meshes require clean and complete input data or involve complex multi-stage pipelines, limiting their practicality in real-world scenarios. The proposed approach directly refines the input point cloud to produce high-quality artist-style meshes. The core of this method is the innovative reformulation of 3D point cloud refinement as a 2D inpainting task, allowing the utilization of powerful generative models. Preliminary results on the ShapeNet dataset illustrate the potential of this framework in generating clean and complete meshes. <br /><br />Summary: <div>
arXiv:2509.12501v1 Announce Type: new 
Abstract: We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery</title>
<link>https://arxiv.org/abs/2509.12511</link>
<guid>https://arxiv.org/abs/2509.12511</guid>
<content:encoded><![CDATA[
<div> Keywords: crop breeding, phenotyping, stalk diameter, computer vision, deep learning

Summary:
This paper introduces a novel computer vision pipeline for accurately estimating stalk diameter from RGB-D imagery, a crucial structural trait in crop breeding programs. Traditional measurement methods are labor-intensive and error-prone, making it challenging to scale phenotyping efforts. The proposed method combines deep learning-based instance segmentation, 3D point cloud reconstruction, and Principal Component Analysis (PCA) to robustly estimate stalk diameter. By addressing challenges such as curvature, occlusion, and noise in images, this approach provides a scalable and reliable solution for high-throughput phenotyping in breeding and agronomic research. This technology can significantly improve the efficiency and accuracy of measuring stalk diameter, supporting the development of improved traits like mechanical stability, biomass production, and disease resistance in crops. 

<br /><br />Summary: <div>
arXiv:2509.12511v1 Announce Type: new 
Abstract: Accurate, high-throughput phenotyping is a critical component of modern crop breeding programs, especially for improving traits such as mechanical stability, biomass production, and disease resistance. Stalk diameter is a key structural trait, but traditional measurement methods are labor-intensive, error-prone, and unsuitable for scalable phenotyping. In this paper, we present a geometry-aware computer vision pipeline for estimating stalk diameter from RGB-D imagery. Our method integrates deep learning-based instance segmentation, 3D point cloud reconstruction, and axis-aligned slicing via Principal Component Analysis (PCA) to perform robust diameter estimation. By mitigating the effects of curvature, occlusion, and image noise, this approach offers a scalable and reliable solution to support high-throughput phenotyping in breeding and agronomic research.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew</title>
<link>https://arxiv.org/abs/2509.12544</link>
<guid>https://arxiv.org/abs/2509.12544</guid>
<content:encoded><![CDATA[
<div> NC-structure, Federated Learning, Multi-label, Neural Collapse, Feature Disentanglement <br />
<br />
Summary: <br />
Federated Learning (FL) faces challenges in deep learning when dealing with decentralized and heterogeneous data. This issue is exacerbated in multi-label scenarios, where label co-occurrence and inter-label dependency are common. The Neural Collapse (NC) theory describes a structure in the feature space that can be utilized to align feature distributions across clients in FL and improve representation learning. A feature disentanglement module is introduced to extract semantically specific features in multi-label settings, allowing for better clustering guided by a shared NC structure. Regularization losses further promote compact clustering. Experimental results on benchmark datasets demonstrate the effectiveness of this approach in addressing the complexities of multi-label scenarios in FL. <div>
arXiv:2509.12544v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. However, the performance of deep learning often deteriorates in FL due to decentralized and heterogeneous data. This challenge is further amplified in multi-label scenarios, where data exhibit complex characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL research primarily focuses on single-label classification, many real-world applications, particularly in domains such as medical imaging, often involve multi-label settings. In this paper, we address this important yet underexplored scenario in FL, where clients hold multi-label data with skewed label distributions. Neural Collapse (NC) describes a geometric structure in the latent feature space where features of each class collapse to their class mean with vanishing intra-class variance, and the class means form a maximally separated configuration. Motivated by this theory, we propose a method to align feature distributions across clients and to learn high-quality, well-clustered representations. To make the NC-structure applicable to multi-label settings, where image-level features may contain multiple semantic concepts, we introduce a feature disentanglement module that extracts semantically specific features. The clustering of these disentangled class-wise features is guided by a predefined shared NC structure, which mitigates potential conflicts between client models due to diverse local data distributions. In addition, we design regularisation losses to encourage compact clustering in the latent feature space. Experiments conducted on four benchmark datasets across eight diverse settings demonstrate that our approach outperforms existing methods, validating its effectiveness in this challenging FL scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection</title>
<link>https://arxiv.org/abs/2509.12546</link>
<guid>https://arxiv.org/abs/2509.12546</guid>
<content:encoded><![CDATA[
<div> keywords: Face forgery detection, Agent4FaceForgery, multi-agent framework, LLM-powered agents, Adaptive Rejection Sampling (ARS)  

Summary:  
Agent4FaceForgery introduces a simulation-driven approach to address challenges in face forgery detection. By utilizing a multi-agent framework with LLM-powered agents, the system can capture diverse intents and simulate the iterative forgery creation process. The agents interact in a simulated social environment to generate labeled samples for nuanced text-image consistency, improving data quality and diversity through Adaptive Rejection Sampling (ARS). The approach leads to significant performance gains for detectors of multiple architectures, bridging the gap between offline benchmarks and real-world efficacy in detecting forgeries. This simulation-driven framework effectively models the complex text-image interactions present in social media forgeries, showcasing the value of a more realistic training data environment.<br /><br />Summary: <div>
arXiv:2509.12546v1 Announce Type: new 
Abstract: Face forgery detection faces a critical challenge: a persistent gap between offline benchmarks and real-world efficacy,which we attribute to the ecological invalidity of training data.This work introduces Agent4FaceForgery to address two fundamental problems: (1) how to capture the diverse intents and iterative processes of human forgery creation, and (2) how to model the complex, often adversarial, text-image interactions that accompany forgeries in social media. To solve this,we propose a multi-agent framework where LLM-poweredagents, equipped with profile and memory modules, simulate the forgery creation process. Crucially, these agents interact in a simulated social environment to generate samples labeled for nuanced text-image consistency, moving beyond simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism ensures data quality and diversity. Extensive experiments validate that the data generated by our simulationdriven approach brings significant performance gains to detectors of multiple architectures, fully demonstrating the effectiveness and value of our framework.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Multimodal Graph Modeling for Human-Object Interaction Detection</title>
<link>https://arxiv.org/abs/2509.12554</link>
<guid>https://arxiv.org/abs/2509.12554</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based methods, Human-Object Interaction detection, Graph Neural Networks, Multimodal Graph Network Modeling, HOI task

Summary: 
Multimodal Graph Network Modeling (MGNM) is proposed to enhance Human-Object Interaction (HOI) detection by leveraging Graph Neural Networks (GNNs). The Transformer architecture lacks explicit relational modeling, hindering interaction recognition. MGNM implements a four-stage graph structure framework for HOI tasks. It incorporates a multi-level feature interaction mechanism to improve information propagation across human-object pairs by leveraging vision and language features. MGNM achieves state-of-the-art performance on HICO-DET and V-COCO benchmarks and exhibits a significant performance boost when integrated with advanced object detectors. It effectively balances performance between rare and non-rare classes. 

<br /><br />Summary: <div>
arXiv:2509.12554v1 Announce Type: new 
Abstract: Transformer-based methods have recently become the prevailing approach for Human-Object Interaction (HOI) detection. However, the Transformer architecture does not explicitly model the relational structures inherent in HOI detection, which impedes the recognition of interactions. In contrast, Graph Neural Networks (GNNs) are inherently better suited for this task, as they explicitly model the relationships between human-object pairs. Therefore, in this paper, we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork \textbf{M}odeling (MGNM) that leverages GNN-based relational structures to enhance HOI detection. Specifically, we design a multimodal graph network framework that explicitly models the HOI task in a four-stage graph structure. Furthermore, we introduce a multi-level feature interaction mechanism within our graph network. This mechanism leverages multi-level vision and language features to enhance information propagation across human-object pairs. Consequently, our proposed MGNM achieves state-of-the-art performance on two widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a more advanced object detector, our method demonstrates a significant performance gain and maintains an effective balance between rare and non-rare classes.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf</title>
<link>https://arxiv.org/abs/2509.12556</link>
<guid>https://arxiv.org/abs/2509.12556</guid>
<content:encoded><![CDATA[
<div> VQT-Light, lighting estimation, computer vision, VQVAE, ViT <br />
<br />
Summary: <br />
Accurate lighting estimation in computer vision and graphics is challenging due to difficulties in restoring detailed textures of illumination maps and maintaining run-time efficiency. To address this, a new framework called VQT-Light is proposed based on VQVAE and ViT architecture. By leveraging these two modules for feature extraction and lighting estimation, VQT-Light extracts discrete features using VQVAE to prevent "posterior collapse" and captures global context and dependencies through ViT rather than CNNs. This approach enhances the prediction of illumination beyond the field of view and formulates lighting estimation as a multiclass classification task. The model achieves fast inference speed of 40FPS, improves multiple evaluation metrics, and produces light maps with richer textures and fidelity compared to existing methods, showcasing superior results in both qualitative and quantitative experiments. <div>
arXiv:2509.12556v1 Announce Type: new 
Abstract: Accurate lighting estimation is a significant yet challenging task in computer vision and graphics. However, existing methods either struggle to restore detailed textures of illumination map, or face challenges in run-ning speed and texture fidelity. To tackle this problem, we propose a novel framework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes two modules: feature extraction and lighting estima-tion. First, we take advantages of VQVAE to extract discrete features of illumination map rather than con-tinuous features to avoid "posterior collapse". Second, we capture global context and dependencies of in-put image through ViT rather than CNNs to improve the prediction of illumination outside the field of view. Combining the above two modules, we formulate the lighting estimation as a multiclass classification task, which plays a key role in our pipeline. As a result, our model predicts light map with richer texture and better fidelity while keeping lightweight and fast. VQT-Light achieves an inference speed of 40FPS and im-proves multiple evaluation metrics. Qualitative and quantitative experiments demonstrate that the proposed method realizes superior results compared to existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sampling Scheduler</title>
<link>https://arxiv.org/abs/2509.12569</link>
<guid>https://arxiv.org/abs/2509.12569</guid>
<content:encoded><![CDATA[
<div> Keywords: consistency distillation, adaptive sampling scheduler, target timestep selection, generation performance, complex generation scenarios

Summary:
The paper introduces an adaptive sampling scheduler for consistency distillation methods in diffusion models. The scheduler incorporates dynamic target timestep selection based on timestep importance, optimized alternating sampling along the solution trajectory, and the use of smoothing clipping and color balancing techniques. These strategies enhance generative performance and stability, allowing for more effective exploration of the solution space and improved generation results in complex scenarios. Experimental evaluations across various consistency distillation methods validate the effectiveness and flexibility of the adaptive sampling scheduler, showcasing significant improvements in generative performance. The method demonstrates strong adaptability and broad applicability in enhancing the sampling potential of diffusion models for practical applications.<br /><br />Summary: <div>
arXiv:2509.12569v1 Announce Type: new 
Abstract: Consistent distillation methods have evolved into effective techniques that significantly accelerate the sampling process of diffusion models. Although existing methods have achieved remarkable results, the selection of target timesteps during distillation mainly relies on deterministic or stochastic strategies, which often require sampling schedulers to be designed specifically for different distillation processes. Moreover, this pattern severely limits flexibility, thereby restricting the full sampling potential of diffusion models in practical applications. To overcome these limitations, this paper proposes an adaptive sampling scheduler that is applicable to various consistency distillation frameworks. The scheduler introduces three innovative strategies: (i) dynamic target timestep selection, which adapts to different consistency distillation frameworks by selecting timesteps based on their computed importance; (ii) Optimized alternating sampling along the solution trajectory by guiding forward denoising and backward noise addition based on the proposed time step importance, enabling more effective exploration of the solution space to enhance generation performance; and (iii) Utilization of smoothing clipping and color balancing techniques to achieve stable and high-quality generation results at high guidance scales, thereby expanding the applicability of consistency distillation models in complex generation scenarios. We validated the effectiveness and flexibility of the adaptive sampling scheduler across various consistency distillation methods through comprehensive experimental evaluations. Experimental results consistently demonstrated significant improvements in generative performance, highlighting the strong adaptability achieved by our method.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title>
<link>https://arxiv.org/abs/2509.12595</link>
<guid>https://arxiv.org/abs/2509.12595</guid>
<content:encoded><![CDATA[
<div> Localization, adversarial attack, LiDAR, deep learning models, self-driving cars
Summary:
- The study introduces a novel adversarial attack framework named DisorientLiDAR that targets LiDAR-based localization in self-driving cars.
- Adversaries reverse-engineer localization models to strategically remove critical keypoints and disrupt LiDAR-based localization.
- The attack is evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, GeoTransformer) using the KITTI dataset, showing a significant degradation in registration accuracy when removing key regions.
- The impact of the attack on the Autoware autonomous driving platform is demonstrated, with even a few critical regions hidden leading to noticeable localization drift.
- The attack is extended to the physical world by concealing critical regions with near-infrared absorptive materials, replicating the effects observed in KITTI data and showcasing a step closer to realistic physical-world attacks. 
<br /><br />Summary: <div>
arXiv:2509.12595v1 Announce Type: new 
Abstract: Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack's impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Spectral Characteristics for Single Image Reflection Removal</title>
<link>https://arxiv.org/abs/2509.12627</link>
<guid>https://arxiv.org/abs/2509.12627</guid>
<content:encoded><![CDATA[
<div> Keywords: Reflection removal, Spectral learning, Spectral Codebook, Spectral prior refinement, Spectrum-Aware Transformer

Summary:
This paper addresses the challenge of removing reflections caused by incident light interacting with reflective surfaces in image restoration. The proposed approach introduces the Spectral Codebook to reconstruct the optical spectrum of reflection images, leveraging wavelength differences to distinguish reflections accurately. Spectral prior refinement modules are designed to enhance spectral differences in the reconstructed spectrum. The Spectrum-Aware Transformer is presented to recover transmitted content in both spectral and pixel domains. Experimental results on multiple reflection benchmarks demonstrate the superior performance and generalization of the proposed method compared to existing models. <div>
arXiv:2509.12627v1 Announce Type: new 
Abstract: Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maps for Autonomous Driving: Full-process Survey and Frontiers</title>
<link>https://arxiv.org/abs/2509.12632</link>
<guid>https://arxiv.org/abs/2509.12632</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, maps, map production, HD maps, lightweight maps, implicit maps<br />
<br />
Summary: 
Maps play a crucial role in autonomous driving, with their evolution categorized into three stages: High-Definition (HD) maps, Lightweight (Lite) maps, and Implicit maps. The article provides a detailed overview of the map production workflow for each stage, highlighting technical challenges and solutions proposed by the academic community. It discusses the advancements in map representations and their integration into autonomous driving frameworks. The evolution of maps has seen a shift towards more sophisticated and efficient mapping technologies, with a focus on improving accuracy, real-time updates, and reducing computational complexity. This progress in map production and representation is essential for the development of safe and reliable autonomous driving systems. <div>
arXiv:2509.12632v1 Announce Type: new 
Abstract: Maps have always been an essential component of autonomous driving. With the advancement of autonomous driving technology, both the representation and production process of maps have evolved substantially. The article categorizes the evolution of maps into three stages: High-Definition (HD) maps, Lightweight (Lite) maps, and Implicit maps. For each stage, we provide a comprehensive review of the map production workflow, with highlighting technical challenges involved and summarizing relevant solutions proposed by the academic community. Furthermore, we discuss cutting-edge research advances in map representations and explore how these innovations can be integrated into end-to-end autonomous driving frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIARD: Cyclic Iterative Adversarial Robustness Distillation</title>
<link>https://arxiv.org/abs/2509.12633</link>
<guid>https://arxiv.org/abs/2509.12633</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial robustness distillation, lightweight student model, dual-teacher framework, contrastive push-loss alignment, continuous adversarial retraining 

Summary: 
The paper introduces a novel approach called Cyclic Iterative Adversarial Robustness Distillation (CIARD) to address the performance degradation issue in existing Adversarial Robustness Distillation (ARD) methods. The key innovations of CIARD include a multi-teacher framework with contrastive push-loss alignment to resolve conflicts in optimization objectives and continuous adversarial retraining to maintain robustness against performance deterioration. Experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets show that CIARD outperforms existing methods, achieving a significant improvement in adversarial defense rates and clean sample accuracy. The proposed method establishes a new benchmark for balancing model robustness and generalization. The code for CIARD is publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2509.12633v1 Announce Type: new 
Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance and robustness from teacher model to lightweight student model, enabling resilient performance on resource-constrained scenarios. Though existing ARD approaches enhance student model's robustness, the inevitable by-product leads to the degraded performance on clean examples. We summarize the causes of this problem inherent in existing methods with dual-teacher framework as: 1. The divergent optimization objectives of dual-teacher models, i.e., the clean and robust teachers, impede effective knowledge transfer to the student model, and 2. The iteratively generated adversarial examples during training lead to performance deterioration of the robust teacher model. To address these challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key innovations: a. A multi-teacher framework with contrastive push-loss alignment to resolve conflicts in dual-teacher optimization objectives, and b. Continuous adversarial retraining to maintain dynamic teacher robustness against performance degradation from the varying adversarial examples. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD achieves remarkable performance with an average 3.53 improvement in adversarial defense rates across various attack scenarios and a 5.87 increase in clean sample accuracy, establishing a new benchmark for balancing model robustness and generalization. Our code is available at https://github.com/eminentgu/CIARD
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations</title>
<link>https://arxiv.org/abs/2509.12653</link>
<guid>https://arxiv.org/abs/2509.12653</guid>
<content:encoded><![CDATA[
<div> Keywords: detection, grounding, multimodal data, manipulation, dataset

Summary:
Detection and grounding of manipulated content in multimodal data is a critical challenge in media forensics. Existing benchmarks suffer from misalignment artifacts, creating easily detectable anomalies that differ from real-world manipulation patterns. To address this gap, the Semantic-Aligned Multimodal Manipulation (SAMM) dataset is introduced, featuring semantically-coordinated manipulations with paired textual descriptions. The Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework relies on external knowledge repositories to retrieve contextual evidence, enhancing detection accuracy. Through a two-stage pipeline, SAMM is generated by applying image manipulations and generating contextually-plausible narratives. RamDG outperforms existing methods, achieving higher detection accuracy on SAMM. The dataset and code are publicly available, facilitating further research in detecting and grounding semantically-consistent manipulations.<br /><br />Summary: <div>
arXiv:2509.12653v1 Announce Type: new 
Abstract: The detection and grounding of manipulated content in multimodal data has emerged as a critical challenge in media forensics. While existing benchmarks demonstrate technical progress, they suffer from misalignment artifacts that poorly reflect real-world manipulation patterns: practical attacks typically maintain semantic consistency across modalities, whereas current datasets artificially disrupt cross-modal alignment, creating easily detectable anomalies. To bridge this gap, we pioneer the detection of semantically-coordinated manipulations where visual edits are systematically paired with semantically consistent textual descriptions. Our approach begins with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM) dataset, generated through a two-stage pipeline: 1) applying state-of-the-art image manipulations, followed by 2) generation of contextually-plausible textual narratives that reinforce the visual deception. Building on this foundation, we propose a Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework. RamDG commences by harnessing external knowledge repositories to retrieve contextual evidence, which serves as the auxiliary texts and encoded together with the inputs through our image forgery grounding and deep manipulation detection modules to trace all manipulations. Extensive experiments demonstrate our framework significantly outperforms existing methods, achieving 2.06\% higher detection accuracy on SAMM compared to state-of-the-art approaches. The dataset and code are publicly available at https://github.com/shen8424/SAMM-RamDG-CAP.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization</title>
<link>https://arxiv.org/abs/2509.12673</link>
<guid>https://arxiv.org/abs/2509.12673</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-view geo-localization, EVA02, Multi-scale Frequency Attention Fusion, MFB block, FSA module <br />
Summary: <br />
Cross-view geo-localization is a challenging task due to appearance variations and difficulty in feature extraction. The proposed MFAF method utilizes Multi-Frequency Branch-wise Blocks (MFB) to capture both low and high-frequency features, enhancing feature consistency. Additionally, the Frequency-aware Spatial Attention (FSA) module focuses on key regions of frequency features to reduce interference from background noise. Experimental results on benchmark datasets show that the MFAF method achieves competitive performance in drone localization and navigation tasks. <div>
arXiv:2509.12673v1 Announce Type: new 
Abstract: Cross-view geo-localization aims to determine the geographical location of a query image by matching it against a gallery of images. This task is challenging due to the significant appearance variations of objects observed from variable views, along with the difficulty in extracting discriminative features. Existing approaches often rely on extracting features through feature map segmentation while neglecting spatial and semantic information. To address these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion (MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block (MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block effectively captures both low-frequency structural features and high-frequency edge details across multiple scales, improving the consistency and robustness of feature representations across various viewpoints. Meanwhile, the FSA module adaptively focuses on the key regions of frequency features, significantly mitigating the interference caused by background noise and viewpoint variability. Extensive experiments on widely recognized benchmarks, including University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method achieves competitive performance in both drone localization and drone navigation tasks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks</title>
<link>https://arxiv.org/abs/2509.12682</link>
<guid>https://arxiv.org/abs/2509.12682</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater imagery, autonomous underwater vehicles, computer vision, YOLO, marine-vision research 

Summary: 
This study evaluates the performance of recent YOLO variants on underwater imagery for autonomous underwater vehicles (AUVs). Two datasets, Coral Disease and Fish Species, were curated, and YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s were trained and compared using identical hyperparameters. The results show that accuracy plateaus after YOLOv9, indicating that architectural innovations primarily focus on efficiency rather than accuracy. However, inference speed improves significantly. It was found that YOLOv10 offers the best speed-accuracy trade-off for embedded AUV deployment. Post-hoc Grad-CAM visualizations were used to analyze feature utilization and localization faithfulness. This study provides a controlled comparison of recent YOLO variants on underwater imagery, showcasing the potential of lightweight YOLOv10 for marine-vision research. To accelerate future research in this field, an open, reproducible benchmark and codebase have been made available. 

<br /><br />Summary: <div>
arXiv:2509.12682v1 Announce Type: new 
Abstract: Autonomous underwater vehicles (AUVs) increasingly rely on on-board computer-vision systems for tasks such as habitat mapping, ecological monitoring, and infrastructure inspection. However, underwater imagery is hindered by light attenuation, turbidity, and severe class imbalance, while the computational resources available on AUVs are limited. One-stage detectors from the YOLO family are attractive because they fuse localization and classification in a single, low-latency network; however, their terrestrial benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how successive YOLO releases perform in the marine domain. We curate two openly available datasets that span contrasting operating conditions: a Coral Disease set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20 classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %, 100 % of the images) while keeping balanced validation and test partitions fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate precision, recall, mAP50, mAP50-95, per-image inference time, and frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature utilization and localization faithfulness. Across both datasets, accuracy saturates after YOLOv9, suggesting architectural innovations primarily target efficiency rather than accuracy. Inference speed, however, improves markedly. Our results (i) provide the first controlled comparison of recent YOLO variants on underwater imagery, (ii) show that lightweight YOLOv10 offers the best speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an open, reproducible benchmark and codebase to accelerate future marine-vision research.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo</title>
<link>https://arxiv.org/abs/2509.12683</link>
<guid>https://arxiv.org/abs/2509.12683</guid>
<content:encoded><![CDATA[
<div> dataset, stereo matching, autonomous driving, synthetic, generalization accuracy

Summary:
The article introduces StereoCarla, a synthetic stereo dataset specifically created for autonomous driving scenarios. Utilizing the CARLA simulator, StereoCarla offers a diverse range of camera configurations and environmental conditions, including varying baselines, viewpoints, lighting changes, weather effects, and road geometries. Comprehensive cross-domain experiments on four evaluation datasets demonstrate that models trained on StereoCarla outperform those trained on existing stereo datasets in terms of generalization accuracy. Integration of StereoCarla into multi-dataset training leads to substantial improvements in generalization accuracy, highlighting its compatibility and scalability. The dataset serves as a valuable benchmark for developing and evaluating stereo algorithms under realistic and controllable settings, ultimately enhancing depth perception systems for autonomous vehicles. The code and data for StereoCarla are available on the provided GitHub and website links. <br /><br />Summary: <div>
arXiv:2509.12683v1 Announce Type: new 
Abstract: Stereo matching plays a crucial role in enabling depth perception for autonomous driving and robotics. While recent years have witnessed remarkable progress in stereo matching algorithms, largely driven by learning-based methods and synthetic datasets, the generalization performance of these models remains constrained by the limited diversity of existing training data. To address these challenges, we present StereoCarla, a high-fidelity synthetic stereo dataset specifically designed for autonomous driving scenarios. Built on the CARLA simulator, StereoCarla incorporates a wide range of camera configurations, including diverse baselines, viewpoints, and sensor placements as well as varied environmental conditions such as lighting changes, weather effects, and road geometries. We conduct comprehensive cross-domain experiments across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury, ETH3D) and demonstrate that models trained on StereoCarla outperform those trained on 11 existing stereo datasets in terms of generalization accuracy across multiple benchmarks. Furthermore, when integrated into multi-dataset training, StereoCarla contributes substantial improvements to generalization accuracy, highlighting its compatibility and scalability. This dataset provides a valuable benchmark for developing and evaluating stereo algorithms under realistic, diverse, and controllable settings, facilitating more robust depth perception systems for autonomous vehicles. Code can be available at https://github.com/XiandaGuo/OpenStereo, and data can be available at https://xiandaguo.net/StereoCarla.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes</title>
<link>https://arxiv.org/abs/2509.12701</link>
<guid>https://arxiv.org/abs/2509.12701</guid>
<content:encoded><![CDATA[
<div> Dataset, Smoke removal algorithms, Surveillance systems, Emergency response, Image desmoking. 
<br />
Summary: 
The article introduces a new benchmark dataset named SmokeBench for early-stage fire scenes, which are crucial for emergency interventions. The dataset addresses the limited development of smoke removal algorithms by providing real-world images comprising both smoke-free and smoke-degraded pairs. This enables supervised learning and rigorous evaluation of desmoking methods. The dataset includes images captured under diverse setups and smoke concentrations to mimic real-world scenarios. By benchmarking various desmoking algorithms on the dataset, researchers can advance the development of robust and practical solutions for improving situational awareness in fire scenes. The SmokeBench dataset is publicly available for download on GitHub, offering a valuable foundation for enhancing image desmoking techniques in emergency response and surveillance systems. 
<br /> <div>
arXiv:2509.12701v1 Announce Type: new 
Abstract: Early-stage fire scenes (0-15 minutes after ignition) represent a crucial temporal window for emergency interventions. During this stage, the smoke produced by combustion significantly reduces the visibility of surveillance systems, severely impairing situational awareness and hindering effective emergency response and rescue operations. Consequently, there is an urgent need to remove smoke from images to obtain clear scene information. However, the development of smoke removal algorithms remains limited due to the lack of large-scale, real-world datasets comprising paired smoke-free and smoke-degraded images. To address these limitations, we present a real-world surveillance image desmoking benchmark dataset named SmokeBench, which contains image pairs captured under diverse scenes setup and smoke concentration. The curated dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of desmoking methods on our dataset. Our dataset provides a valuable foundation for advancing robust and practical image desmoking in real-world fire scenes. This dataset has been released to the public and can be downloaded from https://github.com/ncfjd/SmokeBench.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2509.12710</link>
<guid>https://arxiv.org/abs/2509.12710</guid>
<content:encoded><![CDATA[
<div> benchmark, text-driven fusion, image segmentation, multimodal, LangGatedFusion<br />
<br />
Summary:<br />
The study introduces RIS-FUSION, a novel framework that combines referring image segmentation (RIS) and text-driven image fusion to enhance semantic alignment. The LangGatedFusion module is at the core of this system, injecting textual features into the fusion backbone for improved performance. A new benchmark called MM-RIS is also introduced, offering a significant amount of training and testing data for multimodal referring image segmentation tasks. Extensive experiments show that RIS-FUSION outperforms existing methods by more than 11% in mIoU. This approach addresses the lack of goal-aligned tasks in current fusion methods and provides a more effective way to incorporate text input into the fusion process. The code and dataset for this research will be made available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2509.12710v1 Announce Type: new 
Abstract: Text-driven infrared and visible image fusion has gained attention for enabling natural language to guide the fusion process. However, existing methods lack a goal-aligned task to supervise and evaluate how effectively the input text contributes to the fusion outcome. We observe that referring image segmentation (RIS) and text-driven fusion share a common objective: highlighting the object referred to by the text. Motivated by this, we propose RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint optimization. At its core is the LangGatedFusion module, which injects textual features into the fusion backbone to enhance semantic alignment. To support multimodal referring image segmentation task, we introduce MM-RIS, a large-scale benchmark with 12.5k training and 3.5k testing triplets, each consisting of an infrared-visible image pair, a segmentation mask, and a referring expression. Extensive experiments show that RIS-FUSION achieves state-of-the-art performance, outperforming existing methods by over 11% in mIoU. Code and dataset will be released at https://github.com/SijuMa2003/RIS-FUSION.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2509.12711</link>
<guid>https://arxiv.org/abs/2509.12711</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositional Zero-Shot Learning, Debiased Feature Augmentation, Disentangle-and-reconstruct framework, Prior knowledge, Generalization

Summary:
Debiased Feature Augmentation (DeFA) introduces a novel approach to Compositional Zero-Shot Learning (CZSL) by leveraging neuroscientific findings on imagination and perception. The method addresses challenges posed by the entangled nature of attributes and objects, as well as long-tailed distributions in real-world data. DeFA integrates a disentangle-and-reconstruct framework with a debiasing strategy to synthesize high-fidelity composition features based on prior knowledge of seen attributes and objects. Experimental results on three datasets show that DeFA achieves state-of-the-art performance in both closed-world and open-world settings. This approach demonstrates the effectiveness of leveraging cognitive processes to improve CZSL performance and highlights the importance of incorporating prior knowledge for compositional generalization.

<br /><br />Summary: Debiased Feature Augmentation (DeFA) enhances Compositional Zero-Shot Learning by leveraging neuroscientific insights and prior knowledge to address challenges in feature augmentation and generalization. Experimental results validate DeFA's effectiveness, showcasing its state-of-the-art performance in closed-world and open-world scenarios. <div>
arXiv:2509.12711v1 Announce Type: new 
Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object compositions by learning prior knowledge of seen primitives, \textit{i.e.}, attributes and objects. Learning generalizable compositional representations in CZSL remains challenging due to the entangled nature of attributes and objects as well as the prevalence of long-tailed distributions in real-world data. Inspired by neuroscientific findings that imagination and perception share similar neural processes, we propose a novel approach called Debiased Feature Augmentation (DeFA) to address these challenges. The proposed DeFA integrates a disentangle-and-reconstruct framework for feature augmentation with a debiasing strategy. DeFA explicitly leverages the prior knowledge of seen attributes and objects by synthesizing high-fidelity composition features to support compositional generalization. Extensive experiments on three widely used datasets demonstrate that DeFA achieves state-of-the-art performance in both \textit{closed-world} and \textit{open-world} settings.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12715</link>
<guid>https://arxiv.org/abs/2509.12715</guid>
<content:encoded><![CDATA[
<div> specialized experts, hierarchical cross-modal interactions, contextual grounding, multimodal tasks, large vision-language models
Summary:
AsyMoE is introduced as a novel architecture for Large Vision-Language Models (LVLMs) to address the challenges faced by existing Mixture of Experts (MoE) approaches due to the asymmetry between visual and linguistic processing. The proposed model includes three specialized expert groups: intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to maintain contextual grounding. By systematically analyzing the behavior of language experts in deeper layers, AsyMoE overcomes the reliance on parametric knowledge and effectively utilizes visual and linguistic information. Extensive experiments show that AsyMoE outperforms vanilla MoE and modality-specific MoE with significant accuracy improvements while requiring fewer activated parameters than dense models. <div>
arXiv:2509.12715v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</title>
<link>https://arxiv.org/abs/2509.12718</link>
<guid>https://arxiv.org/abs/2509.12718</guid>
<content:encoded><![CDATA[
<div> dynamic spatial reasoning, benchmarks, memory mechanism, spatial understanding, adaptive planning

Summary:
This article introduces new dynamic spatial benchmarks that focus on long-horizon reasoning and memory utilization in partially observable and changing environments. The benchmarks, locally observable maze navigation and match-2 elimination, assess models' abilities in spatial understanding and adaptive planning in dynamic settings. The challenges include local perception, environment feedback, and global objectives that interact with each action taken, necessitating continuous cognitive updates. A subjective experience-based memory mechanism is proposed for cross-task experience transfer and validation. Experiments demonstrate the limitations of current models in dynamic spatial reasoning and long-term memory. The benchmarks provide a comprehensive platform for improving methodological approaches in this area. The code and data are available for further research and development at the provided link. 

<br /><br />Summary: <div>
arXiv:2509.12718v1 Announce Type: new 
Abstract: Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate models' abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at https://anonymous.4open.science/r/EvoEmpirBench-143C/.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation</title>
<link>https://arxiv.org/abs/2509.12721</link>
<guid>https://arxiv.org/abs/2509.12721</guid>
<content:encoded><![CDATA[
<div> Keywords: single-view 3D generative models, multiview diffusion priors, Spherical Projection (SP) representation, consistency, efficiency

Summary: 
SPGen is a new single-view 3D generative model that uses a Spherical Projection (SP) representation to encode geometry information on a bounding sphere. This approach eliminates view inconsistency, supports nested internal structures, and allows for efficient computation. SPGen outperforms existing models in geometric quality and computational efficiency. The injective SP mapping ensures consistency by encoding surface geometry with a single viewpoint, eliminating view ambiguity. The multi-layer SP maps enable representation of complex internal structures and direct lifting to watertight or open 3D surfaces. Operating solely in the image domain, SPGen inherits powerful 2D diffusion priors and allows for efficient fine-tuning with limited computational resources. Overall, SPGen offers a flexible and efficient solution for generating high-quality 3D reconstructions. 

<br /><br />Summary: <div>
arXiv:2509.12721v1 Announce Type: new 
Abstract: Existing single-view 3D generative models typically adopt multiview diffusion priors to reconstruct object surfaces, yet they remain prone to inter-view inconsistencies and are unable to faithfully represent complex internal structure or nontrivial topologies. In particular, we encode geometry information by projecting it onto a bounding sphere and unwrapping it into a compact and structural multi-layer 2D Spherical Projection (SP) representation. Operating solely in the image domain, SPGen offers three key advantages simultaneously: (1) Consistency. The injective SP mapping encodes surface geometry with a single viewpoint which naturally eliminates view inconsistency and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal structures and support direct lifting to watertight or open 3D surfaces; (3) Efficiency. The image-domain formulation allows the direct inheritance of powerful 2D diffusion priors and enables efficient finetuning with limited computational resources. Extensive experiments demonstrate that SPGen significantly outperforms existing baselines in geometric quality and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12724</link>
<guid>https://arxiv.org/abs/2509.12724</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, jailbreak attacks, Defense2Attack, adversarial perturbations, reinforcement fine-tuning

Summary:
Defense2Attack is a novel method for jailbreaking Vision-Language Models (VLMs) that incorporates weak defense mechanisms to enhance the effectiveness and efficiency of attacks. The method consists of three components: a visual optimizer that embeds adversarial perturbations with positive semantics, a textual optimizer that refines inputs using defense-styled prompts, and a red-team suffix generator for reinforcement fine-tuning. Empirical evaluations on four VLMs and safety benchmarks show that Defense2Attack outperforms existing attack methods by achieving superior jailbreak performance in a single attempt. This approach offers a new perspective on jailbreaking VLMs, showcasing the potential of leveraging defensive patterns to guide prompt design and improve attack success rates.<br /><br />Summary: Defense2Attack is a novel method for enhancing the effectiveness and efficiency of jailbreak attacks on Vision-Language Models by incorporating weak defense mechanisms. Through the use of adversarial perturbations with positive semantics, defense-styled prompts, and reinforcement fine-tuning, this method outperforms existing attack techniques by achieving superior performance in a single attempt. The results highlight the importance of considering defensive strategies in designing jailbreaks for VLMs, paving the way for more successful attacks in the future. <div>
arXiv:2509.12724v1 Announce Type: new 
Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks. While recent jailbreaks have achieved notable progress, their effectiveness and efficiency can still be improved. In this work, we reveal an interesting phenomenon: incorporating weak defense into the attack pipeline can significantly enhance both the effectiveness and the efficiency of jailbreaks on VLMs. Building on this insight, we propose Defense2Attack, a novel jailbreak method that bypasses the safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak prompt design. Specifically, Defense2Attack consists of three key components: (1) a visual optimizer that embeds universal adversarial perturbations with affirmative and encouraging semantics; (2) a textual optimizer that refines the input using a defense-styled prompt; and (3) a red-team suffix generator that enhances the jailbreak through reinforcement fine-tuning. We empirically evaluate our method on four VLMs and four safety benchmarks. The results demonstrate that Defense2Attack achieves superior jailbreak performance in a single attempt, outperforming state-of-the-art attack methods that often require multiple tries. Our work offers a new perspective on jailbreaking VLMs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Gaussian Management for High-fidelity Object Reconstruction</title>
<link>https://arxiv.org/abs/2509.12742</link>
<guid>https://arxiv.org/abs/2509.12742</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian management, object reconstruction, Gaussian Splatting, spherical harmonics, normal activation<br />
Summary:<br />
This paper presents an innovative Gaussian management approach for high-fidelity object reconstruction, addressing the limitations of existing Gaussian Splatting methods. The approach leverages a densification strategy that dynamically activates spherical harmonics or normals, guided by a surface reconstruction module to enhance reconstruction quality. A lightweight Gaussian representation is developed, allowing adaptive adjustments of the Gaussian orders based on gradient magnitudes, ensuring efficient representation while balancing parameter quantity. The approach is model-agnostic and seamlessly integrates into various frameworks, improving performance and reducing model size. Extensive experiments demonstrate superior reconstruction quality and efficiency compared to state-of-the-art methods, achieving exceptional performance with significantly fewer parameters. <div>
arXiv:2509.12742v1 Announce Type: new 
Abstract: This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and analysis of the 8 filters from the "master key filters hypothesis" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory</title>
<link>https://arxiv.org/abs/2509.12746</link>
<guid>https://arxiv.org/abs/2509.12746</guid>
<content:encoded><![CDATA[
<div> Keywords: master key filters, clustering, deep networks, receptive fields, scale-space filters

Summary:<br />
This paper analyzes and models a set of 8 "master key filters" extracted from depthwise-separable deep networks using the ConvNeXt architecture. The filters are clustered based on receptive fields, showing separable filtering operations and close spatial offsets. The authors model the filters using difference operators and spatial smoothing with Gaussian kernels, finding good qualitative similarities. Two modeling approaches with different scale parameters are tested, with fitting based on spatial spread measures or norm minimization. Experimental results demonstrate that the idealized models accurately predict the learned filtersâ behavior in deep networks. This suggests that discrete scale-space filters can effectively approximate filters in depthwise-separable deep networks. <div>
arXiv:2509.12746v1 Announce Type: new 
Abstract: This paper presents the results of analysing and modelling a set of 8 ``master key filters'', which have been extracted by applying a clustering approach to the receptive fields learned in depthwise-separable deep networks based on the ConvNeXt architecture.
  For this purpose, we first compute spatial spread measures in terms of weighted mean values and weighted variances of the absolute values of the learned filters, which support the working hypotheses that: (i) the learned filters can be modelled by separable filtering operations over the spatial domain, and that (ii) the spatial offsets of the those learned filters that are non-centered are rather close to half a grid unit. Then, we model the clustered ``master key filters'' in terms of difference operators applied to a spatial smoothing operation in terms of the discrete analogue of the Gaussian kernel, and demonstrate that the resulting idealized models of the receptive fields show good qualitative similarity to the learned filters.
  This modelling is performed in two different ways: (i) using possibly different values of the scale parameters in the coordinate directions for each filter, and (ii) using the same value of the scale parameter in both coordinate directions. Then, we perform the actual model fitting by either (i) requiring spatial spread measures in terms of spatial variances of the absolute values of the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or $l_2$-norms between the idealized receptive field models and the learned filters.
  Complementary experimental results then demonstrate the idealized models of receptive fields have good predictive properties for replacing the learned filters by idealized filters in depthwise-separable deep networks, thus showing that the learned filters in depthwise-separable deep networks can be well approximated by discrete scale-space filters.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment</title>
<link>https://arxiv.org/abs/2509.12750</link>
<guid>https://arxiv.org/abs/2509.12750</guid>
<content:encoded><![CDATA[
<div> Attributes, image quality, multimodal LLMs, human judgments, dataset <br />
Summary: 
Automated evaluation of text-to-image models is complex. Multimodal LLMs have been used to assess image quality, but their understanding of human-relevant concepts like image style is unclear. A study was conducted on image attributes of aesthetics, lack of artifacts, anatomical accuracy, composition correctness, object adherence, and style for both LLMs and humans. Human preferences were collected through synthetic image pairs, showing strong correlations between image quality features for human judgment. In contrast, LLMs displayed weaker links between attributes. Further analysis revealed that while humans could easily assess image quality based on specific attributes, such as aesthetics, LLMs struggled with certain aspects like anatomical accuracy. These findings highlight differences in how humans and LLMs perceive and evaluate images. <br /> <div>
arXiv:2509.12750v1 Announce Type: new 
Abstract: Automated evaluation of generative text-to-image models remains a challenging problem. Recent works have proposed using multimodal LLMs to judge the quality of images, but these works offer little insight into how multimodal LLMs make use of concepts relevant to humans, such as image style or composition, to generate their overall assessment. In this work, we study what attributes of an image--specifically aesthetics, lack of artifacts, anatomical accuracy, compositional correctness, object adherence, and style--are important for both LLMs and humans to make judgments on image quality. We first curate a dataset of human preferences using synthetically generated image pairs. We use inter-task correlation between each pair of image quality attributes to understand which attributes are related in making human judgments. Repeating the same analysis with LLMs, we find that the relationships between image quality attributes are much weaker. Finally, we study individual image quality attributes by generating synthetic datasets with a high degree of control for each axis. Humans are able to easily judge the quality of an image with respect to all of the specific image quality attributes (e.g. high vs. low aesthetic image), however we find that some attributes, such as anatomical accuracy, are much more difficult for multimodal LLMs to learn to judge. Taken together, these findings reveal interesting differences between how humans and multimodal LLMs perceive images.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Cross-View Object Geo-Localization</title>
<link>https://arxiv.org/abs/2509.12757</link>
<guid>https://arxiv.org/abs/2509.12757</guid>
<content:encoded><![CDATA[
<div> Transformer, cross-view object geo-localization, recurrent localization, knowledge distillation, reference feature enhancement <br />
Summary: 
The paper introduces ReCOT, a Recurrent Cross-view Object geo-localization Transformer, for the task of determining object locations in high-resolution satellite imagery. ReCOT reformulates the task as a recurrent localization problem, using learnable tokens to encode task-specific intent and iteratively refine predicted locations. The model incorporates a knowledge distillation strategy to transfer segmentation priors for clearer semantic guidance and a Reference Feature Enhancement Module to emphasize object-relevant regions in reference features. Experimental results demonstrate that ReCOT achieves state-of-the-art performance on standard benchmarks while reducing parameters by 60% compared to previous approaches. <div>
arXiv:2509.12757v1 Announce Type: new 
Abstract: Cross-view object geo-localization (CVOGL) aims to determine the location of a specific object in high-resolution satellite imagery given a query image with a point prompt. Existing approaches treat CVOGL as a one-shot detection task, directly regressing object locations from cross-view information aggregation, but they are vulnerable to feature noise and lack mechanisms for error correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object geo-localization Transformer, which reformulates CVOGL as a recurrent localization task. ReCOT introduces a set of learnable tokens that encode task-specific intent from the query image and prompt embeddings, and iteratively attend to the reference features to refine the predicted location. To enhance this recurrent process, we incorporate two complementary modules: (1) a SAM-based knowledge distillation strategy that transfers segmentation priors from the Segment Anything Model (SAM) to provide clearer semantic guidance without additional inference cost, and (2) a Reference Feature Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize object-relevant regions in the reference features. Extensive experiments on standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art (SOTA) performance while reducing parameters by 60% compared to previous SOTA approaches.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
<link>https://arxiv.org/abs/2509.12759</link>
<guid>https://arxiv.org/abs/2509.12759</guid>
<content:encoded><![CDATA[
<div> Keywords: True Digital Orthophoto Map, On-the-Fly SfM, 3DGS optimization, real-time, rendering quality 

Summary: 
The article introduces A-TDOM, a method for generating True Digital Orthophoto Maps (TDOM) in near real-time. Traditional TDOM generation methods often face delays due to complex offline processes. A-TDOM utilizes On-the-Fly Structure from Motion (SfM) to compute pose and sparse point clouds for images as they are acquired. It integrates new Gaussians and optimizes them into regions that were previously unseen or coarsely reconstructed. By incorporating orthogonal splatting, A-TDOM can render updates of the 3DGS field after each new image. Initial experiments demonstrate that A-TDOM can efficiently render TDOM in near real-time, with 3DGS optimization for each new image completed in seconds, while maintaining acceptable rendering quality and geometric accuracy. <br /><br />Summary: <div>
arXiv:2509.12759v1 Announce Type: new 
Abstract: True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in various fields such as urban management, city planning, land surveying, etc. However, traditional TDOM generation methods generally rely on a complex offline photogrammetric pipeline, resulting in delays that hinder real-time applications. Moreover, the quality of TDOM may degrade due to various challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and scene occlusions. To address these challenges, this work introduces A-TDOM, a near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As each image is acquired, its pose and sparse point cloud are computed via On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously unseen or coarsely reconstructed regions. By integrating with orthogonal splatting, A-TDOM can render just after each update of a new 3DGS field. Initial experiments on multiple benchmarks show that the proposed A-TDOM is capable of actively rendering TDOM in near real-time, with 3DGS optimization for each new image in seconds while maintaining acceptable rendering quality and TDOM geometric accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.12763</link>
<guid>https://arxiv.org/abs/2509.12763</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical image segmentation, DyGLNet, global and local features, dynamic upsampling mechanism, efficient

Summary:<br /><br />
The paper introduces DyGLNet, a novel approach for medical image segmentation that combines global and local features with a dynamic upsampling mechanism. The model incorporates a hybrid feature extraction module (SHDCBlock) that utilizes single-head self-attention and multi-scale dilated convolutions to capture both local details and global context. Additionally, a dynamic adaptive upsampling module (DyFusionUp) is introduced to enhance feature map reconstruction based on learnable offsets. The lightweight design of DyGLNet reduces computational overhead while maintaining high segmentation accuracy. Experimental results on seven public datasets demonstrate superior performance compared to existing methods, especially in boundary accuracy and small-object segmentation. DyGLNet offers an efficient and reliable solution for clinical medical image analysis, providing a balance between accuracy and computational complexity. The code for DyGLNet will be released soon for further exploration and implementation. <div>
arXiv:2509.12763v1 Announce Type: new 
Abstract: Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers</title>
<link>https://arxiv.org/abs/2509.12768</link>
<guid>https://arxiv.org/abs/2509.12768</guid>
<content:encoded><![CDATA[
arXiv:2509.12768v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision applications. However, their performance in few-shot learning is limited by challenges in refining token-level interactions, struggling with limited training data, and developing a strong inductive bias. Existing methods often depend on inflexible token matching or basic similarity measures, which limit the effective incorporation of global context and localized feature refinement. To address these challenges, we propose Bi-Level Adaptive Token Refinement for Few-Shot Transformers (BATR-FST), a two-stage approach that progressively improves token representations and maintains a robust inductive bias for few-shot classification. During the pre-training phase, Masked Image Modeling (MIM) provides Vision Transformers (ViTs) with transferable patch-level representations by recreating masked image regions, providing a robust basis for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to capture localized interactions, Uncertainty-Aware Token Weighting to prioritize dependable features, and a Bi-Level Attention mechanism to balance intra-cluster and inter-cluster relationships, thereby facilitating thorough token refinement. Furthermore, Graph Token Propagation ensures semantic consistency between support and query instances, while a Class Separation Penalty preserves different class borders, enhancing discriminative capability. Extensive experiments on three benchmark few-shot datasets demonstrate that BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and improves the few-shot classification via transformers.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT</title>
<link>https://arxiv.org/abs/2509.12777</link>
<guid>https://arxiv.org/abs/2509.12777</guid>
<content:encoded><![CDATA[
arXiv:2509.12777v1 Announce Type: new 
Abstract: Contrast-enhanced computed tomography (CECT) is the primary imaging technique that provides valuable spatial-temporal information about lesions, enabling the accurate diagnosis and subclassification of pancreatic tumors. However, the high heterogeneity and variability of pancreatic tumors still pose substantial challenges for precise subtyping diagnosis. Previous methods fail to effectively explore the contextual information across multiple CECT phases commonly used in radiologists' diagnostic workflows, thereby limiting their performance. In this paper, we introduce, for the first time, an automatic way to combine the multi-phase CECT data to discriminate between pancreatic tumor subtypes, among which the key is using Mamba with promising learnability and simplicity to encourage both temporal and spatial modeling from multi-phase CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware Mamba module incorporating two novel spatial and temporal sampling sequences to explore intra and inter-phase contrast variations of lesions. A similarity-guided refinement module is also imposed into the temporal scanning modeling to emphasize the learning on local tumor regions with more obvious temporal variations. Moreover, we design the space complementary integrator and multi-granularity fusion module to encode and aggregate the semantics across different scales, achieving more efficient learning for subtyping pancreatic tumors. The experimental results on an in-house dataset of 270 clinical cases achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors (PNETs), demonstrating its potential as a more accurate and efficient tool.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection</title>
<link>https://arxiv.org/abs/2509.12784</link>
<guid>https://arxiv.org/abs/2509.12784</guid>
<content:encoded><![CDATA[
arXiv:2509.12784v1 Announce Type: new 
Abstract: Human-Object Interaction (HOI) detection aims to simultaneously localize human-object pairs and recognize their interactions. While recent two-stage approaches have made significant progress, they still face challenges due to incomplete context modeling. In this work, we introduce a Contextualized Representation Learning Network that integrates both affordance-guided reasoning and contextual prompts with visual cues to better capture complex interactions. We enhance the conventional HOI detection framework by expanding it beyond simple human-object pairs to include multivariate relationships involving auxiliary entities like tools. Specifically, we explicitly model the functional role (affordance) of these auxiliary objects through triplet structures . This enables our model to identify tool-dependent interactions such as 'filling'. Furthermore, the learnable prompt is enriched with instance categories and subsequently integrated with contextual visual features using an attention mechanism. This process aligns language with image content at both global and regional levels. These contextualized representations equip the model with enriched relational cues for more reliable reasoning over complex, context-dependent interactions. Our proposed method demonstrates superior performance on both the HICO-Det and V-COCO datasets in most scenarios. Codes will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Helix Diffusion for Cross-Domain Anomaly Image Generation</title>
<link>https://arxiv.org/abs/2509.12787</link>
<guid>https://arxiv.org/abs/2509.12787</guid>
<content:encoded><![CDATA[
arXiv:2509.12787v1 Announce Type: new 
Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation</title>
<link>https://arxiv.org/abs/2509.12791</link>
<guid>https://arxiv.org/abs/2509.12791</guid>
<content:encoded><![CDATA[
arXiv:2509.12791v1 Announce Type: new 
Abstract: Superpixels are widely used in computer vision to simplify image representation and reduce computational complexity. While traditional methods rely on low-level features, deep learning-based approaches leverage high-level features but also tend to sacrifice regularity of superpixels to capture complex objects, leading to accurate but less interpretable segmentations. In this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework for segmenting images into accurate yet regular superpixels. We train a model to extract image features for superpixel generation, and at inference, we leverage a large-scale pretrained model for semantic-agnostic segmentation to ensure that superpixels align with object masks. SPAM can handle any prior high-level segmentation, resolving uncertainty regions, and is able to interactively focus on specific objects. Comprehensive experiments demonstrate that SPAM qualitatively and quantitatively outperforms state-of-the-art methods on segmentation tasks, making it a valuable and robust tool for various applications. Code and pre-trained models are available here: https://github.com/waldo-j/spam.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation</title>
<link>https://arxiv.org/abs/2509.12815</link>
<guid>https://arxiv.org/abs/2509.12815</guid>
<content:encoded><![CDATA[
arXiv:2509.12815v1 Announce Type: new 
Abstract: The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention</title>
<link>https://arxiv.org/abs/2509.12817</link>
<guid>https://arxiv.org/abs/2509.12817</guid>
<content:encoded><![CDATA[
arXiv:2509.12817v1 Announce Type: new 
Abstract: While Transformer architecture excel at modeling long-range dependencies contributing to its widespread adoption in vision tasks the quadratic complexity of softmax-based attention mechanisms imposes a major bottleneck, particularly when processing high-resolution images. Linear attention presents a promising alternative by reformulating the attention computation from $(QK)V$ to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$ while preserving the global receptive field. However, most existing methods compress historical key-value (KV) information uniformly, which can lead to feature redundancy and the loss of directional alignment with the query (Q). This uniform compression results in low-rank $KV$ feature maps, contributing to a performance gap compared to softmax attention. To mitigate this limitation, we propose \textbf{S}elective \textbf{A}daptive \textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which introduces input-adaptive learnable gates to selectively modulate information aggregation into the $KV$ feature map. These gates enhance semantic diversity and alleviate the low-rank constraint inherent in conventional linear attention. Additionally, we propose an efficient Hadamard-product decomposition method for gate computation, which introduces no additional memory overhead. Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency and model effectiveness.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Scaling Laws for Radiology Foundation Models</title>
<link>https://arxiv.org/abs/2509.12818</link>
<guid>https://arxiv.org/abs/2509.12818</guid>
<content:encoded><![CDATA[
arXiv:2509.12818v1 Announce Type: new 
Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M chest x-rays from a single institution, holding compute and evaluation protocols constant. We evaluate on classification (radiology findings, lines and tubes), segmentation (lines and tubes), and radiology report generation. While prior work has primarily focused on tasks related to radiology findings, we include lines and tubes tasks to counterbalance this bias and evaluate a model's ability to extract features that preserve continuity along elongated structures. Our experiments show that MI2 scales more effectively for finding-related tasks, while RAD-DINO is stronger on tube-related tasks. Surprisingly, continually pretraining MI2 with both reports and structured labels using UniCL improves performance, underscoring the value of structured supervision at scale. We further show that for some tasks, as few as 30k in-domain samples are sufficient to surpass open-weights foundation models. These results highlight the utility of center-specific continual pretraining, enabling medical institutions to derive significant performance gains by utilizing in-domain data.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Metric Fusion for Evaluation of NeRFs</title>
<link>https://arxiv.org/abs/2509.12836</link>
<guid>https://arxiv.org/abs/2509.12836</guid>
<content:encoded><![CDATA[
arXiv:2509.12836v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses</title>
<link>https://arxiv.org/abs/2509.12866</link>
<guid>https://arxiv.org/abs/2509.12866</guid>
<content:encoded><![CDATA[
arXiv:2509.12866v1 Announce Type: new 
Abstract: It is well-established that more data generally improves AI model performance. However, data collection can be challenging for certain tasks due to the rarity of occurrences or high costs. These challenges are evident in our use case, where we apply AI models to a novel approach for visually documenting the musculoskeletal condition of dogs. Here, abnormalities are marked as colored strokes on a body map of a dog. Since these strokes correspond to distinct muscles or joints, they can be mapped to the textual domain in which large language models (LLMs) operate. LLMs have demonstrated impressive capabilities across a wide range of tasks, including medical applications, offering promising potential for generating synthetic training data. In this work, we investigate whether LLMs can effectively generate synthetic visual training data for canine musculoskeletal diagnoses. For this, we developed a mapping that segments visual documentations into over 200 labeled regions representing muscles or joints. Using techniques like guided decoding, chain-of-thought reasoning, and few-shot prompting, we generated 1,000 synthetic visual documentations for patellar luxation (kneecap dislocation) diagnosis, the diagnosis for which we have the most real-world data. Our analysis shows that the generated documentations are sensitive to location and severity of the diagnosis while remaining independent of the dog's sex. We further generated 1,000 visual documentations for various other diagnoses to create a binary classification dataset. A model trained solely on this synthetic data achieved an F1 score of 88% on 70 real-world documentations. These results demonstrate the potential of LLM-generated synthetic data, which is particularly valuable for addressing data scarcity in rare diseases. While our methodology is tailored to the medical domain, the insights and techniques can be adapted to other fields.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment</title>
<link>https://arxiv.org/abs/2509.12871</link>
<guid>https://arxiv.org/abs/2509.12871</guid>
<content:encoded><![CDATA[
arXiv:2509.12871v1 Announce Type: new 
Abstract: Evaluating object detection models in deployment is challenging because ground-truth annotations are rarely available. We introduce the Cumulative Consensus Score (CCS), a label-free metric that enables continuous monitoring and comparison of detectors in real-world settings. CCS applies test-time data augmentation to each image, collects predicted bounding boxes across augmented views, and computes overlaps using Intersection over Union. Maximum overlaps are normalized and averaged across augmentation pairs, yielding a measure of spatial consistency that serves as a proxy for reliability without annotations. In controlled experiments on Open Images and KITTI, CCS achieved over 90% congruence with F1-score, Probabilistic Detection Quality, and Optimal Correction Cost. The method is model-agnostic, working across single-stage and two-stage detectors, and operates at the case level to highlight under-performing scenarios. Altogether, CCS provides a robust foundation for DevOps-style monitoring of object detectors.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.12878</link>
<guid>https://arxiv.org/abs/2509.12878</guid>
<content:encoded><![CDATA[
arXiv:2509.12878v1 Announce Type: new 
Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder</title>
<link>https://arxiv.org/abs/2509.12883</link>
<guid>https://arxiv.org/abs/2509.12883</guid>
<content:encoded><![CDATA[
arXiv:2509.12883v1 Announce Type: new 
Abstract: Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.
  Code is available: https://github.com/xiaomi-research/lego-edit.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</title>
<link>https://arxiv.org/abs/2509.12888</link>
<guid>https://arxiv.org/abs/2509.12888</guid>
<content:encoded><![CDATA[
arXiv:2509.12888v1 Announce Type: new 
Abstract: Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at https://github.com/wmchen/RKSovler_DDTA.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization</title>
<link>https://arxiv.org/abs/2509.12893</link>
<guid>https://arxiv.org/abs/2509.12893</guid>
<content:encoded><![CDATA[
arXiv:2509.12893v1 Announce Type: new 
Abstract: Surgical triplet recognition, which involves identifying instrument, verb, target, and their combinations, is a complex surgical scene understanding challenge plagued by long-tailed data distribution. The mainstream multi-task learning paradigm benefiting from cross-task collaborative promotion has shown promising performance in identifying triples, but two key challenges remain: 1) inter-task optimization conflicts caused by entangling task-generic and task-specific representations; 2) intra-task optimization conflicts due to class-imbalanced training data. To overcome these difficulties, we propose the MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and intra-task optimization for surgical triplet recognition. For inter-task optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning scheme that decomposes representations into task-shared and task-specific components. To enhance task-shared representations, we construct a Multimodal Large Language Model (MLLM) powered probabilistic prompt pool to dynamically augment visual features with expert-level semantic cues. Additionally, comprehensive task-specific cues are modeled via distinct task prompts covering the temporal-spatial dimensions, effectively mitigating inter-task ambiguities. To tackle intra-task optimization conflicts, we develop a Coordinated Gradient Learning (CGL) strategy, which dissects and rebalances the positive-negative gradients originating from head and tail classes for more coordinated learning behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets demonstrate the superiority of our proposed framework, validating its effectiveness in handling optimization conflicts.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialNav: Multi-turn Dialog Navigation with a Remote Guide</title>
<link>https://arxiv.org/abs/2509.12894</link>
<guid>https://arxiv.org/abs/2509.12894</guid>
<content:encoded><![CDATA[
arXiv:2509.12894v1 Announce Type: new 
Abstract: We introduce DialNav, a novel collaborative embodied dialog task, where a navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn dialog to reach a goal location. Unlike prior work, DialNav aims for holistic evaluation and requires the Guide to infer the Navigator's location, making communication essential for task success. To support this task, we collect and release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog paired with navigation trajectories in photorealistic environments. We design a comprehensive benchmark to evaluate both navigation and dialog, and conduct extensive experiments analyzing the impact of different Navigator and Guide models. We highlight key challenges and publicly release the dataset, code, and evaluation framework to foster future research in embodied dialog.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12897</link>
<guid>https://arxiv.org/abs/2509.12897</guid>
<content:encoded><![CDATA[
arXiv:2509.12897v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in images, yet their attention to these objects tends to be very brief. Motivated by the hypothesis that sustained focus on key objects can improve LVLMs' visual capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of CLVS is to incorporate a vision memory that smooths the attention distribution across layers. Specifically, we initialize this vision memory with position-unbiased visual attention in the first layer. In subsequent layers, the model's visual attention jointly considers the vision memory from previous layers, while the memory is updated iteratively, thereby maintaining smooth attention on key objects. Given that visual understanding primarily occurs in the early and middle layers of the model, we use uncertainty as an indicator of completed visual understanding and terminate the smoothing process accordingly. Experiments on four benchmarks across three LVLMs confirm the effectiveness and generalizability of our method. CLVS achieves state-of-the-art performance on a variety of visual understanding tasks, with particularly significant improvements in relation and attribute understanding.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2509.12901</link>
<guid>https://arxiv.org/abs/2509.12901</guid>
<content:encoded><![CDATA[
arXiv:2509.12901v1 Announce Type: new 
Abstract: Infrared and visible image fusion has garnered considerable attention owing to the strong complementarity of these two modalities in complex, harsh environments. While deep learning-based fusion methods have made remarkable advances in feature extraction, alignment, fusion, and reconstruction, they still depend largely on low-level visual cues, such as texture and contrast, and struggle to capture the high-level semantic information embedded in images. Recent attempts to incorporate text as a source of semantic guidance have relied on unstructured descriptions that neither explicitly model entities, attributes, and relationships nor provide spatial localization, thereby limiting fine-grained fusion performance. To overcome these challenges, we introduce MSGFusion, a multimodal scene graph-guided fusion framework for infrared and visible imagery. By deeply coupling structured scene graphs derived from text and vision, MSGFusion explicitly represents entities, attributes, and spatial relations, and then synchronously refines high-level semantics and low-level details through successive modules for scene graph representation, hierarchical aggregation, and graph-driven fusion. Extensive experiments on multiple public benchmarks show that MSGFusion significantly outperforms state-of-the-art approaches, particularly in detail preservation and structural clarity, and delivers superior semantic consistency and generalizability in downstream tasks such as low-light object detection, semantic segmentation, and medical image fusion.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring</title>
<link>https://arxiv.org/abs/2509.12905</link>
<guid>https://arxiv.org/abs/2509.12905</guid>
<content:encoded><![CDATA[
arXiv:2509.12905v1 Announce Type: new 
Abstract: Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</title>
<link>https://arxiv.org/abs/2509.12913</link>
<guid>https://arxiv.org/abs/2509.12913</guid>
<content:encoded><![CDATA[
arXiv:2509.12913v1 Announce Type: new 
Abstract: Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Compression Framework for YOLOv8: Achiev-ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation</title>
<link>https://arxiv.org/abs/2509.12918</link>
<guid>https://arxiv.org/abs/2509.12918</guid>
<content:encoded><![CDATA[
arXiv:2509.12918v1 Announce Type: new 
Abstract: Efficient deployment of deep learning models for aerial object detection on resource-constrained devices requires significant compression without com-promising performance. In this study, we propose a novel three-stage compression pipeline for the YOLOv8 object detection model, integrating sparsity-aware training, structured channel pruning, and Channel-Wise Knowledge Distillation (CWD). First, sparsity-aware training introduces dynamic sparsity during model optimization, effectively balancing parameter reduction and detection accuracy. Second, we apply structured channel pruning by leveraging batch normalization scaling factors to eliminate redundant channels, significantly reducing model size and computational complexity. Finally, to mitigate the accuracy drop caused by pruning, we employ CWD to transfer knowledge from the original model, using an adjustable temperature and loss weighting scheme tailored for small and medium object detection. Extensive experiments on the VisDrone dataset demonstrate the effectiveness of our approach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model parameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to 13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The resulting compressed model achieves 47.9 AP50 and boosts inference speed from 26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge devices. We further apply TensorRT as a lightweight optimization step. While this introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly improves inference speed from 45 to 68 FPS, demonstrating the practicality of our approach for high-throughput, re-source-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATTER: Multiscale Attention for Registration Error Regression</title>
<link>https://arxiv.org/abs/2509.12924</link>
<guid>https://arxiv.org/abs/2509.12924</guid>
<content:encoded><![CDATA[
arXiv:2509.12924v1 Announce Type: new 
Abstract: Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e.,~{\it PCR quality validation}, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar</title>
<link>https://arxiv.org/abs/2509.12931</link>
<guid>https://arxiv.org/abs/2509.12931</guid>
<content:encoded><![CDATA[
arXiv:2509.12931v1 Announce Type: new 
Abstract: 3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings</title>
<link>https://arxiv.org/abs/2509.12938</link>
<guid>https://arxiv.org/abs/2509.12938</guid>
<content:encoded><![CDATA[
arXiv:2509.12938v1 Announce Type: new 
Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive "bags of embeddings" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain</title>
<link>https://arxiv.org/abs/2509.12959</link>
<guid>https://arxiv.org/abs/2509.12959</guid>
<content:encoded><![CDATA[
arXiv:2509.12959v1 Announce Type: new 
Abstract: The integration of event cameras and spiking neural networks holds great promise for energy-efficient visual processing. However, the limited availability of event data and the sparse nature of DVS outputs pose challenges for effective training. Although some prior work has attempted to transfer semantic knowledge from RGB datasets to DVS, they often overlook the significant distribution gap between the two modalities. In this paper, we propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing strategy that exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time-steps. To enable label mixing in cross-modal scenarios, we further introduce modality-aware auxiliary learning objectives. These objectives support the time-step mixup process and enhance the model's ability to discriminate effectively across different modalities. Our approach enables smoother knowledge transfer, alleviates modality shift during training, and achieves superior performance in spiking image classification tasks. Extensive experiments demonstrate the effectiveness of our method across multiple datasets. The code will be released after the double-blind review process.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMS: Multi-Modal Multi-Surface Interactive Segmentation</title>
<link>https://arxiv.org/abs/2509.12963</link>
<guid>https://arxiv.org/abs/2509.12963</guid>
<content:encoded><![CDATA[
arXiv:2509.12963v1 Announce Type: new 
Abstract: In this paper, we present a method to interactively create segmentation masks on the basis of user clicks. We pay particular attention to the segmentation of multiple surfaces that are simultaneously present in the same image. Since these surfaces may be heavily entangled and adjacent, we also present a novel extended evaluation metric that accounts for the challenges of this scenario. Additionally, the presented method is able to use multi-modal inputs to facilitate the segmentation task. At the center of this method is a network architecture which takes as input an RGB image, a number of non-RGB modalities, an erroneous mask, and encoded clicks. Based on this input, the network predicts an improved segmentation mask. We design our architecture such that it adheres to two conditions: (1) The RGB backbone is only available as a black-box. (2) To reduce the response time, we want our model to integrate the interaction-specific information after the image feature extraction and the multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface interactive segmentation (MMMS). We are able to show the effectiveness of our multi-modal fusion strategy. Using additional modalities, our system reduces the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to 1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline achieves competitive, and in some cases even superior performance when tested in a classical, single-mask interactive segmentation scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)</title>
<link>https://arxiv.org/abs/2509.12965</link>
<guid>https://arxiv.org/abs/2509.12965</guid>
<content:encoded><![CDATA[
arXiv:2509.12965v1 Announce Type: new 
Abstract: Text line segmentation is a critical step in handwritten document image analysis. Segmenting text lines in historical handwritten documents, however, presents unique challenges due to irregular handwriting, faded ink, and complex layouts with overlapping lines and non-linear text flow. Furthermore, the scarcity of large annotated datasets renders fully supervised learning approaches impractical for such materials. To address these challenges, we introduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents (FEST) Competition. Participants are tasked with developing systems capable of segmenting text lines in U-DIADS-TL dataset, using only three annotated images per manuscript for training. The competition dataset features a diverse collection of ancient manuscripts exhibiting a wide range of layouts, degradation levels, and non-standard formatting, closely reflecting real-world conditions. By emphasizing few-shot learning, FEST competition aims to promote the development of robust and adaptable methods that can be employed by humanities scholars with minimal manual annotation effort, thus fostering broader adoption of automated document analysis tools in historical research.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHREC 2025: Protein surface shape retrieval including electrostatic potential</title>
<link>https://arxiv.org/abs/2509.12976</link>
<guid>https://arxiv.org/abs/2509.12976</guid>
<content:encoded><![CDATA[
arXiv:2509.12976v1 Announce Type: new 
Abstract: This SHREC 2025 track dedicated to protein surface shape retrieval involved 9 participating teams. We evaluated the performance in retrieval of 15 proposed methods on a large dataset of 11,555 protein surfaces with calculated electrostatic potential (a key molecular surface descriptor). The performance in retrieval of the proposed methods was evaluated through different metrics (Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best retrieval performance was achieved by the proposed methods that used the electrostatic potential complementary to molecular surface shape. This observation was also valid for classes with limited data which highlights the importance of taking into account additional molecular surface descriptors.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER</title>
<link>https://arxiv.org/abs/2509.12980</link>
<guid>https://arxiv.org/abs/2509.12980</guid>
<content:encoded><![CDATA[
arXiv:2509.12980v1 Announce Type: new 
Abstract: We identify and address a fundamental limitation of sinusoidal representation networks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann et al. (2020), when not initialized appropriately, can struggle at fitting signals that fall outside their frequency support. In extreme cases, when the network's frequency support misaligns with the target spectrum, a 'spectral bottleneck' phenomenon is observed, where the model yields to a near-zero output and fails to recover even the frequency components that are within its representational capacity. To overcome this, we propose WINNER - Weight Initialization with Noise for Neural Representations. WINNER perturbs uniformly initialized weights of base SIREN with Gaussian noise - whose noise scales are adaptively determined by the spectral centroid of the target signal. Similar to random Fourier embeddings, this mitigates 'spectral bias' but without introducing additional trainable parameters. Our method achieves state-of-the-art audio fitting and significant gains in image and 3D shape fitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new avenues in adaptive, target-aware initialization strategies for optimizing deep neural network training. For code and data visit cfdlabtechnion.github.io/siren_square/.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</title>
<link>https://arxiv.org/abs/2509.12989</link>
<guid>https://arxiv.org/abs/2509.12989</guid>
<content:encoded><![CDATA[
arXiv:2509.12989v1 Announce Type: new 
Abstract: Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</title>
<link>https://arxiv.org/abs/2509.12990</link>
<guid>https://arxiv.org/abs/2509.12990</guid>
<content:encoded><![CDATA[
arXiv:2509.12990v1 Announce Type: new 
Abstract: In this report, we address the problem of determining whether a user performs an action incorrectly from egocentric video data. To handle the challenges posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted Mixture-of-Experts (DR-MoE) framework. In the first stage, features are extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are combined through a feature-level expert module. In the second stage, three classifiers are trained with different objectives: reweighted cross-entropy to mitigate class imbalance, AUC loss to improve ranking under skewed distributions, and label-aware loss with sharpness-aware minimization to enhance calibration and generalization. Their predictions are fused using a classification-level expert module. The proposed method achieves strong performance, particularly in identifying rare and ambiguous mistake instances. The code is available at https://github.com/boyuh/DR-MoE.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection</title>
<link>https://arxiv.org/abs/2509.12995</link>
<guid>https://arxiv.org/abs/2509.12995</guid>
<content:encoded><![CDATA[
arXiv:2509.12995v1 Announce Type: new 
Abstract: While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\%.
  Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire</title>
<link>https://arxiv.org/abs/2509.12997</link>
<guid>https://arxiv.org/abs/2509.12997</guid>
<content:encoded><![CDATA[
arXiv:2509.12997v1 Announce Type: new 
Abstract: Small drones are an increasing threat to both military personnel and civilian infrastructure, making early and automated detection crucial. In this work we develop a system that uses spiking neural networks and neuromorphic cameras (event cameras) to detect drones. The detection model is deployed on a neuromorphic chip making this a fully neuromorphic system. Multiple detection units can be deployed to create a virtual tripwire which detects when and where drones enter a restricted zone. We show that our neuromorphic solution is several orders of magnitude more energy efficient than a reference solution deployed on an edge GPU, allowing the system to run for over a year on battery power. We investigate how synthetically generated data can be used for training, and show that our model most likely relies on the shape of the drone rather than the temporal characteristics of its propellers. The small size and low power consumption allows easy deployment in contested areas or locations that lack power infrastructure.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</title>
<link>https://arxiv.org/abs/2509.13013</link>
<guid>https://arxiv.org/abs/2509.13013</guid>
<content:encoded><![CDATA[
arXiv:2509.13013v1 Announce Type: new 
Abstract: With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.13031</link>
<guid>https://arxiv.org/abs/2509.13031</guid>
<content:encoded><![CDATA[
arXiv:2509.13031v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has proven highly effective in eliciting the reasoning capabilities of large language models (LLMs). Inspired by this success, recent studies have explored applying similar techniques to vision-language models (VLMs), aiming to enhance their reasoning performance. However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as the tasks faced by VLMs are inherently more complex. Specifically, VLMs must first accurately perceive and understand visual inputs before reasoning can be effectively performed. To address this challenge, we propose a two-stage reinforcement learning framework designed to jointly enhance both the perceptual and reasoning capabilities of VLMs. To mitigate the vanishing advantage issue commonly observed in RL training, we first perform dataset-level sampling to selectively strengthen specific capabilities using distinct data sources. During training, the first stage focuses on improving the model's visual perception through coarse- and fine-grained visual understanding, while the second stage targets the enhancement of reasoning abilities. After the proposed two-stage reinforcement learning process, we obtain PeBR-R1, a vision-language model with significantly enhanced perceptual and reasoning capabilities. Experimental results on seven benchmark datasets demonstrate the effectiveness of our approach and validate the superior performance of PeBR-R1 across diverse visual reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.13067</link>
<guid>https://arxiv.org/abs/2509.13067</guid>
<content:encoded><![CDATA[
arXiv:2509.13067v1 Announce Type: new 
Abstract: By cropping high-resolution images into local tiles and encoding them independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have demonstrated remarkable fine-grained visual understanding capabilities. However, this divide-and-conquer paradigm significantly increases the number of visual tokens, resulting in substantial computational and memory overhead. To better understand and address this challenge, we empirically investigate visual token utilization in HR-LVLMs and uncover three key findings: (1) the local tiles have varying importance, jointly determined by visual saliency and task relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage attention pattern across layers, with each stage attending to different types of visual tokens; (3) the visual tokens emphasized at different stages encode information at varying levels of granularity, playing complementary roles within LVLMs. Building on these insights, we propose HERO, a High-resolution visual token early dropping framework that integrates content-adaptive token budget allocation with function-aware token selection. By accurately estimating tile-level importance and selectively retaining visual tokens with complementary roles, HERO achieves superior efficiency-accuracy trade-offs across diverse benchmarks and model scales, all in a training-free manner. This study provides both empirical insights and practical solutions toward efficient inference in HR-LVLMs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2509.13070</link>
<guid>https://arxiv.org/abs/2509.13070</guid>
<content:encoded><![CDATA[
arXiv:2509.13070v1 Announce Type: new 
Abstract: Referring Image Segmentation (RIS) is a task that segments image regions based on language expressions, requiring fine-grained alignment between two modalities. However, existing methods often struggle with multimodal misalignment and language semantic loss, especially in complex scenes containing multiple visually similar objects, where uniquely described targets are frequently mislocalized or incompletely segmented. To tackle these challenges, this paper proposes TFANet, a Three-stage Image-Text Feature Alignment Network that systematically enhances multimodal alignment through a hierarchical framework comprising three stages: Knowledge Plus Stage (KPS), Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the first stage, we design the Multiscale Linear Cross-Attention Module (MLAM), which facilitates bidirectional semantic exchange between visual features and textual representations across multiple scales. This establishes rich and efficient alignment between image regions and different granularities of linguistic descriptions. Subsequently, the KFS further strengthens feature alignment through the Cross-modal Feature Scanning Module (CFSM), which applies multimodal selective scanning to capture long-range dependencies and construct a unified multimodal representation. This is essential for modeling long-range cross-modal dependencies and enhancing alignment accuracy in complex scenes. Finally, in the KIS, we propose the Word-level Linguistic Feature-guided Semantic Deepening Module (WFDM) to compensate for semantic degradation introduced in earlier stages.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2509.13083</link>
<guid>https://arxiv.org/abs/2509.13083</guid>
<content:encoded><![CDATA[
arXiv:2509.13083v1 Announce Type: new 
Abstract: In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2509.13084</link>
<guid>https://arxiv.org/abs/2509.13084</guid>
<content:encoded><![CDATA[
arXiv:2509.13084v1 Announce Type: new 
Abstract: Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control</title>
<link>https://arxiv.org/abs/2509.13089</link>
<guid>https://arxiv.org/abs/2509.13089</guid>
<content:encoded><![CDATA[
arXiv:2509.13089v1 Announce Type: new 
Abstract: Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge</title>
<link>https://arxiv.org/abs/2509.13107</link>
<guid>https://arxiv.org/abs/2509.13107</guid>
<content:encoded><![CDATA[
arXiv:2509.13107v1 Announce Type: new 
Abstract: The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.13116</link>
<guid>https://arxiv.org/abs/2509.13116</guid>
<content:encoded><![CDATA[
arXiv:2509.13116v1 Announce Type: new 
Abstract: Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</title>
<link>https://arxiv.org/abs/2509.13133</link>
<guid>https://arxiv.org/abs/2509.13133</guid>
<content:encoded><![CDATA[
arXiv:2509.13133v1 Announce Type: new 
Abstract: As automatic parking systems evolve, the accurate detection of parking slots has become increasingly critical. This study focuses on parking slot detection using surround-view cameras, which offer a comprehensive bird's-eye view of the parking environment. However, the current datasets are limited in scale, and the scenes they contain are seldom disrupted by real-world noise (e.g., light, occlusion, etc.). Moreover, manual data annotation is prone to errors and omissions due to the complexity of real-world conditions, significantly increasing the cost of annotating large-scale datasets. To address these issues, we first construct a large-scale parking slot detection dataset (named CRPS-D), which includes various lighting distributions, diverse weather conditions, and challenging parking slot variants. Compared with existing datasets, the proposed dataset boasts the largest data scale and consists of a higher density of parking slots, particularly featuring more slanted parking slots. Additionally, we develop a semi-supervised baseline for parking slot detection, termed SS-PSD, to further improve performance by exploiting unlabeled data. To our knowledge, this is the first semi-supervised approach in parking slot detection, which is built on the teacher-student model with confidence-guided mask consistency and adaptive feature perturbation. Experimental results demonstrate the superiority of SS-PSD over the existing state-of-the-art (SoTA) solutions on both the proposed dataset and the existing dataset. Particularly, the more unlabeled data there is, the more significant the gains brought by our semi-supervised scheme. The relevant source codes and the dataset have been made publicly available at https://github.com/zzh362/CRPS-D.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</title>
<link>https://arxiv.org/abs/2509.13149</link>
<guid>https://arxiv.org/abs/2509.13149</guid>
<content:encoded><![CDATA[
arXiv:2509.13149v1 Announce Type: new 
Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images</title>
<link>https://arxiv.org/abs/2509.13151</link>
<guid>https://arxiv.org/abs/2509.13151</guid>
<content:encoded><![CDATA[
arXiv:2509.13151v1 Announce Type: new 
Abstract: Recognizing textual attributes such as bold, italic, underline and strikeout is essential for understanding text semantics, structure, and visual presentation. These attributes highlight key information, making them crucial for document analysis. Existing methods struggle with computational efficiency or adaptability in noisy, multilingual settings. To address this, we introduce TexTAR, a multi-task, context-aware Transformer for Textual Attribute Recognition (TAR). Our novel data selection pipeline enhances context awareness, and our architecture employs a 2D RoPE (Rotary Positional Embedding)-style mechanism to incorporate input context for more accurate attribute predictions. We also introduce MMTAD, a diverse, multilingual, multi-domain dataset annotated with text attributes across real-world documents such as legal records, notices, and textbooks. Extensive evaluations show TexTAR outperforms existing methods, demonstrating that contextual awareness contributes to state-of-the-art TAR performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)</title>
<link>https://arxiv.org/abs/2509.13161</link>
<guid>https://arxiv.org/abs/2509.13161</guid>
<content:encoded><![CDATA[
arXiv:2509.13161v1 Announce Type: new 
Abstract: Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</title>
<link>https://arxiv.org/abs/2509.13172</link>
<guid>https://arxiv.org/abs/2509.13172</guid>
<content:encoded><![CDATA[
arXiv:2509.13172v1 Announce Type: new 
Abstract: Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: https://github.com/WHU-USI3DV/WHU-STree.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era</title>
<link>https://arxiv.org/abs/2509.13175</link>
<guid>https://arxiv.org/abs/2509.13175</guid>
<content:encoded><![CDATA[
arXiv:2509.13175v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (>96\% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale "silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this "silver-standard" dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at https://github.com/SadVoxel/More-performant-and-scalable.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Road Obstacle Video Segmentation</title>
<link>https://arxiv.org/abs/2509.13181</link>
<guid>https://arxiv.org/abs/2509.13181</guid>
<content:encoded><![CDATA[
arXiv:2509.13181v1 Announce Type: new 
Abstract: With the growing deployment of autonomous driving agents, the detection and segmentation of road obstacles have become critical to ensure safe autonomous navigation. However, existing road-obstacle segmentation methods are applied on individual frames, overlooking the temporal nature of the problem, leading to inconsistent prediction maps between consecutive frames. In this work, we demonstrate that the road-obstacle segmentation task is inherently temporal, since the segmentation maps for consecutive frames are strongly correlated. To address this, we curate and adapt four evaluation benchmarks for road-obstacle video segmentation and evaluate 11 state-of-the-art image- and video-based segmentation methods on these benchmarks. Moreover, we introduce two strong baseline methods based on vision foundation models. Our approach establishes a new state-of-the-art in road-obstacle video segmentation for long-range video sequences, providing valuable insights and direction for future research.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</title>
<link>https://arxiv.org/abs/2509.13210</link>
<guid>https://arxiv.org/abs/2509.13210</guid>
<content:encoded><![CDATA[
arXiv:2509.13210v1 Announce Type: new 
Abstract: Violence detection in public surveillance is critical for public safety. This study addresses challenges such as small-scale targets, complex environments, and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal framework that integrates an enhanced YOLOv8 with a Temporal Segment Network (TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as a lightweight backbone, an exponential moving average (EMA) attention mechanism, and pruning to reduce computational cost while maintaining accuracy. YOLOv8 and TSN are trained separately on pedestrian and violence datasets, where YOLOv8 extracts human regions and TSN performs binary classification of violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming existing methods in both accuracy and efficiency, demonstrating its effectiveness for public safety surveillance. Code is available at https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection</title>
<link>https://arxiv.org/abs/2509.13214</link>
<guid>https://arxiv.org/abs/2509.13214</guid>
<content:encoded><![CDATA[
arXiv:2509.13214v1 Announce Type: new 
Abstract: The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</title>
<link>https://arxiv.org/abs/2509.13229</link>
<guid>https://arxiv.org/abs/2509.13229</guid>
<content:encoded><![CDATA[
arXiv:2509.13229v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Vacuum Thermoforming Process</title>
<link>https://arxiv.org/abs/2509.13250</link>
<guid>https://arxiv.org/abs/2509.13250</guid>
<content:encoded><![CDATA[
arXiv:2509.13250v1 Announce Type: new 
Abstract: Ensuring consistent quality in vacuum thermoforming presents challenges due to variations in material properties and tooling configurations. This research introduces a vision-based quality control system to predict and optimise process parameters, thereby enhancing part quality with minimal data requirements. A comprehensive dataset was developed using visual data from vacuum-formed samples subjected to various process parameters, supplemented by image augmentation techniques to improve model training. A k-Nearest Neighbour algorithm was subsequently employed to identify adjustments needed in process parameters by mapping low-quality parts to their high-quality counterparts. The model exhibited strong performance in adjusting heating power, heating time, and vacuum time to reduce defects and improve production efficiency.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResidualViT for Efficient Temporally Dense Video Encoding</title>
<link>https://arxiv.org/abs/2509.13255</link>
<guid>https://arxiv.org/abs/2509.13255</guid>
<content:encoded><![CDATA[
arXiv:2509.13255v1 Announce Type: new 
Abstract: Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadGame: An AI-Powered Platform for Radiology Education</title>
<link>https://arxiv.org/abs/2509.13270</link>
<guid>https://arxiv.org/abs/2509.13270</guid>
<content:encoded><![CDATA[
arXiv:2509.13270v1 Announce Type: new 
Abstract: We introduce RadGame, an AI-powered gamified platform for radiology education that targets two core skills: localizing findings and generating reports. Traditional radiology training is based on passive exposure to cases or active practice with real-time input from supervising radiologists, limiting opportunities for immediate and scalable feedback. RadGame addresses this gap by combining gamification with large-scale public datasets and automated, AI-driven feedback that provides clear, structured guidance to human learners. In RadGame Localize, players draw bounding boxes around abnormalities, which are automatically compared to radiologist-drawn annotations from public datasets, and visual explanations are generated by vision-language models for user missed findings. In RadGame Report, players compose findings given a chest X-ray, patient age and indication, and receive structured AI feedback based on radiology report generation metrics, highlighting errors and omissions compared to a radiologist's written ground truth report from public datasets, producing a final performance and style score. In a prospective evaluation, participants using RadGame achieved a 68% improvement in localization accuracy compared to 17% with traditional passive methods and a 31% improvement in report-writing accuracy compared to 4% with traditional methods after seeing the same cases. RadGame highlights the potential of AI-driven gamification to deliver scalable, feedback-rich radiology training and reimagines the application of medical AI resources in education.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Realness Assessment and Localization with Multimodal Features</title>
<link>https://arxiv.org/abs/2509.13289</link>
<guid>https://arxiv.org/abs/2509.13289</guid>
<content:encoded><![CDATA[
arXiv:2509.13289v1 Announce Type: new 
Abstract: A reliable method of quantifying the perceptual realness of AI-generated images and identifying visually inconsistent regions is crucial for practical use of AI-generated images and for improving photorealism of generative AI via realness feedback during training. This paper introduces a framework that accomplishes both overall objective realness assessment and local inconsistency identification of AI-generated images using textual descriptions of visual inconsistencies generated by vision-language models trained on large datasets that serve as reliable substitutes for human annotations. Our results demonstrate that the proposed multimodal approach improves objective realness prediction performance and produces dense realness maps that effectively distinguish between realistic and unrealistic spatial regions.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance</title>
<link>https://arxiv.org/abs/2509.13301</link>
<guid>https://arxiv.org/abs/2509.13301</guid>
<content:encoded><![CDATA[
arXiv:2509.13301v1 Announce Type: new 
Abstract: Creating 3D assets that follow the texture and geometry style of existing ones is often desirable or even inevitable in practical applications like video gaming and virtual reality. While impressive progress has been made in generating 3D objects from text or images, creating style-controllable 3D assets remains a complex and challenging problem. In this work, we propose StyleSculptor, a novel training-free approach for generating style-guided 3D assets from a content image and one or more style images. Unlike previous works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner, enabling fine-grained 3D style control that captures the texture, geometry, or both styles of user-provided style images. At the core of StyleSculptor is a novel Style Disentangled Attention (SD-Attn) module, which establishes a dynamic interaction between the input content image and style image for style-guided 3D asset generation via a cross-3D attention mechanism, enabling stable feature fusion and effective style-guided generation. To alleviate semantic content leakage, we also introduce a style-disentangled feature selection strategy within the SD-Attn module, which leverages the variance of 3D feature patches to disentangle style- and content-significant channels, allowing selective feature injection within the attention framework. With SD-Attn, the network can dynamically compute texture-, geometry-, or both-guided features to steer the 3D generation process. Built upon this, we further propose the Style Guided Control (SGC) mechanism, which enables exclusive geometry- or texture-only stylization, as well as adjustable style intensity control. Extensive experiments demonstrate that StyleSculptor outperforms existing baseline methods in producing high-fidelity 3D assets.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Aware Region Prompted Vision Language Model</title>
<link>https://arxiv.org/abs/2509.13317</link>
<guid>https://arxiv.org/abs/2509.13317</guid>
<content:encoded><![CDATA[
arXiv:2509.13317v1 Announce Type: new 
Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction</title>
<link>https://arxiv.org/abs/2509.12234</link>
<guid>https://arxiv.org/abs/2509.12234</guid>
<content:encoded><![CDATA[
arXiv:2509.12234v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high inter-patient variance in rate of cognitive decline. AD progression prediction aims to forecast patient cognitive decline and benefits from incorporating multiple neuroimaging modalities. However, existing multimodal models fail to make accurate predictions when many modalities are missing during inference, as is often the case in clinical settings. To increase multimodal model flexibility under high modality missingness, we introduce PerM-MoE, a novel sparse mixture-of-experts method that uses independent routers for each modality in place of the conventional, single router. Using T1-weighted MRI, FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art Flex-MoE, and unimodal neuroimaging models on predicting two-year change in Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of modality missingness. PerM-MoE outperforms the state of the art in most variations of modality missingness and demonstrates more effective utility of experts than Flex-MoE.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction</title>
<link>https://arxiv.org/abs/2509.12237</link>
<guid>https://arxiv.org/abs/2509.12237</guid>
<content:encoded><![CDATA[
arXiv:2509.12237v1 Announce Type: cross 
Abstract: Accurate prediction of machining deformation in structural components is essential for ensuring dimensional precision and reliability. Such deformation often originates from residual stress fields, whose distribution and influence vary significantly with geometric complexity. Conventional numerical methods for modeling the coupling between residual stresses and deformation are computationally expensive, particularly when diverse geometries are considered. Neural operators have recently emerged as a powerful paradigm for efficiently solving partial differential equations, offering notable advantages in accelerating residual stress-deformation analysis. However, their direct application across changing geometric domains faces theoretical and practical limitations. To address this challenge, a novel framework based on diffeomorphic embedding neural operators named neural diffeomorphic-neural operator (NDNO) is introduced. Complex three-dimensional geometries are explicitly mapped to a common reference domain through a diffeomorphic neural network constrained by smoothness and invertibility. The neural operator is then trained on this reference domain, enabling efficient learning of deformation fields induced by residual stresses. Once trained, both the diffeomorphic neural network and the neural operator demonstrate efficient prediction capabilities, allowing rapid adaptation to varying geometries. The proposed method thus provides an effective and computationally efficient solution for deformation prediction in structural components subject to varying geometries. The proposed method is validated to predict both main-direction and multi-direction deformation fields, achieving high accuracy and efficiency across parts with diverse geometries including component types, dimensions and features.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2509.12239</link>
<guid>https://arxiv.org/abs/2509.12239</guid>
<content:encoded><![CDATA[
arXiv:2509.12239v1 Announce Type: cross 
Abstract: This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifies trajectory properties, including displacement, velocity, clustering, and drift field dynamics, using statistical metrics such as Wasserstein distance and cosine similarity. By enhancing model transparency, InJecteD supports human AI collaboration by enabling practitioners to debug and refine generative models. Experiments reveal distinct denoising phases: initial noise exploration, rapid shape formation, and final refinement, with dataset-specific behaviors example, bullseyes concentric convergence vs. dinos complex contour formation. We evaluate four model configurations, varying embeddings and noise schedules, demonstrating that Fourier based embeddings improve trajectory stability and reconstruction quality
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams</title>
<link>https://arxiv.org/abs/2509.12251</link>
<guid>https://arxiv.org/abs/2509.12251</guid>
<content:encoded><![CDATA[
arXiv:2509.12251v1 Announce Type: cross 
Abstract: This paper develops an autonomous agentic framework called V-Math that aims to assist Vietnamese high school students in preparing for the National High School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates three specialized AI agents: a specification-matrix-conditioned question generator, a solver/explainer for detailed step-by-step reasoning, and a personalized tutor that adapts to student performance. Beyond enabling self-paced student practice, V-Math supports teachers by generating innovative, compliant exam questions and building diverse, high-quality question banks. This reduces manual workload and enriches instructional resources. We describe the system architecture, focusing on practice modes for learners and teacher-oriented features for question generation. Preliminary evaluations demonstrate that V-Math produces matrix-aligned exams with high solution accuracy, delivers coherent explanations, and enhances the variety of practice materials. These results highlight its potential to support scalable, equitable mathematics preparation aligned with national standards while also empowering teachers through AI-assisted exam creation.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT</title>
<link>https://arxiv.org/abs/2509.12274</link>
<guid>https://arxiv.org/abs/2509.12274</guid>
<content:encoded><![CDATA[
arXiv:2509.12274v1 Announce Type: cross 
Abstract: Controlling environmental conditions and monitoring plant status in greenhouses is critical to promptly making appropriate management decisions aimed at promoting crop production. The primary objective of this research study was to develop and test a smart aeroponic greenhouse on an experimental scale where the status of Geranium plant and environmental conditions are continuously monitored through the integration of the internet of things (IoT) and artificial intelligence (AI). An IoT-based platform was developed to control the environmental conditions of plants more efficiently and provide insights to users to make informed management decisions. In addition, we developed an AI-based disease detection framework using VGG-19, InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured periodically after an intentional inoculation. The performance of the AI framework was compared with an expert's evaluation of disease status. Preliminary results showed that the IoT system implemented in the greenhouse environment is able to publish data such as temperature, humidity, water flow, and volume of charge tanks online continuously to users and adjust the controlled parameters to provide an optimal growth environment for the plants. Furthermore, the results of the AI framework demonstrate that the VGG-19 algorithm was able to identify drought stress and rust leaves from healthy leaves with the highest accuracy, 92% among the other algorithms.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Radiographic Disease Detection with MetaCheX, a Context-Aware Multimodal Model</title>
<link>https://arxiv.org/abs/2509.12287</link>
<guid>https://arxiv.org/abs/2509.12287</guid>
<content:encoded><![CDATA[
arXiv:2509.12287v1 Announce Type: cross 
Abstract: Existing deep learning models for chest radiology often neglect patient metadata, limiting diagnostic accuracy and fairness. To bridge this gap, we introduce MetaCheX, a novel multimodal framework that integrates chest X-ray images with structured patient metadata to replicate clinical decision-making. Our approach combines a convolutional neural network (CNN) backbone with metadata processed by a multilayer perceptron through a shared classifier. Evaluated on the CheXpert Plus dataset, MetaCheX consistently outperformed radiograph-only baseline models across multiple CNN architectures. By integrating metadata, the overall diagnostic accuracy was significantly improved, measured by an increase in AUROC. The results of this study demonstrate that metadata reduces algorithmic bias and enhances model generalizability across diverse patient populations. MetaCheX advances clinical artificial intelligence toward robust, context-aware radiographic disease detection.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Gr\"obner Bases of (Universal) Multiview Ideals</title>
<link>https://arxiv.org/abs/2509.12376</link>
<guid>https://arxiv.org/abs/2509.12376</guid>
<content:encoded><![CDATA[
arXiv:2509.12376v1 Announce Type: cross 
Abstract: Multiview ideals arise from the geometry of image formation in pinhole cameras, and universal multiview ideals are their analogs for unknown cameras. We prove that a natural collection of polynomials form a universal Gr\"obner basis for both types of ideals using a criterion introduced by Huang and Larson, and include a proof of their criterion in our setting. Symmetry reduction and induction enable the method to be deployed on an infinite family of ideals. We also give an explicit description of the matroids on which the methodology depends, in the context of multiview ideals.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2509.12458</link>
<guid>https://arxiv.org/abs/2509.12458</guid>
<content:encoded><![CDATA[
arXiv:2509.12458v1 Announce Type: cross 
Abstract: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams. Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control. A near-real-time (near-RT) process uses Structure from Motion (SfM) to generate an instantaneous pointcloud of the object. The system analyzes the model quality on the fly and dynamically adapts the UAV's trajectory to intelligently capture new images of poorly covered areas. This ensures comprehensive data acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR) approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB) location data to achieve superior accuracy. We implemented and validated this architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both single- and multi-UAV configurations, conclusively show that dynamic trajectory adaptation consistently improves reconstruction quality over static flight paths. This work demonstrates a scalable and autonomous solution that unlocks the potential of miniaturized UAVs for fine-grained 3D reconstruction in constrained environments, a capability previously limited to much larger platforms.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification</title>
<link>https://arxiv.org/abs/2509.12512</link>
<guid>https://arxiv.org/abs/2509.12512</guid>
<content:encoded><![CDATA[
arXiv:2509.12512v1 Announce Type: cross 
Abstract: Anomaly detection and classification in medical imaging are critical for early diagnosis but remain challenging due to limited annotated data, class imbalance, and the high cost of expert labeling. Emerging vision foundation models such as DINOv2, pretrained on extensive, unlabeled datasets, offer generalized representations that can potentially alleviate these limitations. In this study, we propose an attention-based global aggregation framework tailored specifically for 3D medical image anomaly classification. Leveraging the self-supervised DINOv2 model as a pretrained feature extractor, our method processes individual 2D axial slices of brain MRIs, assigning adaptive slice-level importance weights through a soft attention mechanism. To further address data scarcity, we employ a composite loss function combining supervised contrastive learning with class-variance regularization, enhancing inter-class separability and intra-class consistency. We validate our framework on the ADNI dataset and an institutional multi-class headache cohort, demonstrating strong anomaly classification performance despite limited data availability and significant class imbalance. Our results highlight the efficacy of utilizing pretrained 2D foundation models combined with attention-based slice aggregation for robust volumetric anomaly detection in medical imaging. Our implementation is publicly available at https://github.com/Rafsani/DinoAtten3D.git.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepEyeNet: Generating Medical Report for Retinal Images</title>
<link>https://arxiv.org/abs/2509.12534</link>
<guid>https://arxiv.org/abs/2509.12534</guid>
<content:encoded><![CDATA[
arXiv:2509.12534v1 Announce Type: cross 
Abstract: The increasing prevalence of retinal diseases poses a significant challenge to the healthcare system, as the demand for ophthalmologists surpasses the available workforce. This imbalance creates a bottleneck in diagnosis and treatment, potentially delaying critical care. Traditional methods of generating medical reports from retinal images rely on manual interpretation, which is time-consuming and prone to errors, further straining ophthalmologists' limited resources. This thesis investigates the potential of Artificial Intelligence (AI) to automate medical report generation for retinal images. AI can quickly analyze large volumes of image data, identifying subtle patterns essential for accurate diagnosis. By automating this process, AI systems can greatly enhance the efficiency of retinal disease diagnosis, reducing doctors' workloads and enabling them to focus on more complex cases. The proposed AI-based methods address key challenges in automated report generation: (1) A multi-modal deep learning approach captures interactions between textual keywords and retinal images, resulting in more comprehensive medical reports; (2) Improved methods for medical keyword representation enhance the system's ability to capture nuances in medical terminology; (3) Strategies to overcome RNN-based models' limitations, particularly in capturing long-range dependencies within medical descriptions; (4) Techniques to enhance the interpretability of the AI-based report generation system, fostering trust and acceptance in clinical practice. These methods are rigorously evaluated using various metrics and achieve state-of-the-art performance. This thesis demonstrates AI's potential to revolutionize retinal disease diagnosis by automating medical report generation, ultimately improving clinical efficiency, diagnostic accuracy, and patient care.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human + AI for Accelerating Ad Localization Evaluation</title>
<link>https://arxiv.org/abs/2509.12543</link>
<guid>https://arxiv.org/abs/2509.12543</guid>
<content:encoded><![CDATA[
arXiv:2509.12543v1 Announce Type: cross 
Abstract: Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining</title>
<link>https://arxiv.org/abs/2509.12553</link>
<guid>https://arxiv.org/abs/2509.12553</guid>
<content:encoded><![CDATA[
arXiv:2509.12553v1 Announce Type: cross 
Abstract: Logit Knowledge Distillation has gained substantial research interest in recent years due to its simplicity and lack of requirement for intermediate feature alignment; however, it suffers from limited interpretability in its decision-making process. To address this, we propose implicit Clustering Distillation (iCD): a simple and effective method that mines and transfers interpretable structural knowledge from logits, without requiring ground-truth labels or feature-space alignment. iCD leverages Gram matrices over decoupled local logit representations to enable student models to learn latent semantic structural patterns. Extensive experiments on benchmark datasets demonstrate the effectiveness of iCD across diverse teacher-student architectures, with particularly strong performance in fine-grained classification tasks -- achieving a peak improvement of +5.08% over the baseline. The code is available at: https://github.com/maomaochongaa/iCD.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning</title>
<link>https://arxiv.org/abs/2509.12594</link>
<guid>https://arxiv.org/abs/2509.12594</guid>
<content:encoded><![CDATA[
arXiv:2509.12594v1 Announce Type: cross 
Abstract: We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2509.12618</link>
<guid>https://arxiv.org/abs/2509.12618</guid>
<content:encoded><![CDATA[
arXiv:2509.12618v1 Announce Type: cross 
Abstract: The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors</title>
<link>https://arxiv.org/abs/2509.12728</link>
<guid>https://arxiv.org/abs/2509.12728</guid>
<content:encoded><![CDATA[
arXiv:2509.12728v1 Announce Type: cross 
Abstract: Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos</title>
<link>https://arxiv.org/abs/2509.12772</link>
<guid>https://arxiv.org/abs/2509.12772</guid>
<content:encoded><![CDATA[
arXiv:2509.12772v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification (UQ) is essential in medical AI. Evidential Deep Learning (EDL) offers a computationally efficient way to quantify model uncertainty alongside predictions, unlike traditional methods such as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these methods often rely on a single expert's annotations as ground truth for model training, overlooking the inter-rater variability in healthcare. To address this issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates uncertainty estimates and predictions from multiple AI experts via EDL models trained with diverse ground truths and modeling strategies. MEGAN's gating network optimally combines predictions and uncertainties from each EDL model, enhancing overall prediction confidence and calibration. We extensively benchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease severity estimation, assessed by visual labeling of Mayo Endoscopic Subscore (MES), where inter-rater variability is prevalent. In large-scale prospective UC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5% reduction in Expected Calibration Error (ECE) compared to existing methods. Furthermore, MEGAN facilitated uncertainty-guided sample stratification, reducing the annotation burden and potentially increasing efficiency and consistency in UC trials.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gesture Evaluation in Virtual Reality</title>
<link>https://arxiv.org/abs/2509.12816</link>
<guid>https://arxiv.org/abs/2509.12816</guid>
<content:encoded><![CDATA[
arXiv:2509.12816v1 Announce Type: cross 
Abstract: Gestures are central to human communication, enriching interactions through non-verbal expression. Virtual avatars increasingly use AI-generated gestures to enhance life-likeness, yet evaluations have largely been confined to 2D. Virtual Reality (VR) provides an immersive alternative that may affect how gestures are perceived. This paper presents a comparative evaluation of computer-generated gestures in VR and 2D, examining three models from the 2023 GENEA Challenge. Results show that gestures viewed in VR were rated slightly higher on average, with the strongest effect observed for motion-capture "true movement." While model rankings remained consistent across settings, VR influenced participants' overall perception and offered unique benefits over traditional 2D evaluation.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration</title>
<link>https://arxiv.org/abs/2509.12846</link>
<guid>https://arxiv.org/abs/2509.12846</guid>
<content:encoded><![CDATA[
arXiv:2509.12846v1 Announce Type: cross 
Abstract: Visual-inertial fusion is crucial for a large amount of intelligent and autonomous applications, such as robot navigation and augmented reality. To bootstrap and achieve optimal state estimation, the spatial-temporal displacements between IMU and cameras must be calibrated in advance. Most existing calibration methods adopt continuous-time state representation, more specifically the B-spline. Despite these methods achieve precise spatial-temporal calibration, they suffer from high computational cost caused by continuous-time state representation. To this end, we propose a novel and extremely efficient calibration method that unleashes the power of discrete-time state representation. Moreover, the weakness of discrete-time state representation in temporal calibration is tackled in this paper. With the increasing production of drones, cellphones and other visual-inertial platforms, if one million devices need calibration around the world, saving one minute for the calibration of each device means saving 2083 work days in total. To benefit both the research and industry communities, our code will be open-source.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use</title>
<link>https://arxiv.org/abs/2509.12867</link>
<guid>https://arxiv.org/abs/2509.12867</guid>
<content:encoded><![CDATA[
arXiv:2509.12867v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in language understanding and reasoning, yet they remain limited when tackling real-world tasks that require up-to-date knowledge, precise operations, or specialized tool use. To address this, we propose Tool-R1, a reinforcement learning framework that enables LLMs to perform general, compositional, and multi-step tool use by generating executable Python code. Tool-R1 supports integration of user-defined tools and standard libraries, with variable sharing across steps to construct coherent workflows. An outcome-based reward function, combining LLM-based answer judgment and code execution success, guides policy optimization. To improve training efficiency, we maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead of costly online sampling. Experiments on the GAIA benchmark show that Tool-R1 substantially improves both accuracy and robustness, achieving about 10\% gain over strong baselines, with larger improvements on complex multi-step tasks. These results highlight the potential of Tool-R1 for enabling reliable and efficient tool-augmented reasoning in real-world applications. Our code will be available at https://github.com/YBYBZhang/Tool-R1.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making</title>
<link>https://arxiv.org/abs/2509.12927</link>
<guid>https://arxiv.org/abs/2509.12927</guid>
<content:encoded><![CDATA[
arXiv:2509.12927v1 Announce Type: cross 
Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning (MARL) algorithms. While StarCraft II-related environments have driven significant advances in MARL, existing benchmarks like SMAC focus primarily on micromanagement, limiting comprehensive evaluation of high-level strategic intelligence. To address this, we introduce HLSMAC, a new cooperative MARL benchmark with 12 carefully designed StarCraft II scenarios based on classical stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a specific stratagem and is designed to challenge agents with diverse strategic elements, including tactical maneuvering, timing coordination, and deception, thereby opening up avenues for evaluating high-level strategic decision-making capabilities. We also propose novel metrics across multiple dimensions beyond conventional win rate, such as ability utilization and advancement efficiency, to assess agents' overall performance within the HLSMAC environment. We integrate state-of-the-art MARL algorithms and LLM-based agents with our benchmark and conduct comprehensive experiments. The results demonstrate that HLSMAC serves as a robust testbed for advancing multi-agent strategic decision-making.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sy-FAR: Symmetry-based Fair Adversarial Robustness</title>
<link>https://arxiv.org/abs/2509.12939</link>
<guid>https://arxiv.org/abs/2509.12939</guid>
<content:encoded><![CDATA[
arXiv:2509.12939v1 Announce Type: cross 
Abstract: Security-critical machine-learning (ML) systems, such as face-recognition systems, are susceptible to adversarial examples, including real-world physically realizable attacks. Various means to boost ML's adversarial robustness have been proposed; however, they typically induce unfair robustness: It is often easier to attack from certain classes or groups than from others. Several techniques have been developed to improve adversarial robustness while seeking perfect fairness between classes. Yet, prior work has focused on settings where security and fairness are less critical. Our insight is that achieving perfect parity in realistic fairness-critical tasks, such as face recognition, is often infeasible -- some classes may be highly similar, leading to more misclassifications between them. Instead, we suggest that seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable because class resemblance is a symmetric relation in most domains. Additionally, as we prove theoretically, symmetry between individuals induces symmetry between any set of sub-groups, in contrast to other fairness notions where group-fairness is often elusive. We develop Sy-FAR, a technique to encourage symmetry while also optimizing adversarial robustness and extensively evaluate it using five datasets, with three model architectures, including against targeted and untargeted realistic attacks. The results show Sy-FAR significantly improves fair adversarial robustness compared to state-of-the-art methods. Moreover, we find that Sy-FAR is faster and more consistent across runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover in this work -- target classes that adversarial examples are likely to be classified into become significantly less vulnerable after inducing symmetry.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2509.13234</link>
<guid>https://arxiv.org/abs/2509.13234</guid>
<content:encoded><![CDATA[
arXiv:2509.13234v1 Announce Type: cross 
Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI systems can expand access to fundus photography screening. Current FDA-cleared systems primarily provide binary referral outputs, where this minimal output may limit clinical trust and utility. Yet, determining the most effective output format to enhance clinician-AI performance is an empirical challenge that is difficult to assess at scale. We evaluated multimodal large language models (MLLMs) for DR detection and their ability to simulate clinical AI assistance across different output types. Two models were tested on IDRiD and Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source medical model. Experiments included: (1) baseline evaluation, (2) simulated AI assistance with synthetic predictions, and (3) actual AI-to-AI collaboration where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at baseline, achieving higher sensitivity and AUROC, while GPT-4o showed near-perfect specificity but low sensitivity. Both models adjusted predictions based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o achieved strong results when guided by MedGemma's descriptive outputs, even without direct image access (AUROC up to 0.96). These findings suggest MLLMs may improve DR screening pipelines and serve as scalable simulators for studying clinical AI assistance across varying output configurations. Open, lightweight models such as MedGemma may be especially valuable in low-resource settings, while descriptive outputs could enhance explainability and clinician trust in clinical workflows.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement</title>
<link>https://arxiv.org/abs/2509.13282</link>
<guid>https://arxiv.org/abs/2509.13282</guid>
<content:encoded><![CDATA[
arXiv:2509.13282v1 Announce Type: cross 
Abstract: Charts are a crucial visual medium for communicating and representing information. While Large Vision-Language Models (LVLMs) have made progress on chart question answering (CQA), the task remains challenging, particularly when models attend to irrelevant regions of the chart. In this work, we present ChartGaze, a new eye-tracking dataset that captures human gaze patterns during chart reasoning tasks. Through a systematic comparison of human and model attention, we find that LVLMs often diverge from human gaze, leading to reduced interpretability and accuracy. To address this, we propose a gaze-guided attention refinement that aligns image-text attention with human fixations. Our approach improves both answer accuracy and attention alignment, yielding gains of up to 2.56 percentage points across multiple models. These results demonstrate the promise of incorporating human gaze to enhance both the reasoning quality and interpretability of chart-focused LVLMs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QDFlow: A Python package for physics simulations of quantum dot devices</title>
<link>https://arxiv.org/abs/2509.13298</link>
<guid>https://arxiv.org/abs/2509.13298</guid>
<content:encoded><![CDATA[
arXiv:2509.13298v1 Announce Type: cross 
Abstract: Recent advances in machine learning (ML) have accelerated progress in calibrating and operating quantum dot (QD) devices. However, most ML approaches rely on access to large, high-quality labeled datasets for training, benchmarking, and validation, with labels capturing key features in the data. Obtaining such datasets experimentally is challenging due to limited data availability and the labor-intensive nature of labeling. QDFlow is an open-source physics simulator for multi-QD arrays that generates realistic synthetic data with ground-truth labels. QDFlow combines a self-consistent Thomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to produce charge stability diagrams and ray-based data closely resembling experiments. With extensive tunable parameters and customizable noise models, QDFlow supports the creation of large, diverse datasets for ML development, benchmarking, and quantum device research.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2212.08328</link>
<guid>https://arxiv.org/abs/2212.08328</guid>
<content:encoded><![CDATA[
arXiv:2212.08328v3 Announce Type: replace 
Abstract: Hinged on the representation power of neural networks, neural radiance fields (NeRF) have recently emerged as one of the promising and widely applicable methods for 3D object and scene representation. However, NeRF faces challenges in practical applications, such as large-scale scenes and edge devices with a limited amount of memory, where data needs to be processed sequentially. Under such incremental learning scenarios, neural networks are known to suffer catastrophic forgetting: easily forgetting previously seen data after training with new data. We observe that previous incremental learning algorithms are limited by either low performance or memory scalability issues. As such, we develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF). MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve as a memory that provides the pixel RGB values, given rays as queries. Upon the motivation, our framework learns which rays to query NeRF to extract previous pixel values. The extracted pixel values are then used to train NeRF in a self-distillation manner to prevent catastrophic forgetting. As a result, MEIL-NeRF demonstrates constant memory consumption and competitive performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport Based Unsupervised Restoration Learning Exploiting Degradation Sparsity</title>
<link>https://arxiv.org/abs/2305.00273</link>
<guid>https://arxiv.org/abs/2305.00273</guid>
<content:encoded><![CDATA[
arXiv:2305.00273v2 Announce Type: replace 
Abstract: Optimal transport (OT) has recently been shown as a promising criterion for unsupervised restoration when no explicit prior model is available. Despite its theoretical appeal, OT still significantly falls short of supervised methods on challenging tasks such as super-resolution, deraining, and dehazing. In this paper, we propose a \emph{sparsity-aware optimal transport} (SOT) framework to bridge this gap by leveraging a key observation: the degradations in these tasks exhibit distinct sparsity in the frequency domain. Incorporating this sparsity prior into OT can significantly reduce the ambiguity of the inverse mapping for restoration and substantially boost performance. We provide analysis to show exploiting degradation sparsity benefits unsupervised restoration learning. Extensive experiments on real-world super-resolution, deraining, and dehazing demonstrate that SOT offers notable performance gains over standard OT, while achieving superior perceptual quality compared to existing supervised and unsupervised methods. In particular, SOT consistently outperforms existing unsupervised methods across all three tasks and narrows the performance gap to supervised counterparts.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Synthetic Face Images: Accuracy, Robustness, Generalization</title>
<link>https://arxiv.org/abs/2406.17547</link>
<guid>https://arxiv.org/abs/2406.17547</guid>
<content:encoded><![CDATA[
arXiv:2406.17547v2 Announce Type: replace 
Abstract: An experimental study on detecting synthetic face images is presented. We collected a dataset, called FF5, of five fake face image generators, including recent diffusion models. We find that a simple model trained on a specific image generator can achieve near-perfect accuracy in separating synthetic and real images. The model handles common image distortions (reduced resolution, compression) by using data augmentation. Moreover, partial manipulations, where synthetic images are blended into real ones by inpainting, are identified and the area of the manipulation is localized by a simple model of YOLO architecture. However, the model turned out to be vulnerable to adversarial attacks and does not generalize to unseen generators. Failure to generalize to detect images produced by a newer generator also occurs for recent state-of-the-art methods, which we tested on Realistic Vision, a fine-tuned version of StabilityAI's Stable Diffusion image generator.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning</title>
<link>https://arxiv.org/abs/2409.13366</link>
<guid>https://arxiv.org/abs/2409.13366</guid>
<content:encoded><![CDATA[
arXiv:2409.13366v4 Announce Type: replace 
Abstract: Aerial Remote Sensing (ARS) vision tasks present significant challenges due to the unique viewing angle characteristics. Existing research has primarily focused on algorithms for specific tasks, which have limited applicability in a broad range of ARS vision applications. This paper proposes RingMo-Aerial, aiming to fill the gap in foundation model research in the field of ARS vision. A Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism is introduced to strengthen the model's capacity for small-object representation. Complementarily, an affine transformation-based contrastive learning method improves its adaptability to the tilted viewing angles inherent in ARS tasks. Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is proposed to improve the model's adaptability and performance in various ARS vision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This indicates the practicality and efficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design</title>
<link>https://arxiv.org/abs/2410.05677</link>
<guid>https://arxiv.org/abs/2410.05677</guid>
<content:encoded><![CDATA[
arXiv:2410.05677v3 Announce Type: replace 
Abstract: In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Prompt Distillation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2411.15244</link>
<guid>https://arxiv.org/abs/2411.15244</guid>
<content:encoded><![CDATA[
arXiv:2411.15244v3 Announce Type: replace 
Abstract: Large pre-trained Vision-Language Models (VLMs) such as Contrastive Language-Image Pre-training (CLIP) have been shown to be susceptible to adversarial attacks, raising concerns about their deployment in safety-critical applications like autonomous driving and medical diagnosis. One promising approach for robustifying pre-trained VLMs is Adversarial Prompt Tuning (APT), which applies adversarial training during the process of prompt tuning. However, existing APT methods are mostly single-modal methods that design prompt(s) for only the visual or textual modality, limiting their effectiveness in either robustness or clean accuracy. In this work, we propose Adversarial Prompt Distillation (APD), a bimodal knowledge distillation framework that enhances APT by integrating it with multi-modal knowledge transfer. APD optimizes prompts for both visual and textual modalities while distilling knowledge from a clean pre-trained teacher CLIP model. Extensive experiments on multiple benchmark datasets demonstrate the superiority of our APD method over the current state-of-the-art APT methods in terms of both adversarial robustness and clean accuracy. The effectiveness of APD also validates the possibility of using a non-robust teacher to improve the generalization and robustness of fine-tuned VLMs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation</title>
<link>https://arxiv.org/abs/2411.19331</link>
<guid>https://arxiv.org/abs/2411.19331</guid>
<content:encoded><![CDATA[
arXiv:2411.19331v3 Announce Type: replace 
Abstract: Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form textual concepts without predefined training classes. While existing vision-language models such as CLIP can generate segmentation masks by leveraging coarse spatial information from Vision Transformers, they face challenges in spatial localization due to their global alignment of image and text features. Conversely, self-supervised visual models like DINO excel in fine-grained visual encoding but lack integration with language. To bridge this gap, we present Talk2DINO, a novel hybrid approach that combines the spatial accuracy of DINOv2 with the language understanding of CLIP. Our approach aligns the textual embeddings of CLIP to the patch-level features of DINOv2 through a learned mapping function without the need to fine-tune the underlying backbones. At training time, we exploit the attention maps of DINOv2 to selectively align local visual patches with textual embeddings. We show that the powerful semantic and localization abilities of Talk2DINO can enhance the segmentation process, resulting in more natural and less noisy segmentations, and that our approach can also effectively distinguish foreground objects from the background. Experimental results demonstrate that Talk2DINO achieves state-of-the-art performance across several unsupervised OVS benchmarks. Source code and models are publicly available at: https://lorebianchi98.github.io/Talk2DINO/.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2412.07825</link>
<guid>https://arxiv.org/abs/2412.07825</guid>
<content:encoded><![CDATA[
arXiv:2412.07825v4 Announce Type: replace 
Abstract: 3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning abilities by balancing data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images from uncommon 6D viewpoints. Our 3DSRBench provide valuable findings and insights about future development of LMMs with strong spatial reasoning abilities. Our project page is available at https://3dsrbench.github.io/.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Free Adversarial Purification with Diffusion Models</title>
<link>https://arxiv.org/abs/2501.13336</link>
<guid>https://arxiv.org/abs/2501.13336</guid>
<content:encoded><![CDATA[
arXiv:2501.13336v2 Announce Type: replace 
Abstract: Adversarial training and adversarial purification are two widely used defense strategies for enhancing model robustness against adversarial attacks. However, adversarial training requires costly retraining, while adversarial purification often suffers from low efficiency. More critically, existing defenses are primarily designed under the perturbation-based adversarial threat model, which is ineffective against recently introduced unrestricted adversarial attacks. In this paper, we propose an effective and efficient defense framework that counters both perturbation-based and unrestricted adversarial attacks. Our approach is motivated by the observation that adversarial examples typically lie near the decision boundary and are highly sensitive to pixel-level perturbations. To address this, we introduce adversarial anti-aliasing, a preprocessing technique that mitigates adversarial noise by reducing the magnitude of pixel-level perturbations. In addition, we propose adversarial super-resolution, which leverages prior knowledge from clean datasets to benignly restore high-quality images from adversarially degraded ones. Unlike image synthesis methods that generate entirely new images, adversarial super-resolution focuses on image restoration, making it more suitable for purification. Importantly, both techniques require no additional training and are computationally efficient since they do not rely on gradient computations. To further improve robustness across diverse datasets, we introduce a contrastive learning-based adversarial deblurring fine-tuning method. By incorporating adversarial priors during fine-tuning on the target dataset, this method enhances purification effectiveness without the need to retrain diffusion models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Experts</title>
<link>https://arxiv.org/abs/2502.18530</link>
<guid>https://arxiv.org/abs/2502.18530</guid>
<content:encoded><![CDATA[
arXiv:2502.18530v3 Announce Type: replace 
Abstract: Large language model (LLM) agents have emerged as a promising solution to automate the workflow of machine learning, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves overall model performance. We also provide some theoretical edvience of the superior properties of this Iterative Refinement. Further, we implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</title>
<link>https://arxiv.org/abs/2503.00972</link>
<guid>https://arxiv.org/abs/2503.00972</guid>
<content:encoded><![CDATA[
arXiv:2503.00972v2 Announce Type: replace 
Abstract: Point cloud registration is important in computer-aided interventions (CAI). While learning-based point cloud registration methods have been developed, their clinical application is hampered by issues of generalizability and explainability. Therefore, classical point cloud registration methods, such as Iterative Closest Point (ICP), are still widely applied in CAI. ICP methods fail to consider that: (1) the points have well-defined semantic meaning, in that each point can be related to a specific anatomical label; (2) the deformation required for registration needs to follow biomechanical energy constraints. In this paper, we present a novel semantic ICP (SemICP) method that handles multiple point labels and uses linear elastic energy regularization. We use semantic labels to improve the robustness of the closest point matching and propose a novel point cloud deformation representation to apply explicit biomechanical energy regularization. Our experiments on a trans-oral robotic surgery ultrasound-computed tomography registration dataset and two public Learn2reg challenge datasets show that our method improves the Hausdorff distance and mean surface distance compared with other point-matching-based registration methods.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering</title>
<link>https://arxiv.org/abs/2503.14957</link>
<guid>https://arxiv.org/abs/2503.14957</guid>
<content:encoded><![CDATA[
arXiv:2503.14957v4 Announce Type: replace 
Abstract: We introduce PKR-QA (Procedural Knowledge Reasoning Question Answering), a new benchmark for question answering over procedural tasks that require structured reasoning. PKR-QA is constructed semi-automatically using a procedural knowledge graph (PKG), which encodes task-specific knowledge across diverse domains. The PKG is built by curating and linking information from the COIN instructional video dataset and the ontology, enriched with commonsense knowledge from ConceptNet and structured outputs from Large Language Models (LLMs), followed by manual verification. To generate question-answer pairs, we design graph traversal templates where each template is applied systematically over PKG. To enable interpretable reasoning, we propose a neurosymbolic approach called Knowledge Module Learning (KML), which learns procedural relations via neural modules and composes them for structured reasoning with LLMs. Experiments demonstrate that this paradigm improves reasoning performance on PKR-QA and enables step-by-step reasoning traces that facilitate interpretability. Code and dataset will be released soon https://github.com/LUNAProject22/KML.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierRelTriple: Guiding Indoor Layout Generation with Hierarchical Relationship Triplet Losses</title>
<link>https://arxiv.org/abs/2503.20289</link>
<guid>https://arxiv.org/abs/2503.20289</guid>
<content:encoded><![CDATA[
arXiv:2503.20289v2 Announce Type: replace 
Abstract: We present a hierarchical triplet-based indoor relationship learning method, coined HierRelTriple, with a focus on spatial relationship learning. Existing approaches often depend on manually defined spatial rules or simplified pairwise representations, which fail to capture complex, multi-object relationships found in real scenarios and lead to overcrowded or physically implausible arrangements. We introduce HierRelTriple, a hierarchical relational triplets modeling framework that first partitions functional regions and then automatically extracts three levels of spatial relationships: object-to-region (O2R), object-to-object (O2O), and corner-to-corner (C2C). By representing these relationships as geometric triplets and employing approaches based on Delaunay Triangulation to establish spatial priors, we derive IoU loss between denoised and ground truth triplets and integrate them seamlessly into the diffusion denoising process. The introduction of the joint formulation of inter-object distances, angular orientations, and spatial relationships enhances the physical realism of the generated scenes. Extensive experiments on unconditional layout synthesis, floorplan-conditioned layout generation, and scene rearrangement demonstrate that HierRelTriple improves spatial-relation metrics by over 15% and substantially reduces collisions and boundary violations compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis</title>
<link>https://arxiv.org/abs/2504.08272</link>
<guid>https://arxiv.org/abs/2504.08272</guid>
<content:encoded><![CDATA[
arXiv:2504.08272v2 Announce Type: replace 
Abstract: Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by adversaries for malicious activities. Despite this growing concern, research on methods to obscure or anonymize palmprints remains largely unexplored. Thus, it is essential to develop a palmprint de-identification technique capable of removing identity-revealing features while retaining the image's utility and preserving non-sensitive information. In this paper, we propose a training-free framework that utilizes pre-trained diffusion models to generate diverse, high-quality palmprint images that conceal identity features for de-identification purposes. To ensure greater stability and controllability in the synthesis process, we incorporate a semantic-guided embedding fusion alongside a prior interpolation mechanism. We further propose the de-identification ratio, a novel metric for intuitive de-identification assessment. Extensive experiments across multiple palmprint datasets and recognition methods demonstrate that our method effectively conceals identity-related traits with significant diversity across de-identified samples. The de-identified samples preserve high visual fidelity and maintain excellent usability, achieving a balance between de-identification and retaining non-identity information.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2504.19075</link>
<guid>https://arxiv.org/abs/2504.19075</guid>
<content:encoded><![CDATA[
arXiv:2504.19075v2 Announce Type: replace 
Abstract: Accurate diagnosis of Alzheimer's disease (AD) requires effectively integrating multimodal data and clinical expertise. However, existing methods often struggle to fully utilize multimodal information and lack structured mechanisms to incorporate dynamic domain knowledge. To address these limitations, we propose HoloDx, a knowledge- and data-driven framework that enhances AD diagnosis by aligning domain knowledge with multimodal clinical data. HoloDx incorporates a knowledge injection module with a knowledge-aware gated cross-attention, allowing the model to dynamically integrate domain-specific insights from both large language models (LLMs) and clinical expertise. Also, a memory injection module with a designed prototypical memory attention enables the model to retain and retrieve subject-specific information, ensuring consistency in decision-making. By jointly leveraging these mechanisms, HoloDx enhances interpretability, improves robustness, and effectively aligns prior knowledge with current subject data. Evaluations on five AD datasets demonstrate that HoloDx outperforms state-of-the-art methods, achieving superior diagnostic accuracy and strong generalization across diverse cohorts. The source code will be released upon publication acceptance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10634</link>
<guid>https://arxiv.org/abs/2505.10634</guid>
<content:encoded><![CDATA[
arXiv:2505.10634v5 Announce Type: replace 
Abstract: Over-reliance on language priors is a major cause of hallucinations in Large Vision-Language Models (LVLMs), often leading to outputs that are linguistically plausible but visually inconsistent. Recent studies have explored contrastive decoding as a training-free solution. However, these methods typically construct contrastive visual inputs by perturbing the original image, resulting in distorted contrastive distributions, incomplete contrastive signals, and excessive suppression of language priors. Motivated by the observation that language priors tend to remain consistent across different images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet effective training-free method that uses unrelated images as contrastive visual inputs. To address the issue of over-suppressing language priors, which can negatively affect the quality of generated responses, we further introduce a dynamic selection mechanism based on the cross-image differences in model behavior. By selectively suppressing language priors, our method reduces hallucinations without compromising the model's performance. Extensive experiments across multiple benchmarks and LVLMs confirm the effectiveness and generalizability of CICD, particularly in image captioning, where language priors are especially dominant.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</title>
<link>https://arxiv.org/abs/2505.21653</link>
<guid>https://arxiv.org/abs/2505.21653</guid>
<content:encoded><![CDATA[
arXiv:2505.21653v2 Announce Type: replace 
Abstract: Recent video diffusion models have demonstrated their great capability in generating visually-pleasing results, while synthesizing the correct physical effects in generated videos remains challenging. The complexity of real-world motions, interactions, and dynamics introduce great difficulties when learning physics from data. In this work, we propose DiffPhy, a generic framework that enables physically-correct and photo-realistic video generation by fine-tuning a pre-trained video diffusion model. Our method leverages large language models (LLMs) to explicitly reason a comprehensive physical context from the text prompt and use it to guide the generation. To incorporate physical context into the diffusion model, we leverage a Multimodal large language model (MLLM) as a supervisory signal and introduce a set of novel training objectives that jointly enforce physical correctness and semantic consistency with the input text. We also establish a high-quality physical video dataset containing diverse phyiscal actions and events to facilitate effective finetuning. Extensive experiments on public benchmarks demonstrate that DiffPhy is able to produce state-of-the-art results across diverse physics-related scenarios. Our project page is available at https://bwgzk-keke.github.io/DiffPhy/
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldExplorer: Towards Generating Fully Navigable 3D Scenes</title>
<link>https://arxiv.org/abs/2506.01799</link>
<guid>https://arxiv.org/abs/2506.01799</guid>
<content:encoded><![CDATA[
arXiv:2506.01799v2 Announce Type: replace 
Abstract: Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedEBench: Diagnosing Reliability in Text-Guided Medical Image Editing</title>
<link>https://arxiv.org/abs/2506.01921</link>
<guid>https://arxiv.org/abs/2506.01921</guid>
<content:encoded><![CDATA[
arXiv:2506.01921v5 Announce Type: replace 
Abstract: Text-guided image editing has seen significant progress in natural image domains, but its application in medical imaging remains limited and lacks standardized evaluation frameworks. Such editing could revolutionize clinical practices by enabling personalized surgical planning, enhancing medical education, and improving patient communication. To bridge this gap, we introduce MedEBench1, a robust benchmark designed to diagnose reliability in text-guided medical image editing. MedEBench consists of 1,182 clinically curated image-prompt pairs covering 70 distinct editing tasks and 13 anatomical regions. It contributes in three key areas: (1) a clinically grounded evaluation framework that measures Editing Accuracy, Context Preservation, and Visual Quality, complemented by detailed descriptions of intended edits and corresponding Region-of-Interest (ROI) masks; (2) a comprehensive comparison of seven state-of-theart models, revealing consistent patterns of failure; and (3) a diagnostic error analysis technique that leverages attention alignment, using Intersection-over-Union (IoU) between model attention maps and ROI masks to identify mislocalization issues, where models erroneously focus on incorrect anatomical regions. MedEBench sets the stage for developing more reliable and clinically effective text-guided medical image editing tools.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMF-MedIT: An Efficient Align-Modulation-Fusion Framework for Medical Image-Tabular Data</title>
<link>https://arxiv.org/abs/2506.19439</link>
<guid>https://arxiv.org/abs/2506.19439</guid>
<content:encoded><![CDATA[
arXiv:2506.19439v2 Announce Type: replace 
Abstract: Multimodal medical analysis combining image and tabular data has gained increasing attention. However, effective fusion remains challenging due to cross-modal discrepancies in feature dimensions and modality contributions, as well as the noise from high-dimensional tabular inputs. To address these problems, we present AMF-MedIT, an efficient Align-Modulation-Fusion framework for medical image and tabular data integration, particularly under data-scarce conditions. Built upon a self-supervised learning strategy, we introduce the Adaptive Modulation and Fusion (AMF) module, a novel, streamlined fusion paradigm that harmonizes dimension discrepancies and dynamically balances modality contributions. It integrates prior knowledge to guide the allocation of modality contributions in the fusion and employs feature masks together with magnitude and leakage losses to adjust the dimensionality and magnitude of unimodal features. Additionally, we develop FT-Mamba, a powerful tabular encoder leveraging a selective mechanism to handle noisy medical tabular data efficiently. Extensive experiments, including simulations of clinical noise, demonstrate that AMF-MedIT achieves superior accuracy, robustness, and data efficiency across multimodal classification tasks. Interpretability analyses further reveal how FT-Mamba shapes multimodal pretraining and enhances the image encoder's attention, highlighting the practical value of our framework for reliable and efficient clinical artificial intelligence applications.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</title>
<link>https://arxiv.org/abs/2507.02844</link>
<guid>https://arxiv.org/abs/2507.02844</guid>
<content:encoded><![CDATA[
arXiv:2507.02844v2 Announce Type: replace 
Abstract: With the emergence of strong vision language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: vision-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct vision-focused strategies, dynamically generating auxiliary images when necessary to construct a vision-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%. Code: https://github.com/Dtc7w3PQ/Visco-Attack.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.03903</link>
<guid>https://arxiv.org/abs/2507.03903</guid>
<content:encoded><![CDATA[
arXiv:2507.03903v2 Announce Type: replace 
Abstract: Reconstruction-based methods have demonstrated very promising results for 3D anomaly detection. However, these methods face great challenges in handling high-precision point clouds due to the large scale and complex structure. In this study, a Down-Up Sampling Network (DUS-Net) is proposed to reconstruct high-precision point clouds for 3D anomaly detection by preserving the group center geometric structure. The DUS-Net first introduces a Noise Generation module to generate noisy patches, which facilitates the diversity of training data and strengthens the feature representation for reconstruction. Then, a Down-sampling Network (Down-Net) is developed to learn an anomaly-free center point cloud from patches with noise injection. Subsequently, an Up-sampling Network (Up-Net) is designed to reconstruct high-precision point clouds by fusing multi-scale up-sampling features. Our method leverages group centers for construction, enabling the preservation of geometric structure and providing a more precise point cloud. Extensive experiments demonstrate the effectiveness of our proposed method, achieving state-of-the-art (SOTA) performance with an Object-level AUROC of 79.9% and 79.5%, and a Point-level AUROC of 71.2% and 84.7% on the Real3D-AD and Anomaly-ShapeNet datasets, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way</title>
<link>https://arxiv.org/abs/2507.08679</link>
<guid>https://arxiv.org/abs/2507.08679</guid>
<content:encoded><![CDATA[
arXiv:2507.08679v2 Announce Type: replace 
Abstract: We introduce ByDeWay, a training-free framework designed to enhance the performance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel prompting strategy called Layered-Depth-Based Prompting (LDP), which improves spatial reasoning and grounding without modifying any model parameters. It segments the scene into closest, mid-range, and farthest layers using monocular depth estimation, then generates region-specific captions with a grounded vision-language model. These structured, depth-aware captions are appended to the image-question prompt, enriching it with spatial context. This guides MLLMs to produce more grounded and less hallucinated responses. Our method is lightweight, modular, and compatible with black-box MLLMs. Experiments on hallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show consistent improvements across multiple MLLMs, validating the effectiveness of depth-aware prompting in a zero-training setting.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</title>
<link>https://arxiv.org/abs/2507.10171</link>
<guid>https://arxiv.org/abs/2507.10171</guid>
<content:encoded><![CDATA[
arXiv:2507.10171v2 Announce Type: replace 
Abstract: Concrete workability is essential for construction quality, with the slump test being the most common on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and prone to inconsistency, limiting its applicability for real-time monitoring. To address these challenges, we propose SlumpGuard, an AI-powered, video-based system that automatically analyzes concrete flow from the truck chute to assess workability in real time. Our system enables full-batch inspection without manual intervention, improving both the accuracy and efficiency of quality control. We present the system design, the construction of a dedicated dataset, and empirical results from real-world deployment, demonstrating the effectiveness of SlumpGuard as a practical solution for modern concrete quality assurance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Canonicalization by Foundation Models for Robust Perception</title>
<link>https://arxiv.org/abs/2507.10375</link>
<guid>https://arxiv.org/abs/2507.10375</guid>
<content:encoded><![CDATA[
arXiv:2507.10375v2 Announce Type: replace 
Abstract: Perception in the real world requires robustness to diverse viewing conditions. Existing approaches often rely on specialized architectures or training with predefined data augmentations, limiting adaptability. Taking inspiration from mental rotation in human vision, we propose FOCAL, a test-time robustness framework that transforms the input into the most typical view. At inference time, FOCAL explores a set of transformed images and chooses the one with the highest likelihood under foundation model priors. This test-time optimization boosts robustness while requiring no retraining or architectural changes. Applied to models like CLIP and SAM, it significantly boosts robustness across a wide range of transformations, including 2D and 3D rotations, contrast and lighting shifts, and day-night changes. We also explore potential applications in active vision. By reframing invariance as a test-time optimization problem, FOCAL offers a general and scalable approach to robustness. Our code is available at: https://github.com/sutkarsh/focal.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions</title>
<link>https://arxiv.org/abs/2507.13773</link>
<guid>https://arxiv.org/abs/2507.13773</guid>
<content:encoded><![CDATA[
arXiv:2507.13773v2 Announce Type: replace 
Abstract: In visual question answering (VQA) context, users often pose ambiguous questions to visual language models (VLMs) due to varying expression habits. Existing research addresses such ambiguities primarily by rephrasing questions. These approaches neglect the inherently interactive nature of user interactions with VLMs, where ambiguities can be clarified through user feedback. However, research on interactive clarification faces two major challenges: (1) Benchmarks are absent to assess VLMs' capacity for resolving ambiguities through interaction; (2) VLMs are trained to prefer answering rather than asking, preventing them from seeking clarification. To overcome these challenges, we introduce \textbf{ClearVQA} benchmark, which targets three common categories of ambiguity in VQA context, and encompasses various VQA scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning</title>
<link>https://arxiv.org/abs/2507.23318</link>
<guid>https://arxiv.org/abs/2507.23318</guid>
<content:encoded><![CDATA[
arXiv:2507.23318v3 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in complex scene understanding and action reasoning, leading to their increasing adoption in end-to-end autonomous driving systems. However, the long visual tokens of VLA models greatly increase computational costs. Current visual token pruning methods in Vision-Language Models (VLM) rely on either visual token similarity or visual-text attention, but both have shown poor performance in autonomous driving scenarios. Given that human drivers concentrate on relevant foreground areas while driving, we assert that retaining visual tokens containing this foreground information is essential for effective decision-making. Inspired by this, we propose FastDriveVLA, a novel reconstruction-based vision token pruning framework designed specifically for autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner called ReconPruner, which prioritizes foreground information through MAE-style pixel reconstruction. A novel adversarial foreground-background reconstruction strategy is designed to train ReconPruner for the visual encoder of VLA models. Once trained, ReconPruner can be seamlessly applied to different VLA models with the same visual encoder without retraining. To train ReconPruner, we also introduce a large-scale dataset called nuScenes-FG, consisting of 241K image-mask pairs with annotated foreground regions. Our approach achieves state-of-the-art results on the nuScenes open-loop planning benchmark across different pruning ratios.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation</title>
<link>https://arxiv.org/abs/2508.00766</link>
<guid>https://arxiv.org/abs/2508.00766</guid>
<content:encoded><![CDATA[
arXiv:2508.00766v2 Announce Type: replace 
Abstract: Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/Sample-Aware-TTA/Code.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.03017</link>
<guid>https://arxiv.org/abs/2508.03017</guid>
<content:encoded><![CDATA[
arXiv:2508.03017v2 Announce Type: replace 
Abstract: Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.06259</link>
<guid>https://arxiv.org/abs/2508.06259</guid>
<content:encoded><![CDATA[
arXiv:2508.06259v4 Announce Type: replace 
Abstract: Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method. Code: https://github.com/zhangquanchen/SIFThinker.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
arXiv:2508.09456v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plane Detection and Ranking via Model Information Optimization</title>
<link>https://arxiv.org/abs/2508.09625</link>
<guid>https://arxiv.org/abs/2508.09625</guid>
<content:encoded><![CDATA[
arXiv:2508.09625v2 Announce Type: replace 
Abstract: Plane detection from depth images is a crucial subtask with broad robotic applications, often accomplished by iterative methods such as Random Sample Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic guarantees, the ambiguity of its inlier threshold criterion makes it susceptible to false positive plane detections. This issue is particularly prevalent in complex real-world scenes, where the true number of planes is unknown and multiple planes coexist. In this paper, we aim to address this limitation by proposing a generalised framework for plane detection based on model information optimization. Building on previous works, we treat the observed depth readings as discrete random variables, with their probability distributions constrained by the ground truth planes. Various models containing different candidate plane constraints are then generated through repeated random sub-sampling to explain our observations. By incorporating the physics and noise model of the depth sensor, we can calculate the information for each model, and the model with the least information is accepted as the most likely ground truth. This information optimization process serves as an objective mechanism for determining the true number of planes and preventing false positive detections. Additionally, the quality of each detected plane can be ranked by summing the information reduction of inlier points for each plane. We validate these properties through experiments with synthetic data and find that our algorithm estimates plane parameters more accurately compared to the default Open3D RANSAC plane segmentation. Furthermore, we accelerate our algorithm by partitioning the depth map using neural network segmentation, which enhances its ability to generate more realistic plane parameters in real-world data.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.13977</link>
<guid>https://arxiv.org/abs/2508.13977</guid>
<content:encoded><![CDATA[
arXiv:2508.13977v2 Announce Type: replace 
Abstract: Depth estimation is a fundamental task for 3D scene understanding in autonomous driving, robotics, and augmented reality. Existing depth datasets, such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from limitations in diversity and scalability. As benchmark performance on these datasets approaches saturation, there is an increasing need for a new generation of large-scale, diverse, and cost-efficient datasets to support the era of foundation models and multi-modal learning. We present ROVR, a large-scale, diverse, and cost-efficient depth dataset designed to capture the complexity of real-world driving. ROVR comprises 200K high-resolution frames across highway, rural, and urban scenarios, spanning day/night and adverse weather conditions. A lightweight acquisition pipeline ensures scalable collection, while sparse but statistically sufficient ground truth supports robust training. Benchmarking with state-of-the-art monocular depth models reveals severe cross-dataset generalization failures: models achieving near-ceiling accuracy on KITTI degrade drastically on ROVR, and even when trained on ROVR, current methods fall short of saturation. These results highlight the unique challenges posed by ROVR-scene diversity, dynamic environments, and sparse ground truth, establishing it as a demanding new platform for advancing depth estimation and building models with stronger real-world robustness. Extensive ablation studies provide a more intuitive understanding of our dataset across different scenarios, lighting conditions, and generalized ability.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting</title>
<link>https://arxiv.org/abs/2509.04545</link>
<guid>https://arxiv.org/abs/2509.04545</guid>
<content:encoded><![CDATA[
arXiv:2509.04545v4 Announce Type: replace 
Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus</title>
<link>https://arxiv.org/abs/2509.04859</link>
<guid>https://arxiv.org/abs/2509.04859</guid>
<content:encoded><![CDATA[
arXiv:2509.04859v2 Announce Type: replace 
Abstract: Mobile reconstruction has the potential to support time-critical tasks such as tele-guidance and disaster response, where operators must quickly gain an accurate understanding of the environment. Full high-fidelity scene reconstruction is computationally expensive and often unnecessary when only specific points of interest (POIs) matter for timely decision making. We address this challenge with CoRe-GS, a semantic POI-focused extension of Gaussian Splatting (GS). Instead of optimizing every scene element uniformly, CoRe-GS first produces a fast segmentation-ready GS representation and then selectively refines splats belonging to semantically relevant POIs detected during data acquisition. This targeted refinement reduces training time to 25\% compared to full semantic GS while improving novel view synthesis quality in the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM) and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs enables faster and higher-quality mobile reconstruction tailored to operational needs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions</title>
<link>https://arxiv.org/abs/2309.07510</link>
<guid>https://arxiv.org/abs/2309.07510</guid>
<content:encoded><![CDATA[
arXiv:2309.07510v5 Announce Type: replace-cross 
Abstract: Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containing a single occluder and generalizing to scenes with complex occluder combinations. Experiments demonstrate the effectiveness of our proposed approach in learning affordance considering environment constraints. Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Transferable Adversarial Images: Systemization, Evaluation, and New Insights</title>
<link>https://arxiv.org/abs/2310.11850</link>
<guid>https://arxiv.org/abs/2310.11850</guid>
<content:encoded><![CDATA[
arXiv:2310.11850v2 Announce Type: replace-cross 
Abstract: Transferable adversarial images raise critical security concerns for computer vision systems in real-world, black-box attack scenarios. Although many transfer attacks have been proposed, existing research lacks a systematic and comprehensive evaluation. In this paper, we systemize transfer attacks into five categories around the general machine learning pipeline and provide the first comprehensive evaluation, with 23 representative attacks against 11 representative defenses, including the recent, transfer-oriented defense and the real-world Google Cloud Vision. In particular, we identify two main problems of existing evaluations: (1) for attack transferability, lack of intra-category analyses with fair hyperparameter settings, and (2) for attack stealthiness, lack of diverse measures. Our evaluation results validate that these problems have indeed caused misleading conclusions and missing points, and addressing them leads to new, \textit{consensus-challenging} insights, such as (1) an early attack, DI, even outperforms all similar follow-up ones, (2) the state-of-the-art (white-box) defense, DiffPure, is even vulnerable to (black-box) transfer attacks, and (3) even under the same $L_p$ constraint, different attacks yield dramatically different stealthiness results regarding diverse imperceptibility metrics, finer-grained measures, and a user study. We hope that our analyses will serve as guidance on properly evaluating transferable adversarial images and advance the design of attacks and defenses. Code is available at https://github.com/ZhengyuZhao/TransferAttackEval.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential</title>
<link>https://arxiv.org/abs/2501.18834</link>
<guid>https://arxiv.org/abs/2501.18834</guid>
<content:encoded><![CDATA[
arXiv:2501.18834v2 Announce Type: replace-cross 
Abstract: Defacing is often applied to head magnetic resonance image (MRI) datasets prior to public release to address privacy concerns. The alteration of facial and nearby voxels has provoked discussions about the true capability of these techniques to ensure privacy as well as their impact on downstream tasks. With advancements in deep generative models, the extent to which defacing can protect privacy is uncertain. Additionally, while the altered voxels are known to contain valuable anatomical information, their potential to support research beyond the anatomical regions directly affected by defacing remains uncertain. To evaluate these considerations, we develop a refacing pipeline that recovers faces in defaced head MRIs using cascaded diffusion probabilistic models (DPMs). The DPMs are trained on images from 180 subjects and tested on images from 484 unseen subjects, 469 of whom are from a different dataset. To assess whether the altered voxels in defacing contain universally useful information, we also predict computed tomography (CT)-derived skeletal muscle radiodensity from facial voxels in both defaced and original MRIs. The results show that DPMs can generate high-fidelity faces that resemble the original faces from defaced images, with surface distances to the original faces significantly smaller than those of a population average face (p < 0.05). This performance also generalizes well to previously unseen datasets. For skeletal muscle radiodensity predictions, using defaced images results in significantly weaker Spearman's rank correlation coefficients compared to using original images (p < 10-4). For shin muscle, the correlation is statistically significant (p < 0.05) when using original images but not statistically significant (p > 0.05) when any defacing method is applied, suggesting that defacing might not only fail to protect privacy but also eliminate valuable information.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Relation Inference via Verb Embeddings</title>
<link>https://arxiv.org/abs/2503.13021</link>
<guid>https://arxiv.org/abs/2503.13021</guid>
<content:encoded><![CDATA[
arXiv:2503.13021v2 Announce Type: replace-cross 
Abstract: CLIP has demonstrated exceptional image-text matching capabilities due to its training on contrastive learning tasks. Past research has suggested that whereas CLIP effectively matches text to images when the matching can be achieved just by matching the text with the objects in the image, CLIP struggles when the matching depends on representing the relationship among the objects in the images (i.e., inferring relations). Previous attempts to address this limitation by training CLIP on relation detection datasets with only linguistic supervision have met with limited success. In this paper, we offer insights and practical methods to advance the field of relation inference from images. This paper approaches the task of creating a model that effectively detects relations among the objects in images by producing text and image embeddings that capture relationships through linguistic supervision. To this end, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which augments the COCO dataset, fine-tunes CLIP with hard negatives subject-relation-object triples and corresponding images, and introduces a novel loss function to improve relation detection. Evaluated on multiple CLIP-based models, our method significantly improves zero-shot relation inference accuracy in both frozen and fine-tuned settings, significantly outperforming CLIP and state-of-the-art models while generalizing well on unseen data.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosis for Less-Prevalent Thyroid Carcinoma Subtype Using a Dual-Branch Attention Deep Network with Ultrasound Images</title>
<link>https://arxiv.org/abs/2505.02211</link>
<guid>https://arxiv.org/abs/2505.02211</guid>
<content:encoded><![CDATA[
arXiv:2505.02211v2 Announce Type: replace-cross 
Abstract: Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDiffuser: Diverse Trajectory Generation with Decorrelated Multi-modal Representation for End-to-end Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.09315</link>
<guid>https://arxiv.org/abs/2505.09315</guid>
<content:encoded><![CDATA[
arXiv:2505.09315v2 Announce Type: replace-cross 
Abstract: In recent years, diffusion models have demonstrated remarkable potential across diverse domains, from vision generation to language modeling. Transferring its generative capabilities to modern end-to-end autonomous driving systems has also emerged as a promising direction. However, existing diffusion-based trajectory generative models often exhibit mode collapse where different random noises converge to similar trajectories after the denoising process.Therefore, state-of-the-art models often rely on anchored trajectories from pre-defined trajectory vocabulary or scene priors in the training set to mitigate collapse and enrich the diversity of generated trajectories, but such inductive bias are not available in real-world deployment, which can be challenged when generalizing to unseen scenarios. In this work, we investigate the possibility of effectively tackling the mode collapse challenge without the assumption of pre-defined trajectory vocabulary or pre-computed scene priors. Specifically, we propose TransDiffuser, an encoder-decoder based generative trajectory planning model, where the encoded scene information and motion states serve as the multi-modal conditional input of the denoising decoder. Different from existing approaches, we exploit a simple yet effective multi-modal representation decorrelation optimization mechanism during the denoising process to enrich the latent representation space which better guides the downstream generation. Without any predefined trajectory anchors or pre-computed scene priors, TransDiffuser achieves the PDMS of 94.85 on the closed-loop planning-oriented benchmark NAVSIM, surpassing previous state-of-the-art methods. Qualitative evaluation further showcases TransDiffuser generates more diverse and plausible trajectories which explore more drivable area.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation</title>
<link>https://arxiv.org/abs/2505.22159</link>
<guid>https://arxiv.org/abs/2505.22159</guid>
<content:encoded><![CDATA[
arXiv:2505.22159v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have advanced general-purpose robotic manipulation by leveraging pretrained visual and linguistic representations. However, they struggle with contact-rich tasks that require fine-grained control involving force, especially under visual occlusion or dynamic uncertainty. To address these limitations, we propose ForceVLA, a novel end-to-end manipulation framework that treats external force sensing as a first-class modality within VLA systems. ForceVLA introduces FVLMoE, a force-aware Mixture-of-Experts fusion module that dynamically integrates pretrained visual-language embeddings with real-time 6-axis force feedback during action decoding. This enables context-aware routing across modality-specific experts, enhancing the robot's ability to adapt to subtle contact dynamics. We also introduce \textbf{ForceVLA-Data}, a new dataset comprising synchronized vision, proprioception, and force-torque signals across five contact-rich manipulation tasks. ForceVLA improves average task success by 23.2% over strong pi_0-based baselines, achieving up to 80% success in tasks such as plug insertion. Our approach highlights the importance of multimodal integration for dexterous manipulation and sets a new benchmark for physically intelligent robotic control. Code and data will be released at https://sites.google.com/view/forcevla2025.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Generalist Vision Language Models (VLMs) Rival Specialist Medical VLMs? Benchmarking and Strategic Insights</title>
<link>https://arxiv.org/abs/2506.17337</link>
<guid>https://arxiv.org/abs/2506.17337</guid>
<content:encoded><![CDATA[
arXiv:2506.17337v2 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) have shown promise in automating image diagnosis and interpretation in clinical settings. However, developing specialist medical VLMs requires substantial computational resources and carefully curated datasets, and it remains unclear under which conditions generalist and specialist medical VLMs each perform best. This study highlights the complementary strengths of specialist medical and generalist VLMs. Specialists remain valuable in modality-aligned use cases, but we find that efficiently fine-tuned generalist VLMs can achieve comparable or even superior performance in most tasks, particularly when transferring to unseen or rare OOD medical modalities. These results suggest that generalist VLMs, rather than being constrained by their lack of specialist medical pretraining, may offer a scalable and cost-effective pathway for advancing clinical AI development.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Open-Source Vision-Language Models to Domain Shift in Object Captioning</title>
<link>https://arxiv.org/abs/2506.19579</link>
<guid>https://arxiv.org/abs/2506.19579</guid>
<content:encoded><![CDATA[
arXiv:2506.19579v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have emerged as powerful tools for generating textual descriptions from visual data. While these models excel on web-scale datasets, their robustness to the domain shifts inherent in many real-world applications remains under-explored. This paper presents a systematic evaluation of VLM performance on a single-view object captioning task when faced with a controlled, physical domain shift. We compare captioning accuracy across two distinct object sets: a collection of multi-material, real-world tools and a set of single-material, 3D-printed items. The 3D-printed set introduces a significant domain shift in texture and material properties, challenging the models' generalization capabilities. Our quantitative results demonstrate that all tested VLMs show a marked performance degradation when describing the 3D-printed objects compared to the real-world tools. This underscores a critical limitation in the ability of current models to generalize beyond surface-level features and highlights the need for more robust architectures for real-world signal processing applications.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction</title>
<link>https://arxiv.org/abs/2508.04929</link>
<guid>https://arxiv.org/abs/2508.04929</guid>
<content:encoded><![CDATA[
arXiv:2508.04929v2 Announce Type: replace-cross 
Abstract: As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from a large collection of noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. Addressing this issue, we introduce cryoGS, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. All these innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoGS over representative baselines. The code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation</title>
<link>https://arxiv.org/abs/2509.04126</link>
<guid>https://arxiv.org/abs/2509.04126</guid>
<content:encoded><![CDATA[
<div> Position-Style-Aware Module, Large Language Models, Multi-Expert Diffusion, Spatial Layout Editing, Style Diversity
Summary:
The Multi-Expert Planning and Generation Framework (MEPG) proposed in this study integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules to improve text-to-image diffusion models. The framework consists of a Position-Style-Aware (PSA) module for precise spatial coordinates and style instructions, and a Multi-Expert Diffusion (MED) module for cross-region generation using expert routing. Specialized models are activated for each spatial partition, enhancing image quality and style diversity. The architecture supports easy integration and replacement of expert models, with real-time spatial layout editing and style selection. Experiments demonstrate that MEPG outperforms baseline models in both image quality and style diversity. <div>
arXiv:2509.04126v2 Announce Type: replace 
Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality
  and style diversity.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs</title>
<link>https://arxiv.org/abs/2509.04719</link>
<guid>https://arxiv.org/abs/2509.04719</guid>
<content:encoded><![CDATA[
<div> Framework, Diffusion Model, Parallel Inference, Heterogeneous GPU, Load Balancing <br />
Summary: STADI is introduced as a novel framework for accelerating diffusion model inference in heterogeneous multi-GPU environments. It utilizes a hybrid scheduler to manage fine-grained parallelism across temporal and spatial dimensions. The framework incorporates a computation-aware step allocator to reduce denoising steps on slower GPUs and optimize execution synchronization. Additionally, an elastic patch parallelism mechanism is implemented to allocate image patches based on GPU computational capability, ensuring balanced workload distribution. Experimental results demonstrate STADI's effectiveness in improving load balancing and mitigating performance bottlenecks, reducing end-to-end inference latency by up to 45% compared to patch parallelism. The framework also showcases enhanced resource utilization on heterogeneous GPUs, showcasing significant advancements in parallel inference techniques. <br /><br /> <div>
arXiv:2509.04719v2 Announce Type: replace-cross 
Abstract: The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-Time Diminished Reality Approach to Privacy in MR Collaboration</title>
<link>https://arxiv.org/abs/2509.10466</link>
<guid>https://arxiv.org/abs/2509.10466</guid>
<content:encoded><![CDATA[
<div> privacy control, diminished reality, mixed reality, semantic segmentation, real-time<br />
Summary:<br />
This thesis introduces a real-time, inpainting-based diminished reality system for privacy control in shared-space mixed reality meetings. The system allows users to remove personal or sensitive objects from their environment using semantic segmentation and precise object selection. It enables real-time object removal from the viewpoint of a secondary observer without requiring a fixed viewpoint or prior 3D scanning. Utilizing YOLOv11 for object detection and a modified DSTT model for video inpainting, the system achieves frame rates exceeding 20 fps at 720p resolution. The system demonstrates the feasibility of real-time diminished reality for practical privacy-preserving applications in mixed reality settings.<br /> <div>
arXiv:2509.10466v1 Announce Type: new 
Abstract: Diminished reality (DR) refers to the digital removal of real-world objects by compositing background content in their place. This thesis presents a real-time, inpainting-based DR system designed to enable privacy control in shared-space mixed reality (MR) meetings. The system allows a primary headset user to selectively remove personal or sensitive items from their environment, ensuring that those objects are no longer visible to other participants. Removal is achieved through semantic segmentation and precise object selection, followed by real-time inpainting from the viewpoint of a secondary observer, implemented using a mobile ZED 2i depth camera. The solution is designed to be portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D scanning of the environment. The system utilises YOLOv11 for object detection and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for high-quality video inpainting. At 720p resolution, the pipeline sustains frame rates exceeding 20 fps, demonstrating the feasibility of real-time diminished reality for practical privacy-preserving MR applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning</title>
<link>https://arxiv.org/abs/2509.10555</link>
<guid>https://arxiv.org/abs/2509.10555</guid>
<content:encoded><![CDATA[
<div> Keywords: Surgical VLP, vision-language dataset, SurgLaVi, dual-modality filtering, SurgCLIP<br />
Summary: SurgLaVi is introduced as the largest surgical vision-language dataset, comprising nearly 240k clip-caption pairs from over 200 procedures, with hierarchical levels at phase, step, and task levels. A fully automated pipeline generates fine-grained transcriptions of surgical videos and filters out irrelevant samples to ensure high-quality annotations. SurgLaVi-eta, a derivative dataset of 113k clip-caption pairs, is released as an open-source resource. SurgCLIP, a CLIP-style video-text contrastive framework with dual encoders, is proposed as a base model, achieving consistent improvements in phase, step, action, and tool recognition tasks. The results demonstrate that large-scale, semantically rich, and hierarchically structured datasets lead to stronger and more generalizable representations, positioning SurgLaVi as a crucial asset for advancing surgical foundation models. <br /><br />Summary: <div>
arXiv:2509.10555v1 Announce Type: new 
Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by aligning language with surgical videos, enabling workflow understanding and transfer across tasks without relying on expert-labeled datasets. However, progress in surgical VLP remains constrained by the limited scale, procedural diversity, semantic quality, and hierarchical structure of existing datasets. In this work, we present SurgLaVi, the largest and most diverse surgical vision-language dataset to date, comprising nearly 240k clip-caption pairs from more than 200 procedures, and comprising hierarchical levels at phase-, step-, and task-level. At the core of SurgLaVi lies a fully automated pipeline that systematically generates fine-grained transcriptions of surgical videos and segments them into coherent procedural units. To ensure high-quality annotations, it applies dual-modality filtering to remove irrelevant and noisy samples. Within this framework, the resulting captions are enriched with contextual detail, producing annotations that are both semantically rich and easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an open-source derivative of 113k clip-caption pairs constructed entirely from public data, which is over four times larger than existing surgical VLP datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP, a CLIP-style video-text contrastive framework with dual encoders, as a representative base model. SurgCLIP achieves consistent improvements across phase, step, action, and tool recognition, surpassing prior state-of-the-art methods, often by large margins. These results validate that large-scale, semantically rich, and hierarchically structured datasets directly translate into stronger and more generalizable representations, establishing SurgLaVi as a key resource for developing surgical foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</title>
<link>https://arxiv.org/abs/2509.10620</link>
<guid>https://arxiv.org/abs/2509.10620</guid>
<content:encoded><![CDATA[
<div> pre-trained, SSL, SimCLR, 3D MRI, neurological diseases
Summary: 
A new high-resolution SimCLR-based self-supervised learning (SSL) foundation model for 3D structural MRI brain scans is developed, pre-trained on a large dataset of 18,759 patients from 11 publicly available datasets. The model is compared to Masked Autoencoders (MAE) and two supervised baselines on four prediction tasks and outperforms them all. It showcases superior performance even with limited labeled training data, achieving great results in predicting Alzheimer's disease with only 20% of labeled samples. The model is released open-source, providing a broadly applicable and accessible tool for clinical brain MRI analysis. The research highlights the potential of SSL in leveraging diverse, unlabeled datasets for developing effective medical imaging models and demonstrates the feasibility of using self-supervised learning approaches for 3D brain MRI analysis. Overall, the study contributes a valuable foundation model for analyzing 3D brain MRI data in clinical settings, showcasing the power of pre-training on diverse neurological disease datasets. 
<br /><br />Summary: <div>
arXiv:2509.10620v1 Announce Type: new 
Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly acquired in clinical settings to monitor a wide range of neurological conditions, including neurodegenerative disorders and stroke. While deep learning models have shown promising results analyzing 3D MRI across a number of brain imaging tasks, most are highly tailored for specific tasks with limited labeled data, and are not able to generalize across tasks and/or populations. The development of self-supervised learning (SSL) has enabled the creation of large medical foundation models that leverage diverse, unlabeled datasets ranging from healthy to diseased data, showing significant success in 2D medical imaging applications. However, even the very few foundation models for 3D brain MRI that have been developed remain limited in resolution, scope, or accessibility. In this work, we present a general, high-resolution SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on 18,759 patients (44,958 scans) from 11 publicly available datasets spanning diverse neurological diseases. We compare our model to Masked Autoencoders (MAE), as well as two supervised baselines, on four diverse downstream prediction tasks in both in-distribution and out-of-distribution settings. Our fine-tuned SimCLR model outperforms all other models across all tasks. Notably, our model still achieves superior performance when fine-tuned using only 20% of labeled training samples for predicting Alzheimer's disease. We use publicly available code and data, and release our trained model at https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly applicable and accessible foundation model for clinical brain MRI analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction</title>
<link>https://arxiv.org/abs/2509.10651</link>
<guid>https://arxiv.org/abs/2509.10651</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral images, RGB-to-HSI reconstruction, nuclear norm regularization, data-adaptive low-rank subspace, deep unfolding solver<br />
Summary:<br />
Reconstructing hyperspectral images (HSIs) from a single RGB image is a challenging task due to misspecification of the camera spectral sensitivity (CSS) and scene illumination. A new approach has been proposed to address this issue by formulating RGB-to-HSI reconstruction as an inverse problem regularized by a nuclear norm in a learnable transform domain. The method explicitly estimates CSS and illumination, ensuring colorimetric consistency in each iteration. To improve efficiency and stability, a data-adaptive low-rank subspace SVT operator is introduced to avoid the costly SVD computations. The developed USCTNet is a deep unfolding solver tailored to HSI, incorporating a parameter estimation module and learnable proximal updates. Experimental results on standard benchmarks demonstrate superior reconstruction accuracy compared to existing RGB-based methods. The code implementation is available for further exploration and application. <br />Summary: <div>
arXiv:2509.10651v1 Announce Type: new 
Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is ill-posed and can become physically inconsistent when the camera spectral sensitivity (CSS) and scene illumination are misspecified. We formulate RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by a nuclear norm in a learnable transform domain, and we explicitly estimate CSS and illumination to define the forward operator embedded in each iteration, ensuring colorimetric consistency. To avoid the cost and instability of full singular-value decompositions (SVDs) required by singular-value thresholding (SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on these components, we develop USCTNet, a deep unfolding solver tailored to HSI that couples a parameter estimation module with learnable proximal updates. Extensive experiments on standard benchmarks show consistent improvements over state-of-the-art RGB-based methods in reconstruction accuracy. Code: https://github.com/psykheXX/USCTNet-Code-Implementation.git
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</title>
<link>https://arxiv.org/abs/2509.10683</link>
<guid>https://arxiv.org/abs/2509.10683</guid>
<content:encoded><![CDATA[
<div> classification, segmentation, large language models, convolutional neural networks, medical imaging <br />
Summary: <br />
- Large Language Models (LLMs) have shown success in text-based healthcare tasks but lack effectiveness in image-based applications. 
- A study compared the performance of LLMs with traditional Convolutional Neural Networks (CNNs) for glioma classification and segmentation using brain MRIs. 
- CNNs achieved higher accuracy, balanced precision and recall in glioma classification compared to LLMs. 
- LLMs struggled with spatial understanding and showed minimal improvement from fine-tuning for image-based tasks. 
- Fine-tuning did not significantly enhance LLM performance in classification or segmentation tasks. 
- CNNs outperformed LLMs in both tasks, indicating the need for more rigorous fine-tuning or alternative training strategies for LLMs to excel in medical imaging applications. <br /> <div>
arXiv:2509.10683v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong performance in text-based healthcare tasks. However, their utility in image-based applications remains unexplored. We investigate the effectiveness of LLMs for medical imaging tasks, specifically glioma classification and segmentation, and compare their performance to that of traditional convolutional neural networks (CNNs). Using the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and balanced precision and recall. The general LLM reached 76% accuracy but suffered from a specificity of only 18%, often misclassifying Low-Grade tumors. Fine-tuning improved specificity to 55%, but overall performance declined (e.g., accuracy dropped to 72%). For segmentation, three methods - center point, bounding box, and polygon extraction, were implemented. CNNs accurately localized gliomas, though small tumors were sometimes missed. In contrast, LLMs consistently clustered predictions near the image center, with no distinction of glioma size, location, or placement. Fine-tuning improved output formatting but failed to meaningfully enhance spatial accuracy. The bounding polygon method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in both tasks. LLMs showed limited spatial understanding and minimal improvement from fine-tuning, indicating that, in their current form, they are not well-suited for image-based tasks. More rigorous fine-tuning or alternative training strategies may be needed for LLMs to achieve better performance, robustness, and utility in the medical space.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</title>
<link>https://arxiv.org/abs/2509.10687</link>
<guid>https://arxiv.org/abs/2509.10687</guid>
<content:encoded><![CDATA[
<div> SP4D, Kinematic part videos, RGB frames, Part segmentation maps, Dual-branch diffusion model

Summary:
SP4D presents a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike traditional part segmentation methods, SP4D focuses on kinematic parts aligned with object articulation and consistent across views and time. It adopts a dual-branch diffusion model to synthesize RGB frames and part segmentation maps. A spatial color encoding scheme simplifies the architecture and enables flexibility in defining part counts. The Bidirectional Diffusion Fusion module enhances cross-branch consistency, supported by a contrastive part consistency loss. SP4D's outputs can be lifted to 3D to derive skeletal structures and skinning weights with minimal manual adjustments. The model is trained and evaluated on the KinematicParts20K dataset, showing strong generalization to various scenarios and producing kinematic-aware outputs suitable for animation and motion-related tasks. <br /><br />Summary: <div>
arXiv:2509.10687v1 Announce Type: new 
Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2509.10710</link>
<guid>https://arxiv.org/abs/2509.10710</guid>
<content:encoded><![CDATA[
<div> Keyword: ISLR, SegSLR, RGB data, pose information, video segmentation

Summary:
SegSLR is a new Isolated Sign Language Recognition (ISLR) system that combines RGB and pose information using promptable zero-shot video segmentation. By segmenting the hands and signer's body in the video based on pose information, SegSLR retains crucial details like hand shape and orientation that are often lost in traditional approaches. This segmentation helps focus the processing of RGB data on the most relevant body parts for ISLR, resulting in improved performance compared to existing methods. Evaluation on the ChaLearn249 IsoGD dataset demonstrated that SegSLR outperforms state-of-the-art approaches. Ablation studies further confirmed the effectiveness of SegSLR's design choices, showing that focusing on the signer's body and hands significantly benefits the recognition process. SegSLR represents a promising advancement in ISLR that addresses the limitations of current approaches through innovative integration of RGB and pose information.

<br /><br />Summary: SegSLR is a novel ISLR system that integrates RGB and pose information using video segmentation. It enhances recognition by maintaining crucial hand shape and orientation details, outperforming existing methods on the ChaLearn249 IsoGD dataset. Ablation studies highlight the benefits of focusing on the signer's body and hands in the recognition process, showcasing the effectiveness of SegSLR's design choices. <div>
arXiv:2509.10710v1 Announce Type: new 
Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB data or signer pose information. However, combining these modalities often results in the loss of crucial details, such as hand shape and orientation, due to imprecise representations like bounding boxes. Therefore, we propose the ISLR system SegSLR, which combines RGB and pose information through promptable zero-shot video segmentation. Given the rough localization of the hands and the signer's body from pose information, we segment the respective parts through the video to maintain all relevant shape information. Subsequently, the segmentations focus the processing of the RGB data on the most relevant body parts for ISLR. This effectively combines RGB and pose information. Our evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR outperforms state-of-the-art methods. Furthermore, ablation studies indicate that SegSLR strongly benefits from focusing on the signer's body and hands, justifying our design choices.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</title>
<link>https://arxiv.org/abs/2509.10748</link>
<guid>https://arxiv.org/abs/2509.10748</guid>
<content:encoded><![CDATA[
<div> Keyword: segmentation, tracking, surgical scene, speech-guided, collaborative perception 
<br />
Summary: 
<br />
Accurate segmentation and tracking of surgical scene elements are critical for intraoperative assistance. Current solutions rely on supervised models with labeled data, limiting adaptability. A new framework, SCOPE, combines language and vision models for on-the-fly segmentation and tracking. A collaborative perception agent integrates language feedback for human-machine collaboration. Results on Cataract1K and skull-base datasets show potential for real-time segmentation. Live mock experiments demonstrate dynamic capabilities. This collaboration showcases the development of adaptable, hands-free tools for surgical environments. 
<br /> <div>
arXiv:2509.10748v1 Announce Type: new 
Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene is crucial to enable context-aware intraoperative assistance and decision making. Current solutions remain tethered to domain-specific, supervised models that rely on labeled data and required domain-specific data to adapt to new surgical scenarios and beyond predefined label categories. Recent advances in prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot segmentation across heterogeneous medical images. However, dependence of these models on manual visual or textual cues restricts their deployment in introperative surgical settings. We introduce a speech-guided collaborative perception (SCOPE) framework that integrates reasoning capabilities of large language model (LLM) with perception capabilities of open-set VFMs to support on-the-fly segmentation, labeling and tracking of surgical instruments and anatomy in intraoperative video streams. A key component of this framework is a collaborative perception agent, which generates top candidates of VFM-generated segmentation and incorporates intuitive speech feedback from clinicians to guide the segmentation of surgical instruments in a natural human-machine collaboration paradigm. Afterwards, instruments themselves serve as interactive pointers to label additional elements of the surgical scene. We evaluated our proposed framework on a subset of publicly available Cataract1k dataset and an in-house ex-vivo skull-base dataset to demonstrate its potential to generate on-the-fly segmentation and tracking of surgical scene. Furthermore, we demonstrate its dynamic capabilities through a live mock ex-vivo experiment. This human-AI collaboration paradigm showcase the potential of developing adaptable, hands-free, surgeon-centric tools for dynamic operating-room environments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation</title>
<link>https://arxiv.org/abs/2509.10759</link>
<guid>https://arxiv.org/abs/2509.10759</guid>
<content:encoded><![CDATA[
<div> Ray Tracing, Camera Effects, Computer Vision, 4D-GRT, Synthetic Scenes<br />
Summary:
The paper introduces 4D Gaussian Ray Tracing (4D-GRT), a novel method for simulating camera effects in computer vision systems. Traditional systems struggle with real-world camera distortions like fisheye and rolling shutter due to limited training data. 4D-GRT overcomes this by combining 4D Gaussian Splatting and ray tracing to accurately model camera effects on multi-view videos. It reconstructs dynamic scenes before applying ray tracing for realistic effect generation. The method is faster and achieves comparable rendering quality to existing approaches. The study includes a benchmark dataset of eight synthetic dynamic scenes in indoor environments with four camera effects for evaluating video generation. 4D-GRT offers a promising solution for training computer vision models to better handle real-world camera distortions. <br /><br /> <div>
arXiv:2509.10759v1 Announce Type: new 
Abstract: Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EditDuet: A Multi-Agent System for Video Non-Linear Editing</title>
<link>https://arxiv.org/abs/2509.10761</link>
<guid>https://arxiv.org/abs/2509.10761</guid>
<content:encoded><![CDATA[
<div> Automated video editing, multi-agent approach, sequential decision making, natural language instructions, learning-based communication <br />
Summary: The article introduces a novel approach to automated video editing using a multi-agent system comprising an Editor agent and a Critic agent. The Editor processes video clips and natural language instructions to produce edited sequences, while the Critic provides feedback based on the output. A learning-based approach facilitates effective communication between the specialized agents. The system's output is evaluated using an LLM-as-a-judge metric and compared with human preferences, demonstrating superior performance in coverage, time constraint satisfaction, and overall preference. The system's effectiveness is verified through qualitative and quantitative evaluation, showcasing its capability to outperform existing video editing approaches. <br /><br />Summary: <div>
arXiv:2509.10761v1 Announce Type: new 
Abstract: Automated tools for video editing and assembly have applications ranging from filmmaking and advertisement to content creation for social media. Previous video editing work has mainly focused on either retrieval or user interfaces, leaving actual editing to the user. In contrast, we propose to automate the core task of video editing, formulating it as sequential decision making process. Ours is a multi-agent approach. We design an Editor agent and a Critic agent. The Editor takes as input a collection of video clips together with natural language instructions and uses tools commonly found in video editing software to produce an edited sequence. On the other hand, the Critic gives natural language feedback to the editor based on the produced sequence or renders it if it is satisfactory. We introduce a learning-based approach for enabling effective communication across specialized agents to address the language-driven video editing task. Finally, we explore an LLM-as-a-judge metric for evaluating the quality of video editing system and compare it with general human preference. We evaluate our system's output video sequences qualitatively and quantitatively through a user study and find that our system vastly outperforms existing approaches in terms of coverage, time constraint satisfaction, and human preference.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging</title>
<link>https://arxiv.org/abs/2509.10767</link>
<guid>https://arxiv.org/abs/2509.10767</guid>
<content:encoded><![CDATA[
<div> Keywords: Glioma, machine learning, MRI, contrast enhancement, stability-aware framework

Summary:
Machine learning was utilized to predict contrast enhancement in glioma patients using non-contrast MRI scans, reducing reliance on contrast agents with potential safety concerns. A stability-aware framework was developed to identify robust machine learning pipelines across multiple centers, demonstrating prediction accuracies ranging from 0.91 to 0.98 in cross-validation and external testing. Features extracted from non-contrast T1WI images were combined with various dimensionality reduction methods and classifiers to create 1,200 pipelines, with the MI linked with ETr pipeline consistently performing well. The model exhibited stable F1, precision, and recall scores, although ROC-AUC varied due to cohort heterogeneity. This approach represents a scalable template for reproducible machine learning in neuro-oncology, enabling reliable prediction of contrast enhancement in glioma patients across different medical centers, ultimately improving diagnostic accuracy and treatment planning options. 

<br /><br />Summary: <div>
arXiv:2509.10767v1 Announce Type: new 
Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but raise safety, cost, and accessibility concerns. Predicting contrast enhancement from non-contrast MRI using machine learning (ML) offers a safer alternative, as enhancement reflects tumor aggressiveness and informs treatment planning. Yet scanner and cohort variability hinder robust model selection. We propose a stability-aware framework to identify reproducible ML pipelines for multicenter prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG). Non-contrast T1WI served as input, with enhancement derived from paired post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were extracted and combined with 48 dimensionality reduction methods and 25 classifiers, yielding 1,200 pipelines. Rotational validation was trained on three datasets and tested on the fourth. Cross-validation prediction accuracies ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM), 0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1, precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr pipeline consistently ranked highest, balancing accuracy and stability. This framework demonstrates that stability-aware model selection enables reliable prediction of contrast enhancement from non-contrast glioma MRI, reducing reliance on GBCAs and improving generalizability across centers. It provides a scalable template for reproducible ML in neuro-oncology and beyond.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection</title>
<link>https://arxiv.org/abs/2509.10779</link>
<guid>https://arxiv.org/abs/2509.10779</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV imagery, object detection, post-processing framework, group evidence, precision-trade-off<br />
<br />
Summary: 
This paper introduces a post-processing framework designed to enhance object detection in UAV imagery. The framework focuses on addressing challenges such as missed objects due to long-range viewpoints, occlusion, and clutter. By converting overlap-induced redundancy into group evidence, the framework improves detection accuracy. Key components include overlapping tiling to recover low-confidence candidates, Spatial and Semantic Gates for validation, confidence reweighting for group calibration, and class-aware NMS fusion for improved precision. Experimental results on VisDrone dataset demonstrate a significant increase in recall and a trade-off in precision, benefiting applications that prioritize recall sensitivity. Ablation studies confirm the effectiveness of each component in enhancing detection performance. The framework seamlessly integrates with existing detectors without the need for retraining, with future work focusing on reducing semantic gating cost and incorporating temporal cues for further enhancement. <div>
arXiv:2509.10779v1 Announce Type: new 
Abstract: Dense small objects in UAV imagery are often missed due to long-range viewpoints, occlusion, and clutter[cite: 5]. This paper presents a detector-agnostic post-processing framework that converts overlap-induced redundancy into group evidence[cite: 6]. Overlapping tiling first recovers low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids) and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group evidence[cite: 7]. Validated groups receive controlled confidence reweighting before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to 0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per image[cite: 10]. These results indicate recall-first, precision-trade-off behavior that benefits recall-sensitive applications such as far-field counting and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects, spatial clustering stabilizes geometry, semantic clustering enforces appearance coherence, and reweighting provides calibrated integration with the baseline[cite: 11]. The framework requires no retraining and integrates with modern detectors[cite: 12]. Future work will reduce semantic gating cost and extend the approach with temporal cues[cite: 13].
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts</title>
<link>https://arxiv.org/abs/2509.10813</link>
<guid>https://arxiv.org/abs/2509.10813</guid>
<content:encoded><![CDATA[
<div> Keywords: Embodied AI, 3D scene datasets, InternScenes, simulatability, object collisions

Summary:
InternScenes is a new large-scale indoor scene dataset that addresses limitations in existing datasets by integrating real-world scans, procedurally generated scenes, and designer-created scenes. It contains approximately 40,000 diverse scenes with 1.96M 3D objects covering 15 common scene types and 288 object classes. The dataset preserves small items, leading to realistic layouts with an average of 41.5 objects per region. A comprehensive data processing pipeline ensures simulatability, enhances interactivity, and resolves object collisions through physical simulations. The dataset presents challenges for scene layout generation and point-goal navigation, showcasing the complexity of realistic layouts. InternScenes enables scaling up model training for these tasks, making generation and navigation in complex scenes possible. The data, models, and benchmarks are open-sourced to benefit the community. 

<br /><br />Summary: <div>
arXiv:2509.10813v1 Announce Type: new 
Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition</title>
<link>https://arxiv.org/abs/2509.10815</link>
<guid>https://arxiv.org/abs/2509.10815</guid>
<content:encoded><![CDATA[
<div> parameterized plane curve, polynomial representation, Legendre basis, computational cost, Chebyshev basis 

Summary:<br />
This article investigates the use of parameterized plane curve polynomial representations for mathematical handwriting, focusing on Legendre and Chebyshev bases. The study aims to determine the optimal basis and polynomial degree for accurate modeling with minimal computational cost. By analyzing the condition number for polynomial evaluation in different bases and bounding variations between symbols using inner products, the research examines the trade-offs between basis choice and computational efficiency. Previous work has shown promise with Legendre and Chebyshev bases, and this study delves deeper into understanding the impact of basis selection on the accuracy and computational complexity of mathematical handwriting representation. <div>
arXiv:2509.10815v1 Announce Type: new 
Abstract: Previous work has made use of a parameterized plane curve polynomial representation for mathematical handwriting, with the polynomials represented in a Legendre or Legendre-Sobolev graded basis. This provides a compact geometric representation for the digital ink. Preliminary results have also been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the trade-offs between basis choice and polynomial degree to achieve accurate modeling with a low computational cost. To do this, we consider the condition number for polynomial evaluation in these bases and bound how the various inner products give norms for the variations between symbols.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression</title>
<link>https://arxiv.org/abs/2509.10824</link>
<guid>https://arxiv.org/abs/2509.10824</guid>
<content:encoded><![CDATA[
<div> Keywords: Glioma, MRI, tumor progression, multitask diffusion framework, uncertainty quantification

Summary:
The study focuses on predicting glioma progression using a multitask diffusion framework that generates future FLAIR sequences and spatial probabilistic tumor evolution maps. The model incorporates a pretrained deformation module to capture temporal dynamics and addresses data scarcity through targeted augmentation. It allows clinicians to assess tumor progression risks at any future temporal milestone based on only two follow-up scans. Additionally, a radiotherapy-weighted focal loss term is introduced to leverage radiation dose maps during model training. The method was trained on a public dataset and evaluated on an internal private dataset, achieving promising results in both cases. This approach offers a valuable tool for accurate evolution prediction of glioma, addressing challenges posed by sparse and irregularly acquired longitudinal MRI data in clinical practice. 

<br /><br />Summary: <div>
arXiv:2509.10824v1 Announce Type: new 
Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and its poor prognosis, poses significant challenges for accurate evolution prediction. These challenges are exacerbated by sparse, irregularly acquired longitudinal MRI data in clinical practice, where incomplete follow-up sequences create data imbalances and make reliable modeling difficult. In this paper, we present a multitask diffusion framework for time-agnostic, pixel-wise prediction of glioma progression. The model simultaneously generates future FLAIR sequences at any chosen time point and estimates spatial probabilistic tumor evolution maps derived using signed distance fields (SDFs), allowing uncertainty quantification. To capture temporal dynamics of tumor evolution across arbitrary intervals, we integrate a pretrained deformation module that models inter-scan changes using deformation fields. Regarding the common clinical limitation of data scarcity, we implement a targeted augmentation pipeline that synthesizes complete sequences of three follow-up scans and imputes missing MRI modalities from available patient studies, improving the stability and accuracy of predictive models. Based on merely two follow-up scans at earlier timepoints, our framework produces flexible time-depending probability maps, enabling clinicians to interrogate tumor progression risks at any future temporal milestone. We further introduce a radiotherapy-weighted focal loss term that leverages radiation dose maps, as these highlight regions of greater clinical importance during model training. The proposed method was trained on a public dataset and evaluated on an internal private dataset, achieving promising results in both cases
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios</title>
<link>https://arxiv.org/abs/2509.10841</link>
<guid>https://arxiv.org/abs/2509.10841</guid>
<content:encoded><![CDATA[
<div> point cloud, semantic segmentation, LiDAR, 2D representations, data augmentation

Summary:
This paper introduces a new method for LiDAR point cloud semantic segmentation that leverages 2D representations through point-plane projections, leading to improved performance without the need for additional sensor data. By incorporating geometry-aware data augmentation techniques, the approach addresses class imbalance and enhances generalization in data-scarce scenarios. The method, named 3PNet, achieves significant improvements in limited-data settings and competitive results on standard datasets like SemanticKITTI and PandaSet. The code for 3PNet is available on GitHub, offering a valuable resource for researchers and practitioners in the field. <div>
arXiv:2509.10841v1 Announce Type: new 
Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D environments in applications such as autonomous driving and robotics. Recent methods achieve strong performance by exploiting different point cloud representations or incorporating data from other sensors, such as cameras or external datasets. However, these approaches often suffer from high computational complexity and require large amounts of training data, limiting their generalization in data-scarce scenarios. In this paper, we improve the performance of point-based methods by effectively learning features from 2D representations through point-plane projections, enabling the extraction of complementary information while relying solely on LiDAR data. Additionally, we introduce a geometry-aware technique for data augmentation that aligns with LiDAR sensor properties and mitigates class imbalance. We implemented and evaluated our method that applies point-plane projections onto multiple informative 2D representations of the point cloud. Experiments demonstrate that this approach leads to significant improvements in limited-data scenarios, while also achieving competitive results on two publicly available standard datasets, as SemanticKITTI and PandaSet. The code of our method is available at https://github.com/SiMoM0/3PNet
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title>
<link>https://arxiv.org/abs/2509.10842</link>
<guid>https://arxiv.org/abs/2509.10842</guid>
<content:encoded><![CDATA[
<div> Keywords: open-vocabulary, semantic segmentation, 3D urban scenes, point clouds, cross-scene generalization

Summary: 
OpenUrban3D is introduced as a framework for open-vocabulary semantic segmentation in large-scale urban scenes. It addresses challenges such as the lack of aligned multi-view images and poor generalization of existing 3D segmentation pipelines. The framework operates without pre-trained networks or manual annotations, generating semantic features directly from raw point clouds. Through multi-view rendering, mask-level vision-language feature extraction, and sample-balanced fusion, OpenUrban3D enables zero-shot segmentation for arbitrary text queries while capturing semantic richness and geometric priors. Extensive experiments on urban benchmarks demonstrate significant improvements in segmentation accuracy and cross-scene generalization compared to existing methods. OpenUrban3D offers a flexible and scalable solution for 3D urban scene understanding. 

<br /><br />Summary: <div>
arXiv:2509.10842v1 Announce Type: new 
Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoOEP -- A Multi-modal Framework for Online Exam Proctoring</title>
<link>https://arxiv.org/abs/2509.10887</link>
<guid>https://arxiv.org/abs/2509.10887</guid>
<content:encoded><![CDATA[
<div> computer vision, machine learning, online exam proctoring, automated proctoring, cheating detection <br />
<br />
Summary: <br />
AutoOEP is a cutting-edge automated online exam proctoring system utilizing computer vision and machine learning to ensure academic integrity during remote examinations. The system employs a dual-camera setup to monitor both the examinee's frontal view and workspace, detecting suspicious cues such as identity verification, head pose, gaze tracking, and hand movements. By integrating a Face Module for identity verification and a Hand Module for object detection, including prohibited items, AutoOEP can accurately classify suspicious activities with 90.7% accuracy. Additionally, the system processes video streams at 2.4 frames per second without the need for a GPU, making it both effective and resource-efficient. Overall, AutoOEP provides a robust solution for automated proctoring, reducing the reliance on human intervention and enhancing the integrity of online assessments. <br /> <div>
arXiv:2509.10887v1 Announce Type: new 
Abstract: The burgeoning of online education has created an urgent need for robust and scalable systems to ensure academic integrity during remote examinations. Traditional human proctoring is often not feasible at scale, while existing automated solutions can be intrusive or fail to detect a wide range of cheating behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a comprehensive, multi-modal framework that leverages computer vision and machine learning to provide effective, automated proctoring. The system utilizes a dual-camera setup to capture both a frontal view of the examinee and a side view of the workspace, minimizing blind spots. Our approach integrates several parallel analyses: the Face Module performs continuous identity verification using ArcFace, along with head pose estimation, gaze tracking, and mouth movement analysis to detect suspicious cues. Concurrently, the Hand Module employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile phones, notes) and tracks hand proximity to these objects. Features from these modules are aggregated and fed into a Long Short-Term Memory (LSTM) network that analyzes temporal patterns to calculate a real-time cheating probability score. We evaluate AutoOEP on a custom-collected dataset simulating diverse exam conditions. Our system achieves an accuracy of 90.7% in classifying suspicious activities. The object detection component obtains a mean Average Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework processes video streams at approximately 2.4 frames per second without a GPU. The results demonstrate that AutoOEP is an effective and resource-efficient solution for automated proctoring, significantly reducing the need for human intervention and enhancing the integrity of online assessments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System</title>
<link>https://arxiv.org/abs/2509.10897</link>
<guid>https://arxiv.org/abs/2509.10897</guid>
<content:encoded><![CDATA[
<div> CASSI, spectral imaging, dual-camera, total variation, subgradient <br />
Summary: <br />
This article introduces a novel dual-camera Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction framework that integrates total variation (TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical model, the computational complexity of solving the inverse problem is reduced. A dynamic regularization strategy is introduced, incorporating normalized gradient constraints from RGB/panchromatic-derived reference images to construct a TV subgradient similarity function. The framework includes an adaptive reference generation and updating mechanism for subgradient guidance from spatial priors of auxiliary cameras. Experimental results demonstrate the method's effectiveness in preserving spatial-spectral structural consistency. The proposed method provides a mathematically well-founded framework for analyzing multi-camera systems in computational spectral imaging and demonstrates robust performance across various reconstruction scenarios. The source code is available for further exploration and implementation. <div>
arXiv:2509.10897v1 Announce Type: new 
Abstract: Spectral imaging technology has long-faced fundamental challenges in balancing spectral, spatial, and temporal resolutions. While compressive sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this trade-off through optical encoding, high compression ratios result in ill-posed reconstruction problems. Traditional model-based methods exhibit limited performance due to reliance on handcrafted inherent image priors, while deep learning approaches are constrained by their black-box nature, which compromises physical interpretability. To address these limitations, we propose a dual-camera CASSI reconstruction framework that integrates total variation (TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical model, we reduce the computational complexity of solving the inverse problem and provide a mathematically well-founded framework for analyzing multi-camera systems. A dynamic regularization strategy is introduced, incorporating normalized gradient constraints from RGB/panchromatic-derived reference images, which constructs a TV subgradient similarity function with strict convex optimization guarantees. Leveraging spatial priors from auxiliary cameras, an adaptive reference generation and updating mechanism is designed to provide subgradient guidance. Experimental results demonstrate that the proposed method effectively preserves spatial-spectral structural consistency. The theoretical framework establishes an interpretable mathematical foundation for computational spectral imaging, demonstrating robust performance across diverse reconstruction scenarios. The source code is available at https://github.com/bestwishes43/ADMM-TVDS.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation</title>
<link>https://arxiv.org/abs/2509.10919</link>
<guid>https://arxiv.org/abs/2509.10919</guid>
<content:encoded><![CDATA[
<div> compact architectures, Earth Observation, Metadata-aware Mixture-of-Experts Masked Autoencoder, transfer learning, efficient

Summary:
The study introduces a compact architecture called Metadata-aware Mixture-of-Experts Masked Autoencoder (MoE-MAE) for Earth Observation models. With only 2.5M parameters, the model incorporates sparse expert routing with geo-temporal conditioning using imagery and metadata. Pretrained on the BigEarthNet-Landsat dataset, the MoE-MAE achieves competitive performance with larger models in transfer learning tasks, showcasing improved efficiency. Despite lacking explicit metadata, the model demonstrates strong generalization on the EuroSAT-Landsat dataset. The results suggest that compact, metadata-aware MoE-MAEs are a promising approach for future Earth Observation foundation models. 

<br /><br />Summary: <div>
arXiv:2509.10919v1 Announce Type: new 
Abstract: Recent advances in Earth Observation have focused on large-scale foundation models. However, these models are computationally expensive, limiting their accessibility and reuse for downstream tasks. In this work, we investigate compact architectures as a practical pathway toward smaller general-purpose EO models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder (MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing with geo-temporal conditioning, incorporating imagery alongside latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen encoder using linear probes. Despite its small size, the model competes with much larger architectures, demonstrating that metadata-aware pretraining improves transfer and label efficiency. To further assess generalization, we evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and still observe competitive performance compared to models with hundreds of millions of parameters. These results suggest that compact, metadata-aware MoE-MAEs are an efficient and scalable step toward future EO foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</title>
<link>https://arxiv.org/abs/2509.10961</link>
<guid>https://arxiv.org/abs/2509.10961</guid>
<content:encoded><![CDATA[
<div> motion artifacts, bone microstructures, HR-pQCT, deep learning, motion correction <br />
<br />Summary: 
This study focuses on addressing rigid-motion artifacts in high-resolution peripheral quantitative computed tomography (HR-pQCT) images, specifically in bone microstructures. The lack of standardized degradation models has hindered the development of motion correction methods. The researchers optimized a sinogram-based method to simulate motion artifacts and created paired datasets for supervised learning. They proposed an Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to correct motion artifacts in both simulated and real-world datasets. The model includes edge-enhancing skip connections, self-attention mechanisms, and a perceptual loss function to preserve trabecular edges and fine micro-structural features. The ESWGAN-GP demonstrated improved signal-to-noise ratio (SNR), structural similarity index measure (SSIM), and visual information fidelity (VIF) for both source and target datasets. While the simulated motion artifacts may not fully represent real-world complexities, these methods provide a promising approach to implementing deep learning-based motion correction in HR-pQCT. <div>
arXiv:2509.10961v1 Announce Type: new 
Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular smearing, hinder in vivo assessment of bone microstructures in high-resolution peripheral quantitative computed tomography (HR-pQCT). Despite various motion grading techniques, no motion correction methods exist due to the lack of standardized degradation models. We optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, creating paired datasets of motion-corrupted images and their corresponding ground truth, which enables seamless integration into supervised learning frameworks for motion correction. As such, we propose an Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts in both simulated (source) and real-world (target) datasets. The model incorporates edge-enhancing skip connections to preserve trabecular edges and self-attention mechanisms to capture long-range dependencies, facilitating motion correction. A visual geometry group (VGG)-based perceptual loss is used to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean signal-to-noise ratio (SNR) of 26.78, structural similarity index measure (SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source dataset, while showing improved performance on the target dataset with an SNR of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a simplified representation of real-world motion that may not fully capture the complexity of in vivo motion artifacts. Nevertheless, because motion artifacts present one of the foremost challenges to more widespread adoption of this modality, these methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze Authentication: Factors Influencing Authentication Performance</title>
<link>https://arxiv.org/abs/2509.10969</link>
<guid>https://arxiv.org/abs/2509.10969</guid>
<content:encoded><![CDATA[
<div> calibration, eye tracking, authentication performance, neural network, signal quality  
Summary:  
- The study focused on factors influencing gaze-based authentication performance using a large dataset and neural network architecture.  
- Various factors like eye tracking signal quality, calibration target depth, and gaze filtering were analyzed.  
- Using the same calibration target depth and fusing calibrated and non-calibrated gaze improved authentication performance.  
- Enhancing eye tracking signal quality was also beneficial for authentication.  
- A simple moving average filter slightly reduced authentication performance, with some exceptions noted.  

<br /><br />Summary: <div>
arXiv:2509.10969v1 Announce Type: new 
Abstract: This paper examines the key factors that influence the performance of state-of-the-art gaze-based authentication. Experiments were conducted on a large-scale, in-house dataset comprising 8,849 subjects collected with Meta Quest Pro equivalent hardware running a video oculography-driven gaze estimation pipeline at 72Hz. The state-of-the-art neural network architecture was employed to study the influence of the following factors on authentication performance: eye tracking signal quality, various aspects of eye tracking calibration, and simple filtering on estimated raw gaze. We found that using the same calibration target depth for eye tracking calibration, fusing calibrated and non-calibrated gaze, and improving eye tracking signal quality all enhance authentication performance. We also found that a simple three-sample moving average filter slightly reduces authentication performance in general. While these findings hold true for the most part, some exceptions were noted.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation</title>
<link>https://arxiv.org/abs/2509.10980</link>
<guid>https://arxiv.org/abs/2509.10980</guid>
<content:encoded><![CDATA[
<div> skin tone recognition, skin tone generation, TrueSkin dataset, model fairness, image analysis<br />
Summary:<br />
The article introduces the TrueSkin dataset containing 7299 images categorized into 6 classes for accurate skin tone recognition and generation. It addresses the challenges faced by existing large multimodal models (LMMs) and image generation models in accurately recognizing and synthesizing skin tones. Existing models show biases, with LMMs misclassifying intermediate skin tones and generative models struggling to produce specified skin tones. TrueSkin serves as a benchmark for evaluating models and training resource, significantly improving classification accuracy and skin tone fidelity in image generation. The dataset showcases the importance of comprehensive datasets in enhancing fairness and accuracy in skin tone recognition and generation tasks. <div>
arXiv:2509.10980v1 Announce Type: new 
Abstract: Skin tone recognition and generation play important roles in model fairness, healthcare, and generative AI, yet they remain challenging due to the lack of comprehensive datasets and robust methodologies. Compared to other human image analysis tasks, state-of-the-art large multimodal models (LMMs) and image generation models struggle to recognize and synthesize skin tones accurately. To address this, we introduce TrueSkin, a dataset with 7299 images systematically categorized into 6 classes, collected under diverse lighting conditions, camera angles, and capture settings. Using TrueSkin, we benchmark existing recognition and generation approaches, revealing substantial biases: LMMs tend to misclassify intermediate skin tones as lighter ones, whereas generative models struggle to accurately produce specified skin tones when influenced by inherent biases from unrelated attributes in the prompts, such as hairstyle or environmental context. We further demonstrate that training a recognition model on TrueSkin improves classification accuracy by more than 20\% compared to LMMs and conventional approaches, and fine-tuning with TrueSkin significantly improves skin tone fidelity in image generation models. Our findings highlight the need for comprehensive datasets like TrueSkin, which not only serves as a benchmark for evaluating existing models but also provides a valuable training resource to enhance fairness and accuracy in skin tone recognition and generation tasks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring</title>
<link>https://arxiv.org/abs/2509.10995</link>
<guid>https://arxiv.org/abs/2509.10995</guid>
<content:encoded><![CDATA[
<div> Keywords: animal health monitoring, population management, UAV-based systems, transfer learning, reinforcement learning

Summary:
This paper addresses the challenge of limited labeled training data in developing effective deep learning models for animal detection using UAV-based systems. The proposed reinforcement learning-based transfer learning framework utilizes an upper confidence bound algorithm to automatically select the most suitable pre-trained model for animal detection tasks. By systematically evaluating and ranking candidate models based on performance, the framework streamlines the model selection process. Experimental results show that the framework achieves a higher detection rate while requiring significantly less computational time compared to traditional methods. This approach has the potential to improve animal health monitoring and population management in wildlife conservation and livestock management. <div>
arXiv:2509.10995v1 Announce Type: new 
Abstract: Animal health monitoring and population management are critical aspects of wildlife conservation and livestock management that increasingly rely on automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV) based systems combined with computer vision offer promising solutions for non-invasive animal monitoring across challenging terrains, limited availability of labeled training data remains an obstacle in developing effective deep learning (DL) models for these applications. Transfer learning has emerged as a potential solution, allowing models trained on large datasets to be adapted for resource-limited scenarios such as those with limited data. However, the vast landscape of pre-trained neural network architectures makes it challenging to select optimal models, particularly for researchers new to the field. In this paper, we propose a reinforcement learning (RL)-based transfer learning framework that employs an upper confidence bound (UCB) algorithm to automatically select the most suitable pre-trained model for animal detection tasks. Our approach systematically evaluates and ranks candidate models based on their performance, streamlining the model selection process. Experimental results demonstrate that our framework achieves a higher detection rate while requiring significantly less computational time compared to traditional methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fungi Prototype Representations for Few-Shot Classification</title>
<link>https://arxiv.org/abs/2509.11020</link>
<guid>https://arxiv.org/abs/2509.11020</guid>
<content:encoded><![CDATA[
<div> competition, fungal species recognition, deep learning, prototypical networks, biodiversity monitoring 
Summary:
The FungiCLEF 2025 competition focuses on automating fungal species recognition using field-collected data. Identifying fungi accurately is crucial for mycologists and citizen scientists for biodiversity monitoring. The competition faces challenges like imbalanced class distributions and limited training samples, especially for rare species. A proposed deep learning method based on prototypical networks improves few-shot fungal classification by enhancing prototype representations. The approach surpasses the competition baseline by over 30 percentage points in Recall@5 on both public and private leaderboards. This technique shows promise in accurately identifying both common and rarely recorded fungal species, aligning with the goals of FungiCLEF 2025. 
<br /><br />Summary: <div>
arXiv:2509.11020v1 Announce Type: new 
Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal species recognition using realistic, field-collected observational data. Accurate identification tools support both mycologists and citizen scientists, greatly enhancing large-scale biodiversity monitoring. Effective recognition systems in this context must handle highly imbalanced class distributions and provide reliable performance even when very few training samples are available for many species, especially rare and under-documented taxa that are often missing from standard training sets. According to competition organizers, about 20\% of all verified fungi observations, representing nearly 20,000 instances, are associated with these rarely recorded species. To tackle this challenge, we propose a robust deep learning method based on prototypical networks, which enhances prototype representations for few-shot fungal classification. Our prototypical network approach exceeds the competition baseline by more than 30 percentage points in Recall@5 on both the public (PB) and private (PR) leaderboards. This demonstrates strong potential for accurately identifying both common and rare fungal species, supporting the main objectives of FungiCLEF 2025.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images</title>
<link>https://arxiv.org/abs/2509.11034</link>
<guid>https://arxiv.org/abs/2509.11034</guid>
<content:encoded><![CDATA[
<div> Instance redundancy, unordered collections, global-local clustering, cluster-level sparsity, computational pathology <br />
Summary: 
- The proposed Cluster-level Sparse MIL (csMIL) framework addresses challenges in Multi-Instance Learning for analyzing weakly labeled datasets like whole-slide images in computational pathology.
- csMIL integrates global-local instance clustering, within-cluster attention, and cluster-level sparsity induction to enhance robustness and interpretability.
- It performs global clustering across all bags, followed by local clustering within each bag, computing attention scores within clusters and applying sparse regularization to cluster weights.
- Theoretical analysis shows csMIL requires O(s log K) bags to recover relevant clusters, aligning with compressed sensing principles.
- Empirically, csMIL achieves state-of-the-art performance on two public histopathology benchmarks (CAMELYON16, TCGA-NSCLC).<br /><br />Summary: <div>
arXiv:2509.11034v1 Announce Type: new 
Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly labeled datasets, such as whole-slide images (WSIs) in computational pathology, where bags comprise unordered collections of instances with sparse diagnostic relevance. Traditional MIL approaches, including early statistical methods and recent attention-based frameworks, struggle with instance redundancy and lack explicit mechanisms for discarding non-informative instances, limiting their robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a novel framework that integrates global-local instance clustering, within-cluster attention, and cluster-level sparsity induction to address these challenges. Our csMIL first performs global clustering across all bags to establish $K$ cluster centers, followed by local clustering within each bag to assign cluster labels. Attention scores are computed within each cluster, and sparse regularization is applied to cluster weights, enabling the selective retention of diagnostically relevant clusters while discarding irrelevant ones. This approach enhances robustness to noisy instances, improves interpretability by identifying critical regions, and reduces computational complexity. Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to recover $s$ relevant clusters, aligning with compressed sensing principles. Empirically, csMIL achieves state-of-the-art performance on two public histopathology benchmarks (CAMELYON16, TCGA-NSCLC).
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.11058</link>
<guid>https://arxiv.org/abs/2509.11058</guid>
<content:encoded><![CDATA[
<div> Zero-Shot Video Anomaly Detection, Skeleton-based approach, Action typicality, Uniqueness learning, State-of-the-art results<br />
<br />
Summary: <br />
Zero-Shot Video Anomaly Detection (ZS-VAD) is a critical task in surveillance with privacy concerns. This paper introduces a novel approach using skeleton data and language-guided semantic typicality modeling to learn normal and abnormal behaviors. It also proposes a test-time context uniqueness analysis module to adapt to scene-specific boundaries. By leveraging these techniques, the method achieves state-of-the-art results on various VAD datasets without the need for target domain training data. <div>
arXiv:2509.11058v1 Announce Type: new 
Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing anomalies without target domain training data, which is a crucial task due to various practical concerns, e.g., data privacy or new surveillance deployments. Skeleton-based approach has inherent generalizable advantages in achieving ZS-VAD as it eliminates domain disparities both in background and human appearance. However, existing methods only learn low-level skeleton representation and rely on the domain-limited normality boundary, which cannot generalize well to new scenes with different normal and abnormal behavior patterns. In this paper, we propose a novel zero-shot video anomaly detection framework, unlocking the potential of skeleton data via action typicality and uniqueness learning. Firstly, we introduce a language-guided semantic typicality modeling module that projects skeleton snippets into action semantic space and distills LLM's knowledge of typical normal and abnormal behaviors during training. Secondly, we propose a test-time context uniqueness analysis module to finely analyze the spatio-temporal differences between skeleton snippets and then derive scene-adaptive boundaries. Without using any training samples from the target domain, our method achieves state-of-the-art results against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech, UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos</title>
<link>https://arxiv.org/abs/2509.11063</link>
<guid>https://arxiv.org/abs/2509.11063</guid>
<content:encoded><![CDATA[
<div> Keywords: organoid, kidney disease, drug discovery, polycystic kidney disease, automated analysis

Summary: 
The article introduces Organoid Tracker, a graphical user interface platform designed for efficient screening in polycystic kidney disease using kidney organoids. This platform enables automated analysis of spatial-temporal microscopy videos, extracting detailed quantitative metrics without the need for programming expertise. Organoid Tracker, built on the Segment Anything Model 2, offers zero-shot segmentation and quantifies key metrics such as cyst formation rate, growth velocity, and morphological changes, providing comprehensive reports. With an open-source framework, Organoid Tracker aims to accelerate research in kidney development, PKD modeling, and therapeutic discovery. This platform revolutionizes the study of human kidney disease mechanisms by enabling scalable, cost-effective research without the need for animal sacrifice. Organoid Tracker is publicly available as open-source software, providing a powerful solution for improving research in kidney development and PKD modeling. <br /><br />Summary: <div>
arXiv:2509.11063v1 Announce Type: new 
Abstract: Recent advances in organoid models have revolutionized the study of human kidney disease mechanisms and drug discovery by enabling scalable, cost-effective research without the need for animal sacrifice. Here, we present a kidney organoid platform optimized for efficient screening in polycystic kidney disease (PKD). While these systems generate rich spatial-temporal microscopy video datasets, current manual approaches to analysis remain limited to coarse classifications (e.g., hit vs. non-hit), often missing valuable pixel-level and longitudinal information. To help overcome this bottleneck, we developed Organoid Tracker, a graphical user interface (GUI) platform designed with a modular plugin architecture, which empowers researchers to extract detailed, quantitative metrics without programming expertise. Built on the cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid Tracker enables zero-shot segmentation and automated analysis of spatial-temporal microscopy videos. It quantifies key metrics such as cyst formation rate, growth velocity, and morphological changes, while generating comprehensive reports. By providing an extensible, open-source framework, Organoid Tracker offers a powerful solution for improving and accelerating research in kidney development, PKD modeling, and therapeutic discovery. The platform is publicly available as open-source software at https://github.com/hrlblab/OrganoidTracker.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge</title>
<link>https://arxiv.org/abs/2509.11071</link>
<guid>https://arxiv.org/abs/2509.11071</guid>
<content:encoded><![CDATA[
<div> vision language model, autonomous driving, LLaVA models, DriveLM-nuScenes dataset, depth information, Chain-of-Thought reasoning

Summary: 
The report discusses the utilization of vision language model systems for the CVPR 2024 Autonomous Grand Challenge, specifically focusing on the Driving with Language track. The systems were built on LLaVA models and enhanced through fine-tuning with LoRA and DoRA methods, utilizing the DriveLM-nuScenes dataset for training. Open-source depth estimation models were integrated to enrich the training and inference processes. A Chain-of-Thought reasoning approach was adopted for inference, particularly with multiple-choice and yes/no questions, to improve accuracy. This comprehensive methodology led to achieving a top score of 0.7799 on the validation set leaderboard, ranking 1st place. <div>
arXiv:2509.11071v1 Announce Type: new 
Abstract: This report outlines our approach using vision language model systems for the Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We have exclusively utilized the DriveLM-nuScenes dataset for training our models. Our systems are built on the LLaVA models, which we enhanced through fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated depth information from open-source depth estimation models to enrich the training and inference processes. For inference, particularly with multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning approach to improve the accuracy of the results. This comprehensive methodology enabled us to achieve a top score of 0.7799 on the validation set leaderboard, ranking 1st on the leaderboard.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation</title>
<link>https://arxiv.org/abs/2509.11082</link>
<guid>https://arxiv.org/abs/2509.11082</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal framework, traversability costmaps, planetary rovers, self-supervised training, LiDAR data

Summary: 
This article presents a robust multi-modal framework for predicting traversability costmaps for planetary rovers, utilizing a fusion of camera and LiDAR data to generate bird's-eye-view terrain costmaps. The model is trained using self-supervised labels derived from IMU data. Updates to the model include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations show that the model is highly robust, with minimal changes in performance when altering inputs. The small performance differences are attributed to the IMU labeling primarily reflecting terrain geometry rather than semantics and the limited diversity of the dataset. The article emphasizes the contributions of a high-fidelity simulation environment, a self-supervised IMU-based labeling pipeline, and a strong multi-modal costmap prediction model. Limitations and future work, such as domain generalization and dataset expansion, are also discussed.<br /><br />Summary: <div>
arXiv:2509.11082v1 Announce Type: new 
Abstract: We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Visual Autonomous Parking via Control-Aided Attention</title>
<link>https://arxiv.org/abs/2509.11090</link>
<guid>https://arxiv.org/abs/2509.11090</guid>
<content:encoded><![CDATA[
<div> Imitation learning, end-to-end system, attention mechanism, self-supervised learning, deep reinforcement learning <br />
Summary:
The paper introduces CAA-Policy, an end-to-end imitation learning system for precise parking that integrates a novel Control-Aided Attention mechanism. This mechanism allows the control signal to guide the learning of visual attention, resulting in more stable and reliable policy decisions. The attention module is trained in a self-supervised manner, enabling it to focus on visual features that influence action outputs. Additionally, CAA-Policy incorporates short-horizon waypoint prediction as an auxiliary task and a motion prediction module to track the target spot over time robustly. Experimental results in the CARLA simulator demonstrate that CAA-Policy outperforms both baseline end-to-end learning and a modular BEV segmentation + hybrid A* pipeline in terms of accuracy, robustness, and interpretability. The code for CAA-Policy is publicly available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2509.11090v1 Announce Type: new 
Abstract: Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at https://github.com/Joechencc/CAAPolicy.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation</title>
<link>https://arxiv.org/abs/2509.11092</link>
<guid>https://arxiv.org/abs/2509.11092</guid>
<content:encoded><![CDATA[
<div> rank, adaptation, panoramic video generation, LoRA, visual quality

Summary:
The article discusses the challenge of generating high-quality 360-degree panoramic videos due to the differences between panoramic and traditional perspective-view projections. Existing solutions often require complex architectures or large-scale training, resulting in suboptimal results. The proposed approach treats panoramic video generation as an adaptation problem from perspective views, utilizing Low-Rank Adaptation (LoRA) to model the transformation between projections. Through theoretical analysis, it is shown that LoRA can effectively handle the task when its rank exceeds the degrees of freedom. The method efficiently fine-tunes a pretrained video diffusion model using approximately 1,000 videos, achieving high-quality panoramic generation. Experimental results demonstrate that the approach maintains proper projection geometry, surpassing previous state-of-the-art methods in visual quality, left-right consistency, and motion diversity. 

<br /><br />Summary: <div>
arXiv:2509.11092v1 Announce Type: new 
Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant challenge due to the fundamental differences between panoramic and traditional perspective-view projections. While perspective videos rely on a single viewpoint with a limited field of view, panoramic content requires rendering the full surrounding environment, making it difficult for standard video generation models to adapt. Existing solutions often introduce complex architectures or large-scale training, leading to inefficiency and suboptimal results. Motivated by the success of Low-Rank Adaptation (LoRA) in style transfer tasks, we propose treating panoramic video generation as an adaptation problem from perspective views. Through theoretical analysis, we demonstrate that LoRA can effectively model the transformation between these projections when its rank exceeds the degrees of freedom in the task. Our approach efficiently fine-tunes a pretrained video diffusion model using only approximately 1,000 videos while achieving high-quality panoramic generation. Experimental results demonstrate that our method maintains proper projection geometry and surpasses previous state-of-the-art approaches in visual quality, left-right consistency, and motion diversity.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing</title>
<link>https://arxiv.org/abs/2509.11093</link>
<guid>https://arxiv.org/abs/2509.11093</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral unmixing, super-resolution, multitask learning, task affinity, convergence

Summary: 
The article discusses the limitations of hyperspectral unmixing due to low spatial resolution and proposes a solution using super-resolution in a multitask learning approach. The main challenges addressed are the lack of task affinity verification and the guaranteeing of unmixing convergence. The proposed method, Super-resolution guided Multi-task Learning for Hyperspectral Unmixing (SMILE), is supported by theoretical analysis that demonstrates the feasibility of multitask learning and task affinity. The framework incorporates positive guidance from super-resolution to enhance unmixing by learning shared and specific representations. Additionally, the proposed theoretical support ensures the convergence of unmixing by proving the optimal solution. Experimental results on synthetic and real datasets validate the effectiveness of the SMILE approach. 

<br /><br />Summary: <div>
arXiv:2509.11093v1 Announce Type: new 
Abstract: The performance of hyperspectral unmixing may be constrained by low spatial resolution, which can be enhanced using super-resolution in a multitask learning way. However, integrating super-resolution and unmixing directly may suffer two challenges: Task affinity is not verified, and the convergence of unmixing is not guaranteed. To address the above issues, in this paper, we provide theoretical analysis and propose super-resolution guided multi-task learning method for hyperspectral unmixing (SMILE). The provided theoretical analysis validates feasibility of multitask learning way and verifies task affinity, which consists of relationship and existence theorems by proving the positive guidance of super-resolution. The proposed framework generalizes positive information from super-resolution to unmixing by learning both shared and specific representations. Moreover, to guarantee the convergence, we provide the accessibility theorem by proving the optimal solution of unmixing. The major contributions of SMILE include providing progressive theoretical support, and designing a new framework for unmixing under the guidance of super-resolution. Our experiments on both synthetic and real datasets have substantiate the usefulness of our work.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing</title>
<link>https://arxiv.org/abs/2509.11096</link>
<guid>https://arxiv.org/abs/2509.11096</guid>
<content:encoded><![CDATA[
<div> Keywords: multitemporal hyperspectral unmixing, copula theory, temporal dependency, endmembers, abundances

Summary: 
This paper introduces a novel method called Cog-TD for multitemporal hyperspectral unmixing. The method leverages copula theory to explicitly model temporal dependencies, addressing limitations of existing approaches in capturing dynamic material evolution. Cog-TD defines a mathematical model that incorporates copula theory to describe temporal dependency structure and constructs a copula-guided framework with two key modules for endmember and abundance estimation. Theoretical support confirms the validity of the estimated copula function and the existence of temporal dependencies in hyperspectral images. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed method in capturing and utilizing temporal information for multitemporal hyperspectral unmixing. <br /><br />Summary: <div>
arXiv:2509.11096v1 Announce Type: new 
Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers and dynamical abundances, which emphasizes the critical temporal information. However, existing methods have limitations in modeling temporal dependency, thus fail to capture the dynamical material evolution. Motivated by the ability of copula theory in modeling dependency structure explicitly, in this paper, we propose a copula-guided temporal dependency method (Cog-TD) for multitemporal hyperspectral unmixing. Cog-TD defines new mathematical model, constructs copula-guided framework and provides two key modules with theoretical support. The mathematical model provides explicit formulations for MTHU problem definition, which describes temporal dependency structure by incorporating copula theory. The copula-guided framework is constructed for utilizing copula function, which estimates dynamical endmembers and abundances with temporal dependency. The key modules consist of copula function estimation and temporal dependency guidance, which computes and employs temporal information to guide unmixing process. Moreover, the theoretical support demonstrates that estimated copula function is valid and the represented temporal dependency exists in hyperspectral images. The major contributions of this paper include redefining MTHU problem with temporal dependency, proposing a copula-guided framework, developing two key modules and providing theoretical support. Our experimental results on both synthetic and real-world datasets demonstrate the utility of the proposed method.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</title>
<link>https://arxiv.org/abs/2509.11097</link>
<guid>https://arxiv.org/abs/2509.11097</guid>
<content:encoded><![CDATA[
<div> Dataset, 3D semantic segmentation, disaster response, UAVs, post-disaster assessment
<br />
Summary:
<br />
The article introduces a new 3D benchmark dataset called 3DAeroRelief, specifically designed for post-disaster assessment. This dataset was collected using low-cost UAVs over hurricane-damaged regions and features dense 3D point clouds with semantic annotations for fine-grained structural damage analysis. Compared to existing datasets, 3DAeroRelief focuses on large-scale outdoor environments in real-world disaster contexts. It highlights the advantages of using UAVs for data collection in hazardous areas, making them ideal for emergency scenarios. The article also evaluates state-of-the-art 3D segmentation models on the dataset, demonstrating the challenges and opportunities of 3D scene understanding in disaster response. Overall, the 3DAeroRelief dataset serves as a valuable resource for advancing 3D vision systems for post-disaster scenarios. 
<br /> <div>
arXiv:2509.11097v1 Announce Type: new 
Abstract: Timely assessment of structural damage is critical for disaster response and recovery. However, most prior work in natural disaster analysis relies on 2D imagery, which lacks depth, suffers from occlusions, and provides limited spatial context. 3D semantic segmentation offers a richer alternative, but existing 3D benchmarks focus mainly on urban or indoor scenes, with little attention to disaster-affected areas. To address this gap, we present 3DAeroRelief--the first 3D benchmark dataset specifically designed for post-disaster assessment. Collected using low-cost unmanned aerial vehicles (UAVs) over hurricane-damaged regions, the dataset features dense 3D point clouds reconstructed via Structure-from-Motion and Multi-View Stereo techniques. Semantic annotations were produced through manual 2D labeling and projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D large-scale outdoor environments with fine-grained structural damage in real-world disaster contexts. UAVs enable affordable, flexible, and safe data collection in hazardous areas, making them particularly well-suited for emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate several state-of-the-art 3D segmentation models on the dataset to highlight both the challenges and opportunities of 3D scene understanding in disaster response. Our dataset serves as a valuable resource for advancing robust 3D vision systems in real-world applications for post-disaster scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.11102</link>
<guid>https://arxiv.org/abs/2509.11102</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal learning, remote sensing, generative models, semantic segmentation, GEMMNet

Summary:
Generative models like AutoEncoder (AE) and Generative Adversarial Network (GAN) have been used to reconstruct missing modality data in multimodal remote sensing scenarios, but they struggle to capture complex semantic context and can introduce bias. In response, this paper introduces the Generative-Enhanced MultiModal learning Network (GEMMNet). GEMMNet includes a Hybrid Feature Extractor (HyFEx) to learn modality-specific representations, Hybrid Fusion with Multiscale Awareness (HyFMA) for capturing semantic context across scales, and a Complementary Loss (CoLoss) scheme to reduce bias by promoting consistency across modalities and tasks. The proposed GEMMNet outperforms traditional generative models and state-of-the-art non-generative approaches on challenging remote sensing datasets like Vaihingen and Potsdam. The source code for GEMMNet is also provided for further research and development. 

<br /><br />Summary: <div>
arXiv:2509.11102v1 Announce Type: new 
Abstract: Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild</title>
<link>https://arxiv.org/abs/2509.11114</link>
<guid>https://arxiv.org/abs/2509.11114</guid>
<content:encoded><![CDATA[
<div> pipeline, dynamic 3D smoke assets, in-the-wild video, interactive simulation, smoke design

Summary:
The article introduces a pipeline for extracting and reconstructing dynamic 3D smoke assets from single in-the-wild videos. It addresses challenges in reconstructing smoke in real-world videos through techniques such as smoke extraction, initialization of smoke particles and camera poses, and inferring multi-view videos. The proposed method surpasses previous methods in smoke reconstruction quality and enables realistic editing of fluid dynamics through smoke asset simulation. The authors provide models, data, and 4D smoke assets on their website. <div>
arXiv:2509.11114v1 Announce Type: new 
Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from a single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at [https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11116</link>
<guid>https://arxiv.org/abs/2509.11116</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, novel view synthesis, SVR-GS, spatial mask aggregation, real-time applications

Summary: 
SVR-GS introduces a spatially variant regularizer for optimizing the number of Gaussians in 3D Gaussian Splatting (3DGS) to improve novel view synthesis. This regularizer generates per-pixel spatial masks from each Gaussian's contribution along the ray, allowing for sparsity pressure on low-importance Gaussians. Multiple spatial mask aggregation strategies are explored and implemented in CUDA, with gradient analysis supporting the final design choice. Experimental results on various datasets show that SVR-GS reduces the number of Gaussians significantly compared to existing methods while maintaining good image quality, leading to smaller, faster, and more memory-efficient models. These improvements make SVR-GS suitable for real-time applications like robotics, AR/VR, and mobile perception. <div>
arXiv:2509.11116v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\&amp;Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and 5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images</title>
<link>https://arxiv.org/abs/2509.11164</link>
<guid>https://arxiv.org/abs/2509.11164</guid>
<content:encoded><![CDATA[
<div> Keywords: reef monitoring, coral growth, 3D volume estimation, surface area estimation, deep learning<br />
Summary:<br />
- A novel learning framework is proposed for estimating the 3D volume and surface area of coral-like objects from 2D multi-view RGB images.
- The approach utilizes a pre-trained module to extract dense point maps from each view and merges them into a unified point cloud enriched with per-view confidence scores.
- Two parallel DGCNN decoder heads are used to jointly output the volume and surface area of the coral, along with corresponding confidence estimates.
- A composite loss function based on Gaussian negative log-likelihood in both real and log domains is introduced to enhance prediction stability and provide uncertainty estimates.
- The method achieves competitive accuracy and generalizes well to unseen morphologies, offering potential applications in coral growth analysis and reef monitoring.<br /> 

Summary: <div>
arXiv:2509.11164v1 Announce Type: new 
Abstract: Effective reef monitoring requires the quantification of coral growth via accurate volumetric and surface area estimates, which is a challenging task due to the complex morphology of corals. We propose a novel, lightweight, and scalable learning framework that addresses this challenge by predicting the 3D volume and surface area of coral-like objects from 2D multi-view RGB images. Our approach utilizes a pre-trained module (VGGT) to extract dense point maps from each view; these maps are merged into a unified point cloud and enriched with per-view confidence scores. The resulting cloud is fed to two parallel DGCNN decoder heads, which jointly output the volume and the surface area of the coral, as well as their corresponding confidence estimate. To enhance prediction stability and provide uncertainty estimates, we introduce a composite loss function based on Gaussian negative log-likelihood in both real and log domains. Our method achieves competitive accuracy and generalizes well to unseen morphologies. This framework paves the way for efficient and scalable coral geometry estimation directly from a sparse set of images, with potential applications in coral growth analysis and reef monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</title>
<link>https://arxiv.org/abs/2509.11165</link>
<guid>https://arxiv.org/abs/2509.11165</guid>
<content:encoded><![CDATA[
<div> keyword: Traffic-MLLM, multimodal, spatiotemporal causality, domain-specific knowledge, TrafficQA<br />
Summary:<br />
The proposed Traffic-MLLM model aims to improve traffic video understanding by addressing challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge. It is built on the Qwen2.5-VL backbone and leverages multimodal datasets for fine-grained traffic analysis. Low-Rank Adaptation (LoRA) is used for lightweight fine-tuning to enhance the model's capacity to model continuous spatiotemporal features in video sequences. An innovative knowledge prompting module fuses Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG) to inject detailed traffic regulations and domain knowledge into the inference process, improving logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show that Traffic-MLLM achieves state-of-the-art performance, demonstrating superior abilities in processing multimodal traffic data and exhibiting zero-shot reasoning and cross-scenario generalization capabilities. <br /> <div>
arXiv:2509.11165v1 Announce Type: new 
Abstract: As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the model's logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields</title>
<link>https://arxiv.org/abs/2509.11169</link>
<guid>https://arxiv.org/abs/2509.11169</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, multispectral information, NeRF, neural architecture, spectral discrepancy

Summary: 
Multispectral-NeRF is introduced as a solution to enhance 3D reconstruction by effectively integrating multispectral information. The traditional 3D reconstruction techniques relying on RGB spectral information have limitations in accuracy and geometric features. This new neural architecture expands hidden layer dimensionality to accommodate 6-band spectral inputs and redesigns residual functions to optimize spectral discrepancy calculations between reconstructed and reference images. Additionally, data compression modules are adapted to handle the increased bit-depth requirements of multispectral imagery. Experimental results demonstrate that Multispectral-NeRF successfully processes multi-band spectral features while accurately preserving the original scenes' spectral characteristics. This advancement in 3D reconstruction technology offers improved precision and quality in reconstruction results, making it a valuable tool for various applications in robotics, autonomous vehicles, and virtual reality systems.<br /><br />Summary: <div>
arXiv:2509.11169v1 Announce Type: new 
Abstract: 3D reconstruction technology generates three-dimensional representations of real-world objects, scenes, or environments using sensor data such as 2D images, with extensive applications in robotics, autonomous vehicles, and virtual reality systems. Traditional 3D reconstruction techniques based on 2D images typically relies on RGB spectral information. With advances in sensor technology, additional spectral bands beyond RGB have been increasingly incorporated into 3D reconstruction workflows. Existing methods that integrate these expanded spectral data often suffer from expensive scheme prices, low accuracy and poor geometric features. Three - dimensional reconstruction based on NeRF can effectively address the various issues in current multispectral 3D reconstruction methods, producing high - precision and high - quality reconstruction results. However, currently, NeRF and some improved models such as NeRFacto are trained on three - band data and cannot take into account the multi - band information. To address this problem, we propose Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can effectively integrates multispectral information. Our technical contributions comprise threefold modifications: Expanding hidden layer dimensionality to accommodate 6-band spectral inputs; Redesigning residual functions to optimize spectral discrepancy calculations between reconstructed and reference images; Adapting data compression modules to address the increased bit-depth requirements of multispectral imagery. Experimental results confirm that Multispectral-NeRF successfully processes multi-band spectral features while accurately preserving the original scenes' spectral characteristics.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2509.11171</link>
<guid>https://arxiv.org/abs/2509.11171</guid>
<content:encoded><![CDATA[
<div> Keywords: Camera-based 3D Semantic Scene Completion, Autonomous driving systems, Neural reconstruction methods, Semantic-PHysical Engaged REpresentation, SPHERE

Summary:
SPHERE is a novel approach for camera-based 3D Semantic Scene Completion in autonomous driving systems. It combines voxel and Gaussian representations to jointly leverage semantic and physical information. The Semantic-guided Gaussian Initialization (SGI) module efficiently initializes Gaus- sian representations using focal voxels as anchors. The Physical-aware Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to enhance physical details and promote semantic-geometry consistency. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks demonstrate the effectiveness of SPHERE in achieving realistic SSC results. This approach fills the gap between existing voxel-based and neural reconstruction methods, providing accurate and detailed scene completion for autonomous driving applications.

Summary: <br /><br />SPHERE combines voxel and Gaussian representations for 3D Semantic Scene Completion in autonomous driving. The SGI module efficiently initializes Gaussian representations using semantic-guided focal voxels, while the PHE module enhances physical details and semantic-geometry consistency through semantic spherical harmonics. Experimental results on benchmark datasets validate the efficacy of SPHERE, bridging the gap between existing methods and offering realistic scene completion for autonomous driving systems. <div>
arXiv:2509.11171v1 Announce Type: new 
Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in autonomous driving systems, assessing voxel-level geometry and semantics for holistic scene perception. While existing voxel-based and plane-based SSC methods have achieved considerable progress, they struggle to capture physical regularities for realistic geometric details. On the other hand, neural reconstruction methods like NeRF and 3DGS demonstrate superior physical awareness, but suffer from high computational cost and slow convergence when handling large-scale, complex autonomous driving scenes, leading to inferior semantic accuracy. To address these issues, we propose the Semantic-PHysical Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel and Gaussian representations for joint exploitation of semantic and physical information. First, the Semantic-guided Gaussian Initialization (SGI) module leverages dual-branch 3D scene representations to locate focal voxels as anchors to guide efficient Gaussian initialization. Then, the Physical-aware Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to model physical-aware contextual details and promote semantic-geometry consistency through focal distribution alignment, generating SSC results with realistic details. Extensive experiments and analyses on the popular SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of SPHERE. The code is available at https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StegOT: Trade-offs in Steganography via Optimal Transport</title>
<link>https://arxiv.org/abs/2509.11178</link>
<guid>https://arxiv.org/abs/2509.11178</guid>
<content:encoded><![CDATA[
<div> Keywords: image hiding, steganography, generative adversarial networks, variational autoencoders, optimal transport theory

Summary:
Image hiding, or steganography, involves hiding a secret image within a cover image. Existing steganography models based on GANs and VAEs often suffer from mode collapse, leading to information imbalance in the resulting stego image. To address this, StegOT, an autoencoder-based steganography model, incorporates optimal transport theory. The MCOT module is designed to transform the feature distribution into a single peak, achieving a trade-off of information between cover and secret images. Experimental results show that StegOT improves the quality of both stego and recovery images. The source code for StegOT will be available on GitHub. <br /><br />Summary: StegOT is a steganography model that uses optimal transport theory to overcome mode collapse and improve information balance in stego images. It incorporates an MCOT module to transform feature distribution for better quality stego and recovery images. <div>
arXiv:2509.11178v1 Announce Type: new 
Abstract: Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on https://github.com/Rss1124/StegOT.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models</title>
<link>https://arxiv.org/abs/2509.11184</link>
<guid>https://arxiv.org/abs/2509.11184</guid>
<content:encoded><![CDATA[
<div> granularity, Fitzpatrick Skin Tone, AI models, bias, lesion classification<br />
Summary:<br />
This study explores the impact of Fitzpatrick Skin Tone (FST) scale granularity on the performance and bias of AI models classifying skin lesions. Results show that using FST-specific data with three groups (FST 1/2, 3/4, 5/6) generally improves model performance compared to a general FST-balanced model. Reducing FST scale granularity can harm performance. The study suggests moving away from the FST scale due to its limited representation of skin tones and possible biases. It advocates for an alternative scale that better captures the diversity of human skin tones. <div>
arXiv:2509.11184v1 Announce Type: new 
Abstract: Artificial intelligence (AI) models to automatically classify skin lesions from dermatology images have shown promising performance but also susceptibility to bias by skin tone. The most common way of representing skin tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has been criticised for having greater granularity in its skin tone categories for lighter-skinned subjects. This paper conducts an investigation of the impact (on performance and bias) on AI classification models of granularity in the FST scale. By training multiple AI models to classify benign vs. malignant lesions using FST-specific data of differing granularity, we show that: (i) when training models using FST-specific data based on three groups (FST 1/2, 3/4 and 5/6), performance is generally better for models trained on FST-specific data compared to a general model trained on FST-balanced data; (ii) reducing the granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a detrimental effect on performance. Our results highlight the importance of the granularity of FST groups when training lesion classification models. Given the question marks over possible human biases in the choice of categories in the FST scale, this paper provides evidence for a move away from the FST scale in fair AI research and a transition to an alternative scale that better represents the diversity of human skin tones.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Forest Vision with Synthetic Data</title>
<link>https://arxiv.org/abs/2509.11201</link>
<guid>https://arxiv.org/abs/2509.11201</guid>
<content:encoded><![CDATA[
<div> Tree segmentation, forest laser scans, synthetic data, 3D forest dataset, AI<br />
<br />
Summary: 
Accurate tree segmentation from forest laser scans is crucial for understanding ecosystem functions. Existing 3D forest datasets are limited, hindering the development of robust segmentation systems. This study explores the use of synthetic data for tree segmentation, reducing the need for real data annotation. A new data generation pipeline combines game-engines and LiDAR simulation to create a diverse, annotated 3D forest dataset on a large scale. Experiments show that pretraining on synthetic data and fine-tuning on minimal real forest plot annotations can produce competitive segmentations. Critical factors for successful synthetic data use include physics, diversity, and scale. This research lays the foundation for more robust 3D forest vision systems in the future, with the data generation pipeline and dataset available for further exploration. <div>
arXiv:2509.11201v1 Announce Type: new 
Abstract: Accurate tree segmentation is a key step in extracting individual tree metrics from forest laser scans, and is essential to understanding ecosystem functions in carbon cycling and beyond. Over the past decade, tree segmentation algorithms have advanced rapidly due to developments in AI. However existing, public, 3D forest datasets are not large enough to build robust tree segmentation systems. Motivated by the success of synthetic data in other domains such as self-driving, we investigate whether similar approaches can help with tree segmentation. In place of expensive field data collection and annotation, we use synthetic data during pretraining, and then require only minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for forest vision tasks, integrating advances in game-engines with physics-based LiDAR simulation. As a result, we have produced a comprehensive, diverse, annotated 3D forest dataset on an unprecedented scale. Extensive experiments with a state-of-the-art tree segmentation algorithm and a popular real dataset show that our synthetic data can substantially reduce the need for labelled real data. After fine-tuning on just a single, real, forest plot of less than 0.1 hectare, the pretrained model achieves segmentations that are competitive with a model trained on the full scale real data. We have also identified critical factors for successful use of synthetic data: physics, diversity, and scale, paving the way for more robust 3D forest vision systems in the future. Our data generation pipeline and the resulting dataset are available at https://github.com/yihshe/CAMP3D.git.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation</title>
<link>https://arxiv.org/abs/2509.11213</link>
<guid>https://arxiv.org/abs/2509.11213</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, realism, customization, GANs, diffusion models

Summary: 
Beyond Sliders is a new framework that enhances image generation by integrating GANs and diffusion models. Unlike existing methods like concept sliders, Beyond Sliders allows for sophisticated image manipulation across various categories, including real world settings. The framework provides fine-grained textual and visual guidance to improve image quality and realism through an adversarial approach. Experimental results demonstrate the robustness and versatility of Beyond Sliders in different applications. This innovative method addresses the limitations of existing techniques and offers a promising solution for generating high-quality, customizable images. 

<br /><br />Summary: <div>
arXiv:2509.11213v1 Announce Type: new 
Abstract: In the realm of image generation, the quest for realism and customization has never been more pressing. While existing methods like concept sliders have made strides, they often falter when it comes to no-AIGC images, particularly images captured in real world settings. To bridge this gap, we introduce Beyond Sliders, an innovative framework that integrates GANs and diffusion models to facilitate sophisticated image manipulation across diverse image categories. Improved upon concept sliders, our method refines the image through fine grained guidance both textual and visual in an adversarial manner, leading to a marked enhancement in image quality and realism. Extensive experimental validation confirms the robustness and versatility of Beyond Sliders across a spectrum of applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometrically Constrained and Token-Based Probabilistic Spatial Transformers</title>
<link>https://arxiv.org/abs/2509.11218</link>
<guid>https://arxiv.org/abs/2509.11218</guid>
<content:encoded><![CDATA[
<div> Spatial Transformer Networks, fine-grained visual classification, geometric variability, component-wise extension, probabilistic modeling<br />
Summary:<br />
The study introduces a novel approach for fine-grained visual classification using Spatial Transformer Networks (STNs) to address geometric variability in object recognition. The proposed method extends traditional STNs by decomposing affine transformations into rotation, scaling, and shearing, regressing each component under geometric constraints, and modeling uncertainty with Gaussian variational posteriors. By leveraging a component-wise alignment loss and sampling-based canonicalization during inference, the method improves robustness in challenging moth classification benchmarks. The approach is backbone-agnostic, flexible, and lacks architectural constraints, offering a scalable solution for overcoming geometric variability in visual classification tasks. The experiments demonstrate consistent improvements in robustness compared to existing STN methods. <br /> <div>
arXiv:2509.11218v1 Announce Type: new 
Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to geometric variability, where objects appear under arbitrary orientations, scales, and perspective distortions. While equivariant architectures address this issue, they typically require substantial computational resources and restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs) as a canonicalization tool for transformer-based vision pipelines, emphasizing their flexibility, backbone-agnostic nature, and lack of architectural constraints. We propose a probabilistic, component-wise extension that improves robustness. Specifically, we decompose affine transformations into rotation, scaling, and shearing, and regress each component under geometric constraints using a shared localization encoder. To capture uncertainty, we model each component with a Gaussian variational posterior and perform sampling-based canonicalization during inference.A novel component-wise alignment loss leverages augmentation parameters to guide spatial alignment. Experiments on challenging moth classification benchmarks demonstrate that our method consistently improves robustness compared to other STNs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning</title>
<link>https://arxiv.org/abs/2509.11219</link>
<guid>https://arxiv.org/abs/2509.11219</guid>
<content:encoded><![CDATA[
<div> framework, deep learning, cattle identification, few-shot learning, model adaptation
Summary:
The paper introduces a novel few-shot learning framework for real-time cattle identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This approach aims to address the limitations faced by deep learning models in cattle identification, such as limited data availability and dynamic herd compositions requiring frequent retraining. The proposed framework leverages the unique muzzle patterns of cattle for accurate identification, achieving superior performance with 98.46% and 97.91% F1 scores. By utilizing CCoMAML with MHAFF, the model demonstrates high adaptability to new data samples without the need for retraining, making it a robust solution for efficient livestock farming management. <div>
arXiv:2509.11219v1 Announce Type: new 
Abstract: Cattle identification is critical for efficient livestock farming management, currently reliant on radio-frequency identification (RFID) ear tags. However, RFID-based systems are prone to failure due to loss, damage, tampering, and vulnerability to external attacks. As a robust alternative, biometric identification using cattle muzzle patterns similar to human fingerprints has emerged as a promising solution. Deep learning techniques have demonstrated success in leveraging these unique patterns for accurate identification. But deep learning models face significant challenges, including limited data availability, disruptions during data collection, and dynamic herd compositions that require frequent model retraining. To address these limitations, this paper proposes a novel few-shot learning framework for real-time cattle identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This model offers great model adaptability to new data through efficient learning from few data samples without retraining. The proposed approach has been rigorously evaluated against current state-of-the-art few-shot learning techniques applied in cattle identification. Comprehensive experimental results demonstrate that our proposed CCoMAML with MHAFF has superior cattle identification performance with 98.46% and 97.91% F1 scores.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification</title>
<link>https://arxiv.org/abs/2509.11220</link>
<guid>https://arxiv.org/abs/2509.11220</guid>
<content:encoded><![CDATA[
<div> Few-Shot Learning, ANROT-HELANet, Hellinger distance-based feature aggregation, adversarial robustness, natural perturbations<br />
<br />
Summary: <br />
ANROT-HELANet is a novel approach in Few-Shot Learning that improves robustness and performance. It introduces a robust Hellinger distance-based feature aggregation scheme that is resilient to adversarial perturbations and Gaussian noise. The network achieves significant improvements on benchmark datasets, with gains in both 1-shot and 5-shot scenarios. A novel Hellinger Similarity contrastive loss function enhances variational few-shot inference scenarios. ANROT-HELANet also outperforms traditional approaches in image reconstruction quality. The combination of Hellinger distance-based feature aggregation, attention mechanisms, and the novel loss function establishes new state-of-the-art performance while maintaining robustness against adversarial and natural perturbations. Extensive experiments on multiple datasets validate the effectiveness of ANROT-HELANet. <div>
arXiv:2509.11220v1 Announce Type: new 
Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian noise up to $\sigma=0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANet's combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction</title>
<link>https://arxiv.org/abs/2509.11232</link>
<guid>https://arxiv.org/abs/2509.11232</guid>
<content:encoded><![CDATA[
<div> Keywords: MIS-LSTM, CNN encoders, LSTM sequence model, sleep quality, stress prediction

Summary:
MIS-LSTM is a hybrid framework combining CNN encoders with an LSTM sequence model for predicting sleep quality and stress levels based on multimodal lifelog data. Sensor streams are partitioned into blocks and converted into multi-channel images, while sparse events are encoded using 1D-CNN. A Convolutional Block Attention Module merges the two modalities to create refined block embeddings, which are then aggregated by an LSTM to capture long-term dependencies. The framework also introduces UALRE, an uncertainty-aware ensemble that improves prediction accuracy by considering confidence levels. Experimental results on the ETRI Lifelog Challenge dataset demonstrate that MIS-LSTM outperforms other baseline models in terms of Macro-F1 score. Ablation studies confirm the effectiveness of multi-channel imaging, 4-hour block granularity, and modality-specific encoding. <br /><br />Summary: <div>
arXiv:2509.11232v1 Announce Type: new 
Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with an LSTM sequence model for sleep quality and stress prediction at the day level from multimodal lifelog data. Continuous sensor streams are first partitioned into N-hour blocks and rendered as multi-channel images, while sparse discrete events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention Module fuses the two modalities into refined block embeddings, which an LSTM then aggregates to capture long-range temporal dependencies. To further boost robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides lowconfidence majority votes with high-confidence individual predictions. Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to 0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm (i) the superiority of multi-channel over stacked-vertical imaging, (ii) the benefit of a 4-hour block granularity, and (iii) the efficacy of modality-specific discrete encoding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States</title>
<link>https://arxiv.org/abs/2509.11247</link>
<guid>https://arxiv.org/abs/2509.11247</guid>
<content:encoded><![CDATA[
arXiv:2509.11247v1 Announce Type: new 
Abstract: Person Re-Identification (ReID) has several challenges in real-world surveillance systems due to clothing changes (CCReID) and the need for maintaining continual learning (LReID). Previous existing methods either develop models specifically for one application, which is mostly a same-cloth (SC) setting or treat CCReID as its own separate sub-problem. In this work, we will introduce the LReID-Hybrid task with the goal of developing a model to achieve both SC and CC while learning in a continual setting. Mismatched representations and forgetting from one task to the next are significant issues, we address this with CMLReID, a CLIP-based framework composed of two novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive prompts, and also incorporates context to align richly multi-grained visual cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection (AKFP) which produces robust SC/CC prototypes through the use of a dual-path learner that aligns features with our Clothing-State-Aware Projection Loss. Experiments performed on a wide range of datasets and illustrate that CMLReID outperforms all state-of-the-art methods with strong robustness and generalization despite clothing variations and a sophisticated process of sequential learning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2509.11264</link>
<guid>https://arxiv.org/abs/2509.11264</guid>
<content:encoded><![CDATA[
arXiv:2509.11264v1 Announce Type: new 
Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where the sets of potential target classes appearing at different time steps are disjoint and are subsets of the source classes. The key to solving this problem lies in avoiding catastrophic forgetting of knowledge about previous target classes during continuously mitigating the domain shift. Most previous works cumbersomely combine two technical components. On one hand, they need to store and utilize rehearsal target sample from previous time steps to avoid catastrophic forgetting; on the other hand, they perform alignment only between classes shared across domains at each time step. Consequently, the memory will continuously increase and the asymmetric alignment may inevitably result in knowledge forgetting. In this paper, we propose to mine and preserve domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task. Specifically, via using CLIP, we extract the class-agnostic properties which we name as "attribute". In our framework, we learn a "key-value" pair to represent an attribute, where the key corresponds to the visual prototype and the value is the textual prompt. We maintain two attribute dictionaries, each corresponding to a different domain. Then we perform attribute alignment across domains to mitigate the domain shift, via encouraging visual attention consistency and prediction consistency. Through attribute modeling and cross-domain alignment, we effectively reduce catastrophic knowledge forgetting while mitigating the domain shift, in a rehearsal-free way. Experiments on three CI-UDA benchmarks demonstrate that our method outperforms previous state-of-the-art methods and effectively alleviates catastrophic forgetting. Code is available at https://github.com/RyunMi/VisTA.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Dataset Evaluation Based on Generalized Cross Validation</title>
<link>https://arxiv.org/abs/2509.11273</link>
<guid>https://arxiv.org/abs/2509.11273</guid>
<content:encoded><![CDATA[
arXiv:2509.11273v1 Announce Type: new 
Abstract: With the rapid advancement of synthetic dataset generation techniques, evaluating the quality of synthetic data has become a critical research focus. Robust evaluation not only drives innovations in data generation methods but also guides researchers in optimizing the utilization of these synthetic resources. However, current evaluation studies for synthetic datasets remain limited, lacking a universally accepted standard framework. To address this, this paper proposes a novel evaluation framework integrating generalized cross-validation experiments and domain transfer learning principles, enabling generalizable and comparable assessments of synthetic dataset quality. The framework involves training task-specific models (e.g., YOLOv5s) on both synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K), forming a cross-performance matrix. Following normalization, a Generalized Cross-Validation (GCV) Matrix is constructed to quantify domain transferability. The framework introduces two key metrics. One measures the simulation quality by quantifying the similarity between synthetic data and real-world datasets, while another evaluates the transfer quality by assessing the diversity and coverage of synthetic data across various real-world scenarios. Experimental validation on Virtual KITTI demonstrates the effectiveness of our proposed framework and metrics in assessing synthetic data fidelity. This scalable and quantifiable evaluation solution overcomes traditional limitations, providing a principled approach to guide synthetic dataset optimization in artificial intelligence research.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSGS: Relightable Outdoor Scenes With Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11275</link>
<guid>https://arxiv.org/abs/2509.11275</guid>
<content:encoded><![CDATA[
arXiv:2509.11275v1 Announce Type: new 
Abstract: Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations</title>
<link>https://arxiv.org/abs/2509.11287</link>
<guid>https://arxiv.org/abs/2509.11287</guid>
<content:encoded><![CDATA[
arXiv:2509.11287v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination problems, where the model-generated responses are inconsistent with the visual inputs. Existing hallucination mitigation methods are mainly based on preference alignment and require external human annotations or auxiliary models for preference data collection, which increase costs and limit sustainable improvement. To tackle these challenges, we propose Autonomous Preference Alignment via Self-Injection (APASI), a novel and generalizable method that mitigates hallucinations without external dependencies. APASI leverages the target LVLM to self-inject hallucinations into a generated response, creating a pair of responses with varying preference levels. During the self-injection process, the dis-preferred response is generated based on three key observations of hallucinations, ensuring it simulates real hallucination patterns. This fidelity offers an accurate learning signal for hallucination mitigation. Moreover, APASI incorporates an iterative alignment training strategy combined with curriculum learning to periodically update the preference data with increasing challenge, enabling stable and continuous enhancement of the LVLM. Extensive experiments across six benchmarks show that APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency, thereby demonstrating its effectiveness and generalization capability. The code is available at https://github.com/davidluciolu/APASI.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
<link>https://arxiv.org/abs/2509.11292</link>
<guid>https://arxiv.org/abs/2509.11292</guid>
<content:encoded><![CDATA[
arXiv:2509.11292v1 Announce Type: new 
Abstract: Unaligned Scene Change Detection aims to detect scene changes between image pairs captured at different times without assuming viewpoint alignment. To handle viewpoint variations, current methods rely solely on 2D visual cues to establish cross-image correspondence to assist change detection. However, large viewpoint changes can alter visual observations, causing appearance-based matching to drift or fail. Additionally, supervision limited to 2D change masks from small-scale SCD datasets restricts the learning of generalizable multi-view knowledge, making it difficult to reliably identify visual overlaps and handle occlusions. This lack of explicit geometric reasoning represents a critical yet overlooked limitation. In this work, we are the first to leverage geometric priors from a Geometric Foundation Model to address the core challenges of unaligned SCD, including reliable identification of visual overlaps, robust correspondence establishment, and explicit occlusion detection. Building on these priors, we propose a training-free framework that integrates them with the powerful representations of a visual foundation model to enable reliable change detection under viewpoint misalignment. Through extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achieves superior and robust performance. Our code will be released at https://github.com/ZilingLiu/GeoSCD.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnLoc: Leveraging Depth Uncertainties for Floorplan Localization</title>
<link>https://arxiv.org/abs/2509.11301</link>
<guid>https://arxiv.org/abs/2509.11301</guid>
<content:encoded><![CDATA[
arXiv:2509.11301v1 Announce Type: new 
Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera localization within floorplans. Floorplan data is readily available, long-term persistent, and robust to changes in visual appearance. We address key limitations of recent methods, such as the lack of uncertainty modeling in depth predictions and the necessity for custom depth networks trained for each environment. We introduce a novel probabilistic model that incorporates uncertainty estimation, modeling depth predictions as explicit probability distributions. By leveraging off-the-shelf pre-trained monocular depth models, we eliminate the need to rely on per-environment-trained depth networks, enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale synthetic and real-world datasets, demonstrating significant improvements over existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$ times higher localization recall on long sequences (100 frames) and $16.7$ times higher on short ones (15 frames) than the state of the art on the challenging LaMAR HGE dataset.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</title>
<link>https://arxiv.org/abs/2509.11323</link>
<guid>https://arxiv.org/abs/2509.11323</guid>
<content:encoded><![CDATA[
arXiv:2509.11323v1 Announce Type: new 
Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency</title>
<link>https://arxiv.org/abs/2509.11328</link>
<guid>https://arxiv.org/abs/2509.11328</guid>
<content:encoded><![CDATA[
arXiv:2509.11328v1 Announce Type: new 
Abstract: Medical Image Computing (MIC) is a broad research topic covering both pixel-wise (e.g., segmentation, registration) and image-wise (e.g., classification, regression) vision tasks. Effective analysis demands models that capture both global long-range context and local subtle visual characteristics, necessitating fine-grained long-range visual dependency modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by intrinsic locality, transformers excel at long-range modeling; however, due to the high computational loads of self-attention, transformers typically cannot process high-resolution features (e.g., full-scale image features before downsampling or patch embedding) and thus face difficulties in modeling fine-grained dependency among subtle medical image details. Concurrently, Multi-layer Perceptron (MLP)-based visual models are recognized as computation/memory-efficient alternatives in modeling long-range visual dependency but have yet to be widely investigated in the MIC community. This doctoral research advances deep learning-based MIC by investigating effective long-range visual dependency modeling. It first presents innovative use of transformers for both pixel- and image-wise medical vision tasks. The focus then shifts to MLPs, pioneeringly developing MLP-based visual models to capture fine-grained long-range visual dependency in medical images. Extensive experiments confirm the critical role of long-range dependency modeling in MIC and reveal a key finding: MLPs provide feasibility in modeling finer-grained long-range dependency among higher-resolution medical features containing enriched anatomical/pathological details. This finding establishes MLPs as a superior paradigm over transformers/CNNs, consistently enhancing performance across various medical vision tasks and paving the way for next-generation medical vision backbones.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Band Video Thermography Near Ambient Conditions</title>
<link>https://arxiv.org/abs/2509.11334</link>
<guid>https://arxiv.org/abs/2509.11334</guid>
<content:encoded><![CDATA[
arXiv:2509.11334v1 Announce Type: new 
Abstract: Long-wave infrared radiation captured by a thermal camera consists of two components: (a) light from the environment reflected or transmitted by a surface, and (b) light emitted by the surface after undergoing heat transport through the object and exchanging heat with the surrounding environment. Separating these components is essential for understanding object properties such as emissivity, temperature, reflectance and shape. Previous thermography studies often assume that only one component is dominant (e.g., in welding) or that the second component is constant and can be subtracted. However, in near-ambient conditions, which are most relevant to computer vision applications, both components are typically comparable in magnitude and vary over time. We introduce the first method that separates reflected and emitted components of light in videos captured by two thermal cameras with different spectral sensitivities. We derive a dual-band thermal image formation model and develop algorithms to estimate the surface's emissivity and its time-varying temperature while isolating a dynamic background. We quantitatively evaluate our approach using carefully calibrated emissivities for a range of materials and show qualitative results on complex everyday scenes, such as a glass filled with hot liquid and people moving in the background.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning</title>
<link>https://arxiv.org/abs/2509.11344</link>
<guid>https://arxiv.org/abs/2509.11344</guid>
<content:encoded><![CDATA[
arXiv:2509.11344v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) conventionally relies on the instance consistency paradigm, assuming that different views of the same image can be treated as positive pairs. However, this assumption breaks down for non-iconic data, where different views may contain distinct objects or semantic information. In this paper, we investigate the effectiveness of SSL when instance consistency is not guaranteed. Through extensive ablation studies, we demonstrate that SSL can still learn meaningful representations even when positive pairs lack strict instance consistency. Furthermore, our analysis further reveals that increasing view diversity, by enforcing zero overlapping or using smaller crop scales, can enhance downstream performance on classification and dense prediction tasks. However, excessive diversity is found to reduce effectiveness, suggesting an optimal range for view diversity. To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to measure mutual information between views, finding that moderate EMD values correlate with improved SSL learning, providing insights for future SSL framework design. We validate our findings across a range of settings, highlighting their robustness and applicability on diverse data sources.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness</title>
<link>https://arxiv.org/abs/2509.11355</link>
<guid>https://arxiv.org/abs/2509.11355</guid>
<content:encoded><![CDATA[
arXiv:2509.11355v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain vulnerable to common corruptions that humans handle with ease. A key reason for this fragility is their reliance on local texture cues rather than global object shapes -- a stark contrast to human perception. To address this, we propose two complementary regularization strategies designed to encourage shape-biased representations and enhance robustness. The first introduces an auxiliary loss that enforces feature consistency between original and low-frequency filtered inputs, discouraging dependence on high-frequency textures. The second incorporates supervised contrastive learning to structure the feature space around class-consistent, shape-relevant representations. Evaluated on the CIFAR-10-C benchmark, both methods improve corruption robustness without degrading clean accuracy. Our results suggest that loss-level regularization can effectively steer CNNs toward more shape-aware, resilient representations.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration</title>
<link>https://arxiv.org/abs/2509.11360</link>
<guid>https://arxiv.org/abs/2509.11360</guid>
<content:encoded><![CDATA[
arXiv:2509.11360v1 Announce Type: new 
Abstract: Video detailed captioning aims to generate comprehensive video descriptions to facilitate video understanding. Recently, most efforts in the video detailed captioning community have been made towards a local-to-global paradigm, which first generates local captions from video clips and then summarizes them into a global caption. However, we find this paradigm leads to less detailed and contextual-inconsistent captions, which can be attributed to (1) no mechanism to ensure fine-grained captions, and (2) weak interaction between local and global captions. To remedy the above two issues, we propose GLaVE-Cap, a Global-Local aligned framework with Vision Expert integration for Captioning, which consists of two core modules: TrackFusion enables comprehensive local caption generation, by leveraging vision experts to acquire cross-frame visual prompts, coupled with a dual-stream structure; while CaptionBridge establishes a local-global interaction, by using global context to guide local captioning, and adaptively summarizing local captions into a coherent global caption. Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark featuring 5X more queries per video than existing benchmarks, covering diverse visual dimensions to facilitate reliable evaluation. We further provide a training dataset GLaVE-1.2M containing 16K high-quality fine-grained video captions and 1.2M related question-answer pairs. Extensive experiments on four benchmarks show that our GLaVE-Cap achieves state-of-the-art performance. Besides, the ablation studies and student model analyses further validate the effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the video understanding community. The source code, model weights, benchmark, and dataset will be open-sourced.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing</title>
<link>https://arxiv.org/abs/2509.11385</link>
<guid>https://arxiv.org/abs/2509.11385</guid>
<content:encoded><![CDATA[
arXiv:2509.11385v1 Announce Type: new 
Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for objective and quantitative dermatological assessment, but no portable, high-resolution device exists that has been validated and used for depth reconstruction across various body locations. We present a compact 3-D skin reconstruction probe based on GelSight tactile imaging with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation. Our probe, integrated into a handheld probe with force sensing for consistent contact, achieves a mean absolute error of 12.55 micron on wrinkle-like test objects. In a study with 15 participants without skin disorders, we provide the first validated wrinkle depth metrics across multiple body regions. We further demonstrate statistically significant reductions in wrinkle height at three locations following over-the-counter moisturizer application. Our work offers a validated tool for clinical and cosmetic skin analysis, with potential applications in diagnosis, treatment monitoring, and skincare efficacy evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation</title>
<link>https://arxiv.org/abs/2509.11394</link>
<guid>https://arxiv.org/abs/2509.11394</guid>
<content:encoded><![CDATA[
arXiv:2509.11394v1 Announce Type: new 
Abstract: We present MixANT, a novel architecture for stochastic long-term dense anticipation of human activities. While recent State Space Models (SSMs) like Mamba have shown promise through input-dependent selectivity on three key parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal memory remains static. We address this limitation by introducing a mixture of experts approach that dynamically selects contextually relevant $\textbf{A}$ matrices based on input features, enhancing representational capacity without sacrificing computational efficiency. Extensive experiments on the 50Salads, Breakfast, and Assembly101 datasets demonstrate that MixANT consistently outperforms state-of-the-art methods across all evaluation settings. Our results highlight the importance of input-dependent forget-gate mechanisms for reliable prediction of human behavior in diverse real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data</title>
<link>https://arxiv.org/abs/2509.11406</link>
<guid>https://arxiv.org/abs/2509.11406</guid>
<content:encoded><![CDATA[
arXiv:2509.11406v1 Announce Type: new 
Abstract: In real world clinical environments, training and applying deep learning models on multi-modal medical imaging data often struggles with partially incomplete data. Standard approaches either discard missing samples, require imputation or repurpose dropout learning schemes, limiting robustness and generalizability. To address this, we propose a hypernetwork-based method that dynamically generates task-specific classification models conditioned on the set of available modalities. Instead of training a fixed model, a hypernetwork learns to predict the parameters of a task model adapted to available modalities, enabling training and inference on all samples, regardless of completeness. We compare this approach with (1) models trained only on complete data, (2) state of the art channel dropout methods, and (3) an imputation-based method, using artificially incomplete datasets to systematically analyze robustness to missing modalities. Results demonstrate superior adaptability of our method, outperforming state of the art approaches with an absolute increase in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of training data with missing modalities). By enabling a single model to generalize across all modality configurations, our approach provides an efficient solution for real-world multi-modal medical data analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Skinning of Gaussian Avatars</title>
<link>https://arxiv.org/abs/2509.11411</link>
<guid>https://arxiv.org/abs/2509.11411</guid>
<content:encoded><![CDATA[
arXiv:2509.11411v1 Announce Type: new 
Abstract: Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery</title>
<link>https://arxiv.org/abs/2509.11436</link>
<guid>https://arxiv.org/abs/2509.11436</guid>
<content:encoded><![CDATA[
arXiv:2509.11436v1 Announce Type: new 
Abstract: Identifying new disease-related patterns in medical imaging data with the help of machine learning enlarges the vocabulary of recognizable findings. This supports diagnostic and prognostic assessment. However, image appearance varies not only due to biological differences, but also due to imaging technology linked to vendors, scanning- or re- construction parameters. The resulting domain shifts impedes data representation learning strategies and the discovery of biologically meaningful cluster appearances. To address these challenges, we introduce an approach to actively learn the domain shift via post-hoc rotation of the data latent space, enabling disentanglement of biological and technical factors. Results on real-world heterogeneous clinical data showcase that the learned disentangled representation leads to stable clusters representing tissue-types across different acquisition settings. Cluster consistency is improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the entangled representation, outperforming four state-of-the-art harmonization methods. When using the clusters to quantify tissue composition on idiopathic pulmonary fibrosis patients, the learned profiles enhance Cox survival prediction. This indicates that the proposed label-free framework facilitates biomarker discovery in multi-center routine imaging data. Code is available on GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder</title>
<link>https://arxiv.org/abs/2509.11442</link>
<guid>https://arxiv.org/abs/2509.11442</guid>
<content:encoded><![CDATA[
arXiv:2509.11442v1 Announce Type: new 
Abstract: Missing input sequences are common in medical imaging data, posing a challenge for deep learning models reliant on complete input data. In this work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our method treats each MRI sequence as a separate input modality, leveraging a late-fusion-style transformer encoder to integrate multi-sequence information (multi-modal) and individual decoder streams for each modality for multi-task reconstruction. This pretraining strategy guides the model to learn rich representations per modality while also equipping it to handle missing inputs through cross-sequence reasoning. The result is a flexible and generalizable encoder for brain MRIs that infers missing sequences from available inputs and can be adapted to various downstream applications. We demonstrate the performance and robustness of our method against an MAE-ViT baseline in downstream segmentation and classification tasks, showing absolute improvement of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing input sequences. Our experiments demonstrate the strength of this pretraining strategy. The implementation is made available.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</title>
<link>https://arxiv.org/abs/2509.11453</link>
<guid>https://arxiv.org/abs/2509.11453</guid>
<content:encoded><![CDATA[
arXiv:2509.11453v1 Announce Type: new 
Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision</title>
<link>https://arxiv.org/abs/2509.11476</link>
<guid>https://arxiv.org/abs/2509.11476</guid>
<content:encoded><![CDATA[
arXiv:2509.11476v1 Announce Type: new 
Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal perception that aims to integrate complementary structural and textural cues from different spectral domains. In this paper, we propose FusionNet, a novel end-to-end fusion framework that explicitly models inter-modality interaction and enhances task-critical regions. FusionNet introduces a modality-aware attention mechanism that dynamically adjusts the contribution of infrared and visible features based on their discriminative capacity. To achieve fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha blending module, which learns spatially-varying fusion weights in an adaptive and content-aware manner. Moreover, we formulate a target-aware loss that leverages weak ROI supervision to preserve semantic consistency in regions containing important objects (e.g., pedestrians, vehicles). Experiments on the public M3FD dataset demonstrate that FusionNet generates fused images with enhanced semantic preservation, high perceptual quality, and clear interpretability. Our framework provides a general and extensible solution for semantic-aware multi-modal image fusion, with benefits for downstream tasks such as object detection and scene understanding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis</title>
<link>https://arxiv.org/abs/2509.11526</link>
<guid>https://arxiv.org/abs/2509.11526</guid>
<content:encoded><![CDATA[
arXiv:2509.11526v1 Announce Type: new 
Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2509.11539</link>
<guid>https://arxiv.org/abs/2509.11539</guid>
<content:encoded><![CDATA[
arXiv:2509.11539v1 Announce Type: new 
Abstract: Camouflaged object detection (COD) aims to segment objects that blend into their surroundings. However, most existing studies overlook the semantic differences among textual prompts of different targets as well as fine-grained frequency features. In this work, we propose a novel Semantic and Frequency Guided Network (SFGNet), which incorporates semantic prompts and frequency-domain features to capture camouflaged objects and improve boundary perception. We further design Multi-Band Fourier Module(MBFM) to enhance the ability of the network in handling complex backgrounds and blurred boundaries. In addition, we design an Interactive Structure Enhancement Block (ISEB) to ensure structural integrity and boundary details in the predictions. Extensive experiments conducted on three COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches. The core code of the model is available at the following link: https://github.com/winter794444/SFGNetICASSP2026.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs</title>
<link>https://arxiv.org/abs/2509.11548</link>
<guid>https://arxiv.org/abs/2509.11548</guid>
<content:encoded><![CDATA[
arXiv:2509.11548v1 Announce Type: new 
Abstract: Graphical user interface (GUI) grounding is a fundamental task for building GUI agents. However, general vision-language models (VLMs) struggle with this task due to a lack of specific optimization. We identify a key gap in this paper: while VLMs exhibit significant latent grounding potential, as demonstrated by their performance measured by Pointing Game, they underperform when tasked with outputting explicit coordinates. To address this discrepancy, and bypass the high data and annotation costs of current fine-tuning approaches, we propose three zero-shot auxiliary reasoning methods. By providing explicit spatial cues such as axes, grids and labeled intersections as part of the input image, these methods enable VLMs to articulate their implicit spatial understanding capabilities. We evaluate these methods on four GUI grounding benchmarks across seven open-source and proprietary VLMs. The evaluation results demonstrate that the proposed methods substantially improve the performance of GUI grounding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps</title>
<link>https://arxiv.org/abs/2509.11574</link>
<guid>https://arxiv.org/abs/2509.11574</guid>
<content:encoded><![CDATA[
arXiv:2509.11574v1 Announce Type: new 
Abstract: While recent Gaussian-based SLAM methods achieve photorealistic reconstruction from RGB-D data, their computational performance remains a critical bottleneck. State-of-the-art techniques operate at less than 20 fps, significantly lagging behind geometry-centric approaches like KinectFusion (hundreds of fps). This limitation stems from the heavy computational burden: modeling scenes requires numerous Gaussians and complex iterative optimization to fit RGB-D data, where insufficient Gaussian counts or optimization iterations cause severe quality degradation. To address this, we propose a Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field (SDF) for smooth geometry and appearance with 3D Gaussians to capture underrepresented details. The SDF is efficiently constructed via RGB-D fusion (as in geometry-centric methods), while Gaussians undergo iterative optimization. Our representation enables drastic Gaussian reduction (50% fewer) by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization (75% fewer iterations) through targeted appearance refinement. Building upon this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time 3D reconstruction system achieving over 150 fps on real-world Azure Kinect sequences -- delivering an order-of-magnitude speedup over state-of-the-art techniques while maintaining comparable reconstruction quality. We will release the source code and data to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2509.11587</link>
<guid>https://arxiv.org/abs/2509.11587</guid>
<content:encoded><![CDATA[
arXiv:2509.11587v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Class Distributions for Bias-Aware Multi-Class Learning</title>
<link>https://arxiv.org/abs/2509.11588</link>
<guid>https://arxiv.org/abs/2509.11588</guid>
<content:encoded><![CDATA[
arXiv:2509.11588v1 Announce Type: new 
Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an iterative, data-centric framework that identifies Pareto optimized class distributions for multi-class image classification. BiCDO enables performance prioritization for specific classes, which is useful in safety-critical scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions, BiCDO determines the optimal number of images per class to enhance reliability and minimize bias and variance in the objective function. BiCDO can be incorporated into existing training pipelines with minimal code changes and supports any labelled multi-class dataset. We have validated BiCDO using EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets, demonstrating improved, balanced model performance through optimized data distribution.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment</title>
<link>https://arxiv.org/abs/2509.11589</link>
<guid>https://arxiv.org/abs/2509.11589</guid>
<content:encoded><![CDATA[
arXiv:2509.11589v1 Announce Type: new 
Abstract: With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: https://github.com/Controller01-ai/MVQA-68K
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework</title>
<link>https://arxiv.org/abs/2509.11598</link>
<guid>https://arxiv.org/abs/2509.11598</guid>
<content:encoded><![CDATA[
arXiv:2509.11598v1 Announce Type: new 
Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its generalization is fundamentally hindered by Shortcut Learning, where models exploit superficial features like texture instead of intrinsic structure. We experimentally verify this flaw within the generative paradigm (e.g., MAE) and argue it is a systemic issue also affecting discriminative methods, identifying it as the root cause of their failure on unseen domains. While existing methods often tackle this at a surface level by aligning or separating domain-specific features, they fail to alter the underlying learning mechanism that fosters shortcut dependency. To address this at its core, we propose HyGDL (Hybrid Generative-Discriminative Learning Framework), a hybrid framework that achieves explicit content-style disentanglement. Our approach is guided by the Invariance Pre-training Principle: forcing a model to learn an invariant essence by systematically varying a bias (e.g., style) at the input while keeping the supervision signal constant. HyGDL operates on a single encoder and analytically defines style as the component of a representation that is orthogonal to its style-invariant content, derived via vector projection.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.11605</link>
<guid>https://arxiv.org/abs/2509.11605</guid>
<content:encoded><![CDATA[
arXiv:2509.11605v1 Announce Type: new 
Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety. However, existing benchmarks are limited to either frame-level or video-level tasks, restricting a holistic view of model generalization. This work first introduces a softmax-based frame allocation strategy that prioritizes anomaly-dense segments while maintaining full-video coverage, enabling balanced sampling across temporal scales. Building on this process, we construct two complementary benchmarks. The image-based benchmark evaluates frame-level reasoning with representative frames, while the video-based benchmark extends to temporally localized segments and incorporates an abnormality scoring task.Experiments on UCF-Crime demonstrate improvements at both the frame and video levels, and ablation studies confirm clear advantages of anomaly-focused sampling over uniform and random baselines.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Controllable 3D Deepfake Generation Framework with Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11624</link>
<guid>https://arxiv.org/abs/2509.11624</guid>
<content:encoded><![CDATA[
arXiv:2509.11624v1 Announce Type: new 
Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed</title>
<link>https://arxiv.org/abs/2509.11638</link>
<guid>https://arxiv.org/abs/2509.11638</guid>
<content:encoded><![CDATA[
arXiv:2509.11638v1 Announce Type: new 
Abstract: Diffusion models have shown promising results in free-form inpainting. Recent studies based on refined diffusion samplers or novel architectural designs led to realistic results and high data consistency. However, random initialization seed (noise) adopted in vanilla diffusion process may introduce mismatched semantic information in masked regions, leading to biased inpainting results, e.g., low consistency and low coherence with the other unmasked area. To address this issue, we propose the Initial Seed refined Diffusion Model (IS-Diff), a completely training-free approach incorporating distributional harmonious seeds to produce harmonious results. Specifically, IS-Diff employs initial seeds sampled from unmasked areas to imitate the masked data distribution, thereby setting a promising direction for the diffusion procedure. Moreover, a dynamic selective refinement mechanism is proposed to detect severe unharmonious inpaintings in intermediate latent and adjust the strength of our initialization prior dynamically. We validate our method on both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet, and Places2 datasets, demonstrating its effectiveness across all metrics compared to state-of-the-art inpainting methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration</title>
<link>https://arxiv.org/abs/2509.11642</link>
<guid>https://arxiv.org/abs/2509.11642</guid>
<content:encoded><![CDATA[
arXiv:2509.11642v1 Announce Type: new 
Abstract: Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at https://github.com/guanqiyuan/WeatherBench.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba</title>
<link>https://arxiv.org/abs/2509.11649</link>
<guid>https://arxiv.org/abs/2509.11649</guid>
<content:encoded><![CDATA[
arXiv:2509.11649v1 Announce Type: new 
Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and monitoring retinal diseases like diabetic retinopathy, age-related macular degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV) segmentation offer insufficient accuracy. To address this, we propose RVMamba, a novel architecture integrating multiple feature extraction modules with the Mamba state-space model. Moreover, existing joint segmentation models for OCTA data exhibit performance imbalance between different tasks. To simultaneously improve the segmentation of the foveal avascular zone (FAZ) and mitigate this imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework. Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba outperforms existing models across evaluation metrics.The code is available at https://github.com/lc-sfis/Joint-OCTAMamba.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition</title>
<link>https://arxiv.org/abs/2509.11661</link>
<guid>https://arxiv.org/abs/2509.11661</guid>
<content:encoded><![CDATA[
arXiv:2509.11661v1 Announce Type: new 
Abstract: Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs</title>
<link>https://arxiv.org/abs/2509.11662</link>
<guid>https://arxiv.org/abs/2509.11662</guid>
<content:encoded><![CDATA[
arXiv:2509.11662v1 Announce Type: new 
Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs. Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers, which enables it to process images at their original variable resolutions. This design avoids the degradation caused by fixed-resolution tiling while preserving fine-grained details and global layouts, which is crucial for visually dense content such as complex charts and diagrams. To ensure the smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a distributed multimodal training framework tailored for Ascend NPUs. To maintain training accuracy, we implement equivalent replacements for certain operators. MindVL undergoes a three-phase training process, namely the warm-up phase, multitask training phase, and supervised instruction tuning phase, to gradually enhance its capabilities. This process starts with basic visual and multimodal pre-training, followed by large-scale multiask trainging and instruction tuning. We also adopt multimodal data packaging and hybrid parallelism techniques, which significantly improve end-to-end training speed. To further boost model performance, we specifically introduce test-time resolution search and model weight averaging. Notably, despite using about 1/10 of the training data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL in evaluations of general multimodal understanding and document/table comprehension. Beyond overall scores, MindVL also delivers leading performance in OCR assessments.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps</title>
<link>https://arxiv.org/abs/2509.11674</link>
<guid>https://arxiv.org/abs/2509.11674</guid>
<content:encoded><![CDATA[
arXiv:2509.11674v1 Announce Type: new 
Abstract: Paper maps remain widely used for hiking and sightseeing because they contain curated trails and locally relevant annotations that are often missing from digital navigation applications such as Google Maps. We propose a pipeline to extract navigable trails from scanned maps, enabling their use in GPS-based navigation. Our method combines georeferencing, U-Net-based binary segmentation, graph construction, and an iterative refinement procedure using a routing engine. We evaluate the full end-to-end pipeline as well as individual components, showing that the approach can robustly recover trail networks from diverse map styles and generate GPS routes suitable for practical use.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects</title>
<link>https://arxiv.org/abs/2509.11680</link>
<guid>https://arxiv.org/abs/2509.11680</guid>
<content:encoded><![CDATA[
arXiv:2509.11680v1 Announce Type: new 
Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception, especially in industrial settings. It enables robots to interact with the environment and manipulate objects. However, existing benchmarks on object 6D pose estimation primarily use everyday objects with rich textures and low-reflectivity, limiting model generalization to industrial scenarios where objects are often metallic, texture-less, and highly reflective. To address this gap, we propose a novel dataset and benchmark namely \textit{Industrial Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset comprises 45 true-to-scale industrial components, captured with an RGB-D camera under natural indoor lighting and varied object arrangements to replicate real-world conditions. The benchmark supports three tasks, including video object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We evaluate existing state-of-the-art models, including XMem and SAM2 for segmentation, and BundleTrack and BundleSDF for pose estimation, to assess model performance in industrial contexts. Evaluation results show that our industrial dataset is more challenging than existing household object datasets. This benchmark provides the baseline for developing and comparing segmentation and pose estimation algorithms that better generalize to industrial robotics scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation</title>
<link>https://arxiv.org/abs/2509.11689</link>
<guid>https://arxiv.org/abs/2509.11689</guid>
<content:encoded><![CDATA[
arXiv:2509.11689v1 Announce Type: new 
Abstract: Uncertainty estimation is critical for reliable medical image segmentation, particularly in retinal vessel analysis, where accurate predictions are essential for diagnostic applications. Deep Ensembles, where multiple networks are trained individually, are widely used to improve medical image segmentation performance. However, training and testing costs increase with the number of ensembles. In this work, we propose Ensemble Distillation as a robust alternative to commonly used uncertainty estimation techniques by distilling the knowledge of multiple ensemble models into a single model. Through extensive experiments on the DRIVE and FIVES datasets, we demonstrate that Ensemble Distillation achieves comparable performance via calibration and segmentation metrics, while significantly reducing computational complexity. These findings suggest that Ensemble distillation provides an efficient and reliable approach for uncertainty estimation in the segmentation of the retinal vessels, making it a promising tool for medical imaging applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quest for Universal Master Key Filters in DS-CNNs</title>
<link>https://arxiv.org/abs/2509.11711</link>
<guid>https://arxiv.org/abs/2509.11711</guid>
<content:encoded><![CDATA[
arXiv:2509.11711v1 Announce Type: new 
Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for convolutional neural network filters. This paper extends this hypothesis by radically constraining its scope to a single set of just 8 universal filters that depthwise separable convolutional networks inherently converge to. While conventional DS-CNNs employ thousands of distinct trained filters, our analysis reveals these filters are predominantly linear shifts (ax+b) of our discovered universal set. Through systematic unsupervised search, we extracted these fundamental patterns across different architectures and datasets. Remarkably, networks initialized with these 8 unique frozen filters achieve over 80% ImageNet accuracy, and even outperform models with thousands of trainable parameters when applied to smaller datasets. The identified master key filters closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives, structures that are not only fundamental to classical image processing but also strikingly similar to receptive fields in mammalian visual systems. Our findings provide compelling evidence that depthwise convolutional layers naturally gravitate toward this fundamental set of spatial operators regardless of task or architecture. This work offers new insights for understanding generalization and transfer learning through the universal language of these master key filters.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Layout Analysis Models for Docling</title>
<link>https://arxiv.org/abs/2509.11720</link>
<guid>https://arxiv.org/abs/2509.11720</guid>
<content:encoded><![CDATA[
arXiv:2509.11720v1 Announce Type: new 
Abstract: This technical report documents the development of novel Layout Analysis models integrated into the Docling document-conversion pipeline. We trained several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly available and proprietary). Post-processing steps were applied to the raw detections to make them more applicable to the document conversion task. We evaluated the effectiveness of the layout analysis on various document benchmarks using different methodologies while also measuring the runtime performance across different environments (CPU, Nvidia and Apple GPUs). We introduce five new document layout models achieving 20.6% - 23.9% mAP improvement over Docling's previous baseline, with comparable or better runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image inference time on a single NVIDIA A100 GPU. Extensive quantitative and qualitative experiments establish best practices for training, evaluating, and deploying document-layout detectors, providing actionable guidance for the document conversion community. All trained checkpoints, code, and documentation are released under a permissive license on HuggingFace.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Microsurgical Instrument Segmentation for Robot-Assisted Surgery</title>
<link>https://arxiv.org/abs/2509.11727</link>
<guid>https://arxiv.org/abs/2509.11727</guid>
<content:encoded><![CDATA[
arXiv:2509.11727v1 Announce Type: new 
Abstract: Accurate segmentation of thin structures is critical for microsurgical scene understanding but remains challenging due to resolution loss, low contrast, and class imbalance. We propose Microsurgery Instrument Segmentation for Robotic Assistance(MISRA), a segmentation framework that augments RGB input with luminance channels, integrates skip attention to preserve elongated features, and employs an Iterative Feedback Module(IFM) for continuity restoration across multiple passes. In addition, we introduce a dedicated microsurgical dataset with fine-grained annotations of surgical instruments including thin objects, providing a benchmark for robust evaluation Dataset available at https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate that MISRA achieves competitive performance, improving the mean class IoU by 5.37% over competing methods, while delivering more stable predictions at instrument contacts and overlaps. These results position MISRA as a promising step toward reliable scene parsing for computer-assisted and robotic microsurgery.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference</title>
<link>https://arxiv.org/abs/2509.11731</link>
<guid>https://arxiv.org/abs/2509.11731</guid>
<content:encoded><![CDATA[
arXiv:2509.11731v1 Announce Type: new 
Abstract: Trajectory data has become a key resource for automated map in-ference due to its low cost, broad coverage, and continuous availability. However, uneven trajectory density often leads to frag-mented roads in sparse areas and redundant segments in dense regions, posing significant challenges for existing methods. To address these issues, we propose DGMap, a dual-decoding framework with global context awareness, featuring Multi-scale Grid Encoding, Mask-enhanced Keypoint Extraction, and Global Context-aware Relation Prediction. By integrating global semantic context with local geometric features, DGMap improves keypoint detection accuracy to reduce road fragmentation in sparse-trajectory areas. Additionally, the Global Context-aware Relation Prediction module suppresses false connections in dense-trajectory regions by modeling long-range trajectory patterns. Experimental results on three real-world datasets show that DGMap outperforms state-of-the-art methods by 5% in APLS, with notable performance gains on trajectory data from the Didi Chuxing platform
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</title>
<link>https://arxiv.org/abs/2509.11752</link>
<guid>https://arxiv.org/abs/2509.11752</guid>
<content:encoded><![CDATA[
arXiv:2509.11752v1 Announce Type: new 
Abstract: Artificial intelligence (AI) that can effectively learn ultrasound representations by integrating multi-source data holds significant promise for advancing clinical care. However, the scarcity of large labeled datasets in real-world clinical environments and the limited generalizability of task-specific models have hindered the development of generalizable clinical AI models for ultrasound applications. In this study, we present EchoCare, a novel ultrasound foundation model for generalist clinical use, developed via self-supervised learning on our curated, publicly available, large-scale dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images, sourced from over 23 countries across 5 continents and acquired via a diverse range of distinct imaging devices, thus encompassing global cohorts that are multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt off-the-shelf vision foundation model architectures, we introduce a hierarchical classifier into EchoCare to enable joint learning of pixel-level and representation-level features, capturing both global anatomical contexts and local ultrasound characteristics. With minimal training, EchoCare outperforms state-of-the-art comparison models across 10 representative ultrasound benchmarks of varying diagnostic difficulties, spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement and report generation. The code and pretrained model are publicly released, rendering EchoCare accessible for fine-tuning and local adaptation, supporting extensibility to additional applications. EchoCare provides a fully open and generalizable foundation model to boost the development of AI technologies for diverse clinical ultrasound applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images</title>
<link>https://arxiv.org/abs/2509.11763</link>
<guid>https://arxiv.org/abs/2509.11763</guid>
<content:encoded><![CDATA[
arXiv:2509.11763v1 Announce Type: new 
Abstract: Reconstructing 3D face from a single unconstrained image remains a challenging problem due to diverse conditions in unconstrained environments. Recently, learning-based methods have achieved notable results by effectively capturing complex facial structures and details across varying conditions. Consequently, many existing approaches employ projection-based losses between generated and input images to constrain model training. However, learning-based methods for 3D face reconstruction typically require substantial amounts of 3D facial data, which is difficult and costly to obtain. Consequently, to reduce reliance on labeled 3D face datasets, many existing approaches employ projection-based losses between generated and input images to constrain model training. Nonetheless, despite these advancements, existing approaches frequently struggle to capture detailed and multi-scale features under diverse facial attributes and conditions, leading to incomplete or less accurate reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained images. Our method integrates multi-scale feature fusion with a focus on multi-attribute learning and leverages a large-kernel attention module to enhance the precision of feature extraction across scales, enabling accurate 3D facial parameter estimation from a single 2D image. Comprehensive experiments on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate that our approach achieves results on par with current state-of-the-art methods, and in some instances, surpasses SOTA performance across challenging conditions.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</title>
<link>https://arxiv.org/abs/2509.11772</link>
<guid>https://arxiv.org/abs/2509.11772</guid>
<content:encoded><![CDATA[
arXiv:2509.11772v1 Announce Type: new 
Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2509.11774</link>
<guid>https://arxiv.org/abs/2509.11774</guid>
<content:encoded><![CDATA[
arXiv:2509.11774v1 Announce Type: new 
Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such as diabetic retinopathy, hypertension, and neurodegenerative disorders. Although SA-UNet introduces spatial attention in the bottleneck, it underuses attention in skip connections and does not address the severe foreground-background imbalance. We propose SA-UNetv2, a lightweight model that injects cross-scale spatial attention into all skip connections to strengthen multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE) plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images, demonstrating strong efficiency and deployability in resource-constrained, CPU-only settings.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning</title>
<link>https://arxiv.org/abs/2509.11796</link>
<guid>https://arxiv.org/abs/2509.11796</guid>
<content:encoded><![CDATA[
arXiv:2509.11796v1 Announce Type: new 
Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has shown potential in general video understanding but faces significant challenges when applied to the inherently complex domain of sports videos. In this work, we propose FineQuest, the first training-free framework that leverages dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for straightforward sports queries and ii) Deliberative Reasoning for more complex ones. To bridge the knowledge gap between general-purpose models and domain-specific sports understanding, FineQuest incorporates SSGraph, a multimodal sports knowledge scene graph spanning nine sports, which encodes both visual instances and domain-specific terminology to enhance reasoning accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA and Diving-QA, derived from the FineGym and FineDiving datasets, enabling diverse and comprehensive evaluation. FineQuest achieves state-of-the-art performance on these benchmarks as well as the existing SPORTU dataset, while maintains strong general VideoQA capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics</title>
<link>https://arxiv.org/abs/2509.11800</link>
<guid>https://arxiv.org/abs/2509.11800</guid>
<content:encoded><![CDATA[
arXiv:2509.11800v1 Announce Type: new 
Abstract: Computer-aided diagnosis systems must make critical decisions from medical images that are often noisy, ambiguous, or conflicting, yet today's models are trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot labels erase inter-rater variability and force models to make overconfident predictions, especially when faced with incomplete or artifact-laden inputs. We address this gap by introducing a novel framework that brings uncertainty back into the label space. Our method leverages neural network training dynamics (NNTD) to assess the inherent difficulty of each training sample. By aggregating and calibrating model predictions during training, we generate uncertainty-aware pseudo-labels that reflect the ambiguity encountered during learning. This label augmentation approach is architecture-agnostic and can be applied to any supervised learning pipeline to enhance uncertainty estimation and robustness. We validate our approach on a challenging echocardiography classification benchmark, demonstrating superior performance over specialized baselines in calibration, selective classification, and multi-view fusion.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio</title>
<link>https://arxiv.org/abs/2509.11811</link>
<guid>https://arxiv.org/abs/2509.11811</guid>
<content:encoded><![CDATA[
arXiv:2509.11811v1 Announce Type: new 
Abstract: Retinal vessel segmentation is critical for the early diagnosis of vision-threatening and systemic diseases, especially in real-world clinical settings with limited computational resources. Although significant improvements have been made in deep learning-based segmentation methods, current models still face challenges in extracting tiny vessels and suffer from high computational costs. In this study, we present LFRA-Net by incorporating focal modulation attention at the encoder-decoder bottleneck and region-aware attention in the selective skip connections. LFRA-Net is a lightweight network optimized for precise and effective retinal vascular segmentation. It enhances feature representation and regional focus by efficiently capturing local and global dependencies. LFRA-Net outperformed many state-of-the-art models while maintaining lightweight characteristics with only 0.17 million parameters, 0.66 MB memory size, and 10.50 GFLOPs. We validated it on three publicly available datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and 74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net provides an ideal ratio between segmentation accuracy and computational cost compared to existing deep learning methods, which makes it suitable for real-time clinical applications in areas with limited resources. The code can be found at https://github.com/Mehwish4593/LFRA-Net.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecVLM: Fast Speculative Decoding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.11815</link>
<guid>https://arxiv.org/abs/2509.11815</guid>
<content:encoded><![CDATA[
arXiv:2509.11815v1 Announce Type: new 
Abstract: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.11817</link>
<guid>https://arxiv.org/abs/2509.11817</guid>
<content:encoded><![CDATA[
arXiv:2509.11817v1 Announce Type: new 
Abstract: Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods. The code is available at https://github.com/Abraham-Einstein/MAFS/.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network</title>
<link>https://arxiv.org/abs/2509.11838</link>
<guid>https://arxiv.org/abs/2509.11838</guid>
<content:encoded><![CDATA[
arXiv:2509.11838v1 Announce Type: new 
Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as medical imaging, autonomous driving, and environmental monitoring, where safety hinges on reliable model behavior under uncertainty. Yet, existing probabilistic verification approaches struggle to scale with the complexity and dimensionality of modern segmentation tasks, often yielding guarantees that are too conservative to be practical. We introduce a probabilistic verification framework that is both architecture-agnostic and scalable to high-dimensional outputs. Our approach combines sampling-based reachability analysis with conformal inference (CI) to deliver provable guarantees while avoiding the excessive conservatism of prior methods. To counteract CI's limitations in high-dimensional settings, we propose novel strategies that reduce conservatism without compromising rigor. Empirical evaluation on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates that our method provides reliable safety guarantees while substantially tightening bounds compared to SOTA. We also provide a toolbox implementing this technique, available on Github.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</title>
<link>https://arxiv.org/abs/2509.11840</link>
<guid>https://arxiv.org/abs/2509.11840</guid>
<content:encoded><![CDATA[
arXiv:2509.11840v1 Announce Type: new 
Abstract: Generative vision-language models (VLMs) exhibit strong high-level image understanding but lack spatially dense alignment between vision and language modalities, as our findings indicate. Orthogonal to advancements in generative VLMs, another line of research has focused on representation learning for vision-language alignment, targeting zero-shot inference for dense tasks like segmentation. In this work, we bridge these two directions by densely aligning images with synthetic descriptions generated by VLMs. Synthetic captions are inexpensive, scalable, and easy to generate, making them an excellent source of high-level semantic understanding for dense alignment methods. Empirically, our approach outperforms prior work on standard zero-shot open-vocabulary segmentation benchmarks/datasets, while also being more data-efficient.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11853</link>
<guid>https://arxiv.org/abs/2509.11853</guid>
<content:encoded><![CDATA[
arXiv:2509.11853v1 Announce Type: new 
Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</title>
<link>https://arxiv.org/abs/2509.11862</link>
<guid>https://arxiv.org/abs/2509.11862</guid>
<content:encoded><![CDATA[
arXiv:2509.11862v1 Announce Type: new 
Abstract: Video Question Answering (VQA) requires models to reason over spatial, temporal, and causal cues in videos. Recent vision language models (VLMs) achieve strong results but often rely on shallow correlations, leading to weak temporal grounding and limited interpretability. We study symbolic scene graphs (SGs) as intermediate grounding signals for VQA. SGs provide structured object-relation representations that complement VLMs holistic reasoning. We introduce SG-VLM, a modular framework that integrates frozen VLMs with scene graph grounding via prompting and visual localization. Across three benchmarks (NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM improves causal and temporal reasoning and outperforms prior baselines, though gains over strong VLMs are limited. These findings highlight both the promise and current limitations of symbolic grounding, and offer guidance for future hybrid VLM-symbolic approaches in video understanding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</title>
<link>https://arxiv.org/abs/2509.11866</link>
<guid>https://arxiv.org/abs/2509.11866</guid>
<content:encoded><![CDATA[
arXiv:2509.11866v1 Announce Type: new 
Abstract: Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/Eurekaleo/Dr.V.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods</title>
<link>https://arxiv.org/abs/2509.11873</link>
<guid>https://arxiv.org/abs/2509.11873</guid>
<content:encoded><![CDATA[
arXiv:2509.11873v1 Announce Type: new 
Abstract: Precision livestock farming requires advanced monitoring tools to meet the increasing management needs of the industry. Computer vision systems capable of long-term multi-animal tracking (MAT) are essential for continuous behavioral monitoring in livestock production. MAT, a specialized subset of multi-object tracking (MOT), shares many challenges with MOT, but also faces domain-specific issues including frequent animal occlusion, highly similar appearances among animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they often underperform compared to state-of-the-art MOT methods, which can result in inaccurate downstream tasks such as behavior analysis, health state estimation, and related applications. In this study, we benchmarked both MAT and MOT approaches for long-term tracking of pigs. We compared tools such as DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT, cross-input consistency, and newer approaches like Track-Anything and PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results demonstrate that, overall, MOT approaches outperform traditional MAT tools, even for long-term tracking scenarios. These findings highlight the potential of recent MOT techniques to enhance the accuracy and reliability of automated livestock tracking.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation</title>
<link>https://arxiv.org/abs/2509.11878</link>
<guid>https://arxiv.org/abs/2509.11878</guid>
<content:encoded><![CDATA[
arXiv:2509.11878v1 Announce Type: new 
Abstract: Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2509.11884</link>
<guid>https://arxiv.org/abs/2509.11884</guid>
<content:encoded><![CDATA[
arXiv:2509.11884v1 Announce Type: new 
Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages reverse parameter configuration and test-time training to enhance its performance on Camouflaged Object Detection (COD), named SAM-TTT. While most existing SAM-based COD models primarily focus on enhancing SAM by extracting favorable features and amplifying its advantageous parameters, a crucial gap is identified: insufficient attention to adverse parameters that impair SAM's semantic understanding in downstream tasks. To tackle this issue, the Reverse SAM Parameter Configuration Module is proposed to effectively mitigate the influence of adverse parameters in a train-free manner by configuring SAM's parameters. Building on this foundation, the T-Visioner Module is unveiled to strengthen advantageous parameters by integrating Test-Time Training layers, originally developed for language tasks, into vision tasks. Test-Time Training layers represent a new class of sequence modeling layers characterized by linear complexity and an expressive hidden state. By integrating two modules, SAM-TTT simultaneously suppresses adverse parameters while reinforcing advantageous ones, significantly improving SAM's semantic understanding in COD task. Our experimental results on various COD benchmarks demonstrate that the proposed approach achieves state-of-the-art performance, setting a new benchmark in the field. The code will be available at https://github.com/guobaoxiao/SAM-TTT.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation</title>
<link>https://arxiv.org/abs/2509.11885</link>
<guid>https://arxiv.org/abs/2509.11885</guid>
<content:encoded><![CDATA[
arXiv:2509.11885v1 Announce Type: new 
Abstract: Monocular depth estimation in bronchoscopy can significantly improve real-time navigation accuracy and enhance the safety of interventions in complex, branching airways. Recent advances in depth foundation models have shown promise for endoscopic scenarios, yet these models often lack anatomical awareness in bronchoscopy, overfitting to local textures rather than capturing the global airway structure, particularly under ambiguous depth cues and poor lighting. To address this, we propose Brea-Depth, a novel framework that integrates airway-specific geometric priors into foundation model adaptation for bronchoscopic depth estimation. Our method introduces a depth-aware CycleGAN, refining the translation between real bronchoscopic images and airway geometries from anatomical data, effectively bridging the domain gap. In addition, we introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. By incorporating anatomical priors, Brea-Depth enhances model generalization and yields more robust, accurate 3D airway reconstructions. To assess anatomical realism, we introduce Airway Depth Structure Evaluation, a new metric for structural consistency. We validate BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic dataset, where it outperforms existing methods in anatomical depth preservation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.11892</link>
<guid>https://arxiv.org/abs/2509.11892</guid>
<content:encoded><![CDATA[
arXiv:2509.11892v1 Announce Type: new 
Abstract: The ability to detect out-of-distribution data is essential not only for ensuring robustness against unknown or unexpected input data but also for improving the generalization performance of the model. Among various out-of-distribution detection methods, Outlier Exposure and Mixture Outlier Exposure are promising approaches that enhance out-of-distribution detection performance by exposing the outlier data during training. However, even with these sophisticated techniques, it remains challenging for models to learn the relationships between classes effectively and to distinguish data sampling from in-distribution and out-of-distribution clearly. Therefore, we focus on the logit space, where the properties between class-wise distributions are distinctly separated from those in the input or feature spaces. Specifically, we propose a linear interpolation technique in the logit space that mixes in-distribution and out-of-distribution data to facilitate smoothing logits between classes and improve the out-of-distribution detection performance, particularly for out-of-distribution data that lie close to the in-distribution data. Additionally, we enforce consistency between the logits obtained through mixing in the logit space and those generated via mixing in the input space. Our experiments demonstrate that our logit-space mixing technique reduces the abrupt fluctuations in the model outputs near the decision boundaries, resulting in smoother and more reliable separation between in-distribution and out-of-distribution data. Furthermore, we evaluate the effectiveness of the proposed method on a fine-grained out-of-distribution detection task.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Prior Observations for Incremental 3D Scene Graph Prediction</title>
<link>https://arxiv.org/abs/2509.11895</link>
<guid>https://arxiv.org/abs/2509.11895</guid>
<content:encoded><![CDATA[
arXiv:2509.11895v1 Announce Type: new 
Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations of environments by explicitly modeling objects, attributes, and relationships. While 3DSSGs have shown promise in robotics and embodied AI, many existing methods rely mainly on sensor data, not integrating further information from semantically rich environments. Additionally, most methods assume access to complete scene reconstructions, limiting their applicability in real-world, incremental settings. This paper introduces a novel heterogeneous graph model for incremental 3DSSG prediction that integrates additional, multi-modal information, such as prior observations, directly into the message-passing process. Utilizing multiple layers, the model flexibly incorporates global and local scene representations without requiring specialized modules or full scene reconstructions. We evaluate our approach on the 3DSSG dataset, showing that GNNs enriched with multi-modal information such as semantic embeddings (e.g., CLIP) and prior observations offer a scalable and generalizable solution for complex, real-world environments. The full source code of the presented architecture will be made available at https://github.com/m4renz/incremental-scene-graph-prediction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.11916</link>
<guid>https://arxiv.org/abs/2509.11916</guid>
<content:encoded><![CDATA[
arXiv:2509.11916v1 Announce Type: new 
Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to generalize across datasets because facial appearance is an indirect and biased proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal distillation framework that transfers brain-informed priors into an image-only FER student via static Valence/Arousal (V/A) prototypes and a depression-inspired geometric prior (D-Geo). A teacher trained on EEG topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face pairing and no non-visual signals at deployment are required. The student (ResNet-18/50) is trained on FERPlus with conventional CE/KD and two lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with affective findings often reported in depression research (e.g., anhedonia-like contraction in high-valence regions). We evaluate both within-domain (FERPlus validation) and cross-dataset protocols (AffectNet-mini; optional CK+), reporting standard 8-way scores alongside present-only Macro-F1 and balanced accuracy to fairly handle label-set mismatch. Ablations attribute consistent gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability. The method is simple, deployable, and improves robustness without architectural complexity.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI</title>
<link>https://arxiv.org/abs/2509.11924</link>
<guid>https://arxiv.org/abs/2509.11924</guid>
<content:encoded><![CDATA[
arXiv:2509.11924v1 Announce Type: new 
Abstract: Multimodal learning has attracted much attention in recent years due to its ability to effectively utilize data features from a variety of different modalities. Diagnosing the vulnerability of atherosclerotic plaques directly from carotid 3D MRI images is relatively challenging for both radiologists and conventional 3D vision networks. In clinical practice, radiologists assess patient conditions using a multimodal approach that incorporates various imaging modalities and domain-specific expertise, paving the way for the creation of multimodal diagnostic networks. In this paper, we have developed an effective strategy to leverage radiologists' domain knowledge to automate the diagnosis of carotid plaque vulnerability through Variation inference and Multimodal knowledge Distillation (VMD). This method excels in harnessing cross-modality prior knowledge from limited image annotations and radiology reports within training data, thereby enhancing the diagnostic network's accuracy for unannotated 3D MRI images. We conducted in-depth experiments on the dataset collected in-house and verified the effectiveness of the VMD strategy we proposed.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization</title>
<link>https://arxiv.org/abs/2509.11926</link>
<guid>https://arxiv.org/abs/2509.11926</guid>
<content:encoded><![CDATA[
arXiv:2509.11926v1 Announce Type: new 
Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random and then optimize each one via stochastic gradient descent (SGD), resulting in substantial risk of poor-performing local minima.Focusing on the image interpolation problem and leveraging a recent theorem that maps a (pseudo-)linear interpolator {\Theta} to a directed graph filter that is a solution to a MAP problem regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator {\Theta}, establishing a baseline performance.Then, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight interpretable neural net.Experimental results demonstrate state-of-the-art image interpolation results, while drastically reducing network parameters.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360{\deg} Videos</title>
<link>https://arxiv.org/abs/2509.11948</link>
<guid>https://arxiv.org/abs/2509.11948</guid>
<content:encoded><![CDATA[
arXiv:2509.11948v1 Announce Type: new 
Abstract: The recent success of immersive applications is pushing the research community to define new approaches to process 360{\deg} images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360{\deg} videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360{\deg} video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation</title>
<link>https://arxiv.org/abs/2509.11952</link>
<guid>https://arxiv.org/abs/2509.11952</guid>
<content:encoded><![CDATA[
arXiv:2509.11952v1 Announce Type: new 
Abstract: Accurate land cover classification from satellite imagery is crucial in environmental monitoring and sustainable resource management. However, it remains challenging due to the complexity of natural landscapes, the visual similarity between classes, and the significant class imbalance in the available datasets. To address these issues, we propose a dual encoder architecture that independently extracts modality-specific features from optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using a cross-modality attention-fusion module named Cross-modality Land cover segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations (CLAIRE). This fusion mechanism highlights complementary spatial and textural features, enabling the network to better capture detailed and diverse land cover patterns. We incorporate a hybrid loss function that utilizes Weighted Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address class imbalance and improve segmentation performance across underrepresented categories. Our model achieves competitive performance across multiple benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and remarkable robustness under cloud-obstructed conditions, achieving an mIoU of 86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce a metric-driven reasoning module generated by a Small Language Model (Phi-3), which generates expert-level, sample-specific justifications for model predictions, thereby enhancing transparency and interpretability.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Generate 4D LiDAR Sequences</title>
<link>https://arxiv.org/abs/2509.11959</link>
<guid>https://arxiv.org/abs/2509.11959</guid>
<content:encoded><![CDATA[
arXiv:2509.11959v1 Announce Type: new 
Abstract: While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Embeddings: Information Loss in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.11986</link>
<guid>https://arxiv.org/abs/2509.11986</guid>
<content:encoded><![CDATA[
arXiv:2509.11986v1 Announce Type: new 
Abstract: Vision--language models (VLMs) often process visual inputs through a pretrained vision encoder, followed by a projection into the language model's embedding space via a connector component. While crucial for modality fusion, the potential information loss induced by this projection step and its direct impact on model capabilities remain understudied. We introduce two complementary approaches to examine and quantify this loss by analyzing the latent representation space. First, we evaluate semantic information preservation by analyzing changes in k-nearest neighbor relationships between image representations, before and after projection. Second, we directly measure information loss by reconstructing visual embeddings from the projected representation, localizing loss at an image patch level. Experiments reveal that connectors substantially distort the local geometry of visual representations, with k-nearest neighbors diverging by 40--60\% post-projection, correlating with degradation in retrieval performance. The patch-level embedding reconstruction provides interpretable insights for model behavior on visually grounded question-answering tasks, finding that areas of high information loss reliably predict instances where models struggle.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness</title>
<link>https://arxiv.org/abs/2509.12024</link>
<guid>https://arxiv.org/abs/2509.12024</guid>
<content:encoded><![CDATA[
arXiv:2509.12024v1 Announce Type: new 
Abstract: Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2509.12039</link>
<guid>https://arxiv.org/abs/2509.12039</guid>
<content:encoded><![CDATA[
arXiv:2509.12039v1 Announce Type: new 
Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at https://github.com/DragonisCV/RAM
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing</title>
<link>https://arxiv.org/abs/2509.12040</link>
<guid>https://arxiv.org/abs/2509.12040</guid>
<content:encoded><![CDATA[
arXiv:2509.12040v1 Announce Type: new 
Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking</title>
<link>https://arxiv.org/abs/2509.12046</link>
<guid>https://arxiv.org/abs/2509.12046</guid>
<content:encoded><![CDATA[
arXiv:2509.12046v1 Announce Type: new 
Abstract: While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset</title>
<link>https://arxiv.org/abs/2509.12047</link>
<guid>https://arxiv.org/abs/2509.12047</guid>
<content:encoded><![CDATA[
arXiv:2509.12047v1 Announce Type: new 
Abstract: Animal behavior analysis plays a crucial role in understanding animal welfare, health status, and productivity in agricultural settings. However, traditional manual observation methods are time-consuming, subjective, and limited in scalability. We present a modular pipeline that leverages open-sourced state-of-the-art computer vision techniques to automate animal behavior analysis in a group housing environment. Our approach combines state-of-the-art models for zero-shot object detection, motion-aware tracking and segmentation, and advanced feature extraction using vision transformers for robust behavior recognition. The pipeline addresses challenges including animal occlusions and group housing scenarios as demonstrated in indoor pig monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset for multiple behavioral tasks. Our temporal model achieved 94.2% overall accuracy, representing a 21.2 percentage point improvement over existing methods. The pipeline demonstrated robust tracking capabilities with 93.3% identity preservation score and 89.3% object detection precision. The modular design suggests potential for adaptation to other contexts, though further validation across species would be required. The open-source implementation provides a scalable solution for behavior monitoring, contributing to precision pig farming and welfare assessment through automated, objective, and continuous analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective</title>
<link>https://arxiv.org/abs/2509.12052</link>
<guid>https://arxiv.org/abs/2509.12052</guid>
<content:encoded><![CDATA[
arXiv:2509.12052v1 Announce Type: new 
Abstract: Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate "Divide and Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation</title>
<link>https://arxiv.org/abs/2509.12062</link>
<guid>https://arxiv.org/abs/2509.12062</guid>
<content:encoded><![CDATA[
arXiv:2509.12062v1 Announce Type: new 
Abstract: Fetal motion is a critical indicator of neurological development and intrauterine health, yet its quantification remains challenging, particularly at earlier gestational ages (GA). Current methods track fetal motion by predicting the location of annotated landmarks on 3D echo planar imaging (EPI) time-series, primarily in third-trimester fetuses. The predicted landmarks enable simplification of the fetal body for downstream analysis. While these methods perform well within their training age distribution, they consistently fail to generalize to early GAs due to significant anatomical changes in both mother and fetus across gestation, as well as the difficulty of obtaining annotated early GA EPI data. In this work, we develop a cross-population data augmentation framework that enables pose estimation models to robustly generalize to younger GA clinical cohorts using only annotated images from older GA cohorts. Specifically, we introduce a fetal-specific augmentation strategy that simulates the distinct intrauterine environment and fetal positioning of early GAs. Our experiments find that cross-population augmentation yields reduced variability and significant improvements across both older GA and challenging early GA cases. By enabling more reliable pose estimation across gestation, our work potentially facilitates early clinical detection and intervention in challenging 4D fetal imaging settings. Code is available at https://github.com/sebodiaz/cross-population-pose.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data</title>
<link>https://arxiv.org/abs/2509.12068</link>
<guid>https://arxiv.org/abs/2509.12068</guid>
<content:encoded><![CDATA[
arXiv:2509.12068v1 Announce Type: new 
Abstract: The fine-grained surface reconstruction of different organs from 3D medical imaging can provide advanced diagnostic support and improved surgical planning. However, the representation of the organs is often limited by the resolution, with a detailed higher resolution requiring more memory and computing footprint. Implicit representations of objects have been proposed to alleviate this problem in general computer vision by providing compact and differentiable functions to represent the 3D object shapes. However, architectural and data-related differences prevent the direct application of these methods to medical images. This work introduces ImplMORe, an end-to-end deep learning method using implicit surface representations for multi-organ reconstruction from 3D medical images. ImplMORe incorporates local features using a 3D CNN encoder and performs multi-scale interpolation to learn the features in the continuous domain using occupancy functions. We apply our method for single and multiple organ reconstructions using the totalsegmentator dataset. By leveraging the continuous nature of occupancy functions, our approach outperforms the discrete explicit representation based surface reconstruction approaches, providing fine-grained surface details of the organ at a resolution higher than the given input image. The source code will be made publicly available at: https://github.com/CAMMA-public/ImplMORe
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2509.12069</link>
<guid>https://arxiv.org/abs/2509.12069</guid>
<content:encoded><![CDATA[
arXiv:2509.12069v1 Announce Type: new 
Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing top 3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with an average inference time of XX (TBC during the ODIN workshop). In Task 2, U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Flow-inspired Unfolding for Spectral Compressive Imaging</title>
<link>https://arxiv.org/abs/2509.12079</link>
<guid>https://arxiv.org/abs/2509.12079</guid>
<content:encoded><![CDATA[
arXiv:2509.12079v1 Announce Type: new 
Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral image (HSI) from a single 2D compressed measurement, which is a highly challenging reconstruction task. Recent deep unfolding networks (DUNs), empowered by explicit data-fidelity updates and implicit deep denoisers, have achieved the state of the art in CASSI reconstruction. However, existing unfolding approaches suffer from uncontrollable reconstruction trajectories, leading to abrupt quality jumps and non-gradual refinement across stages. Inspired by diffusion trajectories and flow matching, we propose a novel trajectory-controllable unfolding framework that enforces smooth, continuous optimization paths from noisy initial estimates to high-quality reconstructions. To achieve computational efficiency, we design an efficient spatial-spectral Transformer tailored for hyperspectral reconstruction, along with a frequency-domain fusion module to gurantee feature consistency. Experiments on simulation and real data demonstrate that our method achieves better reconstruction quality and efficiency than prior state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</title>
<link>https://arxiv.org/abs/2509.12090</link>
<guid>https://arxiv.org/abs/2509.12090</guid>
<content:encoded><![CDATA[
arXiv:2509.12090v1 Announce Type: new 
Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2509.12105</link>
<guid>https://arxiv.org/abs/2509.12105</guid>
<content:encoded><![CDATA[
arXiv:2509.12105v1 Announce Type: new 
Abstract: Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at https://github.com/fornib/FS-SAM2
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
<link>https://arxiv.org/abs/2509.12125</link>
<guid>https://arxiv.org/abs/2509.12125</guid>
<content:encoded><![CDATA[
arXiv:2509.12125v1 Announce Type: new 
Abstract: Tram-human interaction safety is an important challenge, given that trams frequently operate in densely populated areas, where collisions can range from minor injuries to fatal outcomes. This paper addresses the issue from the perspective of designing a solution leveraging digital image processing, deep learning, and artificial intelligence to improve the safety of pedestrians, drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a real-time framework that fuses semantic segmentation, object detection and a rule-based Distance Assessor to highlight track intrusions. Using only monocular video, the system identifies rails, localises nearby objects and classifies their risk by comparing projected distances with the standard 1435mm rail gauge. Experiments on the diverse RailSem19 dataset show that a class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU), while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore delivers accurate, annotation-light scene understanding that can warn drivers before dangerous situations escalate. Code available at https://github.com/oValach/RailSafeNet.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12132</link>
<guid>https://arxiv.org/abs/2509.12132</guid>
<content:encoded><![CDATA[
arXiv:2509.12132v1 Announce Type: new 
Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical challenges: Effective "slow thinking" in VRMs requires \textbf{visual reflection}, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, \textbf{Reflection-V} demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</title>
<link>https://arxiv.org/abs/2509.12143</link>
<guid>https://arxiv.org/abs/2509.12143</guid>
<content:encoded><![CDATA[
arXiv:2509.12143v1 Announce Type: new 
Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</title>
<link>https://arxiv.org/abs/2509.12145</link>
<guid>https://arxiv.org/abs/2509.12145</guid>
<content:encoded><![CDATA[
arXiv:2509.12145v1 Announce Type: new 
Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines online temporal action localization with free-form description generation. Given the scarcity of datasets with hierarchical and fine-grained temporal annotations, we demonstrate that LLMs can effectively group atomic actions into higher-level events, enriching existing datasets. We then propose OpenHOUSE (Open-ended Hierarchical Online Understanding System for Events), which extends streaming action perception beyond action classification. OpenHOUSE features a specialized streaming module that accurately detects boundaries between closely adjacent actions, nearly doubling the performance of direct extensions of existing methods. We envision the future of streaming action perception in the integration of powerful generative models, with OpenHOUSE representing a key step in that direction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Anatomy X-Ray Foundation Model</title>
<link>https://arxiv.org/abs/2509.12146</link>
<guid>https://arxiv.org/abs/2509.12146</guid>
<content:encoded><![CDATA[
arXiv:2509.12146v1 Announce Type: new 
Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury</title>
<link>https://arxiv.org/abs/2509.12155</link>
<guid>https://arxiv.org/abs/2509.12155</guid>
<content:encoded><![CDATA[
arXiv:2509.12155v1 Announce Type: new 
Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloGarment: 360{\deg} Novel View Synthesis of In-the-Wild Garments</title>
<link>https://arxiv.org/abs/2509.12187</link>
<guid>https://arxiv.org/abs/2509.12187</guid>
<content:encoded><![CDATA[
arXiv:2509.12187v1 Announce Type: new 
Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due significant occlusions, complex human poses, and cloth deformations. Prior methods rely on synthetic 3D training data consisting of mostly unoccluded and static objects, leading to poor generalization on real-world clothing. In this paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3 images or a continuous video of a person wearing a garment and generates 360{\deg} novel views of the garment in a canonical pose. Our key insight is to bridge the domain gap between real and synthetic data with a novel implicit training paradigm leveraging a combination of large-scale real video data and small-scale synthetic 3D data to optimize a shared garment embedding space. During inference, the shared embedding space further enables dynamic video-to-360{\deg} NVS through the construction of a garment "atlas" representation by finetuning a garment embedding on a specific real-world video. The atlas captures garment-specific geometry and texture across all viewpoints, independent of body pose or motion. Extensive experiments show that HoloGarment achieves state-of-the-art performance on NVS of in-the-wild garments from images and videos. Notably, our method robustly handles challenging real-world artifacts -- such as wrinkling, pose variation, and occlusion -- while maintaining photorealism, view consistency, fine texture details, and accurate geometry. Visit our project page for additional results: https://johannakarras.github.io/HoloGarment
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Adaptive Pretraining Improves Primate Behavior Recognition</title>
<link>https://arxiv.org/abs/2509.12193</link>
<guid>https://arxiv.org/abs/2509.12193</guid>
<content:encoded><![CDATA[
arXiv:2509.12193v1 Announce Type: new 
Abstract: Computer vision for animal behavior offers promising tools to aid research in ecology, cognition, and to support conservation efforts. Video camera traps allow for large-scale data collection, but high labeling costs remain a bottleneck to creating large-scale datasets. We thus need data-efficient learning approaches. In this work, we show that we can utilize self-supervised learning to considerably improve action recognition on primate behavior. On two datasets of great ape behavior (PanAf and ChimpACT), we outperform published state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt. mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining with in-domain data. We show that most of the performance gain stems from the DAP. Our method promises great potential for improving the recognition of animal behavior, as DAP does not require labeled samples. Code is available at https://github.com/ecker-lab/dap-behavior
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</title>
<link>https://arxiv.org/abs/2509.12197</link>
<guid>https://arxiv.org/abs/2509.12197</guid>
<content:encoded><![CDATA[
arXiv:2509.12197v1 Announce Type: new 
Abstract: In this paper, we present a comprehensive review of 3D human pose estimation and human mesh recovery from in-the-wild LiDAR point clouds. We compare existing approaches across several key dimensions, and propose a structured taxonomy to classify these methods. Following this taxonomy, we analyze each method's strengths, limitations, and design choices. In addition, (i) we perform a quantitative comparison of the three most widely used datasets, detailing their characteristics; (ii) we compile unified definitions of all evaluation metrics; and (iii) we establish benchmark tables for both tasks on these datasets to enable fair comparisons and promote progress in the field. We also outline open challenges and research directions critical for advancing LiDAR-based 3D human understanding. Moreover, we maintain an accompanying webpage that organizes papers according to our taxonomy and continuously update it with new studies: https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</title>
<link>https://arxiv.org/abs/2509.12201</link>
<guid>https://arxiv.org/abs/2509.12201</guid>
<content:encoded><![CDATA[
arXiv:2509.12201v1 Announce Type: new 
Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence</title>
<link>https://arxiv.org/abs/2509.12203</link>
<guid>https://arxiv.org/abs/2509.12203</guid>
<content:encoded><![CDATA[
arXiv:2509.12203v1 Announce Type: new 
Abstract: The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Character-Centric Understanding of Animated Movies</title>
<link>https://arxiv.org/abs/2509.12204</link>
<guid>https://arxiv.org/abs/2509.12204</guid>
<content:encoded><![CDATA[
arXiv:2509.12204v1 Announce Type: new 
Abstract: Animated movies are captivating for their unique character designs and imaginative storytelling, yet they pose significant challenges for existing recognition systems. Unlike the consistent visual patterns detected by conventional face recognition methods, animated characters exhibit extreme diversity in their appearance, motion, and deformation. In this work, we propose an audio-visual pipeline to enable automatic and robust animated character recognition, and thereby enhance character-centric understanding of animated movies. Central to our approach is the automatic construction of an audio-visual character bank from online sources. This bank contains both visual exemplars and voice (audio) samples for each character, enabling subsequent multi-modal character recognition despite long-tailed appearance distributions. Building on accurate character recognition, we explore two downstream applications: Audio Description (AD) generation for visually impaired audiences, and character-aware subtitling for the hearing impaired. To support research in this domain, we introduce CMD-AM, a new dataset of 75 animated movies with comprehensive annotations. Our character-centric pipeline demonstrates significant improvements in both accessibility and narrative comprehension for animated content over prior face-detection-based approaches. For the code and dataset, visit https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2410.06927</link>
<guid>https://arxiv.org/abs/2410.06927</guid>
<content:encoded><![CDATA[
arXiv:2410.06927v2 Announce Type: cross 
Abstract: Convolutional neural networks (CNNs) are widely used in computer vision. They can be used not only for conventional digital image material to recognize patterns, but also for feature extraction from digital imagery representing spectral and rhythm features extracted from time-domain digital audio signals for the acoustic classification of sounds. Different spectral and rhythm feature representations like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams are investigated in terms of the audio classification performance using a deep convolutional neural network. It can be clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCCs) perform significantly better than the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs. The experiments were carried out with the aid of the ESC-50 dataset with 2,000 labeled environmental audio recordings.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Bottleneck in Deep Neural Networks: Noise is All You Need</title>
<link>https://arxiv.org/abs/2509.09719</link>
<guid>https://arxiv.org/abs/2509.09719</guid>
<content:encoded><![CDATA[
arXiv:2509.09719v1 Announce Type: cross 
Abstract: Deep neural networks are known to exhibit a spectral learning bias, wherein low-frequency components are learned early in training, while high-frequency modes emerge more gradually in later epochs. However, when the target signal lacks low-frequency components and is dominated by broadband high frequencies, training suffers from a 'spectral bottleneck', and the model fails to reconstruct the entire signal, including the frequency components that lie within the network's representational capacity. We examine such a scenario in the context of implicit neural representations (INRs) with sinusoidal representation networks (SIRENs), focusing on the challenge of fitting high-frequency-dominant signals that are susceptible to spectral bottleneck. To effectively fit any target signal irrespective of it's frequency content, we propose a generalized target-aware 'weight perturbation scheme' (WINNER - weight initialization with noise for neural representations) for network initialization. The scheme perturbs uniformly initialized weights with Gaussian noise, where the noise scales are adaptively determined by the spectral centroid of the target signal. We show that the noise scales can provide control over the spectra of network activations and the eigenbasis of the empirical neural tangent kernel. This method not only addresses the spectral bottleneck but also yields faster convergence and with improved representation accuracy, outperforming state-of-the-art approaches in audio fitting and achieving notable gains in image fitting and denoising tasks. Beyond signal reconstruction, our approach opens new directions for adaptive weight initialization strategies in computer vision and scientific machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results</title>
<link>https://arxiv.org/abs/2509.10463</link>
<guid>https://arxiv.org/abs/2509.10463</guid>
<content:encoded><![CDATA[
arXiv:2509.10463v1 Announce Type: cross 
Abstract: This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop's objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph</title>
<link>https://arxiv.org/abs/2509.10467</link>
<guid>https://arxiv.org/abs/2509.10467</guid>
<content:encoded><![CDATA[
arXiv:2509.10467v1 Announce Type: cross 
Abstract: Current general-purpose large language models (LLMs) commonly exhibit knowledge hallucination and insufficient domain-specific adaptability in domain-specific tasks, limiting their effectiveness in specialized question answering scenarios. Retrieval-augmented generation (RAG) effectively tackles these challenges by integrating external knowledge to enhance accuracy and relevance. However, traditional RAG still faces limitations in domain knowledge accuracy and context modeling.To enhance domain-specific question answering performance, this work focuses on a graph-based RAG framework, emphasizing the critical role of knowledge graph quality during the generation process. We propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven retrieval-augmented generation framework designed for domain-specific applications. Our approach leverages domain-specific documents as the primary knowledge source, integrating heterogeneous information such as text, images, and tables to construct a multimodal knowledge graph covering both conceptual and instance layers. Building on this foundation, we introduce semantic pruning and structured subgraph retrieval mechanisms, combining knowledge graph context and vector retrieval results to guide the language model towards producing more reliable responses. Evaluations using the Langfuse multidimensional scoring mechanism show that our method excels in domain-specific question answering, validating the efficacy of integrating multimodal knowledge graphs with retrieval-augmented generation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIDOG 2025 Track 2: A Deep Learning Model for Classification of Atypical and Normal Mitotic Figures under Class and Hardness Imbalances</title>
<link>https://arxiv.org/abs/2509.10502</link>
<guid>https://arxiv.org/abs/2509.10502</guid>
<content:encoded><![CDATA[
arXiv:2509.10502v1 Announce Type: cross 
Abstract: Motivation: Accurate classification of mitotic figures into normal and atypical types is crucial for tumor prognostication in digital pathology. However, developing robust deep learning models for this task is challenging due to the subtle morphological differences, as well as significant class and hardness imbalances in real-world histopathology datasets. Methods: We propose a novel deep learning approach based on a ResNet backbone with specialized classification heads. Our architecture uniquely models both the mitotic figure phenotype and the instance difficulty simultaneously. This method is specifically designed to handle the challenges of diverse tissue types, scanner variability, and imbalanced data. We employed focal loss to effectively mitigate the pronounced class imbalance, and a comprehensive data augmentation pipeline was implemented to enhance the model's robustness and generalizability. Results: Our approach demonstrated strong and consistent performance. In a 5-fold cross-validation on the MIDOG 2025 Track 2 dataset, it achieved a mean balanced accuracy of 0.8744 +/- 0.0093 and an ROC AUC of 0.9505 +/- 0.029. The model showed robust generalization across preliminary leaderboard evaluations, achieving an overall balanced accuracy of 0.8736 +/- 0.0204. Conclusion: The proposed method offers a reliable and generalizable solution for the classification of atypical and normal mitotic figures. By addressing the inherent challenges of real world data, our approach has the potential to support precise prognostic assessments in clinical practice and improve consistency in pathological diagnosis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free</title>
<link>https://arxiv.org/abs/2509.10503</link>
<guid>https://arxiv.org/abs/2509.10503</guid>
<content:encoded><![CDATA[
arXiv:2509.10503v1 Announce Type: cross 
Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a global object detection model without accessing their local data from diverse domains. However, significant variations in environment, weather, and other domain specific factors hinder performance, making cross domain generalization a key challenge. Existing FOD methods often overlook the hardware constraints of edge devices and introduce local training regularizations that incur high computational costs, limiting real-world applicability. In this paper, we propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without introducing additional local computational overhead. FEDEXCHANGE employs a server side dynamic model exchange strategy that enables each client to gain insights from other clients' domain data without direct data sharing. Specifically, FEDEXCHANGE allows the server to alternate between model aggregation and model exchange. During aggregation rounds, the server aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters and exchanges local models based on distance measures, allowing local models to learn from a variety of domains. As all operations are performed on the server side, clients can achieve improved cross domain utility without any additional computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE enhances FOD performance, achieving 1.6X better mean average precision in challenging domains, such as rainy conditions, while requiring only 0.8X the computational resources compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification</title>
<link>https://arxiv.org/abs/2509.10510</link>
<guid>https://arxiv.org/abs/2509.10510</guid>
<content:encoded><![CDATA[
arXiv:2509.10510v1 Announce Type: cross 
Abstract: Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets; however, standard GNNs often operate as black boxes, limiting transparency and usability, particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction</title>
<link>https://arxiv.org/abs/2509.10522</link>
<guid>https://arxiv.org/abs/2509.10522</guid>
<content:encoded><![CDATA[
arXiv:2509.10522v1 Announce Type: cross 
Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense airspace, where accurate workload modeling is critical for safety and efficiency. This paper proposes a multimodal deep learning framework that integrates structured data, trajectory sequences, and image features to estimate two key parameters in the ATCO command lifecycle: the time offset between a command and the resulting aircraft maneuver, and the command duration. A high-quality dataset was constructed, with maneuver points detected using sliding window and histogram-based methods. A CNN-Transformer ensemble model was developed for accurate, generalizable, and interpretable predictions. By linking trajectories to voice commands, this work offers the first model of its kind to support intelligent command generation and provides practical value for workload assessment, staffing, and scheduling.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay</title>
<link>https://arxiv.org/abs/2509.10529</link>
<guid>https://arxiv.org/abs/2509.10529</guid>
<content:encoded><![CDATA[
arXiv:2509.10529v1 Announce Type: cross 
Abstract: Continual learning -- the ability to acquire knowledge incrementally without forgetting previous skills -- is fundamental to natural intelligence. While the human brain excels at this, artificial neural networks struggle with "catastrophic forgetting," where learning new tasks erases previously acquired knowledge. This challenge is particularly severe for text-to-image diffusion models, which generate images from textual prompts. Additionally, these models face "mode collapse," where their outputs become increasingly repetitive over time. To address these challenges, we apply Latent Replay, a neuroscience-inspired approach, to diffusion models. Traditional replay methods mitigate forgetting by storing and revisiting past examples, typically requiring large collections of images. Latent Replay instead retains only compact, high-level feature representations extracted from the model's internal architecture. This mirrors the hippocampal process of storing neural activity patterns rather than raw sensory inputs, reducing memory usage while preserving critical information. Through experiments with five sequentially learned visual concepts, we demonstrate that Latent Replay significantly outperforms existing methods in maintaining model versatility. After learning all concepts, our approach retained 77.59% Image Alignment (IA) on the earliest concept, 14% higher than baseline methods, while maintaining diverse outputs. Surprisingly, random selection of stored latent examples outperforms similarity-based strategies. Our findings suggest that Latent Replay enables efficient continual learning for generative AI models, paving the way for personalized text-to-image models that evolve with user needs without excessive computational costs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Cervical Os Segmentation for Camera-Guided, Speculum-Free Screening</title>
<link>https://arxiv.org/abs/2509.10593</link>
<guid>https://arxiv.org/abs/2509.10593</guid>
<content:encoded><![CDATA[
arXiv:2509.10593v1 Announce Type: cross 
Abstract: Cervical cancer is highly preventable, yet persistent barriers to screening limit progress toward elimination goals. Speculum-free devices that integrate imaging and sampling could improve access, particularly in low-resource settings, but require reliable visual guidance. This study evaluates deep learning methods for real-time segmentation of the cervical os in transvaginal endoscopic images. Five encoder-decoder architectures were compared using 913 frames from 200 cases in the IARC Cervical Image Dataset, annotated by gynaecologists. Performance was assessed using IoU, DICE, detection rate, and distance metrics with ten-fold cross-validation. EndoViT/DPT, a vision transformer pre-trained on surgical video, achieved the highest DICE (0.50 \pm 0.31) and detection rate (0.87 \pm 0.33), outperforming CNN-based approaches. External validation with phantom data demonstrated robust segmentation under variable conditions at 21.5 FPS, supporting real-time feasibility. These results establish a foundation for integrating automated os recognition into speculum-free cervical screening devices to support non-expert use in both high- and low-resource contexts.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning</title>
<link>https://arxiv.org/abs/2509.10635</link>
<guid>https://arxiv.org/abs/2509.10635</guid>
<content:encoded><![CDATA[
arXiv:2509.10635v1 Announce Type: cross 
Abstract: Machine learning has shown promise in facial dysmorphology, where characteristic facial features provide diagnostic clues for rare genetic disorders. GestaltMatcher, a leading framework in this field, has demonstrated clinical utility across multiple studies, but its reliance on centralized datasets limits further development, as patient data are siloed across institutions and subject to strict privacy regulations. We introduce a federated GestaltMatcher service based on a cross-silo horizontal federated learning framework, which allows hospitals to collaboratively train a global ensemble feature extractor without sharing patient images. Patient data are mapped into a shared latent space, and a privacy-preserving kernel matrix computation framework enables syndrome inference and discovery while safeguarding confidentiality. New participants can directly benefit from and contribute to the system by adopting the global feature extractor and kernel configuration from previous training rounds. Experiments show that the federated service retains over 90% of centralized performance and remains robust to both varying silo numbers and heterogeneous data distributions.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction</title>
<link>https://arxiv.org/abs/2509.10698</link>
<guid>https://arxiv.org/abs/2509.10698</guid>
<content:encoded><![CDATA[
arXiv:2509.10698v1 Announce Type: cross 
Abstract: Predicting the success of start-up companies, defined as achieving an exit through acquisition or IPO, is a critical problem in entrepreneurship and innovation research. Datasets such as Crunchbase provide both structured information (e.g., funding rounds, industries, investor networks) and unstructured text (e.g., company descriptions), but effectively leveraging this heterogeneous data for prediction remains challenging. Traditional machine learning approaches often rely only on structured features and achieve moderate accuracy, while large language models (LLMs) offer rich reasoning abilities but struggle to adapt directly to domain-specific business data. We present \textbf{CrunchLLM}, a domain-adapted LLM framework for startup success prediction. CrunchLLM integrates structured company attributes with unstructured textual narratives and applies parameter-efficient fine-tuning strategies alongside prompt optimization to specialize foundation models for entrepreneurship data. Our approach achieves accuracy exceeding 80\% on Crunchbase startup success prediction, significantly outperforming traditional classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM provides interpretable reasoning traces that justify its predictions, enhancing transparency and trustworthiness for financial and policy decision makers. This work demonstrates how adapting LLMs with domain-aware fine-tuning and structured--unstructured data fusion can advance predictive modeling of entrepreneurial outcomes. CrunchLLM contributes a methodological framework and a practical tool for data-driven decision making in venture capital and innovation policy.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration</title>
<link>https://arxiv.org/abs/2509.10704</link>
<guid>https://arxiv.org/abs/2509.10704</guid>
<content:encoded><![CDATA[
arXiv:2509.10704v1 Announce Type: cross 
Abstract: Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2509.10784</link>
<guid>https://arxiv.org/abs/2509.10784</guid>
<content:encoded><![CDATA[
arXiv:2509.10784v1 Announce Type: cross 
Abstract: Medical Vision Foundation Models (Med-VFMs) have superior capabilities of interpreting medical images due to the knowledge learned from self-supervised pre-training with extensive unannotated images. To improve their performance on adaptive downstream evaluations, especially segmentation, a few samples from target domains are selected randomly for fine-tuning them. However, there lacks works to explore the way of adapting Med-VFMs to achieve the optimal performance on target domains efficiently. Thus, it is highly demanded to design an efficient way of fine-tuning Med-VFMs by selecting informative samples to maximize their adaptation performance on target domains. To achieve this, we propose an Active Source-Free Domain Adaptation (ASFDA) method to efficiently adapt Med-VFMs to target domains for volumetric medical image segmentation. This ASFDA employs a novel Active Learning (AL) method to select the most informative samples from target domains for fine-tuning Med-VFMs without the access to source pre-training samples, thus maximizing their performance with the minimal selection budget. In this AL method, we design an Active Test Time Sample Query strategy to select samples from the target domains via two query metrics, including Diversified Knowledge Divergence (DKD) and Anatomical Segmentation Difficulty (ASD). DKD is designed to measure the source-target knowledge gap and intra-domain diversity. It utilizes the knowledge of pre-training to guide the querying of source-dissimilar and semantic-diverse samples from the target domains. ASD is designed to evaluate the difficulty in segmentation of anatomical structures by measuring predictive entropy from foreground regions adaptively. Additionally, our ASFDA method employs a Selective Semi-supervised Fine-tuning to improve the performance and efficiency of fine-tuning by identifying samples with high reliability from unqueried ones.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis</title>
<link>https://arxiv.org/abs/2509.10804</link>
<guid>https://arxiv.org/abs/2509.10804</guid>
<content:encoded><![CDATA[
arXiv:2509.10804v1 Announce Type: cross 
Abstract: Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient parasitic plant that threatens tomato production by extracting nutrients from the host, with reported yield losses up to 80 percent. Its mostly subterranean life cycle and prolific seed production (more than 200,000 seeds per plant, viable for up to 20 years) make early detection essential. We present an end-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to identify broomrape-infested tomato fields in California. Regions of interest were defined from farmer-reported infestations, and images with less than 10 percent cloud cover were retained. We processed 12 spectral bands and sun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and derived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy Chlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation, and Fractional Vegetation Cover) using a neural network calibrated with ground-truth and synthetic data. Trends in Canopy Chlorophyll Content delineated transplanting-to-harvest periods, and phenology was aligned using growing degree days. Vegetation pixels were segmented and used to train a Long Short-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day time points. The model achieved 88 percent training accuracy and 87 percent test accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation feature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a chlorophyll red-edge index as most informative, consistent with the physiological effects of infestation. Results show the promise of satellite-driven time-series modeling for scalable detection of parasitic stress in tomato farms.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nav-R1: Reasoning and Navigation in Embodied Scenes</title>
<link>https://arxiv.org/abs/2509.10884</link>
<guid>https://arxiv.org/abs/2509.10884</guid>
<content:encoded><![CDATA[
arXiv:2509.10884v1 Announce Type: cross 
Abstract: Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustifying Diffusion-Denoised Smoothing Against Covariate Shift</title>
<link>https://arxiv.org/abs/2509.10913</link>
<guid>https://arxiv.org/abs/2509.10913</guid>
<content:encoded><![CDATA[
arXiv:2509.10913v1 Announce Type: cross 
Abstract: Randomized smoothing is a well-established method for achieving certified robustness against l2-adversarial perturbations. By incorporating a denoiser before the base classifier, pretrained classifiers can be seamlessly integrated into randomized smoothing without significant performance degradation. Among existing methods, Diffusion Denoised Smoothing - where a pretrained denoising diffusion model serves as the denoiser - has produced state-of-the-art results. However, we show that employing a denoising diffusion model introduces a covariate shift via misestimation of the added noise, ultimately degrading the smoothed classifier's performance. To address this issue, we propose a novel adversarial objective function focused on the added noise of the denoising diffusion model. This approach is inspired by our understanding of the origin of the covariate shift. Our goal is to train the base classifier to ensure it is robust against the covariate shift introduced by the denoiser. Our method significantly improves certified accuracy across three standard classification benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art performance in l2-adversarial perturbations. Our implementation is publicly available at https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.11003</link>
<guid>https://arxiv.org/abs/2509.11003</guid>
<content:encoded><![CDATA[
arXiv:2509.11003v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Ensemble Weather Forecasting with Diffusion Models</title>
<link>https://arxiv.org/abs/2509.11047</link>
<guid>https://arxiv.org/abs/2509.11047</guid>
<content:encoded><![CDATA[
arXiv:2509.11047v1 Announce Type: cross 
Abstract: Although numerical weather forecasting methods have dominated the field, recent advances in deep learning methods, such as diffusion models, have shown promise in ensemble weather forecasting. However, such models are typically autoregressive and are thus computationally expensive. This is a challenge in climate science, where data can be limited, costly, or difficult to work with. In this work, we explore the impact of curated data selection on these autoregressive diffusion models. We evaluate several data sampling strategies and show that a simple time stratified sampling approach achieves performance similar to or better than full-data training. Notably, it outperforms the full-data model on certain metrics and performs only slightly worse on others while using only 20% of the training data. Our results demonstrate the feasibility of data-efficient diffusion training, especially for weather forecasting, and motivates future work on adaptive or model-aware sampling methods that go beyond random or purely temporal sampling.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees</title>
<link>https://arxiv.org/abs/2509.11054</link>
<guid>https://arxiv.org/abs/2509.11054</guid>
<content:encoded><![CDATA[
arXiv:2509.11054v1 Announce Type: cross 
Abstract: We establish the first information-theoretic limits for multimodal retrieval. Casting ranking as lossy source coding, we derive a single-letter rate-distortion function $R(D)$ for reciprocal-rank distortion and prove a converse bound that splits into a modality-balanced term plus a skew penalty $\kappa\,\Delta H$ capturing entropy imbalance and cross-modal redundancy. We then construct an explicit entropy-weighted stochastic quantizer with an adaptive, per-modality temperature decoder; a Blahut-Arimoto argument shows this scheme achieves distortion within $O(n^{-1})$ of $R(D)$ using $n$ training triples. A VC-type analysis yields the first finite-sample excess-risk bound whose complexity scales sub-linearly in both the number of modalities and the entropy gap. Experiments on controlled Gaussian mixtures and Flickr30k confirm that our adaptive codes sit within two percentage points of the theoretical frontier, while fixed-temperature and naive CLIP baselines lag significantly. Taken together, our results give a principled answer to "how many bits per query are necessary" for high-quality multimodal retrieval and provide design guidance for entropy-aware contrastive objectives, continual-learning retrievers, and retrieval-augmented generators.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SH-SAS: An Implicit Neural Representation for Complex Spherical-Harmonic Scattering Fields for 3D Synthetic Aperture Sonar</title>
<link>https://arxiv.org/abs/2509.11087</link>
<guid>https://arxiv.org/abs/2509.11087</guid>
<content:encoded><![CDATA[
arXiv:2509.11087v1 Announce Type: cross 
Abstract: Synthetic aperture sonar (SAS) reconstruction requires recovering both the spatial distribution of acoustic scatterers and their direction-dependent response. Time-domain backprojection is the most common 3D SAS reconstruction algorithm, but it does not model directionality and can suffer from sampling limitations, aliasing, and occlusion. Prior neural volumetric methods applied to synthetic aperture sonar treat each voxel as an isotropic scattering density, not modeling anisotropic returns. We introduce SH-SAS, an implicit neural representation that expresses the complex acoustic scattering field as a set of spherical harmonic (SH) coefficients. A multi-resolution hash encoder feeds a lightweight MLP that outputs complex SH coefficients up to a specified degree L. The zeroth-order coefficient acts as an isotropic scattering field, which also serves as the density term, while higher orders compactly capture directional scattering with minimal parameter overhead. Because the model predicts the complex amplitude for any transmit-receive baseline, training is performed directly from 1-D time-of-flight signals without the need to beamform intermediate images for supervision. Across synthetic and real SAS (both in-air and underwater) benchmarks, results show that SH-SAS performs better in terms of 3D reconstruction quality and geometric metrics than previous methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraUPConvNet: A UPerNet- and ConvNeXt-Based Multi-Task Network for Ultrasound Tissue Segmentation and Disease Prediction</title>
<link>https://arxiv.org/abs/2509.11108</link>
<guid>https://arxiv.org/abs/2509.11108</guid>
<content:encoded><![CDATA[
arXiv:2509.11108v1 Announce Type: cross 
Abstract: Ultrasound imaging is widely used in clinical practice due to its cost-effectiveness, mobility, and safety. However, current AI research often treats disease prediction and tissue segmentation as two separate tasks and their model requires substantial computational overhead. In such a situation, we introduce UltraUPConvNet, a computationally efficient universal framework designed for both ultrasound image classification and segmentation. Trained on a large-scale dataset containing more than 9,700 annotations across seven different anatomical regions, our model achieves state-of-the-art performance on certain datasets with lower computational overhead. Our model weights and codes are available at https://github.com/yyxl123/UltraUPConvNet
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations</title>
<link>https://arxiv.org/abs/2509.11125</link>
<guid>https://arxiv.org/abs/2509.11125</guid>
<content:encoded><![CDATA[
arXiv:2509.11125v1 Announce Type: cross 
Abstract: Deploying visual reinforcement learning (RL) policies in real-world manipulation is often hindered by camera viewpoint changes. A policy trained from a fixed front-facing camera may fail when the camera is shifted--an unavoidable situation in real-world settings where sensor placement is hard to manage appropriately. Existing methods often rely on precise camera calibration or struggle with large perspective changes. To address these limitations, we propose ManiVID-3D, a novel 3D RL architecture designed for robotic manipulation, which learns view-invariant representations through self-supervised disentangled feature learning. The framework incorporates ViewNet, a lightweight yet effective module that automatically aligns point cloud observations from arbitrary viewpoints into a unified spatial coordinate system without the need for extrinsic calibration. Additionally, we develop an efficient GPU-accelerated batch rendering module capable of processing over 5000 frames per second, enabling large-scale training for 3D visual RL at unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world tasks demonstrates that our approach achieves a 44.7% higher success rate than state-of-the-art methods under viewpoint variations while using 80% fewer parameters. The system's robustness to severe perspective changes and strong sim-to-real performance highlight the effectiveness of learning geometrically consistent representations for scalable robotic manipulation in unstructured environments. Our project website can be found in https://zheng-joe-lee.github.io/manivid3d/.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2509.11197</link>
<guid>https://arxiv.org/abs/2509.11197</guid>
<content:encoded><![CDATA[
arXiv:2509.11197v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\% and 18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realistic Environmental Injection Attacks on GUI Agents</title>
<link>https://arxiv.org/abs/2509.11250</link>
<guid>https://arxiv.org/abs/2509.11250</guid>
<content:encoded><![CDATA[
arXiv:2509.11250v1 Announce Type: cross 
Abstract: GUI agents built on LVLMs are increasingly used to interact with websites. However, their exposure to open-world content makes them vulnerable to Environmental Injection Attacks (EIAs) that hijack agent behavior via webpage elements. Many recent studies assume the attacker to be a regular user who can only upload a single trigger image, which is more realistic than earlier assumptions of website-level administrative control. However, these works still fall short of realism: (1) the trigger's position and surrounding context remain largely fixed between training and testing, failing to capture the dynamic nature of real webpages and (2) the trigger often occupies an unrealistically large area, whereas real-world images are typically small. To better reflect real-world scenarios, we introduce a more realistic threat model where the attacker is a regular user and the trigger image is small and embedded within a dynamically changing environment. As a result, existing attacks prove largely ineffective under this threat model.
  To better expose the vulnerabilities of GUI agents, we propose Chameleon, an attack framework with two main novelties. The first is LLM-Driven Environment Simulation, which automatically generates diverse and high-fidelity webpage simulations. The second is Attention Black Hole, which transforms attention weights into explicit supervisory signals that guide the agent's focus toward the trigger region. We evaluate Chameleon on 6 realistic websites and 4 representative LVLM-powered GUI agents, where it significantly outperforms existing methods. Ablation studies confirm that both novelties are critical to performance. Our findings reveal underexplored vulnerabilities in modern GUI agents and establish a robust foundation for future research on defense in open-world GUI agent systems. The code is publicly available at https://github.com/zhangyitonggg/attack2gui.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing</title>
<link>https://arxiv.org/abs/2509.11265</link>
<guid>https://arxiv.org/abs/2509.11265</guid>
<content:encoded><![CDATA[
arXiv:2509.11265v1 Announce Type: cross 
Abstract: Deep neural networks tend to memorize noisy labels, severely degrading their generalization performance. Although Mixup has demonstrated effectiveness in improving generalization and robustness, existing Mixup-based methods typically perform indiscriminate mixing without principled guidance on sample selection and mixing strategy, inadvertently propagating noisy supervision. To overcome these limitations, we propose SelectMix, a confidence-guided mixing framework explicitly tailored for noisy labels. SelectMix first identifies potentially noisy or ambiguous samples through confidence based mismatch analysis using K-fold cross-validation, then selectively blends identified uncertain samples with confidently predicted peers from their potential classes. Furthermore, SelectMix employs soft labels derived from all classes involved in the mixing process, ensuring the labels accurately represent the composition of the mixed samples, thus aligning supervision signals closely with the actual mixed inputs. Through extensive theoretical analysis and empirical evaluations on multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that SelectMix consistently outperforms strong baseline methods, validating its effectiveness and robustness in learning with noisy labels.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to a Low-Cost AI-Powered GUI for Unstained Cell Culture Analysis</title>
<link>https://arxiv.org/abs/2509.11354</link>
<guid>https://arxiv.org/abs/2509.11354</guid>
<content:encoded><![CDATA[
arXiv:2509.11354v1 Announce Type: cross 
Abstract: This article presents a novel microscopy image analysis framework designed for low-budget labs equipped with a standard CPU desktop. The Python-based program enables cytometric analysis of live, unstained cells in culture through an advanced computer vision and machine learning pipeline. Crucially, the framework operates on label-free data, requiring no manually annotated training data or training phase. It is accessible via a user-friendly, cross-platform GUI that requires no programming skills, while also providing a scripting interface for programmatic control and integration by developers. The end-to-end workflow performs semantic and instance segmentation, feature extraction, analysis, evaluation, and automated report generation. Its modular architecture supports easy maintenance and flexible integration while supporting both single-image and batch processing. Validated on several unstained cell types from the public dataset of livecells, the framework demonstrates superior accuracy and reproducibility compared to contemporary tools like Cellpose and StarDist. Its competitive segmentation speed on a CPU-based platform highlights its significant potential for basic research and clinical applications -- particularly in cell transplantation for personalized medicine and muscle regeneration therapies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</title>
<link>https://arxiv.org/abs/2509.11362</link>
<guid>https://arxiv.org/abs/2509.11362</guid>
<content:encoded><![CDATA[
arXiv:2509.11362v1 Announce Type: cross 
Abstract: Understanding human behavior traits is central to applications in human-computer interaction, computational social science, and personalized AI systems. Such understanding often requires integrating multiple modalities to capture nuanced patterns and relationships. However, existing resources rarely provide datasets that combine behavioral descriptors with complementary modalities such as facial attributes and biographical information. To address this gap, we present PersonaX, a curated collection of multimodal datasets designed to enable comprehensive analysis of public traits across modalities. PersonaX consists of (1) CelebPersona, featuring 9444 public figures from diverse occupations, and (2) AthlePersona, covering 4181 professional athletes across 7 major sports leagues. Each dataset includes behavioral trait assessments inferred by three high-performing large language models, alongside facial imagery and structured biographical features. We analyze PersonaX at two complementary levels. First, we abstract high-level trait scores from text descriptions and apply five statistical independence tests to examine their relationships with other modalities. Second, we introduce a novel causal representation learning (CRL) framework tailored to multimodal and multi-measurement data, providing theoretical identifiability guarantees. Experiments on both synthetic and real-world data demonstrate the effectiveness of our approach. By unifying structured and unstructured analysis, PersonaX establishes a foundation for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</title>
<link>https://arxiv.org/abs/2509.11417</link>
<guid>https://arxiv.org/abs/2509.11417</guid>
<content:encoded><![CDATA[
arXiv:2509.11417v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs</title>
<link>https://arxiv.org/abs/2509.11480</link>
<guid>https://arxiv.org/abs/2509.11480</guid>
<content:encoded><![CDATA[
arXiv:2509.11480v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Analysis of Magnetic Labyrinthine Stripe Evolution via U-Net Segmentation</title>
<link>https://arxiv.org/abs/2509.11485</link>
<guid>https://arxiv.org/abs/2509.11485</guid>
<content:encoded><![CDATA[
arXiv:2509.11485v1 Announce Type: cross 
Abstract: Labyrinthine stripe patterns are common in many physical systems, yet their lack of long-range order makes quantitative characterization challenging. We investigate the evolution of such patterns in bismuth-doped yttrium iron garnet (Bi:YIG) films subjected to a magnetic field annealing protocol. A U-Net deep learning model, trained with synthetic degradations including additive white Gaussian and Simplex noise, enables robust segmentation of experimental magneto-optical images despite noise and occlusions. Building on this segmentation, we develop a geometric analysis pipeline based on skeletonization, graph mapping, and spline fitting, which quantifies local stripe propagation through length and curvature measurements. Applying this framework to 444 images from 12 annealing protocol trials, we analyze the transition from the "quenched" state to a more parallel and coherent "annealed" state, and identify two distinct evolution modes (Type A and Type B) linked to field polarity. Our results provide a quantitative analysis of geometric and topological properties in magnetic stripe patterns and offer new insights into their local structural evolution, and establish a general tool for analyzing complex labyrinthine systems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching</title>
<link>https://arxiv.org/abs/2509.11628</link>
<guid>https://arxiv.org/abs/2509.11628</guid>
<content:encoded><![CDATA[
arXiv:2509.11628v1 Announce Type: cross 
Abstract: Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{https://github.com/Shenyi-Z/Cache4Diffusion}
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering</title>
<link>https://arxiv.org/abs/2509.11663</link>
<guid>https://arxiv.org/abs/2509.11663</guid>
<content:encoded><![CDATA[
arXiv:2509.11663v1 Announce Type: cross 
Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem, introduces a corresponding benchmark, and proposes a system to tackle the problem. Classical Embodied Question Answering (EQA) is typically formulated as answering one single question by actively exploring a 3D environment. Real deployments, however, often demand handling multiple questions that may arrive asynchronously and carry different urgencies. We formalize this setting as Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group memory module shared among questions to reduce redundant exploration, and a priority-planning module to dynamically schedule questions. To evaluate this setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs) benchmark containing 40 indoor scenes and five questions per scene (200 in total), featuring asynchronous follow-up questions and urgency labels. We further propose metrics for EQsA performance: Direct Answer Rate (DAR), and Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency and responsiveness of this system. ParaEQsA consistently outperforms strong sequential baselines adapted from recent EQA systems, while reducing exploration and delay. Empirical evaluations investigate the relative contributions of priority, urgency modeling, spatial scope, reward estimation, and dependency reasoning within our framework. Together, these results demonstrate that urgency-aware, parallel scheduling is key to making embodied agents responsive and efficient under realistic, multi-question workloads.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model</title>
<link>https://arxiv.org/abs/2509.11698</link>
<guid>https://arxiv.org/abs/2509.11698</guid>
<content:encoded><![CDATA[
arXiv:2509.11698v1 Announce Type: cross 
Abstract: Motion instruction is a crucial task that helps athletes refine their technique by analyzing movements and providing corrective guidance. Although recent advances in multimodal models have improved motion understanding, generating precise and sport-specific instruction remains challenging due to the highly domain-specific nature of sports and the need for informative guidance. We propose CoachMe, a reference-based model that analyzes the differences between a learner's motion and a reference under temporal and physical aspects. This approach enables both domain-knowledge learning and the acquisition of a coach-like thinking process that identifies movement errors effectively and provides feedback to explain how to improve. In this paper, we illustrate how CoachMe adapts well to specific sports such as skating and boxing by learning from general movements and then leveraging limited data. Experiments show that CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. Analysis further confirms that it elaborates on errors and their corresponding improvement methods in the generated instructions. You can find CoachMe here: https://motionxperts.github.io/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAG: Data Reconstruction Attack using Guided Diffusion</title>
<link>https://arxiv.org/abs/2509.11724</link>
<guid>https://arxiv.org/abs/2509.11724</guid>
<content:encoded><![CDATA[
arXiv:2509.11724v1 Announce Type: cross 
Abstract: With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDM's learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios. Code is available at: https://github.com/ntuaislab/DRAG.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDAF: Federated Domain Adaptation Using Model Functional Distance</title>
<link>https://arxiv.org/abs/2509.11819</link>
<guid>https://arxiv.org/abs/2509.11819</guid>
<content:encoded><![CDATA[
arXiv:2509.11819v1 Announce Type: cross 
Abstract: Federated Domain Adaptation (FDA) is a federated learning (FL) approach that improves model performance at the target client by collaborating with source clients while preserving data privacy. FDA faces two primary challenges: domain shifts between source and target data and limited labeled data at the target. Most existing FDA methods focus on domain shifts, assuming ample target data, yet often neglect the combined challenges of both domain shifts and data scarcity. Moreover, approaches that address both challenges fail to prioritize sharing relevant information from source clients according to the target's objective. In this paper, we propose FedDAF, a novel approach addressing both challenges in FDA. FedDAF uses similarity-based aggregation of the global source model and target model by calculating model functional distance from their mean gradient fields computed on target data. This enables effective model aggregation based on the target objective, constructed using target data, even with limited data. While computing model functional distance between these two models, FedDAF computes the angle between their mean gradient fields and then normalizes with the Gompertz function. To construct the global source model, all the local source models are aggregated using simple average in the server. Experiments on real-world datasets demonstrate FedDAF's superiority over existing FL, PFL, and FDA methods in terms of achieving better test accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning</title>
<link>https://arxiv.org/abs/2509.11839</link>
<guid>https://arxiv.org/abs/2509.11839</guid>
<content:encoded><![CDATA[
arXiv:2509.11839v1 Announce Type: cross 
Abstract: Imitation learning (IL) enables efficient skill acquisition from demonstrations but often struggles with long-horizon tasks and high-precision control due to compounding errors. Residual policy learning offers a promising, model-agnostic solution by refining a base policy through closed-loop corrections. However, existing approaches primarily focus on local corrections to the base policy, lacking a global understanding of state evolution, which limits robustness and generalization to unseen scenarios. To address this, we propose incorporating global dynamics modeling to guide residual policy updates. Specifically, we leverage Koopman operator theory to impose linear time-invariant structure in a learned latent space, enabling reliable state transitions and improved extrapolation for long-horizon prediction and unseen environments. We introduce KORR (Koopman-guided Online Residual Refinement), a simple yet effective framework that conditions residual corrections on Koopman-predicted latent states, enabling globally informed and stable action refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture assembly tasks under various perturbations. Results demonstrate consistent gains in performance, robustness, and generalization over strong baselines. Our findings further highlight the potential of Koopman-based modeling to bridge modern learning methods with classical control theory. For more details, please refer to https://jiachengliu3.github.io/TrajBooster.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning</title>
<link>https://arxiv.org/abs/2509.12001</link>
<guid>https://arxiv.org/abs/2509.12001</guid>
<content:encoded><![CDATA[
arXiv:2509.12001v1 Announce Type: cross 
Abstract: A healthy smile plays a significant role in functional as well as esthetic considerations, improving confidence. It is difficult for dental professionals to strike a balance between esthetic requirements and functional requirements. Traditional smile design has had heavy reliance on dentist expertise and used plaster models and hand drawings, raising questions about the outcome for patients. Digital technology, led by Dr. Christian Coachman in 2007, allows photographic and videographic assessments, enabling improved intercommunication among specialists and patients. Advances in artificial intelligence (AI) and big data have supported analysis of facial features and development of personalized smile designs in the last few years. Outputs are, however, susceptible to practitioner bias or limitations of training data, and may be suboptimal for individual users. The study presented here suggests a comprehensive system integrating AI, big data, and recognition technologies to automate the smile design process so that both experienced and inexperienced dentists can generate pleasing aesthetics with ease. The system has a Facial Feature Extraction Module and an Image Generation Module, serving diverse practitioner and patient needs. User data can be incorporated in future research for design optimization and testing of virtual and augmented reality for real-time previewing. Data gathered can also be employed in aesthetic preference analyses, which can enhance our knowledge of smile design in dental practice.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning</title>
<link>https://arxiv.org/abs/2509.12074</link>
<guid>https://arxiv.org/abs/2509.12074</guid>
<content:encoded><![CDATA[
arXiv:2509.12074v1 Announce Type: cross 
Abstract: Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic weed that threatens tomato production by extracting nutrients from the host. We investigate early detection using leaf-level spectral reflectance (400-2500 nm) and ensemble machine learning. In a field experiment in Woodland, California, we tracked 300 tomato plants across growth stages defined by growing degree days (GDD). Leaf reflectance was acquired with a portable spectrometer and preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing, correlation-based band reduction). Clear class differences were observed near 1500 nm and 2000 nm water absorption features, consistent with reduced leaf water content in infected plants at early stages. An ensemble combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at 585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and weed interference. Despite the small number of infected plants and environmental confounders, results show that proximal sensing with ensemble learning enables timely detection of broomrape before canopy symptoms are visible, supporting targeted interventions and reduced yield losses.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Medical Artificial Intelligence Using a Century of Cases</title>
<link>https://arxiv.org/abs/2509.12194</link>
<guid>https://arxiv.org/abs/2509.12194</guid>
<content:encoded><![CDATA[
arXiv:2509.12194v1 Announce Type: cross 
Abstract: BACKGROUND: For over a century, the New England Journal of Medicine Clinicopathological Conferences (CPCs) have tested the reasoning of expert physicians and, recently, artificial intelligence (AI). However, prior AI evaluations have focused on final diagnoses without addressing the multifaceted reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025), we conducted extensive physician annotation and automated processing to create CPC-Bench, a physician-validated benchmark spanning 10 text-based and multimodal tasks, against which we evaluated leading large language models (LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce written and slide-based video presentations using only the case presentation, modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the final diagnosis first in 60% of cases and within the top ten in 84% of cases, outperforming a 20-physician baseline; next-test selection accuracy reached 98%. Event-level physician annotations quantified AI diagnostic accuracy per unit of information. Performance was lower on literature search and image tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image challenges. In blinded comparisons of CaBot vs. human expert-generated text, physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions. To promote research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based differential diagnosis and convincingly emulate expert medical presentations, but image interpretation and literature retrieval remain weaker. CPC-Bench and CaBot may enable transparent and continued tracking of progress in medical AI.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-based Sign Language Recognition without Temporal Segmentation</title>
<link>https://arxiv.org/abs/1801.10111</link>
<guid>https://arxiv.org/abs/1801.10111</guid>
<content:encoded><![CDATA[
arXiv:1801.10111v2 Announce Type: replace 
Abstract: Millions of hearing impaired people around the world routinely use some variants of sign languages to communicate, thus the automatic translation of a sign language is meaningful and important. Currently, there are two sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that recognizes word by word and continuous SLR that translates entire sentences. Existing continuous SLR methods typically utilize isolated SLRs as building blocks, with an extra layer of preprocessing (temporal segmentation) and another layer of post-processing (sentence synthesis). Unfortunately, temporal segmentation itself is non-trivial and inevitably propagates errors into subsequent steps. Worse still, isolated SLR methods typically require strenuous labeling of each word separately in a sentence, severely limiting the amount of attainable training data. To address these challenges, we propose a novel continuous sign recognition framework, the Hierarchical Attention Network with Latent Space (LS-HAN), which eliminates the preprocessing of temporal segmentation. The proposed LS-HAN consists of three components: a two-stream Convolutional Neural Network (CNN) for video feature representation generation, a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention Network (HAN) for latent space based recognition. Experiments are carried out on two large scale datasets. Experimental results demonstrate the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAIF: Sparse Adversarial and Imperceptible Attack Framework</title>
<link>https://arxiv.org/abs/2212.07495</link>
<guid>https://arxiv.org/abs/2212.07495</guid>
<content:encoded><![CDATA[
arXiv:2212.07495v4 Announce Type: replace 
Abstract: Adversarial attacks hamper the decision-making ability of neural networks by perturbing the input signal. The addition of calculated small distortion to images, for instance, can deceive a well-trained image classification network. In this work, we propose a novel attack technique called Sparse Adversarial and Interpretable Attack Framework (SAIF). Specifically, we design imperceptible attacks that contain low-magnitude perturbations at a small number of pixels and leverage these sparse attacks to reveal the vulnerability of classifiers. We use the Frank-Wolfe (conditional gradient) algorithm to simultaneously optimize the attack perturbations for bounded magnitude and sparsity with $O(1/\sqrt{T})$ convergence. Empirical results show that SAIF computes highly imperceptible and interpretable adversarial examples, and outperforms state-of-the-art sparse attack methods on the ImageNet dataset.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRSNetwork: Siamese Reconstruction-Segmentation Networks based on Dynamic-Parameter Convolution</title>
<link>https://arxiv.org/abs/2312.01741</link>
<guid>https://arxiv.org/abs/2312.01741</guid>
<content:encoded><![CDATA[
arXiv:2312.01741v2 Announce Type: replace 
Abstract: Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations.Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: https://github.com/fidshu/SRSNet.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Tailed 3D Detection via Multi-Modal Fusion</title>
<link>https://arxiv.org/abs/2312.10986</link>
<guid>https://arxiv.org/abs/2312.10986</guid>
<content:encoded><![CDATA[
arXiv:2312.10986v5 Announce Type: replace 
Abstract: Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors. While class labels naturally follow a long-tailed distribution in the real world, existing benchmarks only focus on a few common classes (e.g., pedestrian and car) and neglect many rare but crucial classes (e.g., emergency vehicle and stroller). However, AVs must reliably detect both common and rare classes for safe operation in the open world. We address this challenge by formally studying the problem of Long-Tailed 3D Detection (LT3D), which evaluates all annotated classes, including those in-the-tail. We address LT3D with hierarchical losses that promote feature sharing across classes, and introduce diagnostic metrics that award partial credit to "reasonable" mistakes with respect to the semantic hierarchy. Further, we point out that rare-class accuracy is particularly improved via multi-modal late fusion (MMLF) of independently trained uni-modal LiDAR and RGB detectors. Such an MMLF framework allows us to leverage large-scale uni-modal datasets (with more examples for rare classes) to train better uni-modal detectors. Finally, we examine three critical components of our simple MMLF approach from first principles: whether to train 2D or 3D RGB detectors for fusion, whether to match RGB and LiDAR detections in 3D or the projected 2D image plane, and how to fuse matched detections. Extensive experiments reveal that 2D RGB detectors achieve better recognition accuracy for rare classes than 3D RGB detectors, matching on the 2D image plane mitigates depth estimation errors for better matching, and score calibration and probabilistic fusion notably improves the final performance further. Our MMLF significantly outperforms prior work for LT3D, particularly improving on the six rarest classes from 12.8 to 20.0 mAP! Our code and models are available on our project page.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep Groupwise Image Registration</title>
<link>https://arxiv.org/abs/2401.02141</link>
<guid>https://arxiv.org/abs/2401.02141</guid>
<content:encoded><![CDATA[
arXiv:2401.02141v3 Announce Type: replace 
Abstract: This article presents a general Bayesian learning framework for multi-modal groupwise image registration. The method builds on probabilistic modelling of the image generative process, where the underlying common anatomy and geometric variations of the observed images are explicitly disentangled as latent variables. Therefore, groupwise image registration is achieved via hierarchical Bayesian inference. We propose a novel hierarchical variational auto-encoding architecture to realise the inference procedure of the latent variables, where the registration parameters can be explicitly estimated in a mathematically interpretable fashion. Remarkably, this new paradigm learns groupwise image registration in an unsupervised closed-loop self-reconstruction process, sparing the burden of designing complex image-based similarity measures. The computationally efficient disentangled network architecture is also inherently scalable and flexible, allowing for groupwise registration on large-scale image groups with variable sizes. Furthermore, the inferred structural representations from multi-modal images via disentanglement learning are capable of capturing the latent anatomy of the observations with visual semantics. Extensive experiments were conducted to validate the proposed framework, including four different datasets from cardiac, brain, and abdominal medical images. The results have demonstrated the superiority of our method over conventional similarity-based approaches in terms of accuracy, efficiency, scalability, and interpretability.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic Image Synthesis</title>
<link>https://arxiv.org/abs/2403.09638</link>
<guid>https://arxiv.org/abs/2403.09638</guid>
<content:encoded><![CDATA[
arXiv:2403.09638v3 Announce Type: replace 
Abstract: Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has set new state-of-the-art results in SIS on Cityscapes, ADE20K and COCO-Stuff, yielding a FID as low as 10.53 on Cityscapes. The code and models can be accessed via the project page.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructHumans: Editing Animated 3D Human Textures with Instructions</title>
<link>https://arxiv.org/abs/2404.04037</link>
<guid>https://arxiv.org/abs/2404.04037</guid>
<content:encoded><![CDATA[
arXiv:2404.04037v2 Announce Type: replace 
Abstract: We present InstructHumans, a novel framework for instruction-driven {animatable} 3D human texture editing. Existing text-based 3D editing methods often directly apply Score Distillation Sampling (SDS). SDS, designed for generation tasks, cannot account for the defining requirement of editing -- maintaining consistency with the source avatar. This work shows that naively using SDS harms editing, as it may destroy consistency. We propose a modified SDS for Editing (SDS-E) that selectively incorporates subterms of SDS across diffusion timesteps. We further enhance SDS-E with spatial smoothness regularization and gradient-based viewpoint sampling for edits with sharp and high-fidelity detailing. Incorporating SDS-E into a 3D human texture editing framework allows us to outperform existing 3D editing methods. Our avatars faithfully reflect the textual edits while remaining consistent with the original avatars. Project page: https://jyzhu.top/instruct-humans/.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSIDMamba: Exploring Bidirectional State-Space Models for Hyperspectral Denoising</title>
<link>https://arxiv.org/abs/2404.09697</link>
<guid>https://arxiv.org/abs/2404.09697</guid>
<content:encoded><![CDATA[
arXiv:2404.09697v2 Announce Type: replace 
Abstract: Effectively modeling global context information in hyperspectral image (HSI) denoising is crucial, but prevailing methods using convolution or transformers still face localized or computational efficiency limitations. Inspired by the emerging Selective State Space Model (Mamba) with nearly linear computational complexity and efficient long-term modeling, we present a novel HSI denoising network named HSIDMamba (HSDM). HSDM is tailored to exploit the capture of potential spatial-spectral dependencies effectively and efficiently for HSI denoising. In particular, HSDM comprises multiple Hyperspectral Continuous Scan Blocks (HCSB) to strengthen spatial-spectral interactions. HCSB links forward and backward scans and enhances information from eight directions through the State Space Model (SSM), strengthening the context representation learning of HSDM and improving denoising performance more effectively. In addition, to enhance the utilization of spectral information and mitigate the degradation problem caused by long-range scanning, spectral attention mechanism. Extensive evaluations against HSI denoising benchmarks validate the superior performance of HSDM, achieving state-of-the-art performance and surpassing the efficiency of the transformer method SERT by 31%.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Diversity Improves Vision-Language Representations</title>
<link>https://arxiv.org/abs/2405.16915</link>
<guid>https://arxiv.org/abs/2405.16915</guid>
<content:encoded><![CDATA[
arXiv:2405.16915v3 Announce Type: replace 
Abstract: Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large. All translated captions and metadata (language, CLIP score, etc.) are available on HuggingFace.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is the Visual Cognition Gap between Humans and Multimodal LLMs?</title>
<link>https://arxiv.org/abs/2406.10424</link>
<guid>https://arxiv.org/abs/2406.10424</guid>
<content:encoded><![CDATA[
arXiv:2406.10424v2 Announce Type: replace 
Abstract: Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning - the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven's Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities. MaRs-VQA is available at huggingface.co/datasets/IrohXu/VCog-Bench. The training code of Qwen2-VCog is available at github.com/IrohXu/Cognition-MLLM.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models</title>
<link>https://arxiv.org/abs/2406.13763</link>
<guid>https://arxiv.org/abs/2406.13763</guid>
<content:encoded><![CDATA[
arXiv:2406.13763v2 Announce Type: replace 
Abstract: Can large multimodal models have a human-like ability for emotional and social reasoning, and if so, how does it work? Recent research has discovered emergent theory-of-mind (ToM) reasoning capabilities in large language models (LLMs). LLMs can reason about people's mental states by solving various text-based ToM tasks that ask questions about the actors' ToM (e.g., human belief, desire, intention). However, human reasoning in the wild is often grounded in dynamic scenes across time. Thus, we consider videos a new medium for examining spatio-temporal ToM reasoning ability. Specifically, we ask explicit probing questions about videos with abundant social and emotional reasoning content. We develop a pipeline for multimodal LLM for ToM reasoning using video and text. We also enable explicit ToM reasoning by retrieving key frames for answering a ToM question, which reveals how multimodal LLMs reason about ToM.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalp Diagnostic System With Label-Free Segmentation and Training-Free Image Translation</title>
<link>https://arxiv.org/abs/2406.17254</link>
<guid>https://arxiv.org/abs/2406.17254</guid>
<content:encoded><![CDATA[
arXiv:2406.17254v3 Announce Type: replace 
Abstract: Scalp disorders are highly prevalent worldwide, yet remain underdiagnosed due to limited access to expert evaluation and the high cost of annotation. Although AI-based approaches hold great promise, their practical deployment is hindered by challenges such as severe data imbalance and the absence of pixel-level segmentation labels. To address these issues, we propose ScalpVision, an AI-driven system for the holistic diagnosis of scalp diseases. In ScalpVision, effective hair segmentation is achieved using pseudo image-label pairs and an innovative prompting method in the absence of traditional hair masking labels. Additionally, ScalpVision introduces DiffuseIT-M, a generative model adopted for dataset augmentation while maintaining hair information, facilitating improved predictions of scalp disease severity. Our experimental results affirm ScalpVision's efficiency in diagnosing a variety of scalp conditions, showcasing its potential as a valuable tool in dermatological care. Our code is available at https://github.com/winston1214/ScalpVision.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Perception of Faces in a Vision-Language Model</title>
<link>https://arxiv.org/abs/2408.14435</link>
<guid>https://arxiv.org/abs/2408.14435</guid>
<content:encoded><![CDATA[
arXiv:2408.14435v2 Announce Type: replace 
Abstract: We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2409.19972</link>
<guid>https://arxiv.org/abs/2409.19972</guid>
<content:encoded><![CDATA[
arXiv:2409.19972v3 Announce Type: replace 
Abstract: Multi-sensor fusion significantly enhances the accuracy and robustness of 3D semantic occupancy prediction, which is crucial for autonomous driving and robotics. However, most existing approaches depend on high-resolution images and complex networks to achieve top performance, hindering their deployment in practical scenarios. Moreover, current multi-sensor fusion approaches mainly focus on improving feature fusion while largely neglecting effective supervision strategies for those features. To address these issues, we propose DAOcc, a novel multi-modal occupancy prediction framework that leverages 3D object detection supervision to assist in achieving superior performance, while using a deployment-friendly image backbone and practical input resolution. In addition, we introduce a BEV View Range Extension strategy to mitigate performance degradation caused by lower image resolution. Extensive experiments demonstrate that DAOcc achieves new state-of-the-art results on both the Occ3D-nuScenes and Occ3D-Waymo benchmarks, and outperforms previous state-of-the-art methods by a significant margin using only a ResNet-50 backbone and 256*704 input resolution. With TensorRT optimization, DAOcc reaches 104.9 FPS while maintaining 54.2 mIoU on an NVIDIA RTX 4090 GPU. Code is available at https://github.com/AlphaPlusTT/DAOcc.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-OOD3D: Supervised and Unsupervised Out-of-Distribution object detection in LiDAR data</title>
<link>https://arxiv.org/abs/2410.23767</link>
<guid>https://arxiv.org/abs/2410.23767</guid>
<content:encoded><![CDATA[
arXiv:2410.23767v3 Announce Type: replace 
Abstract: Autonomous systems rely on accurate 3D object detection from LiDAR data, yet most detectors are limited to a predefined set of known classes, making them vulnerable to unexpected out-of-distribution (OOD) objects. In this work, we present HD-OOD3D, a novel two-stage method for detecting unknown objects. We demonstrate the superiority of two-stage approaches over single-stage methods, achieving more robust detection of unknown objects while addressing key challenges in the evaluation protocol. Furthermore, we conduct an in-depth analysis of the standard evaluation protocol for OOD detection, revealing the critical impact of hyperparameter choices. To address the challenge of scaling the learning of unknown objects, we explore unsupervised training strategies to generate pseudo-labels for unknowns. Among the different approaches evaluated, our experiments show that top-5 auto-labelling offers more promising performance compared to simple resizing techniques.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning</title>
<link>https://arxiv.org/abs/2411.10070</link>
<guid>https://arxiv.org/abs/2411.10070</guid>
<content:encoded><![CDATA[
arXiv:2411.10070v2 Announce Type: replace 
Abstract: Existing cross-domain few-shot learning (CDFSL) methods, which develop source-domain training strategies to enhance model transferability, face challenges with large-scale pre-trained models (LMs) due to inaccessible source data and training strategies. Moreover, fine-tuning LMs for CDFSL demands substantial computational resources, limiting practicality. This paper addresses the source-free CDFSL (SF-CDFSL) problem, tackling few-shot learning (FSL) in the target domain using only pre-trained models and a few target samples without source data or strategies. To overcome the challenge of inaccessible source data, this paper introduces Step-wise Distribution Alignment Guided Style Prompt Tuning (StepSPT), which implicitly narrows domain gaps through prediction distribution optimization. StepSPT proposes a style prompt to align target samples with the desired distribution and adopts a dual-phase optimization process. In the external process, a step-wise distribution alignment strategy factorizes prediction distribution optimization into a multi-step alignment problem to tune the style prompt. In the internal process, the classifier is updated using standard cross-entropy loss. Evaluations on five datasets demonstrate that StepSPT outperforms existing prompt tuning-based methods and SOTAs. Ablation studies further verify its effectiveness. Code will be made publicly available at https://github.com/xuhuali-mxj/StepSPT.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Undefined: Chain-of-Action for Generative Semantic Labels</title>
<link>https://arxiv.org/abs/2411.17406</link>
<guid>https://arxiv.org/abs/2411.17406</guid>
<content:encoded><![CDATA[
arXiv:2411.17406v2 Announce Type: replace 
Abstract: Recent advances in vision-language models (VLMs) have demonstrated remarkable capabilities in image classification by leveraging predefined sets of labels to construct text prompts for zero-shot reasoning. However, these approaches face significant limitations in undefined domains, where the label space is vocabulary-unknown and composite. We thus introduce Generative Semantic Labels (GSLs), a novel task that aims to predict a comprehensive set of semantic labels for an image without being constrained by a predefined labels set. Unlike traditional zero-shot classification, GSLs generates multiple semantic-level labels, encompassing objects, scenes, attributes, and relationships, thereby providing a richer and more accurate representation of image content. In this paper, we propose Chain-of-Action (CoA), an innovative method designed to tackle the GSLs task. CoA is motivated by the observation that enriched contextual information significantly improves generative performance during inference. Specifically, CoA decomposes the GSLs task into a sequence of detailed actions. Each action extracts and merges key information from the previous step, passing enriched context to the next, ultimately guiding the VLM to generate comprehensive and accurate semantic labels. We evaluate the effectiveness of CoA through extensive experiments on widely-used benchmark datasets. The results demonstrate significant improvements across key performance metrics, validating the capability of CoA to generate accurate and contextually rich semantic labels. Our work not only advances the state-of-the-art in generative semantic labels but also opens new avenues for applying VLMs in open-ended and dynamic real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote Sensing SpatioTemporal Vision-Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2412.02573</link>
<guid>https://arxiv.org/abs/2412.02573</guid>
<content:encoded><![CDATA[
arXiv:2412.02573v3 Announce Type: replace 
Abstract: The interpretation of multi-temporal remote sensing imagery is critical for monitoring Earth's dynamic processes-yet previous change detection methods, which produce binary or semantic masks, fall short of providing human-readable insights into changes. Recent advances in Vision-Language Models (VLMs) have opened a new frontier by fusing visual and linguistic modalities, enabling spatio-temporal vision-language understanding: models that not only capture spatial and temporal dependencies to recognize changes but also provide a richer interactive semantic analysis of temporal images (e.g., generate descriptive captions and answer natural-language queries). In this survey, we present the first comprehensive review of RS-STVLMs. The survey covers the evolution of models from early task-specific models to recent general foundation models that leverage powerful large language models. We discuss progress in representative tasks, such as change captioning, change question answering, and change grounding. Moreover, we systematically dissect the fundamental components and key technologies underlying these models, and review the datasets and evaluation metrics that have driven the field. By synthesizing task-level insights with a deep dive into shared architectural patterns, we aim to illuminate current achievements and chart promising directions for future research in spatio-temporal vision-language understanding for remote sensing. We will keep tracing related works at https://github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE: Learning to Think with Vision Specialists</title>
<link>https://arxiv.org/abs/2412.05479</link>
<guid>https://arxiv.org/abs/2412.05479</guid>
<content:encoded><![CDATA[
arXiv:2412.05479v4 Announce Type: replace 
Abstract: While open-source vision-language models perform well on simple question-answering, they still struggle with complex questions that require both perceptual and reasoning capabilities. We propose LATTE, a family of vision-language models that have LeArned to Think wiTh vision spEcialists. By offloading perception to state-of-the-art vision models, our approach enables vision-language models to focus solely on reasoning over high-quality perceptual information. To train LATTE, we synthesize and filter a large dataset of 293K multi-modal reasoning traces over perceptual outputs of vision specialists. LATTE trained on this data achieves significant 4-5% gains over baselines across 6 benchmarks covering both perception and reasoning abilities. Ablation studies reveal that the effectiveness of multi-modal reasoning traces depends on the data sources, formats, and quality of thoughts.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Mesh Editing using Masked LRMs</title>
<link>https://arxiv.org/abs/2412.08641</link>
<guid>https://arxiv.org/abs/2412.08641</guid>
<content:encoded><![CDATA[
arXiv:2412.08641v2 Announce Type: replace 
Abstract: We present a novel approach to shape editing, building on recent progress in 3D reconstruction from multi-view images. We formulate shape editing as a conditional reconstruction problem, where the model must reconstruct the input shape with the exception of a specified 3D region, in which the geometry should be generated from the conditional signal. To this end, we train a conditional Large Reconstruction Model (LRM) for masked reconstruction, using multi-view consistent masks rendered from a randomly generated 3D occlusion, and using one clean viewpoint as the conditional signal. During inference, we manually define a 3D region to edit and provide an edited image from a canonical viewpoint to fill that region. We demonstrate that, in just a single forward pass, our method not only preserves the input geometry in the unmasked region through reconstruction capabilities on par with SoTA, but is also expressive enough to perform a variety of mesh edits from a single image guidance that past works struggle with, while being 2-10x faster than the top-performing prior work.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion</title>
<link>https://arxiv.org/abs/2412.13389</link>
<guid>https://arxiv.org/abs/2412.13389</guid>
<content:encoded><![CDATA[
arXiv:2412.13389v2 Announce Type: replace 
Abstract: Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Depth-Based Pipeline for Selfie Image Rectification</title>
<link>https://arxiv.org/abs/2412.19189</link>
<guid>https://arxiv.org/abs/2412.19189</guid>
<content:encoded><![CDATA[
arXiv:2412.19189v2 Announce Type: replace 
Abstract: Portraits or selfie images taken from a close distance typically suffer from perspective distortion. In this paper, we propose an end-to-end deep learning-based rectification pipeline to mitigate the effects of perspective distortion. We learn to predict the facial depth by training a deep CNN. The estimated depth is utilized to adjust the camera-to-subject distance by moving the camera farther, increasing the camera focal length, and reprojecting the 3D image features to the new perspective. The reprojected features are then fed to an inpainting module to fill in the missing pixels. We leverage a differentiable renderer to enable end-to-end training of our depth estimation and feature extraction nets to improve the rectified outputs. To boost the results of the inpainting module, we incorporate an auxiliary module to predict the horizontal movement of the camera which decreases the area that requires hallucination of challenging face parts such as ears. Unlike previous works, we process the full-frame input image at once without cropping the subject's face and processing it separately from the rest of the body, eliminating the need for complex post-processing steps to attach the face back to the subject's body. To train our network, we utilize the popular game engine Unreal Engine to generate a large synthetic face dataset containing various subjects, head poses, expressions, eyewear, clothes, and lighting. Quantitative and qualitative results show that our rectification pipeline outperforms previous methods, and produces comparable results with a time-consuming 3D GAN-based method while being more than 260 times faster.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning</title>
<link>https://arxiv.org/abs/2502.00848</link>
<guid>https://arxiv.org/abs/2502.00848</guid>
<content:encoded><![CDATA[
arXiv:2502.00848v3 Announce Type: replace 
Abstract: Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Blind and Low-Vision Individuals Prefer Large Vision-Language Model-Generated Scene Descriptions</title>
<link>https://arxiv.org/abs/2502.14883</link>
<guid>https://arxiv.org/abs/2502.14883</guid>
<content:encoded><![CDATA[
arXiv:2502.14883v2 Announce Type: replace 
Abstract: For individuals with blindness or low vision (BLV), navigating complex environments can pose serious risks. Large Vision-Language Models (LVLMs) show promise for generating scene descriptions, but their effectiveness for BLV users remains underexplored. To address this gap, we conducted a user study with eight BLV participants to systematically evaluate preferences for six types of LVLM descriptions. While they helped to reduce fear and improve actionability, user ratings showed wide variation in sufficiency and conciseness. Furthermore, GPT-4o--despite its strong potential to refine descriptions--was not consistently preferred by participants. We use the insights obtained from the user study to build training data for building our new automatic evaluation metric that can capture BLV preferences effectively. Our findings underscore the urgent need for BLV-centered evaluation metrics and human-in-the-loop feedback to advance LVLM description quality for accessibility.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction</title>
<link>https://arxiv.org/abs/2502.14894</link>
<guid>https://arxiv.org/abs/2502.14894</guid>
<content:encoded><![CDATA[
arXiv:2502.14894v2 Announce Type: replace 
Abstract: Per- and polyfluoroalkyl substances (PFAS), chemicals found in products like non-stick cookware, are unfortunately persistent environmental pollutants with severe health risks. Accurately mapping PFAS contamination is crucial for guiding targeted remediation efforts and protecting public and environmental health, yet detection across large regions remains challenging due to the cost of testing and the difficulty of simulating their spread. In this work, we introduce FOCUS, a geospatial deep learning framework with a label noise-aware loss function, to predict PFAS contamination in surface water over large regions. By integrating hydrological flow data, land cover information, and proximity to known PFAS sources, our approach leverages both spatial and environmental context to improve prediction accuracy. We evaluate the performance of our approach through extensive ablation studies, robustness analysis, real-world validation, and comparative analyses against baselines like sparse segmentation, as well as existing scientific methods, including Kriging and pollutant transport simulations. Results and expert feedback highlight our framework's potential for scalable PFAS monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</title>
<link>https://arxiv.org/abs/2502.20220</link>
<guid>https://arxiv.org/abs/2502.20220</guid>
<content:encoded><![CDATA[
arXiv:2502.20220v2 Announce Type: replace 
Abstract: Traditionally, creating photo-realistic 3D head avatars requires a studio-level multi-view capture setup and expensive optimization during test-time, limiting the use of digital human doubles to the VFX industry or offline renderings.
  To address this shortcoming, we present Avat3r, which regresses a high-quality and animatable 3D head avatar from just a few input images, vastly reducing compute requirements during inference. More specifically, we make Large Reconstruction Models animatable and learn a powerful prior over 3D human heads from a large multi-view video dataset. For better 3D head reconstructions, we employ position maps from DUSt3R and generalized feature maps from the human foundation model Sapiens. To animate the 3D head, our key discovery is that simple cross-attention to an expression code is already sufficient. Finally, we increase robustness by feeding input images with different expressions to our model during training, enabling the reconstruction of 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture with accidental movement, or frames from a monocular video.
  We compare Avat3r with current state-of-the-art methods for few-input and single-input scenarios, and find that our method has a competitive advantage in both tasks. Finally, we demonstrate the wide applicability of our proposed model, creating 3D head avatars from images of different sources, smartphone captures, single images, and even out-of-domain inputs like antique busts.
  Project website: https://tobias-kirschstein.github.io/avat3r/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLF: Extreme Image Compression with Dual-generative Latent Fusion</title>
<link>https://arxiv.org/abs/2503.01428</link>
<guid>https://arxiv.org/abs/2503.01428</guid>
<content:encoded><![CDATA[
arXiv:2503.01428v3 Announce Type: replace 
Abstract: Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Project: https://dlfcodec.github.io/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA</title>
<link>https://arxiv.org/abs/2503.02034</link>
<guid>https://arxiv.org/abs/2503.02034</guid>
<content:encoded><![CDATA[
arXiv:2503.02034v2 Announce Type: replace 
Abstract: Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of interpreting CTPA scans and generating accurate radiology reports remains a significant challenge. This paper introduces Abn-BLIP (Abnormality-aligned Bootstrapping Language-Image Pretraining), an advanced diagnosis model designed to align abnormal findings to generate the accuracy and comprehensiveness of radiology reports. By leveraging learnable queries and cross-modal attention mechanisms, our model demonstrates superior performance in detecting abnormalities, reducing missed findings, and generating structured reports compared to existing methods. Our experiments show that Abn-BLIP outperforms state-of-the-art medical vision-language models and 3D report generation methods in both accuracy and clinical relevance. These results highlight the potential of integrating multimodal learning strategies for improving radiology reporting. The source code is available at https://github.com/zzs95/abn-blip.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization of Representation Uncertainty in Earth Observation</title>
<link>https://arxiv.org/abs/2503.07082</link>
<guid>https://arxiv.org/abs/2503.07082</guid>
<content:encoded><![CDATA[
arXiv:2503.07082v2 Announce Type: replace 
Abstract: Recent advances in Computer Vision have introduced the concept of pretrained representation uncertainty, enabling zero-shot uncertainty estimation. This holds significant potential for Earth Observation (EO), where trustworthiness is critical, yet the complexity of EO data poses challenges to uncertainty-aware methods. In this work, we investigate the generalization of representation uncertainty in EO, considering the domain's unique semantic characteristics. We pretrain uncertainties on large EO datasets and propose an evaluation framework to assess their zero-shot performance in multi-label classification and segmentation EO tasks. Our findings reveal that, unlike uncertainties pretrained on natural images, EO-pretraining exhibits strong generalization across unseen EO domains, geographic locations, and target granularities, while maintaining sensitivity to variations in ground sampling distance. We demonstrate the practical utility of pretrained uncertainties showcasing their alignment with task-specific uncertainties in downstream tasks, their sensitivity to real-world EO image noise, and their ability to generate spatial uncertainty estimates out-of-the-box. Initiating the discussion on representation uncertainty in EO, our study provides insights into its strengths and limitations, paving the way for future research in the field. Code and weights are available at: https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2503.09040</link>
<guid>https://arxiv.org/abs/2503.09040</guid>
<content:encoded><![CDATA[
arXiv:2503.09040v3 Announce Type: replace 
Abstract: Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene Reconstruction</title>
<link>https://arxiv.org/abs/2504.00763</link>
<guid>https://arxiv.org/abs/2504.00763</guid>
<content:encoded><![CDATA[
arXiv:2504.00763v2 Announce Type: replace 
Abstract: Reconstructing and decomposing dynamic urban scenes is crucial for autonomous driving, urban planning, and scene editing. However, existing methods fail to perform instance-aware decomposition without manual annotations, which is crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian Splatting (3DGS) based approach that decomposes a scene into a static background and individual dynamic instances using only RGB images and LiDAR point clouds. At its core, we introduce 4D superpoints, a novel representation that clusters multi-frame LiDAR points in 4D space, enabling unsupervised instance separation based on spatiotemporal correlations. These 4D superpoints serve as the foundation for our decomposed 4D initialization, i.e., providing spatial and temporal initialization to train a dynamic 3DGS for arbitrary dynamic classes without requiring bounding boxes or object templates.Furthermore, we introduce a smoothness regularization strategy in both 2D and 3D space, further improving the temporal stability.Experiments on benchmark datasets show that our method outperforms existing methods in decomposed dynamic scene reconstruction while enabling accurate and flexible instance-level editing, making it a practical solution for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GISE-TTT:A Framework for Global InformationSegmentation and Enhancement</title>
<link>https://arxiv.org/abs/2504.00879</link>
<guid>https://arxiv.org/abs/2504.00879</guid>
<content:encoded><![CDATA[
arXiv:2504.00879v3 Announce Type: replace 
Abstract: This paper addresses the challenge of capturing global temporaldependencies in long video sequences for Video Object Segmentation (VOS). Existing architectures often fail to effectively model these dependencies acrossextended temporal horizons. To overcome this limitation, we introduce GISE-TTT, anovel architecture that integrates Temporal Transformer (TTT) layers intotransformer-based frameworks through a co-designed hierarchical approach.The TTTlayer systematically condenses historical temporal information into hidden states thatencode globally coherent contextual representations. By leveraging multi-stagecontextual aggregation through hierarchical concatenation, our frameworkprogressively refines spatiotemporal dependencies across network layers. This designrepresents the first systematic empirical evidence that distributing global informationacross multiple network layers is critical for optimal dependency utilization in videosegmentation tasks.Ablation studies demonstrate that incorporating TTT modules athigh-level feature stages significantly enhances global modeling capabilities, therebyimproving the network's ability to capture long-range temporal relationships. Extensive experiments on DAVIS 2017 show that GISE-TTT achieves a 3.2%improvement in segmentation accuracy over the baseline model, providingcomprehensive evidence that global information should be strategically leveragedthroughout the network architecture.The code will be made available at:https://github.com/uuool/GISE-TTT.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation</title>
<link>https://arxiv.org/abs/2504.01020</link>
<guid>https://arxiv.org/abs/2504.01020</guid>
<content:encoded><![CDATA[
arXiv:2504.01020v2 Announce Type: replace 
Abstract: Our objective is the automatic generation of Audio Descriptions (ADs) for edited video material, such as movies and TV series. To achieve this, we propose a two-stage framework that leverages "shots" as the fundamental units of video understanding. This includes extending temporal context to neighbouring shots and incorporating film grammar devices, such as shot scales and thread structures, to guide AD generation. Our method is compatible with both open-source and proprietary Visual-Language Models (VLMs), integrating expert knowledge from add-on modules without requiring additional training of the VLMs. We achieve state-of-the-art performance among all prior training-free approaches and even surpass fine-tuned methods on several benchmarks. To evaluate the quality of predicted ADs, we introduce a new evaluation measure -- an action score -- specifically targeted to assessing this important aspect of AD. Additionally, we propose a novel evaluation protocol that treats automatic frameworks as AD generation assistants and asks them to generate multiple candidate ADs for selection.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model</title>
<link>https://arxiv.org/abs/2504.06027</link>
<guid>https://arxiv.org/abs/2504.06027</guid>
<content:encoded><![CDATA[
arXiv:2504.06027v2 Announce Type: replace 
Abstract: Multimodal remote sensing image registration aligns images from different sensors for data fusion and analysis. However, existing methods often struggle to extract modality-invariant features when faced with large nonlinear radiometric differences, such as those between SAR and optical images. To address these challenges, we propose OSDM-MReg, a novel multimodal image registration framework that bridges the modality gap through image-to-image translation. Specifically, we introduce a one-step unaligned target-guided conditional diffusion model (UTGOS-CDM) to translate source and target images into a unified representation domain. Unlike traditional conditional DDPM that require hundreds of iterative steps for inference, our model incorporates a novel inverse translation objective during training to enable direct prediction of the translated image in a single step at test time, significantly accelerating the registration process. After translation, we design a multimodal multiscale registration network (MM-Reg) that extracts and fuses both unimodal and translated multimodal images using the proposed multimodal fusion strategy, enhancing the robustness and precision of alignment across scales and modalities. Extensive experiments on the OSdataset demonstrate that OSDM-MReg achieves superior registration accuracy compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering</title>
<link>https://arxiv.org/abs/2504.21403</link>
<guid>https://arxiv.org/abs/2504.21403</guid>
<content:encoded><![CDATA[
arXiv:2504.21403v2 Announce Type: replace 
Abstract: Video question answering benefits from the rich information in videos, enabling various applications. However, the large volume of tokens generated from long videos presents challenges to memory efficiency and model performance. To alleviate this, existing works propose to compress video inputs, but often overlook the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. We propose a novel token selection strategy, \textsc{explore-then-select}, that adaptively adjusts static and dynamic information based on question requirements. Our framework first explores different token allocations between key frames, which preserve spatial details, and delta frames, which capture temporal changes. Then it employs a query-aware attention-based metric to select the optimal token combination without model updates. Our framework is plug-and-play and can be seamlessly integrated within diverse video language models. Extensive experiments show that our method achieves significant performance improvements (up to 5.8\%) on multiple video question answering benchmarks. Our code is available at https://github.com/ANDgate99/Explore-Then-Select .
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
<link>https://arxiv.org/abs/2505.01571</link>
<guid>https://arxiv.org/abs/2505.01571</guid>
<content:encoded><![CDATA[
arXiv:2505.01571v4 Announce Type: replace 
Abstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities - including RGB, synthetic thermal, and estimated depth videos - and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 75 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment. The foundation model's architecture (code) and weights are available at: https://github.com/GkikasStefanos/PainFormer.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVVNet: A Cross-Vertical-View Network for Gait Recognition</title>
<link>https://arxiv.org/abs/2505.01837</link>
<guid>https://arxiv.org/abs/2505.01837</guid>
<content:encoded><![CDATA[
arXiv:2505.01837v3 Announce Type: replace 
Abstract: Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\%$ improvement on DroneGait and $2\%$ on Gait3D compared with the best existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</title>
<link>https://arxiv.org/abs/2505.03154</link>
<guid>https://arxiv.org/abs/2505.03154</guid>
<content:encoded><![CDATA[
arXiv:2505.03154v2 Announce Type: replace 
Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. Cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. Previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. Constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. In this work, we present StableMotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. The core component of our method is the introduction of motion quality indicators, which can be easily annotated - through manual labeling or heuristic algorithms - and enable training of quality-aware motion generation models on raw motion data with mixed quality. At test time, the model can be prompted to generate high-quality motions using the quality indicators. Our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. We demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying StableMotion to SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. The trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. Results and code are available at https://yxmu.foo/stablemotion-page
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending 3D Geometry and Machine Learning for Multi-View Stereopsis</title>
<link>https://arxiv.org/abs/2505.03470</link>
<guid>https://arxiv.org/abs/2505.03470</guid>
<content:encoded><![CDATA[
arXiv:2505.03470v4 Announce Type: replace 
Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric and geometric consistency constraints. In contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3D geometry, applying explicit geometric consistency (GC) checks only as a post-processing step, with no impact on the learning process itself. In this work, we introduce GC MVSNet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see Fig. 1). This integrated GC check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other MVS methods. Furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. Extensive experiments demonstrate that our approach achieves a new state of the art on the DTU and BlendedMVS datasets and secures second place on the Tanks and Temples benchmark. To our knowledge, GC MVSNet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. Our code is available.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDmamba: Rethinking Efficient Event Denoising with Spatiotemporal Decoupled SSMs</title>
<link>https://arxiv.org/abs/2505.05391</link>
<guid>https://arxiv.org/abs/2505.05391</guid>
<content:encoded><![CDATA[
arXiv:2505.05391v3 Announce Type: replace 
Abstract: Event cameras provide micro-second latency and broad dynamic range, yet their raw streams are marred by spatial artifacts (e.g., hot pixels) and temporally inconsistent background activity. Existing methods jointly process the entire 4D event volume (x, y, p, t), forcing heavy spatio-temporal attention that inflates parameters, FLOPs, and latency. We introduce EDmamba, a compact event-denoising framework that embraces the key insight that spatial and temporal noise arise from different physical mechanisms and can therefore be suppressed independently. A polarity- and geometry-aware encoder first extracts coarse cues, which are then routed to two lightweight state-space branches: a Spatial-SSM that learns location-conditioned filters to silence persistent artifacts, and a Temporal-SSM that models causal signal dynamics to eliminate bursty background events. This decoupled design distills the network to only 88.9K parameters and 2.27GFLOPs, enabling real-time throughput of 100K events in 68ms on a single GPU, 36x faster than recent Transformer baselines. Despite its economy, EDmamba establishes new state-of-the-art accuracy on four public benchmarks, outscoring the strongest prior model by 2.1 percentage points.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2505.16146</link>
<guid>https://arxiv.org/abs/2505.16146</guid>
<content:encoded><![CDATA[
arXiv:2505.16146v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on multimodal tasks. However, they still suffer from hallucinations, generating text inconsistent with visual input, posing significant risks in real-world applications. Existing approaches to address this issue focus on incorporating external knowledge bases, alignment training, or decoding strategies, all of which require substantial computational cost and time. Recent works try to explore more efficient alternatives by adjusting LVLMs' internal representations. Although promising, these methods may cause hallucinations to be insufficiently suppressed or lead to excessive interventions that negatively affect normal semantics. In this work, we leverage sparse autoencoders (SAEs) to identify semantic directions closely associated with faithfulness or hallucination, extracting more precise and disentangled hallucination-related representations. Our analysis demonstrates that interventions along the identified faithful direction can mitigate hallucinations, while those along the hallucinatory direction can exacerbate them. Building on these insights, we propose Steering LVLMs via SAE Latent Directions (SSL), a plug-and-play method based on SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive experiments demonstrate that SSL significantly outperforms existing decoding approaches in mitigating hallucinations, while maintaining transferability across different model architectures with negligible additional time overhead. The code is available at https://github.com/huazhenglin2003/SSL.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection</title>
<link>https://arxiv.org/abs/2505.18660</link>
<guid>https://arxiv.org/abs/2505.18660</guid>
<content:encoded><![CDATA[
arXiv:2505.18660v3 Announce Type: replace 
Abstract: Recent advances in AI-powered generative models have enabled the creation of increasingly realistic synthetic images, posing significant risks to information integrity and public trust on social media platforms. While robust detection frameworks and diverse, large-scale datasets are essential to mitigate these risks, existing academic efforts remain limited in scope: current datasets lack the diversity, scale, and realism required for social media contexts, while detection methods struggle with generalization to unseen generative technologies. To bridge this gap, we introduce So-Fake-Set, a comprehensive social media-oriented dataset with over 2 million high-quality images, diverse generative sources, and photorealistic imagery synthesized using 35 state-of-the-art generative models. To rigorously evaluate cross-domain robustness, we establish a novel and large-scale (100K) out-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from commercial models explicitly excluded from the training distribution, creating a realistic testbed for evaluating real-world performance. Leveraging these resources, we present So-Fake-R1, an advanced vision-language framework that employs reinforcement learning for highly accurate forgery detection, precise localization, and explainable inference through interpretable visual rationales. Extensive experiments show that So-Fake-R1 outperforms the second-best method, with a 1.3% gain in detection accuracy and a 4.5% increase in localization IoU. By integrating a scalable dataset, a challenging OOD benchmark, and an advanced detection framework, this work establishes a new foundation for social media-centric forgery detection research. The code, models, and datasets will be released publicly.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title>
<link>https://arxiv.org/abs/2505.23716</link>
<guid>https://arxiv.org/abs/2505.23716</guid>
<content:encoded><![CDATA[
arXiv:2505.23716v2 Announce Type: replace 
Abstract: We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Signature: In-generation Watermarking for Latent Video Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00652</link>
<guid>https://arxiv.org/abs/2506.00652</guid>
<content:encoded><![CDATA[
arXiv:2506.00652v3 Announce Type: replace 
Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has led to significant progress in video generation but also raises serious concerns about intellectual property protection and reliable content tracing. Watermarking is a widely adopted solution to this issue, but existing methods for video generation mainly follow a post-generation paradigm, which introduces additional computational overhead and often fails to effectively balance the trade-off between video quality and watermark extraction. To address these issues, we propose Video Signature (VIDSIG), an in-generation watermarking method for latent video diffusion models, which enables implicit and adaptive watermark integration during generation. Specifically, we achieve this by partially fine-tuning the latent decoder, where Perturbation-Aware Suppression (PAS) pre-identifies and freezes perceptually sensitive layers to preserve visual quality. Beyond spatial fidelity, we further enhance temporal consistency by introducing a lightweight Temporal Alignment module that guides the decoder to generate coherent frame sequences during fine-tuning. Experimental results show that VIDSIG achieves the best overall performance in watermark extraction, visual quality, and generation efficiency. It also demonstrates strong robustness against both spatial and temporal tampering, highlighting its practicality in real-world scenarios. Our code is available at \href{https://github.com/hardenyu21/Video-Signature}{here}
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</title>
<link>https://arxiv.org/abs/2506.01064</link>
<guid>https://arxiv.org/abs/2506.01064</guid>
<content:encoded><![CDATA[
arXiv:2506.01064v3 Announce Type: replace 
Abstract: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive ``fighting fire with fire'' strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code is available at https://github.com/btzyd/F3.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAquaCluster: Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation</title>
<link>https://arxiv.org/abs/2506.08214</link>
<guid>https://arxiv.org/abs/2506.08214</guid>
<content:encoded><![CDATA[
arXiv:2506.08214v2 Announce Type: replace 
Abstract: In recent years the wide availability of high-resolution radar satellite images along with the advancement of computer vision models have enabled the remote monitoring of wetland surface areas. However, these models require large amounts of manually annotated satellite images, which are slow and expensive to produce. To overcome this problem we use self-supervised training methods to train a model called DeepAquaCluster to segment radar satellite images into areas that separate water from land without the use of any manual annotations. Our final model outperforms other radar-based water detection techniques in our test dataset, achieving a 0.08 improvement in the Intersection Over Union metric.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LH2Face: Loss function for Hard High-quality Face</title>
<link>https://arxiv.org/abs/2506.23555</link>
<guid>https://arxiv.org/abs/2506.23555</guid>
<content:encoded><![CDATA[
arXiv:2506.23555v2 Announce Type: replace 
Abstract: In current practical face authentication systems, most face recognition (FR) algorithms are based on cosine similarity with softmax classification. Despite its reliable classification performance, this method struggles with hard samples. A popular strategy to improve FR performance is incorporating angular or cosine margins. However, it does not take face quality or recognition hardness into account, simply increasing the margin value and thus causing an overly uniform training strategy. To address this problem, a novel loss function is proposed, named Loss function for Hard High-quality Face (LH2Face). Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution is stated, specifically focusing on the logarithm of the Probability Density Function (PDF), which represents the distance between a probability distribution and a vector. Then, an adaptive margin-based multi-classification method using softmax, called the Uncertainty-Aware Margin Function, is implemented in the article. Furthermore, proxy-based loss functions are used to apply extra constraints between the proxy and sample to optimize their representation space distribution. Finally, a renderer is constructed that optimizes FR through face reconstruction and vice versa. Our LH2Face is superior to similiar schemes on hard high-quality face datasets, achieving 49.39% accuracy on the IJB-B dataset, which surpasses the second-place method by 2.37%.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion Prediction</title>
<link>https://arxiv.org/abs/2507.07515</link>
<guid>https://arxiv.org/abs/2507.07515</guid>
<content:encoded><![CDATA[
arXiv:2507.07515v2 Announce Type: replace 
Abstract: Human motion is a continuous physical process in 3D space, governed by complex dynamic and kinematic constraints. Existing methods typically represent the human pose as an abstract graph structure, neglecting the intrinsic physical dependencies between joints, which increases learning difficulty and makes the model prone to generating unrealistic motions. In this paper, we propose GGMotion, a group graph dynamics-kinematics network that models human topology in groups to better leverage dynamics and kinematics priors. To preserve the geometric equivariance in 3D space, we propose a novel radial field for the graph network that captures more comprehensive spatio-temporal dependencies by aggregating joint features through spatial and temporal edges. Inter-group and intra-group interaction modules are employed to capture the dependencies of joints at different scales. Combined with equivariant multilayer perceptrons (MLP), joint position features are updated in each group through parallelized dynamics-kinematics propagation to improve physical plausibility. Meanwhile, we introduce an auxiliary loss to supervise motion priors during training. Extensive experiments on three standard benchmarks, including Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and superiority of our approach, achieving a significant performance margin in short-term motion prediction. The code is available at https://github.com/inkcat520/GGMotion.git.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occlusion-Aware Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction</title>
<link>https://arxiv.org/abs/2507.08137</link>
<guid>https://arxiv.org/abs/2507.08137</guid>
<content:encoded><![CDATA[
arXiv:2507.08137v3 Announce Type: replace 
Abstract: We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction</title>
<link>https://arxiv.org/abs/2507.17594</link>
<guid>https://arxiv.org/abs/2507.17594</guid>
<content:encoded><![CDATA[
arXiv:2507.17594v3 Announce Type: replace 
Abstract: The introduction of the neural implicit representation has notably propelled the advancement of online dense reconstruction techniques. Compared to traditional explicit representations, such as TSDF, it improves the mapping completeness and memory efficiency. However, the lack of reconstruction details and the time-consuming learning of neural representations hinder the widespread application of neural-based methods to large-scale online reconstruction. We introduce RemixFusion, a novel residual-based mixed representation for scene reconstruction and camera pose estimation dedicated to high-quality and large-scale online RGB-D reconstruction. In particular, we propose a residual-based map representation comprised of an explicit coarse TSDF grid and an implicit neural module that produces residuals representing fine-grained details to be added to the coarse grid. Such mixed representation allows for detail-rich reconstruction with bounded time and memory budget, contrasting with the overly-smoothed results by the purely implicit representations, thus paving the way for high-quality camera tracking. Furthermore, we extend the residual-based representation to handle multi-frame joint pose optimization via bundle adjustment (BA). In contrast to the existing methods, which optimize poses directly, we opt to optimize pose changes. Combined with a novel technique for adaptive gradient amplification, our method attains better optimization convergence and global optimality. Furthermore, we adopt a local moving volume to factorize the mixed scene representation with a divide-and-conquer design to facilitate efficient online learning in our residual-based framework. Extensive experiments demonstrate that our method surpasses all state-of-the-art ones, including those based either on explicit or implicit representations, in terms of the accuracy of both mapping and tracking on large-scale scenes.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models</title>
<link>https://arxiv.org/abs/2507.19808</link>
<guid>https://arxiv.org/abs/2507.19808</guid>
<content:encoded><![CDATA[
arXiv:2507.19808v2 Announce Type: replace 
Abstract: Entrusted with the goal of pixel-level object classification, the semantic segmentation networks entail the laborious preparation of pixel-level annotation masks. To obtain pixel-level annotation masks for a given class without human efforts, recent few works have proposed to generate pairs of images and annotation masks by employing image and text relationships modeled by text-to-image generative models, especially Stable Diffusion. However, these works do not fully exploit the capability of text-guided Diffusion models and thus require a pre-trained segmentation network, careful text prompt tuning, or the training of a segmentation network to generate final annotation masks. In this work, we take a closer look at attention mechanisms of Stable Diffusion, from which we draw connections with classical seeded segmentation approaches. In particular, we show that cross-attention alone provides very coarse object localization, which however can provide initial seeds. Then, akin to region expansion in seeded segmentation, we utilize the semantic-correspondence-modeling capability of self-attention to iteratively spread the attention to the whole class from the seeds using multi-scale self-attention maps. We also observe that a simple-text-guided synthetic image often has a uniform background, which is easier to find correspondences, compared to complex-structured objects. Thus, we further refine a mask using a more accurate background mask. Our proposed method, dubbed SeeDiff, generates high-quality masks off-the-shelf from Stable Diffusion, without additional training procedure, prompt tuning, or a pre-trained segmentation network.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for Human Image Generation</title>
<link>https://arxiv.org/abs/2507.20083</link>
<guid>https://arxiv.org/abs/2507.20083</guid>
<content:encoded><![CDATA[
arXiv:2507.20083v2 Announce Type: replace 
Abstract: Recent methods using diffusion models have made significant progress in Human Image Generation (HIG) with various control signals such as pose priors. In HIG, both accurate human poses and coherent visual quality are crucial for image generation. However, most existing methods mainly focus on pose accuracy while neglecting overall image quality, often improving pose alignment at the cost of image quality. To address this, we propose Knowledge-Based Global Guidance and Dynamic pose Masking for human image Generation (KB-DMGen). The Knowledge Base (KB), implemented as a visual codebook, provides coarse, global guidance based on input text-related visual features, improving pose accuracy while maintaining image quality, while the Dynamic pose Mask (DM) offers fine-grained local control to enhance precise pose accuracy. By injecting KB and DM at different stages of the diffusion process, our framework enhances pose accuracy through both global and local control without compromising image quality. Experiments demonstrate the effectiveness of KB-DMGen, achieving new state-of-the-art results in terms of AP and CAP on the HumanArt dataset. The project page and code are available at https://lushbng.github.io/KBDMGen.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</title>
<link>https://arxiv.org/abs/2508.07307</link>
<guid>https://arxiv.org/abs/2508.07307</guid>
<content:encoded><![CDATA[
arXiv:2508.07307v2 Announce Type: replace 
Abstract: Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Building Heritage Assessment Using Street-Level Imagery</title>
<link>https://arxiv.org/abs/2508.11486</link>
<guid>https://arxiv.org/abs/2508.11486</guid>
<content:encoded><![CDATA[
arXiv:2508.11486v2 Announce Type: replace 
Abstract: Detailed data is required to quantify energy conservation measures in buildings, such as envelop retrofits, without compromising cultural heritage. Novel artificial intelligence tools may improve efficiency in identifying heritage values in buildings compared to costly and time-consuming traditional inventories. In this study, the large language model GPT was used to detect various aspects of cultural heritage value in fa\c{c}ade images. Using this data and building register data as features, machine learning models were trained to classify multi-family and non-residential buildings in Stockholm, Sweden. Validation against an expert-created inventory shows a macro F1-score of 0.71 using a combination of register data and features retrieved from GPT, and a score of 0.60 using only GPT-derived data. The presented methodology can contribute to a higher-quality database and thus support careful energy efficiency measures and integrated consideration of heritage value in large-scale energetic refurbishment scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements</title>
<link>https://arxiv.org/abs/2508.12023</link>
<guid>https://arxiv.org/abs/2508.12023</guid>
<content:encoded><![CDATA[
arXiv:2508.12023v2 Announce Type: replace 
Abstract: Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To enable full automation, a contour-aware SL placement approach is proposed in this work, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level- mimicking clinical guidelines. Building on this foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode. \textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application. The source code is publicly available at https://github.com/SFI-Visual-Intelligence/wiselvam.git.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.14707</link>
<guid>https://arxiv.org/abs/2508.14707</guid>
<content:encoded><![CDATA[
arXiv:2508.14707v2 Announce Type: replace 
Abstract: Vision foundation models (VFMs) are predominantly developed using data-centric methods. These methods require training on vast amounts of data usually with high-quality labels, which poses a bottleneck for most institutions that lack both large-scale data and high-end GPUs. On the other hand, many open-source vision models have been pretrained on domain-specific data, enabling them to distill and represent core knowledge in a form that is transferable across diverse applications. Even though these models are highly valuable assets, they remain largely under-explored in empowering the development of a general-purpose VFM. In this paper, we present a new model-driven approach for training VFMs through joint knowledge transfer and preservation. Our method unifies multiple pre-trained teacher models in a shared latent space to mitigate the ``imbalanced transfer'' issue caused by their distributional gaps. Besides, we introduce a knowledge preservation strategy to take a general-purpose teacher as a knowledge base for integrating knowledge from the remaining purpose-specific teachers using an adapter module. By unifying and aggregating existing models, we build a powerful VFM to inherit teachers' expertise without needing to train on a large amount of labeled data. Our model not only provides generalizable visual features, but also inherently supports multiple downstream tasks. Extensive experiments demonstrate that our VFM outperforms existing data-centric models across four fundamental vision tasks, including image classification, object detection, semantic and instance segmentation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2508.15313</link>
<guid>https://arxiv.org/abs/2508.15313</guid>
<content:encoded><![CDATA[
arXiv:2508.15313v2 Announce Type: replace 
Abstract: Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements. \textcolor{blue} {Code: https://github.com/Lwt-diamond/RAG-SEG.}
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-LLMs with Temporal Visual Screening</title>
<link>https://arxiv.org/abs/2508.21094</link>
<guid>https://arxiv.org/abs/2508.21094</guid>
<content:encoded><![CDATA[
arXiv:2508.21094v2 Announce Type: replace 
Abstract: Humans naturally perform temporal screening by dragging the progress bar and focusing on salient temporal segments, but current Video Large Language Models (Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse frame sampling and insufficient inter-frame reasoning supervision during their training. To address this, Inspired by well-established cognitive science principles, we propose Temporal Visual Screening (TVS), a new task that universally pre-processes video question answering and instruction tuning data by: (1) retaining focus-critical video segments, (2) synchronously reconstructing queries to their most direct form while preserving answer consistency, and (3) keeping the invariance and consistency for any possible answer. TVS is formulated as a modular front-end adapter task that can be seamlessly integrated into both Video Instruction Tuning (training) and Video Question Answering (inference) pipelines. TVS optimizes distribution of reasoning burden and cognitive load; during training, it aligns queries with focus-critical visual information; at inference, it enables query-aware segment focus and streamlined query representations. In particular, we curate the first benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming while achieving competitive query rewriting performance. Experiments demonstrate that incorporating TVS yields relative gains of 7.33% (training) and 34.6% (inference), demonstrating the effectiveness of temporal information screening for improving video-language understanding.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoGeDE: Generalizable Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes</title>
<link>https://arxiv.org/abs/2509.01206</link>
<guid>https://arxiv.org/abs/2509.01206</guid>
<content:encoded><![CDATA[
arXiv:2509.01206v2 Announce Type: replace 
Abstract: Self-supervised monocular depth estimation is a significant task for low-cost and efficient 3D scene perception in endoscopy. In recent years, a series of methods are proposed to address the illumination inconsistency, while certain works also focus on the generalization of the model by efficiently finetuning the foundation models. However, the variety of illumination conditions and scene features is still the primary challenges for depth estimation in endoscopic scenes. In this work, a self-supervised framework is proposed for monocular depth estimation in diverse endoscopy. Firstly, considering the diverse features in endoscopic scenes with different tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to efficiently finetune the foundation model for endoscopic depth estimation. In the proposed module, based on the input feature, different experts with a small amount of trainable parameters are adaptively selected for weighted inference, from low-rank experts which are allocated based on the generalization of each block. Moreover, a novel self-supervised training framework is proposed to jointly cope with brightness inconsistency and reflectance interference. The proposed method outperforms state-of-the-art works on SCARED dataset and SimCol dataset. Furthermore, the proposed network also achieves the best generalization based on zero-shot depth estimation on C3VD, Hamlyn and SERV-CT dataset. The outstanding performance of our model is further demonstrated with 3D reconstruction and ego-motion estimation. The proposed method could contribute to accurate endoscopy for minimally invasive measurement and surgery. The evaluation codes will be released upon acceptance, while the demo videos can be found on: https://endo-gede.netlify.app/.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</title>
<link>https://arxiv.org/abs/2307.12255</link>
<guid>https://arxiv.org/abs/2307.12255</guid>
<content:encoded><![CDATA[
arXiv:2307.12255v2 Announce Type: replace-cross 
Abstract: The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2406.09070</link>
<guid>https://arxiv.org/abs/2406.09070</guid>
<content:encoded><![CDATA[
arXiv:2406.09070v4 Announce Type: replace-cross 
Abstract: In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in text to image models through Chain of Thought (CoT) reasoning within multimodal generative large language models. FairCoT employs iterative CoT refinement to systematically mitigate biases, and dynamically adjusts textual prompts in real time, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across popular text-to-image systems including DALLE and various Stable Diffusion variants, demonstrate that FairCoT significantly enhances fairness and diversity without sacrificing image quality or semantic fidelity. By combining robust reasoning, lightweight deployment, and extensibility to multiple models, FairCoT represents a promising step toward more socially responsible and transparent AI driven content generation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis</title>
<link>https://arxiv.org/abs/2408.04909</link>
<guid>https://arxiv.org/abs/2408.04909</guid>
<content:encoded><![CDATA[
arXiv:2408.04909v3 Announce Type: replace-cross 
Abstract: The task of image captioning has recently been gaining popularity, and with it the complex task of evaluating the quality of image captioning models. In this work, we present the first survey and taxonomy of over 70 different image captioning metrics and their usage in hundreds of papers, specifically designed to help users select the most suitable metric for their needs. We find that despite the diversity of proposed metrics, the vast majority of studies rely on only five popular metrics, which we show to be weakly correlated with human ratings. We hypothesize that combining a diverse set of metrics can enhance correlation with human ratings. As an initial step, we demonstrate that a linear regression-based ensemble method, which we call EnsembEval, trained on one human ratings dataset, achieves improved correlation across five additional datasets, showing there is a lot of room for improvement by leveraging a diverse set of metrics.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2408.10123</link>
<guid>https://arxiv.org/abs/2408.10123</guid>
<content:encoded><![CDATA[
arXiv:2408.10123v2 Announce Type: replace-cross 
Abstract: Affordance, defined as the potential actions that an object offers, is crucial for embodied AI agents. For example, such knowledge directs an agent to grasp a knife by the handle for cutting or by the blade for safe handover. While existing approaches have made notable progress, affordance research still faces three key challenges: data scarcity, poor generalization, and real-world deployment. Specifically, there is a lack of large-scale affordance datasets with precise segmentation maps, existing models struggle to generalize across different domains or novel object and affordance classes, and little work demonstrates deployability in real-world scenarios. In this work, we address these issues by proposing a complete affordance learning system that (1) takes in egocentric videos and outputs precise affordance annotations without human labeling, (2) leverages geometric information and vision foundation models to improve generalization, and (3) introduces a framework that facilitates affordance-oriented robotic manipulation such as tool grasping and robot-to-human tool handover. Experimental results show that our model surpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves 77.1% successful grasping among 179 trials, including evaluations on seen, unseen classes, and cluttered scenes. Project page: https://reagan1311.github.io/affgrasp.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2411.02482</link>
<guid>https://arxiv.org/abs/2411.02482</guid>
<content:encoded><![CDATA[
arXiv:2411.02482v3 Announce Type: replace-cross 
Abstract: Training a policy that can generalize to unknown objects is a long standing challenge within the field of robotics. The performance of a policy often drops significantly in situations where an object in the scene was not seen during training. To solve this problem, we present NeRF-Aug, a novel method that is capable of teaching a policy to interact with objects that are not present in the dataset. This approach differs from existing approaches by leveraging the speed, photorealism, and 3D consistency of a neural radiance field for augmentation. NeRF-Aug both creates more photorealistic data and runs 63% faster than existing methods. We demonstrate the effectiveness of our method on 5 tasks with 9 novel objects that are not present in the expert demonstrations. We achieve an average performance boost of 55.6% when comparing our method to the next best method. You can see video results at https://nerf-aug.github.io.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Representational Similarity Measures from the Lens of Functional Correspondence</title>
<link>https://arxiv.org/abs/2411.14633</link>
<guid>https://arxiv.org/abs/2411.14633</guid>
<content:encoded><![CDATA[
arXiv:2411.14633v2 Announce Type: replace-cross 
Abstract: Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain -- spanning alignment-based, Canonical Correlation Analysis (CCA)-based, inner product kernel-based, and nearest-neighbor methods -- we found that metrics like linear Centered Kernel Alignment (CKA) and Procrustes distance, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering</title>
<link>https://arxiv.org/abs/2412.07030</link>
<guid>https://arxiv.org/abs/2412.07030</guid>
<content:encoded><![CDATA[
arXiv:2412.07030v5 Announce Type: replace-cross 
Abstract: Multimodal multihop question answering (MMQA) requires reasoning over images and text from multiple sources. Despite advances in visual question answering, this multihop setting remains underexplored due to a lack of quality datasets. Existing methods focus on single-hop, single-modality, or short texts, limiting real-world applications like interpreting educational documents with long, multimodal content. To fill this gap, we introduce FM2DS, the first framework for creating a high-quality dataset for MMQA. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure data quality. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks: MultimodalQA and WebQA. Our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) score on average. Additionally, we introduce M2QA-Bench with 1k samples, the first benchmark for MMQA on long documents, generated using FM2DS and refined by human annotators. We believe our data synthesis method will serve as a strong foundation for training and evaluating MMQA models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
<link>https://arxiv.org/abs/2501.19047</link>
<guid>https://arxiv.org/abs/2501.19047</guid>
<content:encoded><![CDATA[
arXiv:2501.19047v5 Announce Type: replace-cross 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable GUI Exploration</title>
<link>https://arxiv.org/abs/2502.03330</link>
<guid>https://arxiv.org/abs/2502.03330</guid>
<content:encoded><![CDATA[
arXiv:2502.03330v2 Announce Type: replace-cross 
Abstract: During the early stages of interface design, designers need to produce multiple sketches to explore a design space. Design tools often fail to support this critical stage, because they insist on specifying more details than necessary. Although recent advances in generative AI have raised hopes of solving this issue, in practice they fail because expressing loose ideas in a prompt is impractical. In this paper, we propose a diffusion-based approach to the low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: A) prompts, B) wireframes, and C) visual flows. The designer can provide any combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification. We present qualitative results for various combinations of input specifications. Additionally, we demonstrate that our model aligns more accurately with these specifications than other models.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction</title>
<link>https://arxiv.org/abs/2503.10156</link>
<guid>https://arxiv.org/abs/2503.10156</guid>
<content:encoded><![CDATA[
arXiv:2503.10156v4 Announce Type: replace-cross 
Abstract: Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC = 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45\%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regist3R: Incremental Registration with Stereo Foundation Model</title>
<link>https://arxiv.org/abs/2504.12356</link>
<guid>https://arxiv.org/abs/2504.12356</guid>
<content:encoded><![CDATA[
arXiv:2504.12356v2 Announce Type: replace-cross 
Abstract: Multi-view 3D reconstruction has remained an essential yet challenging problem in the field of computer vision. While DUSt3R and its successors have achieved breakthroughs in 3D reconstruction from unposed images, these methods exhibit significant limitations when scaling to multi-view scenarios, including high computational cost and cumulative error induced by global alignment. To address these challenges, we propose Regist3R, a novel stereo foundation model tailored for efficient and scalable incremental reconstruction. Regist3R leverages an incremental reconstruction paradigm, enabling large-scale 3D reconstructions from unordered and many-view image collections. We evaluate Regist3R on public datasets for camera pose estimation and 3D reconstruction. Our experiments demonstrate that Regist3R achieves comparable performance with optimization-based methods while significantly improving computational efficiency, and outperforms existing multi-view reconstruction models. Furthermore, to assess its performance in real-world applications, we introduce a challenging oblique aerial dataset which has long spatial spans and hundreds of views. The results highlight the effectiveness of Regist3R. We also demonstrate the first attempt to reconstruct large-scale scenes encompassing over thousands of views through pointmap-based foundation models, showcasing its potential for practical applications in large-scale 3D reconstruction tasks, including urban modeling, aerial mapping, and beyond.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
arXiv:2504.15133v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation</title>
<link>https://arxiv.org/abs/2505.12748</link>
<guid>https://arxiv.org/abs/2505.12748</guid>
<content:encoded><![CDATA[
arXiv:2505.12748v2 Announce Type: replace-cross 
Abstract: Teleoperation is a cornerstone of embodied-robot learning, and bimanual dexterous teleoperation in particular provides rich demonstrations that are difficult to obtain with fully autonomous systems. While recent studies have proposed diverse hardware pipelines-ranging from inertial motion-capture gloves to exoskeletons and vision-based interfaces-there is still no unified benchmark that enables fair, reproducible comparison of these systems. In this paper, we introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual dexterous teleoperation. TeleOpBench contains 30 high-fidelity task environments that span pick-and-place, tool use, and collaborative manipulation, covering a broad spectrum of kinematic and force-interaction difficulty. Within this benchmark we implement four representative teleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand exoskeletons, and (iv) monocular vision tracking-and evaluate them with a common protocol and metric suite. To validate that performance in simulation is predictive of real-world behavior, we conduct mirrored experiments on a physical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10 held-out tasks we observe a strong correlation between simulator and hardware performance, confirming the external validity of TeleOpBench. TeleOpBench establishes a common yardstick for teleoperation research and provides an extensible platform for future algorithmic and hardware innovation. Codes is now available at https://github.com/cyjdlhy/TeleOpBench .
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRICT: Stress Test of Rendering Images Containing Text</title>
<link>https://arxiv.org/abs/2505.18985</link>
<guid>https://arxiv.org/abs/2505.18985</guid>
<content:encoded><![CDATA[
arXiv:2505.18985v2 Announce Type: replace-cross 
Abstract: While diffusion models have revolutionized text-to-image generation with their ability to synthesize realistic and diverse scenes, they continue to struggle to generate consistent and legible text within images. This shortcoming is commonly attributed to the locality bias inherent in diffusion-based generation, which limits their ability to model long-range spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a benchmark designed to systematically stress-test the ability of diffusion models to render coherent and instruction-aligned text in images. Our benchmark evaluates models across multiple dimensions: (1) the maximum length of readable text that can be generated; (2) the correctness and legibility of the generated text, and (3) the ratio of not following instructions for generating text. We evaluate several state-of-the-art models, including proprietary and open-source variants, and reveal persistent limitations in long-range consistency and instruction-following capabilities. Our findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling. We release our entire evaluation pipeline at https://github.com/tianyu-z/STRICT-Bench.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples</title>
<link>https://arxiv.org/abs/2506.03004</link>
<guid>https://arxiv.org/abs/2506.03004</guid>
<content:encoded><![CDATA[
arXiv:2506.03004v2 Announce Type: replace-cross 
Abstract: We present PartComposer: a framework for part-level concept learning from single-image examples that enables text-to-image diffusion models to compose novel objects from meaningful components. Existing methods either struggle with effectively learning fine-grained concepts or require a large dataset as input. We propose a dynamic data synthesis pipeline generating diverse part compositions to address one-shot data scarcity. Most importantly, we propose to maximize the mutual information between denoised latents and structured concept codes via a concept predictor, enabling direct regulation on concept disentanglement and re-composition supervision. Our method achieves strong disentanglement and controllable composition, outperforming subject and part-level baselines when mixing concepts from the same, or different, object categories.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</title>
<link>https://arxiv.org/abs/2506.10968</link>
<guid>https://arxiv.org/abs/2506.10968</guid>
<content:encoded><![CDATA[
arXiv:2506.10968v2 Announce Type: replace-cross 
Abstract: Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-shot HDR using conventional image sensor shutter functions and optical randomization</title>
<link>https://arxiv.org/abs/2506.22426</link>
<guid>https://arxiv.org/abs/2506.22426</guid>
<content:encoded><![CDATA[
arXiv:2506.22426v2 Announce Type: replace-cross 
Abstract: High-dynamic-range (HDR) imaging is an essential technique for overcoming the dynamic range limits of image sensors. The classic method relies on multiple exposures, which slows capture time, resulting in motion artifacts when imaging dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR data into a single exposure, then computationally recovering it. Many established methods use strong image priors to recover improperly exposed image detail. These approaches struggle with extended highlight regions. We utilize the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR shutter mode applies a longer exposure time to rows closer to the bottom of the sensor. We use optics that relay a randomly permuted (shuffled) image onto the sensor, effectively creating spatially randomized exposures across the scene. The exposure diversity allows us to recover HDR data by solving an optimization problem with a simple total variation image prior. In simulation, we demonstrate that our method outperforms other single-shot methods when many sensor pixels are saturated (10% or more), and is competitive at a modest saturation (1%). Finally, we demonstrate a physical lab prototype that uses an off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with 48dB dynamic range.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning</title>
<link>https://arxiv.org/abs/2508.13229</link>
<guid>https://arxiv.org/abs/2508.13229</guid>
<content:encoded><![CDATA[
arXiv:2508.13229v3 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven "annotation-reasoning-annotation" closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs.Code and resources are available at: https://github.com/HSH55/RISE.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images</title>
<link>https://arxiv.org/abs/2508.13776</link>
<guid>https://arxiv.org/abs/2508.13776</guid>
<content:encoded><![CDATA[
arXiv:2508.13776v2 Announce Type: replace-cross 
Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods</title>
<link>https://arxiv.org/abs/2509.03108</link>
<guid>https://arxiv.org/abs/2509.03108</guid>
<content:encoded><![CDATA[
<div> Keywords: Face recognition, Spoofing attacks, Deep learning, Backdoor poisoning, Anti-spoofing detection <br />
<br />
Summary: 
This paper discusses the vulnerability of face recognition systems to spoofing attacks, where an attacker uses photos to gain unauthorized access. Existing methods for detecting such attacks rely heavily on deep learning and require ample training data. However, the authors introduce a novel backdoor poisoning attack method that can allow certain spoofing attacks to go undetected by embedding features from a spoofed image into a live one without any visual changes. This poses a significant threat to current anti-spoofing systems as it could lead to false positives. Through experiments on public datasets, the authors demonstrate that their proposed method is a realistic threat. <div>
arXiv:2509.03108v2 Announce Type: replace 
Abstract: Face recognition systems are robust against environmental changes and noise, and thus may be vulnerable to illegal authentication attempts using user face photos, such as spoofing attacks. To prevent such spoofing attacks, it is crucial to discriminate whether the input image is a live user image or a spoofed image prior to the face recognition process. Most existing spoofing attack detection methods utilize deep learning, which necessitates a substantial amount of training data. Consequently, if malicious data is injected into a portion of the training dataset, a specific spoofing attack may be erroneously classified as live, leading to false positives. In this paper, we propose a novel backdoor poisoning attack method to demonstrate the latent threat of backdoor poisoning within face anti-spoofing detection. The proposed method enables certain spoofing attacks to bypass detection by embedding features extracted from the spoofing attack's face image into a live face image without inducing any perceptible visual alterations. Through experiments conducted on public datasets, we demonstrate that the proposed method constitutes a realistic threat to existing spoofing attack detection systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</title>
<link>https://arxiv.org/abs/2509.03498</link>
<guid>https://arxiv.org/abs/2509.03498</guid>
<content:encoded><![CDATA[
<div> transformer architecture, multimodal model, autoregressive modeling, Mixture-of-Experts, vision tokenizer <br />
Summary: <br />
The article introduces OneCAT, a unified multimodal model that integrates understanding, generation, and editing in a pure decoder-only transformer architecture. By using a modality-specific Mixture-of-Experts structure trained with a single autoregressive objective, the need for external components like Vision Transformers or vision tokenizers during inference is eliminated, leading to efficiency gains for high-resolution inputs. The model also supports dynamic resolutions and incorporates a multi-scale visual autoregressive mechanism within the Large Language Model. OneCAT outperforms existing unified multimodal models in benchmarks for generation, editing, and understanding, showcasing the potential of pure autoregressive modeling as a strong foundation for multimodal intelligence. <div>
arXiv:2509.03498v2 Announce Type: replace 
Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision</title>
<link>https://arxiv.org/abs/2509.09720</link>
<guid>https://arxiv.org/abs/2509.09720</guid>
<content:encoded><![CDATA[
<div> dataset, 3D textured meshes, supermarket items, robotics, computer vision

Summary:
The Australian Supermarket Object Set (ASOS) is a new dataset that includes 50 common supermarket items with high-quality 3D textured meshes for benchmarking in robotics and computer vision applications. Unlike other datasets that use synthetic models or specialized objects, ASOS features readily available household items from a major Australian supermarket chain. The dataset covers 10 categories with various shapes, sizes, and weights. The 3D meshes are obtained using structure-from-motion techniques and high-resolution imaging to create watertight meshes. ASOS is designed to be accessible and applicable in real-world scenarios, making it valuable for tasks such as object detection, pose estimation, and robotics applications.<br /><br />Summary: <div>
arXiv:2509.09720v1 Announce Type: new 
Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a comprehensive dataset comprising 50 readily available supermarket items with high-quality 3D textured meshes designed for benchmarking in robotics and computer vision applications. Unlike existing datasets that rely on synthetic models or specialized objects with limited accessibility, ASOS provides a cost-effective collection of common household items that can be sourced from a major Australian supermarket chain. The dataset spans 10 distinct categories with diverse shapes, sizes, and weights. 3D meshes are acquired by a structure-from-motion techniques with high-resolution imaging to generate watertight meshes. The dataset's emphasis on accessibility and real-world applicability makes it valuable for benchmarking object detection, pose estimation, and robotics applications.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval</title>
<link>https://arxiv.org/abs/2509.09721</link>
<guid>https://arxiv.org/abs/2509.09721</guid>
<content:encoded><![CDATA[
<div> framework, multimodal retrieval-augmented generation, damage evaluation, natural disasters, insurance claims

Summary:
The article introduces a novel multimodal retrieval-augmented generation (MM-RAG) framework for accurate evaluations of housing damage after natural disasters. The framework utilizes a two-branch multimodal encoder structure with a visual encoder for image analysis and a BERT retriever for text vectorization and policy matching. A cross-modal interaction module ensures semantic alignment between image and text, while a modal attention gating mechanism controls the role of visual evidence and text information during generation. The framework is trained end-to-end with multi-task optimization objectives, combining comparison, retrieval, and generation losses. Results show improved retrieval accuracy and classification of damage severity, with a 9.6% increase in Top-1 retrieval accuracy. This collaborative learning approach enhances image understanding and policy matching in post-disaster scenarios. <br /><br />Summary: <div>
arXiv:2509.09721v1 Announce Type: new 
Abstract: After natural disasters, accurate evaluations of damage to housing are important for insurance claims response and planning of resources. In this work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG) framework. On top of classical RAG architecture, we further the framework to devise a two-branch multimodal encoder structure that the image branch employs a visual encoder composed of ResNet and Transformer to extract the characteristic of building damage after disaster, and the text branch harnesses a BERT retriever for the text vectorization of posts as well as insurance policies and for the construction of a retrievable restoration index. To impose cross-modal semantic alignment, the model integrates a cross-modal interaction module to bridge the semantic representation between image and text via multi-head attention. Meanwhile, in the generation module, the introduced modal attention gating mechanism dynamically controls the role of visual evidence and text prior information during generation. The entire framework takes end-to-end training, and combines the comparison loss, the retrieval loss and the generation loss to form multi-task optimization objectives, and achieves image understanding and policy matching in collaborative learning. The results demonstrate superior performance in retrieval accuracy and classification index on damage severity, where the Top-1 retrieval accuracy has been improved by 9.6%.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving MLLM Historical Record Extraction with Test-Time Image</title>
<link>https://arxiv.org/abs/2509.09722</link>
<guid>https://arxiv.org/abs/2509.09722</guid>
<content:encoded><![CDATA[
<div> Keywords: ensemble framework, LLM, text extraction, historical documents, transcription accuracy

Summary:
- The article introduces a novel ensemble framework to enhance text extraction from noisy historical documents using LLM.
- The framework transcribes augmented variants of each image with Gemini 2.0 Flash and combines them using a custom aligner to generate a consensus transcription and confidence score.
- A new dataset of 622 Pennsylvania death records is presented for evaluation, showing a 4% improvement in transcription accuracy compared to a single shot baseline.
- Padding and blurring techniques are found to be most effective in enhancing accuracy, while grid warp perturbations help distinguish high and low confidence cases.
- The approach is described as straightforward, scalable, and readily applicable to other collections of documents and transcription models. 

<br /><br />Summary: <div>
arXiv:2509.09722v1 Announce Type: new 
Abstract: We present a novel ensemble framework that stabilizes LLM based text extraction from noisy historical documents. We transcribe multiple augmented variants of each image with Gemini 2.0 Flash and fuse these outputs with a custom Needleman Wunsch style aligner that yields both a consensus transcription and a confidence score. We present a new dataset of 622 Pennsylvania death records, and demonstrate our method improves transcription accuracy by 4 percentage points relative to a single shot baseline. We find that padding and blurring are the most useful for improving accuracy, while grid warp perturbations are best for separating high and low confidence cases. The approach is simple, scalable, and immediately deployable to other document collections and transcription models.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance</title>
<link>https://arxiv.org/abs/2509.09730</link>
<guid>https://arxiv.org/abs/2509.09730</guid>
<content:encoded><![CDATA[
<div> dataset, multimodal, Intelligent Traffic Surveillance, LMMs, performance 

Summary: 
The article introduces MITS, a new benchmark dataset specifically designed for Intelligent Traffic Surveillance (ITS) tasks. MITS includes 170,400 real-world ITS images annotated with ITS-specific objects and events. Through a systematic data generation pipeline, high-quality captions and visual question-answer pairs are created to address five critical ITS tasks. Fine-tuning mainstream Large Multimodal Models (LMMs) on MITS significantly improves their performance in ITS applications, with notable increases in various evaluation metrics. The dataset, along with code and models, is released as open-source to advance research in both ITS and LMMs. <div>
arXiv:2509.09730v1 Announce Type: new 
Abstract: General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs, addressing five critical ITS tasks: object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source, providing high-value resources to advance both ITS and LMM research.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2509.09732</link>
<guid>https://arxiv.org/abs/2509.09732</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision language models, structured reasoning, fine-grained classification, decision trees, interpretable classification<br />
Summary:<br />
The study focuses on enhancing the performance of Vision Language Models (VLMs) in fine-grained and coarse-grained visual classification tasks using structured, tree-based reasoning. The framework decomposes classification into interpretable decisions using decision trees, aiming to improve model accuracy. However, the results indicate that despite achieving high accuracy in understanding tree knowledge, the tree-based reasoning approach consistently underperforms standard zero-shot prompting in visual classification tasks. The researchers explore ways to enhance the tree prompts by incorporating Language Language Model (LLM)-generated classes and image descriptions to improve alignment. It is observed that adding descriptions enhances the performance of both tree-based and zero-shot methods. The study reveals limitations of structured reasoning in visual classification and provides insights for designing more interpretable VLM systems<br /> 
Summary: <div>
arXiv:2509.09732v1 Announce Type: new 
Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but their performance on fine-grained tasks and large hierarchical label spaces is understudied. This paper investigates whether structured, tree-based reasoning can enhance VLM performance. We introduce a framework that decomposes classification into interpretable decisions using decision trees and evaluates it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the model achieves 98.2% accuracy in understanding the tree knowledge, tree-based reasoning consistently underperforms standard zero-shot prompting. We also explore enhancing the tree prompts with LLM-generated classes and image descriptions to improve alignment. The added description enhances the performance of the tree-based and zero-shot methods. Our findings highlight limitations of structured reasoning in visual classification and offer insights for designing more interpretable VLM systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Modeling with Probabilistic Structure Integration</title>
<link>https://arxiv.org/abs/2509.09737</link>
<guid>https://arxiv.org/abs/2509.09737</guid>
<content:encoded><![CDATA[
<div> Probabilistic Structure Integration, PSI, world models, richly controllable, flexibly promptable, probabilistic prediction, graphical model, random-access autoregressive sequence model, structure extraction, low-dimensional properties, intermediate structures, integration, token types, training diet, conditioning signals, prediction targets, LLM-like universal prompting language.<br />
Summary:<br />
Probabilistic Structure Integration (PSI) is a system for learning world models by a three-step cycle. First, Probabilistic prediction involves building a graphical model Psi for data. Second, Structure extraction uncovers low-dimensional properties in the data via causal inference on Psi. Finally, Integration converts these structures into new token types for conditioning signals and prediction targets. With training on a massive dataset, PSI achieves state-of-the-art results in optical flow, depth, and object segmentation. It supports video prediction and understanding inferences, highlighting its rich control and flexible prompting capabilities. Each iteration of the cycle enhances PSI's modeling and control abilities, resembling a universal prompting language like LLM. <div>
arXiv:2509.09737v1 Announce Type: new 
Abstract: We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful "intermediate structures", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning</title>
<link>https://arxiv.org/abs/2509.09742</link>
<guid>https://arxiv.org/abs/2509.09742</guid>
<content:encoded><![CDATA[
<div> privacy-preserving, federated learning, gradient inversion attacks, video data, feature extractors 
Summary:
- This paper examines the threat of gradient inversion attacks on video data in the context of federated learning (FL).
- The study evaluates two video classification approaches and finds that using pre-trained feature extractors can provide greater resilience against attacks.
- Image super-resolution techniques can enhance frames extracted through attacks, allowing for the reconstruction of higher-quality videos.
- The research demonstrates that video data leakage is possible in FL, especially when classifiers lack complexity.
- Further investigation is necessary to understand the specific conditions under which video data leakage occurs in federated learning.
<br /><br />Summary: <div>
arXiv:2509.09742v1 Announce Type: new 
Abstract: Federated learning (FL) allows multiple entities to train a shared model collaboratively. Its core, privacy-preserving principle is that participants only exchange model updates, such as gradients, and never their raw, sensitive data. This approach is fundamental for applications in domains where privacy and confidentiality are important. However, the security of this very mechanism is threatened by gradient inversion attacks, which can reverse-engineer private training data directly from the shared gradients, defeating the purpose of FL. While the impact of these attacks is known for image, text, and tabular data, their effect on video data remains an unexamined area of research. This paper presents the first analysis of video data leakage in FL using gradient inversion attacks. We evaluate two common video classification approaches: one employing pre-trained feature extractors and another that processes raw video frames with simple transformations. Our initial results indicate that the use of feature extractors offers greater resilience against gradient inversion attacks. We also demonstrate that image super-resolution techniques can enhance the frames extracted through gradient inversion attacks, enabling attackers to reconstruct higher-quality videos. Our experiments validate this across scenarios where the attacker has access to zero, one, or more reference frames from the target environment. We find that although feature extractors make attacks more challenging, leakage is still possible if the classifier lacks sufficient complexity. We, therefore, conclude that video data leakage in FL is a viable threat, and the conditions under which it occurs warrant further investigation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images</title>
<link>https://arxiv.org/abs/2509.09750</link>
<guid>https://arxiv.org/abs/2509.09750</guid>
<content:encoded><![CDATA[
<div> Framework, Object detection, Retail environments, Semi-supervised, Co-training

Summary:
The study introduces a semi-supervised co-training framework for object detection in densely packed retail settings. It combines Faster R-CNN and YOLO to handle complex conditions and limited labeled data effectively. By incorporating an ensemble of XGBoost, Random Forest, and SVM for classification, the approach enhances accuracy and robustness in challenging scenes with occlusion and overlapping objects. The framework optimizes hyperparameters using a metaheuristic-driven algorithm, improving precision and efficiency across models. With reduced reliance on manual labeling, the approach minimizes annotation costs and adapts well to frequent product and layout changes in retail environments. Experiments on the SKU-110k dataset demonstrate the framework's strong performance, showcasing its scalability and practicality for real-world retail applications such as automated inventory tracking, product monitoring, and checkout systems. 

<br /><br />Summary: <div>
arXiv:2509.09750v1 Announce Type: new 
Abstract: This study proposes a semi-supervised co-training framework for object detection in densely packed retail environments, where limited labeled data and complex conditions pose major challenges. The framework combines Faster R-CNN (utilizing a ResNet backbone) for precise localization with YOLO (employing a Darknet backbone) for global context, enabling mutual pseudo-label exchange that improves accuracy in scenes with occlusion and overlapping objects. To strengthen classification, it employs an ensemble of XGBoost, Random Forest, and SVM, utilizing diverse feature representations for higher robustness. Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing precision and efficiency across models. By minimizing reliance on manual labeling, the approach reduces annotation costs and adapts effectively to frequent product and layout changes common in retail. Experiments on the SKU-110k dataset demonstrate strong performance, highlighting the scalability and practicality of the proposed framework for real-world retail applications such as automated inventory tracking, product monitoring, and checkout systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging</title>
<link>https://arxiv.org/abs/2509.09785</link>
<guid>https://arxiv.org/abs/2509.09785</guid>
<content:encoded><![CDATA[
<div> Token Purging, Test-time adaptation, 3D point cloud classification, Domain shifts, Attention layers

Summary:
Token Purging (PG) is introduced as a backpropagation-free approach for test-time adaptation in 3D point cloud classification to mitigate performance degradation from distribution shifts. PG removes highly affected tokens before reaching attention layers, operating at the token level for robust adaptation without iterative updates. Two variants, PG-SP utilizing source statistics and PG-SF a source-free version relying on CLS-token-driven adaptation, are proposed. Evaluation on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C shows PG-SP achieving +10.3% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation. PG is also significantly faster and more memory-efficient than the baseline, making it suitable for real-world deployment. The code is available at the provided GitHub link. 

Summary: <br />Token Purging addresses distribution shifts in 3D point cloud classification with backpropagation-free test-time adaptation. Operating at the token level, PG removes tokens highly affected by domain shifts to improve robustness without iterative updates, offering faster and more memory-efficient adaptation methods. Two variants, PG-SP and PG-SF, demonstrate superior performance compared to existing methods and set new benchmarks for source-free adaptation. Extensive evaluations on benchmark datasets confirm the efficacy of Token Purging in mitigating performance degradation caused by distribution shifts, offering a promising solution for real-world deployment. <div>
arXiv:2509.09785v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation caused by distribution shifts in 3D point cloud classification. In this work, we introduce Token Purging (PG), a novel backpropagation-free approach that removes tokens highly affected by domain shifts before they reach attention layers. Unlike existing TTA methods, PG operates at the token level, ensuring robust adaptation without iterative updates. We propose two variants: PG-SP, which leverages source statistics, and PG-SF, a fully source-free version relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of +10.3\% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is 12.4 times faster and 5.5 times more memory efficient than our baseline, making it suitable for real-world deployment. Code is available at \hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors</title>
<link>https://arxiv.org/abs/2509.09792</link>
<guid>https://arxiv.org/abs/2509.09792</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained cross-view localization, local feature matching, monocular depth prior, Procrustes alignment, relative depth models

Summary:
The article proposes a novel method for fine-grained cross-view localization, aiming to estimate the 3 Degrees of Freedom pose of a ground-level image by matching its local features with a reference aerial image. Instead of transforming the ground image into a bird's-eye view, the method establishes correspondences between ground and aerial images and uses a monocular depth prior to lift matched keypoints to BEV space. This approach supports both metric and relative depth, with a scale-aware Procrustes alignment to estimate camera pose and recover scale when using relative depth. The method shows superior localization performance, especially in challenging conditions like cross-area generalization and unknown orientation. It is also compatible with various relative depth models without needing per-model finetuning, making it suitable for real-world deployment.<br /><br />Summary: <div>
arXiv:2509.09792v1 Announce Type: new 
Abstract: We propose an accurate and highly interpretable fine-grained cross-view localization method that estimates the 3 Degrees of Freedom pose of a ground-level image by matching its local features with a reference aerial image. Previous methods typically transform the ground image into a bird's-eye view (BEV) representation and then align it with the aerial image for localization. However, this transformation often leads to information loss due to perspective distortion or compression of height information, thereby degrading alignment quality with the aerial view. In contrast, our method directly establishes correspondences between ground and aerial images and lifts only the matched keypoints to BEV space using monocular depth prior. Notably, modern depth predictors can provide reliable metric depth when the test samples are similar to the training data. When the depth distribution differs, they still produce consistent relative depth, i.e., depth accurate up to an unknown scale. Our method supports both metric and relative depth. It employs a scale-aware Procrustes alignment to estimate the camera pose from the correspondences and optionally recover the scale when using relative depth. Experimental results demonstrate that, with only weak supervision on camera pose, our method learns accurate local feature correspondences and achieves superior localization performance under challenging conditions, such as cross-area generalization and unknown orientation. Moreover, our method is compatible with various relative depth models without requiring per-model finetuning. This flexibility, combined with strong localization performance, makes it well-suited for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test</title>
<link>https://arxiv.org/abs/2509.09808</link>
<guid>https://arxiv.org/abs/2509.09808</guid>
<content:encoded><![CDATA[
<div> Keywords: visual impairments, Bruckner test, mobile device, deep neural networks, pediatric vision screenings <br />
<br />
Summary: <br />
This paper introduces KidsVisionCheck, a mobile application that uses red-eye reflex images for vision screening in young children. The app utilizes deep neural networks trained on labeled pupil images, achieving 90% accuracy on unseen test data. By eliminating the need for specialized equipment, KidsVisionCheck enables accessible pediatric vision screenings and early intervention for vision abnormalities globally. The study identifies optimal data collection conditions and provides immediate feedback to users, enhancing the effectiveness of the screening process. Through combining smartphone technology and artificial intelligence, this research presents a significant advancement in detecting visual impairments and conducting the Bruckner test outside traditional clinical settings. The potential impact of this work is highlighted in improving healthcare access and outcomes for children with vision issues. <br /> <div>
arXiv:2509.09808v1 Announce Type: new 
Abstract: Numerous visual impairments can be detected in red-eye reflex images from young children. The so-called Bruckner test is traditionally performed by ophthalmologists in clinical settings. Thanks to the recent technological advances in smartphones and artificial intelligence, it is now possible to recreate the Bruckner test using a mobile device. In this paper, we present a first study conducted during the development of KidsVisionCheck, a free application that can perform vision screening with a mobile device using red-eye reflex images. The underlying model relies on deep neural networks trained on children's pupil images collected and labeled by an ophthalmologist. With an accuracy of 90% on unseen test data, our model provides highly reliable performance without the necessity of specialist equipment. Furthermore, we can identify the optimal conditions for data collection, which can in turn be used to provide immediate feedback to the users. In summary, this work marks a first step toward accessible pediatric vision screenings and early intervention for vision abnormalities worldwide.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</title>
<link>https://arxiv.org/abs/2509.09828</link>
<guid>https://arxiv.org/abs/2509.09828</guid>
<content:encoded><![CDATA[
<div> fusion, semantic perception, autonomous vehicles, depth information, lidar<br />
<br />
Summary: 
The article introduces a novel depth-guided multimodal fusion method called DGFusion for robust semantic perception in autonomous vehicles. It addresses the limitation of traditional sensor fusion approaches by incorporating depth information from lidar sensors. DGFusion treats multimodal segmentation as a multi-task problem, leveraging lidar measurements for learning depth and as input for the model. By using an auxiliary depth head, the network learns depth-aware features that are encoded into spatially varying local depth tokens for condition-aware fusion. These tokens, along with a global condition token, allow for dynamic adaptation of sensor fusion based on the reliability of each sensor across different spatial regions, which is influenced by depth. A robust loss function is proposed to handle sparse and noisy lidar inputs in challenging conditions. The method achieves state-of-the-art performance on the MUSES and DELIVER datasets. <div>
arXiv:2509.09828v1 Announce Type: new 
Abstract: Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DELIVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework</title>
<link>https://arxiv.org/abs/2509.09841</link>
<guid>https://arxiv.org/abs/2509.09841</guid>
<content:encoded><![CDATA[
<div> Keywords: Rosacea, deep learning, patch-based detection, patient privacy, dermatological diagnostics

Summary: 
Various image patches of different sizes, shapes, and locations are extracted from facial images to develop patch-based automatic rosacea detection strategies using the ResNet-18 deep learning framework. Investigation studies reveal the impact of localized visual information on model performance, showing that patch-based strategies outperform full-image methods. These strategies focus on clinically relevant regions, offering competitive accuracy and sensitivity while preserving patient privacy by excluding identifiable facial features. The deep learning model is guided to enhance interpretability and robustness, ultimately providing practical insights for improving automated dermatological diagnostics.<br /><br /> <div>
arXiv:2509.09841v1 Announce Type: new 
Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with facial redness, papules, and visible blood vessels, often requirs precise and early detection for significantly improving treatment effectiveness. This paper presents new patch-based automatic rosacea detection strategies using the ResNet-18 deep learning framework. The contributions of the proposed strategies come from the following aspects. First, various image pateches are extracted from the facial images of people in different sizes, shapes, and locations. Second, a number of investigation studies are carried out to evaluate how the localized visual information influences the deep learing model performance. Third, thorough experiments are implemented to reveal that several patch-based automatic rosacea detection strategies achieve competitive or superior accuracy and sensitivity than the full-image based methods. And finally, the proposed patch-based strategies, which use only localized patches, inherently preserve patient privacy by excluding any identifiable facial features from the data. The experimental results indicate that the proposed patch-based strategies guide the deep learning model to focus on clinically relevant regions, enhance robustness and interpretability, and protect patient privacy. As a result, the proposed strategies offer practical insights for improving automated dermatological diagnostics.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection</title>
<link>https://arxiv.org/abs/2509.09844</link>
<guid>https://arxiv.org/abs/2509.09844</guid>
<content:encoded><![CDATA[
<div> Keywords: Rosacea, automated detection, privacy-preserving, synthetic data, ResNet-18

Summary: 
A new method for automated detection of rosacea, a common but underdiagnosed inflammatory skin condition, is introduced in this paper. Leveraging clinical priors, the method focuses on central facial erythema, a key symptom of rosacea, using a redness-informed mask to exclude identity-revealing features in facial images. By training a ResNet-18 deep learning model on synthetic data with the mask, the method achieves better performance in terms of accuracy, recall, and F1 score compared to full-face baseline models when tested on real-world data. This approach shows promise in enabling accurate and ethical dermatological AI systems, especially for privacy-sensitive applications like telemedicine and large-scale screening. The use of synthetic data and clinical priors together enhances the potential for effective rosacea detection while respecting privacy concerns. 

<br /><br />Summary: <div>
arXiv:2509.09844v1 Announce Type: new 
Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that primarily affects the central face and presents with subtle redness, pustules, and visible blood vessels. Automated detection remains challenging due to the diffuse nature of symptoms, the scarcity of labeled datasets, and privacy concerns associated with using identifiable facial images. A novel privacy-preserving automated rosacea detection method inspired by clinical priors and trained entirely on synthetic data is presented in this paper. Specifically, the proposed method, which leverages the observation that rosacea manifests predominantly through central facial erythema, first constructs a fixed redness-informed mask by selecting regions with consistently high red channel intensity across facial images. The mask thus is able to focus on diagnostically relevant areas such as the cheeks, nose, and forehead and exclude identity-revealing features. Second, the ResNet-18 deep learning method, which is trained on the masked synthetic images, achieves superior performance over the full-face baselines with notable gains in terms of accuracy, recall and F1 score when evaluated using the real-world test data. The experimental results demonstrate that the synthetic data and clinical priors can jointly enable accurate and ethical dermatological AI systems, especially for privacy sensitive applications in telemedicine and large-scale screening.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking</title>
<link>https://arxiv.org/abs/2509.09849</link>
<guid>https://arxiv.org/abs/2509.09849</guid>
<content:encoded><![CDATA[
<div> ablation study, ULW framework, laparoscopic image desmoking, U-Net, mean squared error loss<br />
Summary:<br />
This paper conducts an ablation study to evaluate the effectiveness and necessity of individual components in the ULW framework for laparoscopic image desmoking. The ULW approach combines a U-Net backbone with a compound loss function containing MSE, SSIM loss, and perceptual loss, as well as a learnable Wiener filter module. Through systematic ablation experiments, the study assesses the impact of removing the Wiener filter and selectively using individual loss terms from the composite function. Benchmarking on a laparoscopic images dataset reveals insights into the specific contributions of each component on the overall performance, with metrics such as SSIM, PSNR, MSE, and CIEDE-2000, along with visual comparisons. The study provides valuable insights into optimizing the ULW framework for enhanced laparoscopic image desmoking. <br /><br />Summary: <div>
arXiv:2509.09849v1 Announce Type: new 
Abstract: To rigorously assess the effectiveness and necessity of individual components within the recently proposed ULW framework for laparoscopic image desmoking, this paper presents a comprehensive ablation study. The ULW approach combines a U-Net based backbone with a compound loss function that comprises mean squared error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The framework also incorporates a differentiable, learnable Wiener filter module. In this study, each component is systematically ablated to evaluate its specific contribution to the overall performance of the whole framework. The analysis includes: (1) removal of the learnable Wiener filter, (2) selective use of individual loss terms from the composite loss function. All variants are benchmarked on a publicly available paired laparoscopic images dataset using quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative visual comparisons.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector</title>
<link>https://arxiv.org/abs/2509.09859</link>
<guid>https://arxiv.org/abs/2509.09859</guid>
<content:encoded><![CDATA[
<div> drone detector, multi-modal, object detection, acoustic signals, Deformable DETR <br />
Summary: <br />
The article introduces a multi-modal WAVE-DETR drone detector that combines visible RGB and acoustic signals for robust UAV object detection. The approach fuses visual and acoustic features using the Deformable DETR and Wav2Vec2 architectures, showing strong performance in challenging environmental conditions. Leveraging the Drone-vs-Bird dataset and the ARDrone dataset with over 7,500 synchronized images and audio segments, the fusion of acoustic information improves the Deformable DETR object detector's performance. Four fusion configurations were developed, with the gated fusion approach being the best performer, enhancing mAP scores by 11.1% to 15.3% for small drones and 3.27% to 5.84% for medium and large drones across all IoU thresholds. The Wav2Vec2 acoustic embeddings fused with the Deformable DETR feature mappings significantly improve overall object detection performance across all drone sizes. <div>
arXiv:2509.09859v1 Announce Type: new 
Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and acoustic signals for robust real-life UAV object detection. Our approach fuses visual and acoustic features in a unified object detector model relying on the Deformable DETR and Wav2Vec2 architectures, achieving strong performance under challenging environmental conditions. Our work leverage the existing Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more than 7,500 synchronized images and audio segments. We show how the acoustic information is used to improve the performance of the Deformable DETR object detector on the real ARDrone dataset. We developed, trained and tested four different fusion configurations based on a gated mechanism, linear layer, MLP and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi resolution feature mappings of the Deformable DETR and enhance the object detection performance over all drones dimensions. The best performer is the gated fusion approach, which improves the mAP of the Deformable DETR object detector on our in-distribution and out-of-distribution ARDrone datasets by 11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9. The mAP scores for medium and large drones are also enhanced, with overall gains across all drone sizes ranging from 3.27% to 5.84%.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate Supervision for Robust and Generalizable Deformable Image Registration</title>
<link>https://arxiv.org/abs/2509.09869</link>
<guid>https://arxiv.org/abs/2509.09869</guid>
<content:encoded><![CDATA[
<div> training paradigm, deep learning, medical image registration, robustness, generalizability

Summary: 
Surrogate supervision is proposed as a training paradigm to enhance the robustness and generalizability of deep learning-based deformable image registration. By decoupling the input domain from the supervision domain using estimated spatial transformations on surrogate images, the framework is able to handle variations in input characteristics such as artifacts, field-of-view mismatch, and modality differences. The approach was evaluated on tasks including artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration, showing strong resilience to input variations while maintaining high performance on well-curated data. Surrogate supervision offers a principled and practical pathway to improving the robustness and generalizability of medical image registration models without increasing complexity, thereby enabling broader applicability in diverse biomedical imaging scenarios.<br /><br />Summary: <div>
arXiv:2509.09869v1 Announce Type: new 
Abstract: Objective: Deep learning-based deformable image registration has achieved strong accuracy, but remains sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality difference. We aim to develop a general training paradigm that improves the robustness and generalizability of registration networks. Methods: We introduce surrogate supervision, which decouples the input domain from the supervision domain by applying estimated spatial transformations to surrogate images. This allows training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined. We evaluate the framework through three representative applications: artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration. Results: Across tasks, surrogate supervision demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences, while maintaining high performance on well-curated data. Conclusions: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity. Significance: Surrogate supervision offers a practical pathway to more robust and generalizable medical image registration, enabling broader applicability in diverse biomedical imaging scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars</title>
<link>https://arxiv.org/abs/2509.09911</link>
<guid>https://arxiv.org/abs/2509.09911</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, dental age estimation, convolutional autoencoder, Vision Transformer, forensic age estimation <br />
<br />
Summary: <br />
- The study addresses the challenge of 'black box' models in forensic age estimation, focusing on the practical adoption of deep learning techniques for dental age estimation.
- A framework combining a convolutional autoencoder (AE) with a Vision Transformer (ViT) is proposed to improve performance and transparency in automated staging of mandibular molars.
- The framework shows increased classification accuracy for teeth 37 and 38 compared to a baseline ViT, offering diagnostic insights through analysis of the AE's latent space metrics and image reconstructions.
- The framework identifies data-centric limitations in the tooth 38 dataset, highlighting high intra-class morphological variability as a key factor impacting performance.
- The study emphasizes the importance of multi-faceted interpretability in model analysis, suggesting that reliance solely on attention maps may overlook underlying data issues, and presents a more robust methodology to support expert decision-making in forensic age estimation. <br /> <div>
arXiv:2509.09911v1 Announce Type: new 
Abstract: The practical adoption of deep learning in high-stakes forensic applications, such as dental age estimation, is often limited by the 'black box' nature of the models. This study introduces a framework designed to enhance both performance and transparency in this context. We use a notable performance disparity in the automated staging of mandibular second (tooth 37) and third (tooth 38) molars as a case study. The proposed framework, which combines a convolutional autoencoder (AE) with a Vision Transformer (ViT), improves classification accuracy for both teeth over a baseline ViT, increasing from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond improving performance, the framework provides multi-faceted diagnostic insights. Analysis of the AE's latent space metrics and image reconstructions indicates that the remaining performance gap is data-centric, suggesting high intra-class morphological variability in the tooth 38 dataset is a primary limiting factor. This work highlights the insufficiency of relying on a single mode of interpretability, such as attention maps, which can appear anatomically plausible yet fail to identify underlying data issues. By offering a methodology that both enhances accuracy and provides evidence for why a model may be uncertain, this framework serves as a more robust tool to support expert decision-making in forensic age estimation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoDA: Self-supervised Continual Domain Adaptation</title>
<link>https://arxiv.org/abs/2509.09935</link>
<guid>https://arxiv.org/abs/2509.09935</guid>
<content:encoded><![CDATA[
<div> SCoDA, Self-supervised Continual Domain Adaptation, domain adaptation, source-free, geometric manifold alignment, SSL<br />
<br />
Summary: SCoDA introduces a novel approach to Source-Free Domain Adaptation (SFDA) by initializing the framework with a teacher model pre-trained via self-supervision and incorporating geometric manifold alignment principles. The student model is trained using a composite objective that combines instance-level feature matching with a Space Similarity Loss to preserve crucial geometric information about the latent manifold of the source model. To prevent catastrophic forgetting, the teacher's parameters are updated through an Exponential Moving Average (EMA) of the student's parameters. Extensive experiments on benchmark datasets show that SCoDA outperforms existing SFDA methods, demonstrating its effectiveness in adapting a model to a target domain without access to source domain data.<br /> <div>
arXiv:2509.09935v1 Announce Type: new 
Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a model to a target domain without access to the data of the source domain. Prevailing methods typically start with a source model pre-trained with full supervision and distill the knowledge by aligning instance-level features. However, these approaches, relying on cosine similarity over L2-normalized feature vectors, inadvertently discard crucial geometric information about the latent manifold of the source model. We introduce Self-supervised Continual Domain Adaptation (SCoDA) to address these limitations. We make two key departures from standard practice: first, we avoid the reliance on supervised pre-training by initializing the proposed framework with a teacher model pre-trained entirely via self-supervision (SSL). Second, we adapt the principle of geometric manifold alignment to the SFDA setting. The student is trained with a composite objective combining instance-level feature matching with a Space Similarity Loss. To combat catastrophic forgetting, the teacher's parameters are updated via an Exponential Moving Average (EMA) of the student's parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA significantly outperforms state-of-the-art SFDA methods.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Anything for Cell Tracking</title>
<link>https://arxiv.org/abs/2509.09943</link>
<guid>https://arxiv.org/abs/2509.09943</guid>
<content:encoded><![CDATA[
<div> zero-shot learning, cell tracking, time-lapse microscopy, deep learning, unsupervised <br />
<br />
Summary: 
The article introduces a novel approach for cell tracking and mitotic event detection in time-lapse microscopy image sequences. Traditional deep learning methods for cell tracking require manually labeled datasets for training, limiting their generalizability to unseen data. The proposed zero-shot cell tracking framework integrates a large foundation model called SAM2 for image and video segmentation, allowing for unsupervised tracking without dataset-specific adaptation. This approach eliminates the need for training on specific datasets, reducing cost and time involved in manual labeling. The method achieves competitive accuracy in both 2D and 3D microscopy videos, overcoming challenges such as dividing objects, low signal-to-noise ratios, and dense clusters. By leveraging the capabilities of SAM2, the framework offers a versatile solution for cell tracking in diverse microscopy datasets. <br /> <div>
arXiv:2509.09943v1 Announce Type: new 
Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image sequences is a crucial task in biomedical research. However, it remains highly challenging due to dividing objects, low signal-tonoise ratios, indistinct boundaries, dense clusters, and the visually similar appearance of individual cells. Existing deep learning-based methods rely on manually labeled datasets for training, which is both costly and time-consuming. Moreover, their generalizability to unseen datasets remains limited due to the vast diversity of microscopy data. To overcome these limitations, we propose a zero-shot cell tracking framework by integrating Segment Anything 2 (SAM2), a large foundation model designed for general image and video segmentation, into the tracking pipeline. As a fully-unsupervised approach, our method does not depend on or inherit biases from any specific training dataset, allowing it to generalize across diverse microscopy datasets without finetuning. Our approach achieves competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos while eliminating the need for dataset-specific adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation</title>
<link>https://arxiv.org/abs/2509.09946</link>
<guid>https://arxiv.org/abs/2509.09946</guid>
<content:encoded><![CDATA[
<div> 3D MTMC, multi-target, multi-camera tracking, depth information, data association

Summary:
The paper introduces a method for extending 2D multi-camera tracking systems to 3D space using depth information to reconstruct targets in point-cloud space. By clustering and refining yaw, the system can recover the 3D box of the targets. Additionally, an enhanced data association mechanism assigns global IDs across frames by leveraging local ID consistency. The framework was evaluated on the 2025 AI City Challenge's 3D MTMC dataset, where it achieved 3rd place on the leaderboard. This approach enables automation of large-scale surveillance tasks by allowing targets to be projected into the 3D environment, providing a more comprehensive understanding of the scene. <div>
arXiv:2509.09946v1 Announce Type: new 
Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision task for automating large-scale surveillance. With camera calibration and depth information, the targets in the scene can be projected into 3D space, offering unparalleled levels of automatic perception of a 3D environment. However, tracking in the 3D space requires replacing all 2D tracking components from the ground up, which may be infeasible for existing MTMC systems. In this paper, we present an approach for extending any online 2D multi-camera tracking system into 3D space by utilizing depth information to reconstruct a target in point-cloud space, and recovering its 3D box through clustering and yaw refinement following tracking. We also introduced an enhanced online data association mechanism that leverages the target's local ID consistency to assign global IDs across frames. The proposed framework is evaluated on the 2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the leaderboard.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification</title>
<link>https://arxiv.org/abs/2509.09958</link>
<guid>https://arxiv.org/abs/2509.09958</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension; zero-shot workflow; visual-language verification; COCO dataset; VLM <br />
Summary:<br />
A new approach to Referring Expression Comprehension (REC) demonstrates competitive or superior performance without task-specific training. By treating REC as box-wise visual-language verification, using a generic detector (YOLO-World) and a general-purpose VLM to answer True/False queries for each region, the method reduces interference between boxes, allows for abstention and multiple matches, and requires no fine-tuning. Results on RefCOCO datasets show that this approach outperforms a zero-shot baseline and models trained with REC-specific data. Controlled studies confirm that verification outperforms selection-based prompting, even with open VLMs. The findings suggest that workflow design is key to achieving strong zero-shot REC performance, rather than task-specific pretraining. <br /> <div>
arXiv:2509.09958v1 Announce Type: new 
Abstract: Referring Expression Comprehension (REC) is usually addressed with task-trained grounding models. We show that a zero-shot workflow, without any REC-specific training, can achieve competitive or superior performance. Our approach reformulates REC as box-wise visual-language verification: given proposals from a COCO-clean generic detector (YOLO-World), a general-purpose VLM independently answers True/False queries for each region. This simple procedure reduces cross-box interference, supports abstention and multiple matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our method not only surpasses a zero-shot GroundingDINO baseline but also exceeds reported results for GroundingDINO trained on REC and GroundingDINO+CRG. Controlled studies with identical proposals confirm that verification significantly outperforms selection-based prompting, and results hold with open VLMs. Overall, we show that workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation</title>
<link>https://arxiv.org/abs/2509.09961</link>
<guid>https://arxiv.org/abs/2509.09961</guid>
<content:encoded><![CDATA[
<div> Keywords: wheat, segmentation, foliar diseases, insect damage, Random Projected Copy-and-Paste (RPCP) augmentation

Summary:
The article discusses the importance of accurate segmentation of foliar diseases and insect damage in wheat for effective crop management. The extreme pixel-level imbalance in insect damage poses a challenge to segmentation performance, leading to overfitting and insufficient learning of rare classes. To address this issue, the authors propose a Random Projected Copy-and-Paste (RPCP) augmentation technique. This method involves extracting rare insect-damage patches from training images, applying geometric transformations, and pasting them in appropriate regions to improve segmentation accuracy. Additionally, a random projection filter refines local features for a natural blend with the background. Experimental results show that the RPCP augmentation technique significantly enhances segmentation performance on insect damage while maintaining or improving accuracy in other categories. This targeted augmentation approach offers a practical solution for agricultural segmentation problems. 

<br /><br />Summary: <div>
arXiv:2509.09961v1 Announce Type: new 
Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is crucial for effective crop management and disease control. However, the insect damage typically occupies only a tiny fraction of annotated pixels. This extreme pixel-level imbalance poses a significant challenge to the segmentation performance, which can result in overfitting to common classes and insufficient learning of rare classes, thereby impairing overall performance. In this paper, we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to address the pixel imbalance problem. Specifically, we extract rare insect-damage patches from annotated training images and apply random geometric transformations to simulate variations. The transformed patches are then pasted in appropriate regions while avoiding overlaps with lesions or existing damaged regions. In addition, we apply a random projection filter to the pasted regions, refining local features and ensuring a natural blend with the new background. Experiments show that our method substantially improves segmentation performance on the insect damage class, while maintaining or even slightly enhancing accuracy on other categories. Our results highlight the effectiveness of targeted augmentation in mitigating extreme pixel imbalance, offering a straightforward yet effective solution for agricultural segmentation problems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock</title>
<link>https://arxiv.org/abs/2509.09962</link>
<guid>https://arxiv.org/abs/2509.09962</guid>
<content:encoded><![CDATA[
<div> Framework, multi-object tracking, Hidden Markov Model, uncertain identities, animal tracking

Summary:
The article introduces a new framework for long-term multi-object tracking (MOT) that addresses the issue of identity switches between objects over time. By incorporating uncertain identities and utilizing a Hidden Markov Model (HMM) formulation, the framework improves tracking performance on a 10-minute pig tracking dataset. The HMM framework also demonstrates robustness to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance is validated on benchmark datasets using leading MOT approaches like ByteTrack and FairMOT. The code for the HMM framework and the new pig tracking dataset are available for further exploration and research. Overall, the framework shows promising results in enhancing tracking accuracy, especially in scenarios where identifications are sporadic or uncertain. 

<br /><br />Summary: <div>
arXiv:2509.09962v1 Announce Type: new 
Abstract: The need for long-term multi-object tracking (MOT) is growing due to the demand for analyzing individual behaviors in videos that span several minutes. Unfortunately, due to identity switches between objects, the tracking performance of existing MOT approaches decreases over time, making them difficult to apply for long-term tracking. However, in many real-world applications, such as in the livestock sector, it is possible to obtain sporadic identifications for some of the animals from sources like feeders. To address the challenges of long-term MOT, we propose a new framework that combines both uncertain identities and tracking using a Hidden Markov Model (HMM) formulation. In addition to providing real-world identities to animals, our HMM framework improves the F1 score of ByteTrack, a leading MOT approach even with re-identification, on a 10 minute pig tracking dataset with 21 identifications at the pen's feeding station. We also show that our approach is robust to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance of our HMM framework was also validated on the MOT17 and MOT20 benchmark datasets using both ByteTrack and FairMOT. The code for this new HMM framework and the new 10-minute pig tracking video dataset are available at: https://github.com/ngobibibnbe/uncertain-identity-aware-tracking
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey</title>
<link>https://arxiv.org/abs/2509.09971</link>
<guid>https://arxiv.org/abs/2509.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: event camera sensors, deep learning, video restoration, 3D reconstruction, visual media enhancement

Summary:
This survey explores the fusion of event-stream captured with traditional frame-based capture, showcasing how this integration benefits video restoration and 3D reconstruction tasks. It reviews major deep learning advancements in image/video enhancement and restoration, focusing on temporal and spatial enhancement techniques such as frame interpolation, motion deblurring, super-resolution, low-light and HDR enhancement, and artifact reduction. The paper also delves into how the 3D reconstruction field is evolving with the fusion of event-driven data. It discusses recent works that improve visual quality under challenging conditions by leveraging event camera systems and deep learning. Additionally, the survey provides a list of openly available datasets for reproducible research and benchmarking purposes. This comprehensive overview aims to inspire further research into the utilization of event camera sensors, particularly in conjunction with deep learning, for advanced visual media restoration and enhancement.<br /><br />Summary: <div>
arXiv:2509.09971v1 Announce Type: new 
Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking</title>
<link>https://arxiv.org/abs/2509.09977</link>
<guid>https://arxiv.org/abs/2509.09977</guid>
<content:encoded><![CDATA[
<div> Transformer, RGB-Event tracking, Artificial Neural Networks, Spiking Neural Networks, ISTASTrack

Summary:<br />
ISTASTrack is a novel transformer-based ANN-SNN hybrid tracker for RGB-Event tracking. It addresses the challenge of effectively fusing features from RGB images and dynamic event streams by using ISTA adapters for bidirectional feature interaction. The model consists of two branches - a vision transformer for spatial context extraction from RGB inputs and a spiking transformer for spatio-temporal dynamics from event streams. An ISTA adapter based on sparse representation theory facilitates feature fusion between ANN and SNN branches. A temporal downsampling attention module aligns SNN features with ANN features for improved temporal fusion. ISTASTrack achieves state-of-the-art performance on RGB-Event tracking benchmarks like FE240hz and VisEvent while being energy efficient. This work demonstrates the efficacy of hybrid ANN-SNN designs in enhancing visual tracking accuracy. <div>
arXiv:2509.09977v1 Announce Type: new 
Abstract: RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction</title>
<link>https://arxiv.org/abs/2509.09988</link>
<guid>https://arxiv.org/abs/2509.09988</guid>
<content:encoded><![CDATA[
<div> deep state space models, class imbalance, FLARE loss, solar flare prediction, multi-wavelength solar image dataset <br />
Summary: 
This study focuses on improving the accuracy of predicting the class of the largest solar flare expected to occur within the next 72 hours. The current forecasting methods lack effectiveness due to class imbalance across flare classes. To address this issue, the researchers propose a solar flare prediction model based on multiple deep state space models. They introduce the FLARE loss, a novel reliability loss, to enhance predictive performance and reliability under class imbalance. Experiments conducted on a comprehensive solar image dataset demonstrate that the proposed method outperforms baseline approaches. The performance and reliability of the model were assessed using standard metrics, such as the Gandin-Murphy-Gerrity score and the true skill statistic, showcasing the model's superior predictive capabilities.  <br /><br />Summary: <div>
arXiv:2509.09988v1 Announce Type: new 
Abstract: Accurate and reliable solar flare predictions are essential to mitigate potential impacts on critical infrastructure. However, the current performance of solar flare forecasting is insufficient. In this study, we address the task of predicting the class of the largest solar flare expected to occur within the next 72 hours. Existing methods often fail to adequately address the severe class imbalance across flare classes. To address this issue, we propose a solar flare prediction model based on multiple deep state space models. In addition, we introduce the frequency & local-boundary-aware reliability loss (FLARE loss) to improve predictive performance and reliability under class imbalance. Experiments were conducted on a multi-wavelength solar image dataset covering a full 11-year solar activity cycle. As a result, our method outperformed baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the true skill statistic, which are standard metrics in terms of the performance and reliability.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion</title>
<link>https://arxiv.org/abs/2509.10005</link>
<guid>https://arxiv.org/abs/2509.10005</guid>
<content:encoded><![CDATA[
<div> Keywords: RGB-thermal, semantic segmentation, feature extraction, cross-modal fusion, real-time efficiency <br />
Summary: <br />
The article introduces a new model called TUNI for RGB-thermal semantic segmentation, addressing issues in existing models. TUNI features an RGB-T encoder that performs multi-modal feature extraction and cross-modal fusion simultaneously, leading to better integration of features. Through large-scale pre-training with RGB and pseudo-thermal data, the encoder learns to efficiently fuse features in a unified manner. By slimming down the thermal branch, TUNI achieves a more compact architecture without compromising performance. An RGB-T local module is also introduced to enhance cross-modal local feature fusion using adaptive cosine similarity. Experimental results demonstrate that TUNI performs competitively with state-of-the-art models on various datasets while having fewer parameters and lower computational costs. Furthermore, TUNI achieves real-time inference speeds of 27 FPS on a Jetson Orin NX, showcasing its efficiency in deployment. The code for TUNI is available on GitHub for further exploration. <br /> <div>
arXiv:2509.10005v1 Announce Type: new 
Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental perception of autonomous platforms in challenging conditions. Prevailing models employ encoders pre-trained on RGB images to extract features from both RGB and infrared inputs, and design additional modules to achieve cross-modal feature fusion. This results in limited thermal feature extraction and suboptimal cross-modal fusion, while the redundant encoders further compromises the model's real-time efficiency. To address the above issues, we propose TUNI, with an RGB-T encoder consisting of multiple stacked blocks that simultaneously perform multi-modal feature extraction and cross-modal fusion. By leveraging large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder learns to integrate feature extraction and fusion in a unified manner. By slimming down the thermal branch, the encoder achieves a more compact architecture. Moreover, we introduce an RGB-T local module to strengthen the encoder's capacity for cross-modal local feature fusion. The RGB-T local module employs adaptive cosine similarity to selectively emphasize salient consistent and distinct local features across RGB-T modalities. Experimental results show that TUNI achieves competitive performance with state-of-the-art models on FMB, PST900 and CART, with fewer parameters and lower computational cost. Meanwhile, it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its real-time capability in deployment. Codes are available at https://github.com/xiaodonguo/TUNI.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Part-Shot Font Generation</title>
<link>https://arxiv.org/abs/2509.10006</link>
<guid>https://arxiv.org/abs/2509.10006</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot font generation, partial design elements, efficiency, font creation, design details 

Summary: 
This paper introduces a new model for few-shot font generation that allows designing entire fonts based on partial design elements, rather than requiring complete character shapes for multiple classes. By utilizing partial shapes as input, this approach improves the efficiency of font creation while also offering insights into how specific design details influence the overall structure of individual characters. The proposed model streamlines the font design process and opens up possibilities for more creative and efficient font creation techniques. Ultimately, this new model provides a fresh perspective on font generation and highlights the importance of understanding the influence of partial design elements on the overall design of fonts. <div>
arXiv:2509.10006v1 Announce Type: new 
Abstract: This paper proposes a novel model of few-part-shot font generation, which designs an entire font based on a set of partial design elements, i.e., partial shapes. Unlike conventional few-shot font generation, which requires entire character shapes for a couple of character classes, our approach only needs partial shapes as input. The proposed model not only improves the efficiency of font creation but also provides insights into how partial design details influence the entire structure of the individual characters.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Accurate Downfacing Visual Inertial Odometry</title>
<link>https://arxiv.org/abs/2509.10021</link>
<guid>https://arxiv.org/abs/2509.10021</guid>
<content:encoded><![CDATA[
<div> Visual Inertial Odometry, VIO, micro-UAVs, RISC-V, feature detection, tracking methods<br />
<br />
Summary:<br />
This paper presents an efficient and accurate Visual Inertial Odometry (VIO) pipeline optimized for micro- and nano-UAVs. The design incorporates state-of-the-art feature detection and tracking methods such as SuperPoint, PX4FLOW, and ORB, quantized for RISC-V-based ultra-low-power parallel systems on chips. By using a rigid body motion model, the pipeline reduces estimation errors and improves accuracy in planar motion scenarios. Real-time VIO suitability on an ultra-low-power SoC is assessed in terms of compute requirements and tracking accuracy after quantization. The implementation on the GAP9 low-power SoC shows a significant reduction in RMSE compared to baseline, with ORB achieving the highest accuracy. The computational complexity analysis reveals that PX4FLOW offers comparable tracking accuracy to ORB at lower runtime for movements below 24 pixels/frame. <div>
arXiv:2509.10021v1 Announce Type: new 
Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that determines an agent's movement through a camera and an IMU sensor. This paper presents an efficient and accurate VIO pipeline optimized for applications on micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and quantized for emerging RISC-V-based ultra-low-power parallel systems on chips (SoCs). Furthermore, by employing a rigid body motion model, the pipeline reduces estimation errors and achieves improved accuracy in planar motion scenarios. The pipeline's suitability for real-time VIO is assessed on an ultra-low-power SoC in terms of compute requirements and tracking accuracy after quantization. The pipeline, including the three feature tracking methods, was implemented on the SoC for real-world validation. This design bridges the gap between high-accuracy VIO pipelines that are traditionally run on computationally powerful systems and lightweight implementations suitable for microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates an average reduction in RMSE of up to a factor of 3.65x over the baseline pipeline when using the ORB feature tracker. The analysis of the computational complexity of the feature trackers further shows that PX4FLOW achieves on-par tracking accuracy with ORB at a lower runtime for movement speeds below 24 pixels/frame.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images</title>
<link>https://arxiv.org/abs/2509.10024</link>
<guid>https://arxiv.org/abs/2509.10024</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D face model, convolutional neural network, in-the-wild images, hierarchical multi-level attention network, semi-supervised training

Summary:
The article introduces a novel approach called Hierarchical Multi-Level Attention Network (MLANet) for reconstructing detailed 3D face models from single in-the-wild images. The model utilizes a convolutional neural network architecture with multi-level attention mechanisms for facial feature extraction. A semi-supervised training strategy is employed, incorporating 3DMM parameters and a differentiable renderer for end-to-end training. The method is evaluated on benchmark datasets AFLW2000-3D and MICC Florence, showcasing its effectiveness in 3D face reconstruction and alignment tasks. The proposed approach predicts facial geometry, texture, pose, and illumination parameters from a single image, addressing the challenges posed by the lack of labeled datasets and complex real-world environments in 3D face modeling. This work contributes to the advancement of 3D face modeling technology and shows promising results for applications in computer vision. 

<br /><br />Summary: <div>
arXiv:2509.10024v1 Announce Type: new 
Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable attention in the computer vision community due to its wide range of potential applications. However, the lack of ground-truth labeled datasets and the complexity of real-world environments remain significant challenges. In this chapter, we propose a convolutional neural network-based approach, the Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face models from single in-the-wild images. Our model predicts detailed facial geometry, texture, pose, and illumination parameters from a single image. Specifically, we employ a pre-trained hierarchical backbone network and introduce multi-level attention mechanisms at different stages of 2D face image feature extraction. A semi-supervised training strategy is employed, incorporating 3D Morphable Model (3DMM) parameters from publicly available datasets along with a differentiable renderer, enabling an end-to-end training process. Extensive experiments, including both comparative and ablation studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The effectiveness of the proposed method was evaluated both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</title>
<link>https://arxiv.org/abs/2509.10026</link>
<guid>https://arxiv.org/abs/2509.10026</guid>
<content:encoded><![CDATA[
<div> Keywords: VLMs, multilingual visual question answering, Chain-of-thought reasoning, LaV-CoT, multi-aspect reward optimization

Summary: 
LaV-CoT is introduced as a Language-aware Visual CoT framework with Multi-Aspect Reward Optimization for multilingual visual question answering. It incorporates a multi-stage reasoning pipeline including Text Summary with Bounding Box, Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. An automated data curation method is designed to generate high-quality multilingual CoT annotations for training data. LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning with Language-aware Group Relative Policy Optimization guided by verifiable multi-aspect rewards. Extensive evaluations demonstrate LaV-CoT's superior performance compared to open-source baselines and even models with larger scales. It outperforms advanced proprietary models like GPT-4o-0513 and Gemini-2.5-flash and shows effectiveness for industrial deployment. Online A/B testing validates the method on real-world data, showcasing its potential for practical applications. The code for LaV-CoT is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2509.10026v1 Announce Type: new 
Abstract: As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A/B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: \href{https://github.com/HJNVR/LaV-CoT}
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation</title>
<link>https://arxiv.org/abs/2509.10058</link>
<guid>https://arxiv.org/abs/2509.10058</guid>
<content:encoded><![CDATA[
<div> Keywords: color alignment, text-to-image generation, large language model, disambiguation, CIELAB color space

Summary:
Accurate color alignment is essential in text-to-image generation for applications like fashion and design. Current models struggle with nuanced color terms, leading to misaligned images. This paper proposes a training-free framework that leverages a large language model to disambiguate color prompts and refine color blending in the text embedding space. By resolving ambiguous color terms and adjusting text embeddings based on spatial relationships in the CIELAB color space, this method enhances color fidelity without additional training or reference images. Experimental results show improved color accuracy without sacrificing image quality, effectively bridging the gap between text semantics and visual generation. <div>
arXiv:2509.10058v1 Announce Type: new 
Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration</title>
<link>https://arxiv.org/abs/2509.10059</link>
<guid>https://arxiv.org/abs/2509.10059</guid>
<content:encoded><![CDATA[
<div> Keywords: AVI-Math, mathematical reasoning, UAV-based remote sensing, vision-language models, multimodal benchmark

Summary:
AVI-Math introduces a benchmark to evaluate mathematical reasoning in aerial vehicle imagery, focusing on UAV-based remote sensing. The dataset includes vehicle-related questions covering various mathematical subjects and topics, collected from real-world UAV scenarios. The evaluation of 14 vision-language models on AVI-Math reveals their limitations in mathematical reasoning tasks. The study suggests the use of Chain-of-Thought prompting and fine-tuning techniques to improve reasoning capabilities. The findings highlight the need for advancements in UAV-based vision-language models for real-world applications. The code and datasets will be made available for further research at the provided GitHub repository. 

Summary: <br /><br />Mathematical reasoning in UAV-based remote sensing is evaluated through the AVI-Math benchmark, revealing limitations in current vision-language models. The dataset includes vehicle-related questions from real-world UAV scenarios, covering diverse mathematical subjects and topics. The study suggests using Chain-of-Thought prompting and fine-tuning techniques to enhance reasoning capabilities for UAV applications. Availability of code and datasets for further research is provided through the GitHub repository. <div>
arXiv:2509.10059v1 Announce Type: new 
Abstract: Mathematical reasoning is critical for tasks such as precise distance and area computations, trajectory estimations, and spatial analysis in unmanned aerial vehicle (UAV) based remote sensing, yet current vision-language models (VLMs) have not been adequately tested in this domain. To address this gap, we introduce AVI-Math, the first benchmark to rigorously evaluate multimodal mathematical reasoning in aerial vehicle imagery, moving beyond simple counting tasks to include domain-specific knowledge in areas such as geometry, logic, and algebra. The dataset comprises 3,773 high-quality vehicle-related questions captured from UAV views, covering 6 mathematical subjects and 20 topics. The data, collected at varying altitudes and from multiple UAV angles, reflects real-world UAV scenarios, ensuring the diversity and complexity of the constructed mathematical problems. In this paper, we benchmark 14 prominent VLMs through a comprehensive evaluation and demonstrate that, despite their success on previous multimodal benchmarks, these models struggle with the reasoning tasks in AVI-Math. Our detailed analysis highlights significant limitations in the mathematical reasoning capabilities of current VLMs and suggests avenues for future research. Furthermore, we explore the use of Chain-of-Thought prompting and fine-tuning techniques, which show promise in addressing the reasoning challenges in AVI-Math. Our findings not only expose the limitations of VLMs in mathematical reasoning but also offer valuable insights for advancing UAV-based trustworthy VLMs in real-world applications. The code, and datasets will be released at https://github.com/VisionXLab/avi-math
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals</title>
<link>https://arxiv.org/abs/2509.10080</link>
<guid>https://arxiv.org/abs/2509.10080</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, trajectory prediction, bird's-eye view, real-time sensor data, deformable attention<br />
Summary:<br />
The article introduces a new trajectory prediction framework called BEVTraj, designed for autonomous driving. Unlike existing approaches that rely on pre-built maps, BEVTraj operates directly in the bird's-eye view (BEV) space using real-time sensor data. The framework uses deformable attention to extract context from dense BEV features efficiently. Additionally, BEVTraj includes a Sparse Goal Candidate Proposal (SGCP) module for end-to-end prediction without post-processing steps. Extensive experiments show that BEVTraj achieves performance comparable to HD map-based models but offers more flexibility by eliminating the need for pre-built maps. The source code for BEVTraj is available on GitHub for further exploration and development. <div>
arXiv:2509.10080v1 Announce Type: new 
Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe and efficient navigation. To improve prediction accuracy, recent approaches often rely on pre-built high-definition (HD) maps or real-time local map construction modules to incorporate static environmental information. However, pre-built HD maps are limited to specific regions and cannot adapt to transient changes. In addition, local map construction modules, which recognize only predefined elements, may fail to capture critical scene details or introduce errors that degrade prediction performance. To overcome these limitations, we propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory prediction framework that operates directly in the bird's-eye view (BEV) space utilizing real-time sensor data without relying on any pre-built maps. The BEVTraj leverages deformable attention to efficiently extract relevant context from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate Proposal (SGCP) module, which enables full end-to-end prediction without requiring any post-processing steps. Extensive experiments demonstrate that the BEVTraj achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating the dependency on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing</title>
<link>https://arxiv.org/abs/2509.10093</link>
<guid>https://arxiv.org/abs/2509.10093</guid>
<content:encoded><![CDATA[
<div> propose, multi-human parsing, occlusions, multi-view information, weak supervision
Summary:
- The article introduces a new approach for multi-human parsing, focusing on segmenting human body parts and associating them with the correct person in cases of overlapping bodies.
- Current state-of-the-art methods struggle with segmenting overlapping people, leading to the proposal of a novel training framework that utilizes multi-view information to improve parsing models under occlusions.
- The proposed method incorporates weak supervision on human instances and a multi-view consistency loss during training to enhance performance.
- To address the lack of suitable datasets, a semi-automatic annotation strategy is proposed to generate human instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
- Experimental results show up to a 4.20% relative improvement in human parsing over the baseline model in scenarios involving occlusions.<br /><br />Summary: <div>
arXiv:2509.10093v1 Announce Type: new 
Abstract: Multi-human parsing is the task of segmenting human body parts while associating each part to the person it belongs to, combining instance-level and part-level information for fine-grained human understanding. In this work, we demonstrate that, while state-of-the-art approaches achieved notable results on public datasets, they struggle considerably in segmenting people with overlapping bodies. From the intuition that overlapping people may appear separated from a different point of view, we propose a novel training framework exploiting multi-view information to improve multi-human parsing models under occlusions. Our method integrates such knowledge during the training process, introducing a novel approach based on weak supervision on human instances and a multi-view consistency loss. Given the lack of suitable datasets in the literature, we propose a semi-automatic annotation strategy to generate human instance segmentation masks from multi-view RGB+D data and 3D human skeletons. The experiments demonstrate that the approach can achieve up to a 4.20\% relative improvement on human parsing over the baseline model in occlusion scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARCO-VISION-2.0 Technical Report</title>
<link>https://arxiv.org/abs/2509.10105</link>
<guid>https://arxiv.org/abs/2509.10105</guid>
<content:encoded><![CDATA[
<div> bilingual, vision-language model, VARCO-VISION-2.0, multi-image understanding, spatial grounding 
<br />
<br />
Summary: 
The article introduces VARCO-VISION-2.0, an open-weight bilingual vision-language model (VLM) for Korean and English that improves on the previous model VARCO-VISION-14B. The new model supports multi-image understanding for complex inputs like documents, charts, and tables, providing layout-aware OCR by predicting textual content and spatial location. Trained with a four-stage curriculum and memory-efficient techniques, VARCO-VISION-2.0 achieves enhanced multimodal alignment while maintaining core language capabilities and improving safety through preference optimization. Extensive benchmark evaluations show strong spatial grounding and competitive results in both languages, with the 14B model ranking 8th on the OpenCompass VLM leaderboard among models of similar scale. Additionally, a 1.7B version optimized for on-device deployment is released alongside the full-scale 14B model. The development of these bilingual VLMs is seen as a step forward in advancing practical applications of vision-language models. <div>
arXiv:2509.10105v1 Announce Type: new 
Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model (VLM) for Korean and English with improved capabilities compared to the previous model VARCO-VISION-14B. The model supports multi-image understanding for complex inputs such as documents, charts, and tables, and delivers layoutaware OCR by predicting both textual content and its spatial location. Trained with a four-stage curriculum with memory-efficient techniques, the model achieves enhanced multimodal alignment, while preserving core language abilities and improving safety via preference optimization. Extensive benchmark evaluations demonstrate strong spatial grounding and competitive results for both languages, with the 14B model achieving 8th place on the OpenCompass VLM leaderboard among models of comparable scale. Alongside the 14B-scale model, we release a 1.7B version optimized for on-device deployment. We believe these models advance the development of bilingual VLMs and their practical applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a full-scale 14B model and a lightweight 1.7B model.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss</title>
<link>https://arxiv.org/abs/2509.10114</link>
<guid>https://arxiv.org/abs/2509.10114</guid>
<content:encoded><![CDATA[
<div> MobileNetV3-Small, ShuffleNetV2, FIQA, lightweight, efficient
Summary:
Our proposed method for Face Image Quality Assessment (FIQA) utilizes a combination of MobileNetV3-Small and ShuffleNetV2 convolutional neural networks to efficiently evaluate face image quality in real-world settings. By incorporating a correlation-aware loss function, we enhance the alignment with human perceptual judgments. The model achieves a high Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894 on the VQualA FIQA benchmark while remaining computationally efficient. This balance between accuracy and computational cost makes our method suitable for practical applications in face recognition systems. The ensemble approach with prediction-level fusion via averaging ensures robust performance in capturing face-specific degradations. Our approach is designed to address the limitations of general-purpose no-reference image quality assessment techniques in uncontrolled environments. Our experiments demonstrate the effectiveness and applicability of the proposed lightweight FIQA model. 
<br /><br />Summary: <div>
arXiv:2509.10114v1 Announce Type: new 
Abstract: Face image quality assessment (FIQA) plays a critical role in face recognition and verification systems, especially in uncontrolled, real-world environments. Although several methods have been proposed, general-purpose no-reference image quality assessment techniques often fail to capture face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be computationally intensive, limiting their practical applicability. We propose a lightweight and efficient method for FIQA, designed for the perceptual evaluation of face images in the wild. Our approach integrates an ensemble of two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2, with prediction-level fusion via simple averaging. To enhance alignment with human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss), combining mean squared error (MSE) with a Pearson correlation regularizer. Our method achieves a strong balance between accuracy and computational cost, making it suitable for real-world deployment. Experiments on the VQualA FIQA benchmark demonstrate that our model achieves a Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894, remaining within competition efficiency constraints.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realism Control One-step Diffusion for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2509.10122</link>
<guid>https://arxiv.org/abs/2509.10122</guid>
<content:encoded><![CDATA[
<div> Efficient Real-ISR, diffusion models, Realism Controlled One-step Diffusion, fidelity-realism trade-offs, degradation-aware sampling<br />
<br />
Summary:<br />
The article introduces the Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR tasks using pre-trained diffusion models. RCOD addresses the limitations of one-step diffusion methods by allowing explicit control over fidelity-realism trade-offs through a latent domain grouping strategy. It also introduces a degradation-aware sampling strategy to enhance trade-off control during the noise prediction phase. A visual prompt injection module replaces text prompts with visual tokens, improving restoration accuracy and semantic consistency. RCOD outperforms current one-step diffusion methods in both quantitative metrics and visual qualities, offering flexible realism control in the inference stage. The method achieves superior fidelity and perceptual quality while maintaining computational efficiency. The code for RCOD will be released for further research and application. <br /> <div>
arXiv:2509.10122v1 Announce Type: new 
Abstract: Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment</title>
<link>https://arxiv.org/abs/2509.10134</link>
<guid>https://arxiv.org/abs/2509.10134</guid>
<content:encoded><![CDATA[
<div> domain adaptation, optic disc, segmentation, glaucoma, fundus imaging

Summary:<br />
- The accurate segmentation of the optic disc and cup is crucial for diagnosing eye diseases like glaucoma.
- Existing segmentation models struggle when applied to new datasets with different imaging conditions.
- The proposed Grad-CL framework addresses this issue by adapting segmentation performance without needing access to original source data.
- Grad-CL combines gradient-based pseudolabel refinement and cosine similarity-based contrastive learning to improve segmentation accuracy and boundary delineation.
- Extensive experiments show that Grad-CL outperforms current unsupervised and source-free domain adaptation methods, achieving superior results on cross-domain fundus imaging datasets. 

Summary: <div>
arXiv:2509.10134v1 Announce Type: new 
Abstract: Accurate segmentation of the optic disc and cup is critical for the early diagnosis and management of ocular diseases such as glaucoma. However, segmentation models trained on one dataset often suffer significant performance degradation when applied to target data acquired under different imaging protocols or conditions. To address this challenge, we propose \textbf{Grad-CL}, a novel source-free domain adaptation framework that leverages a pre-trained source model and unlabeled target data to robustly adapt segmentation performance without requiring access to the original source data. Grad-CL combines a gradient-guided pseudolabel refinement module with a cosine similarity-based contrastive learning strategy. In the first stage, salient class-specific features are extracted via a gradient-based mechanism, enabling more accurate uncertainty quantification and robust prototype estimation for refining noisy pseudolabels. In the second stage, a contrastive loss based on cosine similarity is employed to explicitly enforce inter-class separability between the gradient-informed features of the optic cup and disc. Extensive experiments on challenging cross-domain fundus imaging datasets demonstrate that Grad-CL outperforms state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and improved boundary delineation. Project and code are available at https://visdomlab.github.io/GCL/.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization</title>
<link>https://arxiv.org/abs/2509.10140</link>
<guid>https://arxiv.org/abs/2509.10140</guid>
<content:encoded><![CDATA[
<div> Keywords: Vector quantization, VQBridge, codebook usage, image generation, autoregressive models 

Summary:
In this study, the researchers address the challenges associated with training vector quantization (VQ) for image generation. They introduce VQBridge, a method that ensures high codebook usage during learning annealing and codebook size expansion. Through a compress-process-recover pipeline, VQBridge optimizes code vectors, leading to stable and effective training of the codebook. The proposed method, termed FVQ (FullVQ), achieves 100% codebook usage across various configurations and demonstrates state-of-the-art reconstruction performance. It is scalable, generalizable, and effective with larger codebooks, higher vector channels, and longer training. Integration of FVQ with LlamaGen enhances image generation performance, outperforming visual autoregressive and diffusion models. This highlights the importance of high-quality tokenizers for robust autoregressive image generation. <br /><br />Summary: <div>
arXiv:2509.10140v1 Announce Type: new 
Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
<link>https://arxiv.org/abs/2509.10156</link>
<guid>https://arxiv.org/abs/2509.10156</guid>
<content:encoded><![CDATA[
<div> progressive layer freezing, self-supervised learning, visual representation, latent prediction, masked-autoencoding <br />
<br />
LayerLock is a novel approach for self-supervised visual representation learning that involves progressively freezing layers in a model during training. By exploiting the observation that shallower layers in video masked-autoencoding (MAE) models converge earlier than deeper layers, LayerLock accelerates training and avoids representation collapse. This approach allows for efficient latent prediction in large models up to 4 billion parameters, outperforming non-latent masked prediction on the 4DS perception suite. Overall, LayerLock introduces a simple yet effective method for transitioning from pixel to latent prediction in visual representation learning, offering improved performance and scalability. <br /><br />Summary: <div>
arXiv:2509.10156v1 Announce Type: new 
Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from "representation collapse". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints</title>
<link>https://arxiv.org/abs/2509.10241</link>
<guid>https://arxiv.org/abs/2509.10241</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit methods, Explicit methods, Novel View Synthesis, Space-based 3D object reconstruction, Embeddings <br />
Summary: 
This study compares implicit and explicit Novel View Synthesis methods for 3D object reconstruction in space environments, focusing on the role of appearance embeddings. While embeddings improve photometric fidelity, they do not significantly enhance geometric accuracy crucial for space robotics applications. The comparison across K-Planes, Gaussian Splatting, and Convex Splatting on the SPEED+ dataset reveals that embeddings reduce the number of primitives in explicit methods but do not improve geometric fidelity. Convex splatting is found to offer more compact and clutter-free representations than Gaussian splatting, making it advantageous for interaction and collision avoidance tasks in safety-critical applications. This study clarifies the limitations of appearance embeddings in geometry-centric tasks and emphasizes the trade-offs between reconstruction quality and representation efficiency in space scenarios. <br /><br />Summary: <div>
arXiv:2509.10241v1 Announce Type: new 
Abstract: We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2509.10250</link>
<guid>https://arxiv.org/abs/2509.10250</guid>
<content:encoded><![CDATA[
<div> AI-generated images, generative models, image detection, domain bias, semantic alignment<br />
Summary:<br />
The article introduces GAMMA, a novel training framework to detect AI-generated images by reducing domain bias and enhancing semantic alignment. By employing diverse manipulation strategies and multi-task supervision with dual segmentation heads and a classification head, GAMMA ensures consistency between manipulated and authentic content. A reverse cross-attention mechanism is introduced to correct biased representations in the classification branch. The method achieves state-of-the-art generalization performance on the GenImage benchmark, improving accuracy by 5.8%. Additionally, GAMMA maintains strong robustness on newly released generative models like GPT-40. <div>
arXiv:2509.10250v1 Announce Type: new 
Abstract: With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI</title>
<link>https://arxiv.org/abs/2509.10257</link>
<guid>https://arxiv.org/abs/2509.10257</guid>
<content:encoded><![CDATA[
<div> Keywords: fetal brain MRI, super-resolution reconstruction, ventriculomegaly, volumetric analysis, diagnostic classification

Summary:
In this study, three state-of-the-art super-resolution reconstruction (SRR) methods were applied to fetal brain MRI scans to generate high-resolution 3D volumes. The SRR methods, including NiftyMIC, SVRTK, and NeSVoR, were evaluated on both healthy controls and pathological cases with ventriculomegaly. NeSVoR demonstrated the highest reconstruction success rate, exceeding 90% across both groups. While significant differences were observed in volumetric estimates between the SRR methods, the diagnostic performance for ventriculomegaly was not affected by the choice of reconstruction method. The study highlights the robustness of the NeSVoR method and its ability to maintain diagnostic performance despite variations in volumetric measurements induced by SRR. The findings suggest the potential of SRR techniques in improving fetal brain MRI quality and diagnostic accuracy. 

<br /><br /> <div>
arXiv:2509.10257v1 Announce Type: new 
Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask Consistency Regularization in Object Removal</title>
<link>https://arxiv.org/abs/2509.10259</link>
<guid>https://arxiv.org/abs/2509.10259</guid>
<content:encoded><![CDATA[
<div> Regularization, object removal, mask hallucination, mask-shape bias, inpainting <br />
Summary:<br />
The article proposes a novel training strategy called Mask Consistency Regularization (MCR) to address challenges in object removal tasks. Current methods struggle with mask hallucination and mask-shape bias, leading to inaccurate inpainting results. MCR introduces mask perturbations, such as dilation and reshape, to enforce consistency between the model's output and the original mask. Dilation aligns the inpainted content with the surrounding context, while reshaping encourages the model to break away from mask-shape bias. Through these strategies, MCR significantly reduces hallucinations and biases, resulting in more contextually coherent object removal. Experimental results support the effectiveness of MCR in improving performance in image inpainting tasks. <br /> <div>
arXiv:2509.10259v1 Announce Type: new 
Abstract: Object removal, a challenging task within image inpainting, involves seamlessly filling the removed region with content that matches the surrounding context. Despite advancements in diffusion models, current methods still face two critical challenges. The first is mask hallucination, where the model generates irrelevant or spurious content inside the masked region, and the second is mask-shape bias, where the model fills the masked area with an object that mimics the mask's shape rather than surrounding content. To address these issues, we propose Mask Consistency Regularization (MCR), a novel training strategy designed specifically for object removal tasks. During training, our approach introduces two mask perturbations: dilation and reshape, enforcing consistency between the outputs of these perturbed branches and the original mask. The dilated masks help align the model's output with the surrounding content, while reshaped masks encourage the model to break the mask-shape bias. This combination of strategies enables MCR to produce more robust and contextually coherent inpainting results. Our experiments demonstrate that MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.10260</link>
<guid>https://arxiv.org/abs/2509.10260</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-image generation, Artifacts assessment, MagicMirror framework, Vision-Language Model, Benchmark evaluation

Summary:
The article introduces the MagicMirror framework, which addresses the challenge of physical artifacts in text-to-image generation. It provides a detailed taxonomy of image artifacts and creates a human-annotated dataset, MagicData340K, for fine-grained artifact labeling. A Vision-Language Model called MagicAssessor is developed to assess artifacts in generated images, using a multi-level reward system and data sampling strategy to overcome challenges. The framework also includes MagicBench, an automated benchmark for evaluating artifact reduction in current T2I models. Evaluation with MagicBench reveals that even top-tier models like GPT-image-1 exhibit significant artifacts, emphasizing the need to prioritize artifact reduction in future T2I development. The MagicMirror project page offers more information on the framework. 

<br /><br />Summary: <div>
arXiv:2509.10260v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</title>
<link>https://arxiv.org/abs/2509.10266</link>
<guid>https://arxiv.org/abs/2509.10266</guid>
<content:encoded><![CDATA[
<div> Keywords: Sign language translation, non-manual cues, SignClip, hierarchical contrastive learning, semantic consistency <br />
Summary:<br />
SignClip is a novel framework for improving sign language translation accuracy by incorporating both manual and non-manual cues, such as lip movement features. It utilizes a hierarchical contrastive learning framework with multi-level alignment objectives to ensure semantic consistency across sign-lip and visual-text modalities. Experimental results on benchmark datasets PHOENIX14T and How2Sign demonstrate SignClip's superior performance compared to previous models. For instance, on PHOENIX14T, SignClip outperforms the state-of-the-art model SpaMo in the Gloss-free setting, with improvements in BLEU-4 from 24.32 to 24.71 and ROUGE from 46.57 to 48.38. This highlights the importance of considering non-manual cues in sign language translation and the effectiveness of the SignClip framework in enhancing translation accuracy. <br /> <div>
arXiv:2509.10266v1 Announce Type: new 
Abstract: Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Text Manipulation in Images using Vision Language Models</title>
<link>https://arxiv.org/abs/2509.10278</link>
<guid>https://arxiv.org/abs/2509.10278</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision Language Models, text manipulation detection, open-source models, closed-source models, image manipulation detection

Summary:
Large Vision Language Models (LVLMs) have been effectively used for image manipulation detection, but their application in text manipulation detection has been lacking. This study aims to fill this gap by analyzing both closed-source models like GPT-4o and open-source models on text manipulation datasets. The results indicate that open-source models are progressing but still lag behind closed-source models. The study also benchmarks image manipulation detection-specific LVLMs for text manipulation detection, revealing a generalization issue. Furthermore, the research evaluates the performance of LVLMs on manipulations of in-the-wild scene texts and fantasy ID cards, showcasing the challenges posed by the latter as they mimic real-world misuse.<br /><br />Summary: <div>
arXiv:2509.10278v1 Announce Type: new 
Abstract: Recent works have shown the effectiveness of Large Vision Language Models (VLMs or LVLMs) in image manipulation detection. However, text manipulation detection is largely missing in these studies. We bridge this knowledge gap by analyzing closed- and open-source VLMs on different text manipulation datasets. Our results suggest that open-source models are getting closer, but still behind closed-source ones like GPT- 4o. Additionally, we benchmark image manipulation detection-specific VLMs for text manipulation detection and show that they suffer from the generalization problem. We benchmark VLMs for manipulations done on in-the-wild scene texts and on fantasy ID cards, where the latter mimic a challenging real-world misuse.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.10282</link>
<guid>https://arxiv.org/abs/2509.10282</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot anomaly detection, 3D objects, Multimodal collaboration learning, RGB images, Text semantics 

Summary:
The paper introduces a novel framework, MCL-AD, for Zero-shot 3D anomaly detection. It leverages multimodal collaboration learning across point clouds, RGB images, and text semantics to achieve superior anomaly detection. The framework includes a Multimodal Prompt Learning Mechanism (MPLM) that enhances representation capability and collaborative learning through object-agnostic decoupled text prompts and multimodal contrastive loss. Additionally, a Collaborative Modulation Mechanism (CMM) is proposed to maximize the use of complementary representations in point clouds and RGB images. Experimental results demonstrate that the MCL-AD framework outperforms existing methods in Zero-shot 3D anomaly detection. <br /><br />Summary: <div>
arXiv:2509.10282v1 Announce Type: new 
Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects without relying on labeled training data, making it especially valuable in scenarios constrained by data scarcity, privacy, or high annotation cost. However, most existing methods focus exclusively on point clouds, neglecting the rich semantic cues available from complementary modalities such as RGB images and texts priors. This paper introduces MCL-AD, a novel framework that leverages multimodal collaboration learning across point clouds, RGB images, and texts semantics to achieve superior zero-shot 3D anomaly detection. Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that enhances the intra-modal representation capability and inter-modal collaborative learning by introducing an object-agnostic decoupled text prompt and a multimodal contrastive loss. In addition, a collaborative modulation mechanism (CMM) is proposed to fully leverage the complementary representations of point clouds and RGB images by jointly modulating the RGB image-guided and point cloud-guided branches. Extensive experiments demonstrate that the proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D anomaly detection.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks</title>
<link>https://arxiv.org/abs/2509.10298</link>
<guid>https://arxiv.org/abs/2509.10298</guid>
<content:encoded><![CDATA[
arXiv:2509.10298v1 Announce Type: new 
Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art performance in computer vision but are highly vulnerable to adversarial perturbations. Standard defenses often incur high computational cost or lack formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath) method, where drop probabilities increase with depth to control the effective Lipschitz constant of the network. This approach regularizes deeper layers, improving robustness while preserving clean accuracy and reducing computation. Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent schedule maintains near-baseline clean accuracy, enhances robustness under FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to baseline and linear DropPath schedules.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments</title>
<link>https://arxiv.org/abs/2509.10310</link>
<guid>https://arxiv.org/abs/2509.10310</guid>
<content:encoded><![CDATA[
arXiv:2509.10310v1 Announce Type: new 
Abstract: In this paper we address the problem of precise geolocation of street furniture in complex urban environments, which is a critical task for effective monitoring and maintenance of public infrastructure by local authorities and private stakeholders. To this end, we propose a probabilistic framework based on energy maps that encode the spatial likelihood of object locations. Representing the energy in a map-based geopositioned format allows the optimisation process to seamlessly integrate external geospatial information, such as GIS layers, road maps, or placement constraints, which improves contextual awareness and localisation accuracy. A stochastic birth-and-death optimisation algorithm is introduced to infer the most probable configuration of assets. We evaluate our approach using a realistic simulation informed by a geolocated dataset of street lighting infrastructure in Dublin city centre, demonstrating its potential for scalable and accurate urban asset mapping. The implementation of the algorithm will be made available in the GitHub repository https://github.com/EMurphy0108/SBD_Street_Furniture.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</title>
<link>https://arxiv.org/abs/2509.10312</link>
<guid>https://arxiv.org/abs/2509.10312</guid>
<content:encoded><![CDATA[
arXiv:2509.10312v1 Announce Type: new 
Abstract: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.10334</link>
<guid>https://arxiv.org/abs/2509.10334</guid>
<content:encoded><![CDATA[
arXiv:2509.10334v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effective strategy to improve efficiency, but ViT-based segmentation models are notoriously fragile under low precision, as quantization errors accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the first fully integer-only ViT segmentation framework. Building on the Segmenter architecture, I-Segmenter systematically replaces floating-point operations with integer-only counterparts. To further stabilize both training and inference, we propose $\lambda$-ShiftGELU, a novel activation function that mitigates the limitations of uniform quantization in handling long-tailed activation distributions. In addition, we remove the L2 normalization layer and replace bilinear interpolation in the decoder with nearest neighbor upsampling, ensuring integer-only execution throughout the computational graph. Extensive experiments show that I-Segmenter achieves accuracy within a reasonable margin of its FP32 baseline (5.1 % on average), while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably, even in one-shot PTQ with a single calibration image, I-Segmenter delivers competitive accuracy, underscoring its practicality for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT</title>
<link>https://arxiv.org/abs/2509.10341</link>
<guid>https://arxiv.org/abs/2509.10341</guid>
<content:encoded><![CDATA[
arXiv:2509.10341v1 Announce Type: new 
Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography</title>
<link>https://arxiv.org/abs/2509.10344</link>
<guid>https://arxiv.org/abs/2509.10344</guid>
<content:encoded><![CDATA[
arXiv:2509.10344v1 Announce Type: new 
Abstract: Mammography screening is an essential tool for early detection of breast cancer. The speed and accuracy of mammography interpretation have the potential to be improved with deep learning methods. However, the development of a foundation visual language model (VLM) is hindered by limited data and domain differences between natural and medical images. Existing mammography VLMs, adapted from natural images, often ignore domain-specific characteristics, such as multi-view relationships in mammography. Unlike radiologists who analyze both views together to process ipsilateral correspondence, current methods treat them as independent images or do not properly model the multi-view correspondence learning, losing critical geometric context and resulting in suboptimal prediction. We propose GLAM: Global and Local Alignment for Multi-view mammography for VLM pretraining using geometry guidance. By leveraging the prior knowledge about the multi-view imaging process of mammograms, our model learns local cross-view alignments and fine-grained local features through joint global and local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED [14], one of the largest open mammography datasets, our model outperforms baselines across multiple datasets under different settings.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Visual Grounding in Visual Language Models</title>
<link>https://arxiv.org/abs/2509.10345</link>
<guid>https://arxiv.org/abs/2509.10345</guid>
<content:encoded><![CDATA[
arXiv:2509.10345v1 Announce Type: new 
Abstract: Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immunizing Images from Text to Image Editing via Adversarial Cross-Attention</title>
<link>https://arxiv.org/abs/2509.10359</link>
<guid>https://arxiv.org/abs/2509.10359</guid>
<content:encoded><![CDATA[
arXiv:2509.10359v1 Announce Type: new 
Abstract: Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learned Image Compression Through Knowledge Distillation</title>
<link>https://arxiv.org/abs/2509.10366</link>
<guid>https://arxiv.org/abs/2509.10366</guid>
<content:encoded><![CDATA[
arXiv:2509.10366v1 Announce Type: new 
Abstract: Learned image compression sits at the intersection of machine learning and image processing. With advances in deep learning, neural network-based compression methods have emerged. In this process, an encoder maps the image to a low-dimensional latent space, which is then quantized, entropy-coded into a binary bitstream, and transmitted to the receiver. At the receiver end, the bitstream is entropy-decoded, and a decoder reconstructs an approximation of the original image. Recent research suggests that these models consistently outperform conventional codecs. However, they require significant processing power, making them unsuitable for real-time use on resource-constrained platforms, which hinders their deployment in mainstream applications. This study aims to reduce the resource requirements of neural networks used for image compression by leveraging knowledge distillation, a training paradigm where smaller neural networks, partially trained on the outputs of larger, more complex models, can achieve better performance than when trained independently. Our work demonstrates that knowledge distillation can be effectively applied to image compression tasks: i) across various architecture sizes, ii) to achieve different image quality/bit rate tradeoffs, and iii) to save processing and energy resources. This approach introduces new settings and hyperparameters, and future research could explore the impact of different teacher models, as well as alternative loss functions. Knowledge distillation could also be extended to transformer-based models. The code is publicly available at: https://github.com/FABallemand/PRIM .
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition</title>
<link>https://arxiv.org/abs/2509.10388</link>
<guid>https://arxiv.org/abs/2509.10388</guid>
<content:encoded><![CDATA[
arXiv:2509.10388v1 Announce Type: new 
Abstract: Decomposing an image into its intrinsic photometric factors--shading and reflectance--is a long-standing challenge due to the lack of extensive ground-truth data for real-world scenes. Recent methods rely on synthetic data or sparse annotations for limited indoor and even fewer outdoor scenes. We introduce a novel training-free approach for intrinsic image decomposition using only a pair of visible and thermal images. We leverage the principle that light not reflected from an opaque surface is absorbed and detected as heat by a thermal camera. This allows us to relate the ordinalities between visible and thermal image intensities to the ordinalities of shading and reflectance, which can densely self-supervise an optimizing neural network to recover shading and reflectance. We perform quantitative evaluations with known reflectance and shading under natural and artificial lighting, and qualitative experiments across diverse outdoor scenes. The results demonstrate superior performance over recent learning-based models and point toward a scalable path to curating real-world ordinal supervision, previously infeasible via manual labeling.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards</title>
<link>https://arxiv.org/abs/2509.10407</link>
<guid>https://arxiv.org/abs/2509.10407</guid>
<content:encoded><![CDATA[
arXiv:2509.10407v1 Announce Type: new 
Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC. While deep learning based CVQE has driven significant progress, existing surveys still suffer from limitations: lack of systematic classification linking methods to specific standards and artifacts, insufficient comparative analysis of architectural paradigms across coding types, and underdeveloped benchmarking practices. To address these gaps, this paper presents three key contributions. First, it introduces a novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization. Second, it proposes a unified benchmarking framework integrating modern compression protocols and standard test sequences for fair multi-criteria evaluation. Third, it provides a systematic analysis of the critical trade-offs between reconstruction performance and computational complexity observed in state-of-the-art methods and highlighting promising directions for future research. This comprehensive review aims to establish a foundation for consistent assessment and informed model selection in CVQE research and deployment.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal SAM-adapter for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.10408</link>
<guid>https://arxiv.org/abs/2509.10408</guid>
<content:encoded><![CDATA[
arXiv:2509.10408v1 Announce Type: new 
Abstract: Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis</title>
<link>https://arxiv.org/abs/2509.10441</link>
<guid>https://arxiv.org/abs/2509.10441</guid>
<content:encoded><![CDATA[
arXiv:2509.10441v1 Announce Type: new 
Abstract: Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</title>
<link>https://arxiv.org/abs/2509.10453</link>
<guid>https://arxiv.org/abs/2509.10453</guid>
<content:encoded><![CDATA[
arXiv:2509.10453v1 Announce Type: new 
Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes memory loss and cognitive decline. While there has been extensive research in applying deep learning models to Alzheimer's prediction tasks, these models remain limited by lack of available labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals between scans. In this study, we adapt three state-of-the-art temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis, and add novel extensions designed to handle variable-length inputs and learn robust spatial features. We aggregate four publicly available datasets comprising 3,161 patients for pre-training, and show the performance of our model across multiple Alzheimer's prediction tasks including diagnosis classification, conversion detection, and future conversion prediction. Importantly, our SSL model implemented with temporal order prediction and contrastive learning outperforms supervised learning on six out of seven downstream tasks. It demonstrates adaptability and generalizability across tasks and number of input images with varying time intervals, highlighting its capacity for robust performance across clinical applications. We release our code and model publicly at https://github.com/emilykaczmarek/SSL-AD.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining</title>
<link>https://arxiv.org/abs/2509.09880</link>
<guid>https://arxiv.org/abs/2509.09880</guid>
<content:encoded><![CDATA[
arXiv:2509.09880v1 Announce Type: cross 
Abstract: Diffusion/score-based models have recently emerged as powerful generative priors for solving inverse problems, including accelerated MRI reconstruction. While their flexibility allows decoupling the measurement model from the learned prior, their performance heavily depends on carefully tuned data fidelity weights, especially under fast sampling schedules with few denoising steps. Existing approaches often rely on heuristics or fixed weights, which fail to generalize across varying measurement conditions and irregular timestep schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method that adaptively tunes fidelity weights across arbitrary noise schedules without requiring retraining of the diffusion prior. ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods, showcasing its ability to deliver high-fidelity reconstructions across varying noise schedules and acquisition settings.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios</title>
<link>https://arxiv.org/abs/2509.09926</link>
<guid>https://arxiv.org/abs/2509.09926</guid>
<content:encoded><![CDATA[
arXiv:2509.09926v1 Announce Type: cross 
Abstract: Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\% of the unlabeled data compared with previous works.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images</title>
<link>https://arxiv.org/abs/2509.09952</link>
<guid>https://arxiv.org/abs/2509.09952</guid>
<content:encoded><![CDATA[
arXiv:2509.09952v1 Announce Type: cross 
Abstract: Material creation and reconstruction are crucial for appearance modeling but traditionally require significant time and expertise from artists. While recent methods leverage visual foundation models to synthesize PBR materials from user-provided inputs, they often fall short in quality, flexibility, and user control. We propose a novel two-stage generate-and-estimate framework for PBR material generation. In the generation stage, a fine-tuned diffusion model synthesizes shaded, tileable texture images aligned with user input. In the estimation stage, we introduce a chained decomposition scheme that sequentially predicts SVBRDF channels by passing previously extracted representation as input into a single-step image-conditional diffusion model. Our method is efficient, high quality, and enables flexible user control. We evaluate our approach against existing material generation and estimation methods, demonstrating superior performance. Our material estimation method shows strong robustness on both generated textures and in-the-wild photographs. Furthermore, we highlight the flexibility of our framework across diverse applications, including text-to-material, image-to-material, structure-guided generation, and material editing.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge</title>
<link>https://arxiv.org/abs/2509.09955</link>
<guid>https://arxiv.org/abs/2509.09955</guid>
<content:encoded><![CDATA[
arXiv:2509.09955v1 Announce Type: cross 
Abstract: Large-scale transformers are central to modern semantic communication, yet their high computational and communication costs hinder deployment on resource-constrained edge devices. This paper introduces a training-free framework for adaptive token merging, a novel mechanism that compresses transformer representations at runtime by selectively merging semantically redundant tokens under per-layer similarity thresholds. Unlike prior fixed-ratio reduction, our approach couples merging directly to input redundancy, enabling data-dependent adaptation that balances efficiency and task relevance without retraining. We cast the discovery of merging strategies as a multi-objective optimization problem and leverage Bayesian optimization to obtain Pareto-optimal trade-offs between accuracy, inference cost, and communication cost. On ImageNet classification, we match the accuracy of the unmodified transformer with 30\% fewer floating-point operations per second and under 20\% of the original communication cost, while for visual question answering our method achieves performance competitive with the full LLaVA model at less than one-third of the compute and one-tenth of the bandwidth. Finally, we show that our adaptive merging is robust across varying channel conditions and provides inherent privacy benefits, substantially degrading the efficacy of model inversion attacks. Our framework provides a practical and versatile solution for deploying powerful transformer models in resource-limited edge intelligence scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms</title>
<link>https://arxiv.org/abs/2509.09972</link>
<guid>https://arxiv.org/abs/2509.09972</guid>
<content:encoded><![CDATA[
arXiv:2509.09972v1 Announce Type: cross 
Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche ramosa) to California's tomato industry, which supplies over 90 percent of U.S. processing tomatoes. The parasite's largely underground life cycle makes early detection difficult, while conventional chemical controls are costly, environmentally harmful, and often ineffective. To address this, we combined drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. Research was conducted on a known broomrape-infested tomato farm in Woodland, Yolo County, CA, across five key growth stages determined by growing degree days (GDD). Multispectral images were processed to isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with 79.09 percent overall accuracy and 70.36 percent recall without integrating later stages. Incorporating sequential growth stages with LSTM improved detection substantially. The best-performing scenario, which integrated all growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy and 95.37 percent recall. These results demonstrate the strong potential of temporal multispectral analysis and LSTM networks for early broomrape detection. While further real-world data collection is needed for practical deployment, this study shows that UAV-based multispectral sensing coupled with deep learning could provide a powerful precision agriculture tool to reduce losses and improve sustainability in tomato production.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</title>
<link>https://arxiv.org/abs/2509.10096</link>
<guid>https://arxiv.org/abs/2509.10096</guid>
<content:encoded><![CDATA[
arXiv:2509.10096v1 Announce Type: cross 
Abstract: The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polarization Denoising and Demosaicking: Dataset and Baseline Method</title>
<link>https://arxiv.org/abs/2509.10098</link>
<guid>https://arxiv.org/abs/2509.10098</guid>
<content:encoded><![CDATA[
arXiv:2509.10098v1 Announce Type: cross 
Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images with multiple polarization orientations in one shot and thus it is valuable for many applications using polarimetric information. The image processing pipeline for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking. While polarization demosaicking for a noise-free case has increasingly been studied, the research for the joint task of polarization denoising and demosaicking is scarce due to the lack of a suitable evaluation dataset and a solid baseline method. In this paper, we propose a novel dataset and method for polarization denoising and demosaicking. Our dataset contains 40 real-world scenes and three noise-level conditions, consisting of pairs of noisy mosaic inputs and noise-free full images. Our method takes a denoising-then-demosaicking approach based on well-accepted signal processing components to offer a reproducible method. Experimental results demonstrate that our method exhibits higher image reconstruction performance than other alternative methods, offering a solid baseline.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-pathology Chest X-ray Classification with Rejection Mechanisms</title>
<link>https://arxiv.org/abs/2509.10348</link>
<guid>https://arxiv.org/abs/2509.10348</guid>
<content:encoded><![CDATA[
arXiv:2509.10348v1 Announce Type: cross 
Abstract: Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2509.10454</link>
<guid>https://arxiv.org/abs/2509.10454</guid>
<content:encoded><![CDATA[
arXiv:2509.10454v1 Announce Type: cross 
Abstract: In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Weighting Game: Evaluating Quality of Explainability Methods</title>
<link>https://arxiv.org/abs/2208.06175</link>
<guid>https://arxiv.org/abs/2208.06175</guid>
<content:encoded><![CDATA[
arXiv:2208.06175v2 Announce Type: replace 
Abstract: The objective of this paper is to assess the quality of explanation heatmaps for image classification tasks. To assess the quality of explainability methods, we approach the task through the lens of accuracy and stability.
  In this work, we make the following contributions. Firstly, we introduce the Weighting Game, which measures how much of a class-guided explanation is contained within the correct class' segmentation mask. Secondly, we introduce a metric for explanation stability, using zooming/panning transformations to measure differences between saliency maps with similar contents.
  Quantitative experiments are produced, using these new metrics, to evaluate the quality of explanations provided by commonly used CAM methods. The quality of explanations is also contrasted between different model architectures, with findings highlighting the need to consider model architecture when choosing an explainability method.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition</title>
<link>https://arxiv.org/abs/2301.02560</link>
<guid>https://arxiv.org/abs/2301.02560</guid>
<content:encoded><![CDATA[
arXiv:2301.02560v4 Announce Type: replace 
Abstract: Current dataset collection methods typically scrape large amounts of data from the web. While this technique is extremely scalable, data collected in this way tends to reinforce stereotypical biases, can contain personally identifiable information, and typically originates from Europe and North America. In this work, we rethink the dataset collection paradigm and introduce GeoDE, a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, with no personally identifiable information, collected by soliciting images from people around the world. We analyse GeoDE to understand differences in images collected in this manner compared to web-scraping. We demonstrate its use as both an evaluation and training dataset, allowing us to highlight and begin to mitigate the shortcomings in current models, despite GeoDE's relatively small size. We release the full dataset and code at https://geodiverse-data-collection.cs.princeton.edu
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential</title>
<link>https://arxiv.org/abs/2304.11631</link>
<guid>https://arxiv.org/abs/2304.11631</guid>
<content:encoded><![CDATA[
arXiv:2304.11631v2 Announce Type: replace 
Abstract: Skeleton-based action recognition has achieved remarkable results in human action recognition with the development of graph convolutional networks (GCNs). However, the recent works tend to construct complex learning mechanisms with redundant training and exist a bottleneck for long time-series. To solve these problems, we propose the Temporal-Spatio Graph ConvNeXt (TSGCNeXt) to explore efficient learning mechanism of long temporal skeleton sequences. Firstly, a new graph learning mechanism with simple structure, Dynamic-Static Separate Multi-graph Convolution (DS-SMG) is proposed to aggregate features of multiple independent topological graphs and avoid the node information being ignored during dynamic convolution. Next, we construct a graph convolution training acceleration mechanism to optimize the back-propagation computing of dynamic graph learning with 55.08\% speed-up. Finally, the TSGCNeXt restructure the overall structure of GCN with three Spatio-temporal learning modules,efficiently modeling long temporal features. In comparison with existing previous methods on large-scale datasets NTU RGB+D 60 and 120, TSGCNeXt outperforms on single-stream networks. In addition, with the ema model introduced into the multi-stream fusion, TSGCNeXt achieves SOTA levels. On the cross-subject and cross-set of the NTU 120, accuracies reach 90.22% and 91.74%.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Evaluators: Towards Human-aligned Metrics for Missing Markers Reconstruction</title>
<link>https://arxiv.org/abs/2410.14334</link>
<guid>https://arxiv.org/abs/2410.14334</guid>
<content:encoded><![CDATA[
arXiv:2410.14334v4 Announce Type: replace 
Abstract: Animation data is often obtained through optical motion capture systems, which utilize a multitude of cameras to establish the position of optical markers. However, system errors or occlusions can result in missing markers, the manual cleaning of which can be time-consuming. This has sparked interest in machine learning-based solutions for missing marker reconstruction in the academic community. Most academic papers utilize a simplistic mean square error as the main metric. In this paper, we show that this metric does not correlate with subjective perception of the fill quality. Additionally, we introduce and evaluate a set of better-correlated metrics that can drive progress in the field.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Image is Secretly the Last Frame of a Pseudo Video</title>
<link>https://arxiv.org/abs/2410.20158</link>
<guid>https://arxiv.org/abs/2410.20158</guid>
<content:encoded><![CDATA[
arXiv:2410.20158v3 Announce Type: replace 
Abstract: Diffusion models, which can be viewed as a special case of hierarchical variational autoencoders (HVAEs), have shown profound success in generating photo-realistic images. In contrast, standard HVAEs often produce images of inferior quality compared to diffusion models. In this paper, we hypothesize that the success of diffusion models can be partly attributed to the additional self-supervision information for their intermediate latent states provided by corrupted images, which along with the original image form a pseudo video. Based on this hypothesis, we explore the possibility of improving other types of generative models with such pseudo videos. Specifically, we first extend a given image generative model to their video generative model counterpart, and then train the video generative model on pseudo videos constructed by applying data augmentation to the original images. Furthermore, we analyze the potential issues of first-order Markov data augmentation methods, which are typically used in diffusion models, and propose to use more expressive data augmentation to construct more useful information in pseudo videos. Our empirical results on the CIFAR10 and CelebA datasets demonstrate that improved image generation quality can be achieved with additional self-supervised information from pseudo videos.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models</title>
<link>https://arxiv.org/abs/2410.21471</link>
<guid>https://arxiv.org/abs/2410.21471</guid>
<content:encoded><![CDATA[
arXiv:2410.21471v3 Announce Type: replace 
Abstract: Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoFi: Vision-Aided Label Generator for Wi-Fi Localization and Tracking</title>
<link>https://arxiv.org/abs/2412.05074</link>
<guid>https://arxiv.org/abs/2412.05074</guid>
<content:encoded><![CDATA[
arXiv:2412.05074v4 Announce Type: replace 
Abstract: Data-driven Wi-Fi localization and tracking have shown great promise due to their lower reliance on specialized hardware compared to model-based methods. However, most existing data collection techniques provide only coarse-grained ground truth or a limited number of labeled points, significantly hindering the advancement of data-driven approaches. While systems like lidar can deliver precise ground truth, their high costs make them inaccessible to many users. To address these challenges, we propose LoFi, a vision-aided label generator for Wi-Fi localization and tracking. LoFi can generate ground truth position coordinates solely from 2D images, offering high precision, low cost, and ease of use. Utilizing our method, we have compiled a Wi-Fi tracking and localization dataset using the ESP32-S3 and a webcam. The code and dataset of this paper are available at https://github.com/RS2002/LoFi.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoPD: Mixture-of-Prompts Distillation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2412.19087</link>
<guid>https://arxiv.org/abs/2412.19087</guid>
<content:encoded><![CDATA[
arXiv:2412.19087v2 Announce Type: replace 
Abstract: Soft prompt learning methods are effective for adapting vision-language models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a tendency of existing methods that they overfit seen classes and exhibit degraded performance on unseen classes. This limitation is due to the inherent bias in the training data towards the seen classes. To address this issue, we propose a novel soft prompt learning method, named Mixture-of-Prompts Distillation (MoPD), which can effectively transfer useful knowledge from hard prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft prompt (a.k.a. student prompt), thereby enhancing the generalization ability of soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a gating network that learns to select hard prompts used for prompt distillation. Extensive experiments demonstrate that the proposed MoPD method outperforms state-of-the-art baselines especially on on unseen classes.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</title>
<link>https://arxiv.org/abs/2502.13818</link>
<guid>https://arxiv.org/abs/2502.13818</guid>
<content:encoded><![CDATA[
arXiv:2502.13818v4 Announce Type: replace 
Abstract: Estimating the construction year of buildings is critical for advancing sustainability, as older structures often lack energy-efficient features. Sustainable urban planning relies on accurate building age data to reduce energy consumption and mitigate climate change. In this work, we introduce MapYourCity, a novel multi-modal benchmark dataset comprising top-view Very High Resolution (VHR) imagery, multi-spectral Earth Observation (EO) data from the Copernicus Sentinel-2 satellite constellation, and co-localized street-view images across various European cities. Each building is labeled with its construction epoch, and the task is formulated as a seven-class classification problem covering periods from 1900 to the present. To advance research in EO generalization and multi-modal learning, we organized a community-driven data challenge in 2024, hosted by ESA $\Phi$-lab, which ran for four months and attracted wide participation.
  This paper presents the Top-4 performing models from the challenge and their evaluation results. We assess model generalization on cities excluded from training to prevent data leakage, and evaluate performance under missing modality scenarios, particularly when street-view data is unavailable. Results demonstrate that building age estimation is both feasible and effective, even in previously unseen cities and when relying solely on top-view satellite imagery (i.e. with VHR and Sentinel-2 images). The MapYourCity dataset thus provides a valuable resource for developing scalable, real-world solutions in sustainable urban analytics.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation</title>
<link>https://arxiv.org/abs/2503.03556</link>
<guid>https://arxiv.org/abs/2503.03556</guid>
<content:encoded><![CDATA[
arXiv:2503.03556v2 Announce Type: replace 
Abstract: Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and Artificial Intelligence (AI). This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive Large Language Models (LLMs) with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 12.1% performance improvement over the best-reported results from non-LLM methods, while also demonstrating a 1.2% enhancement compared to our previous conference paper. Additionally, it maintains a compact 187M parameter size and infers nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?</title>
<link>https://arxiv.org/abs/2503.07890</link>
<guid>https://arxiv.org/abs/2503.07890</guid>
<content:encoded><![CDATA[
arXiv:2503.07890v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily employ objectives like contrastive learning or masked image modeling, owing to their proven success in learning transferable representations. However, generative diffusion models, which demonstrate the potential to capture multi-grained semantics essential for RS tasks during image generation, remain underexplored for discriminative applications. This prompts the question: can generative diffusion models also excel and serve as GFMs with sufficient discriminative power? In this work, we answer this question with SatDiFuser, a framework that transforms a diffusion-based generative geospatial foundation model into a powerful pretraining tool for discriminative RS. By systematically analyzing multi-stage, noise-dependent diffusion features, we develop three fusion strategies to effectively leverage these diverse representations. Extensive experiments on remote sensing benchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving gains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in classification, demonstrating the capacity of diffusion-based generative foundation models to rival or exceed discriminative GFMs. The source code is available at: https://github.com/yurujaja/SatDiFuser.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.08336</link>
<guid>https://arxiv.org/abs/2503.08336</guid>
<content:encoded><![CDATA[
arXiv:2503.08336v2 Announce Type: replace 
Abstract: Embodied outdoor scene understanding forms the foundation for autonomous agents to perceive, analyze, and react to dynamic driving environments. However, existing 3D understanding is predominantly based on 2D Vision-Language Models (VLMs), which collect and process limited scene-aware contexts. In contrast, compared to the 2D planar visual information, point cloud sensors such as LiDAR provide rich depth and fine-grained 3D representations of objects. Even better the emerging 4D millimeter-wave radar detects the motion trend, velocity, and reflection intensity of each object. The integration of these two modalities provides more flexible querying conditions for natural language, thereby supporting more accurate 3D visual grounding. To this end, we propose a novel method called TPCNet, the first outdoor 3D visual grounding model upon the paradigm of prompt-guided point cloud sensor combination, including both LiDAR and radar sensors. To optimally combine the features of these two sensors required by the prompt, we design a multi-fusion paradigm called Two-Stage Heterogeneous Modal Adaptive Fusion. Specifically, this paradigm initially employs Bidirectional Agent Cross-Attention (BACA), which feeds both-sensor features, characterized by global receptive fields, to the text features for querying. Moreover, we design a Dynamic Gated Graph Fusion (DGGF) module to locate the regions of interest identified by the queries. To further enhance accuracy, we devise an C3D-RECHead, based on the nearest object edge to the ego-vehicle. Experimental results demonstrate that our TPCNet, along with its individual modules, achieves the state-of-the-art performance on both the Talk2Radar and Talk2Car datasets. We release the code at https://github.com/GuanRunwei/TPCNet.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse</title>
<link>https://arxiv.org/abs/2503.16365</link>
<guid>https://arxiv.org/abs/2503.16365</guid>
<content:encoded><![CDATA[
arXiv:2503.16365v2 Announce Type: replace 
Abstract: Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Motion Blending for Versatile Motion Editing</title>
<link>https://arxiv.org/abs/2503.20724</link>
<guid>https://arxiv.org/abs/2503.20724</guid>
<content:encoded><![CDATA[
arXiv:2503.20724v2 Announce Type: replace 
Abstract: Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets, which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models. Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</title>
<link>https://arxiv.org/abs/2504.04191</link>
<guid>https://arxiv.org/abs/2504.04191</guid>
<content:encoded><![CDATA[
arXiv:2504.04191v2 Announce Type: replace 
Abstract: Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedM-VL: What Makes a Good Medical LVLM?</title>
<link>https://arxiv.org/abs/2504.04323</link>
<guid>https://arxiv.org/abs/2504.04323</guid>
<content:encoded><![CDATA[
arXiv:2504.04323v3 Announce Type: replace 
Abstract: Medical image analysis is essential in modern healthcare. Deep learning has redirected research focus toward complex medical multimodal tasks, including report generation and visual question answering. Traditional task-specific models often fall short in handling these challenges. Large vision-language models (LVLMs) offer new solutions for solving such tasks. In this study, we build on the popular LLaVA framework to systematically explore model architectures and training strategies for both 2D and 3D medical LVLMs. We present extensive empirical findings and practical guidance. To support reproducibility and future research, we release a modular codebase, MedM-VL, and two pre-trained models: MedM-VL-2D for 2D medical image analysis and MedM-VL-CT-Chest for 3D CT-based applications. The code is available at: https://github.com/MSIIP/MedM-VL
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Say the Word: Annotation-Free Fine-Grained Object Counting</title>
<link>https://arxiv.org/abs/2504.11705</link>
<guid>https://arxiv.org/abs/2504.11705</guid>
<content:encoded><![CDATA[
arXiv:2504.11705v3 Announce Type: replace 
Abstract: Fine-grained object counting remains a major challenge for class-agnostic counting models, which overcount visually similar but incorrect instances (e.g., jalape\~no vs. poblano). Addressing this by annotating new data and fully retraining the model is time-consuming and does not guarantee generalization to additional novel categories at test time. Instead, we propose an alternative paradigm: Given a category name, tune a compact concept embedding derived from the prompt using synthetic images and pseudo-labels generated by a text-to-image diffusion model. This embedding conditions a specialization module that refines raw overcounts from any frozen counter into accurate, category-specific estimates\textemdash without requiring real images or human annotations. We validate our approach on \textsc{Lookalikes}, a challenging new benchmark containing 1,037 images across 27 fine-grained subcategories, and show substantial improvements over strong baselines. Code will be released upon acceptance. Dataset - https://dalessandro.dev/datasets/lookalikes/
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.16763</link>
<guid>https://arxiv.org/abs/2505.16763</guid>
<content:encoded><![CDATA[
arXiv:2505.16763v2 Announce Type: replace 
Abstract: Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets</title>
<link>https://arxiv.org/abs/2506.14765</link>
<guid>https://arxiv.org/abs/2506.14765</guid>
<content:encoded><![CDATA[
arXiv:2506.14765v2 Announce Type: replace 
Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures.
  We fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image</title>
<link>https://arxiv.org/abs/2506.21152</link>
<guid>https://arxiv.org/abs/2506.21152</guid>
<content:encoded><![CDATA[
arXiv:2506.21152v2 Announce Type: replace 
Abstract: Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To tackle these issues, we present a novel method that seamlessly integrates geometry and perception information without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we incorporate geometry and perception priors to initialize the Gaussian branches and guide their parameter optimization. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we introduce a stable Score Distillation Sampling for fine-grained prior distillation to ensure effective knowledge transfer. The model is further enhanced by a reprojection-based strategy that enforces depth consistency. Experimental results show that we outperform existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title>
<link>https://arxiv.org/abs/2507.01607</link>
<guid>https://arxiv.org/abs/2507.01607</guid>
<content:encoded><![CDATA[
arXiv:2507.01607v3 Announce Type: replace 
Abstract: The widespread deployment of Deep Learning-based Face Recognition Systems raises multiple security concerns. While prior research has identified backdoor vulnerabilities on isolated components, Backdoor Attacks on real-world, unconstrained pipelines remain underexplored. This paper presents the first comprehensive system-level analysis of Backdoor Attacks targeting Face Recognition Systems and provides three contributions. We first show that face feature extractors trained with large margin metric learning losses are susceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15 attack scenarios, we then reveal that a single backdoor can compromise an entire Face Recognition System. Finally, we propose effective best practices and countermeasures for stakeholders.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</title>
<link>https://arxiv.org/abs/2508.05084</link>
<guid>https://arxiv.org/abs/2508.05084</guid>
<content:encoded><![CDATA[
arXiv:2508.05084v2 Announce Type: replace 
Abstract: Pathology foundation models (PFMs) have demonstrated strong representational capabilities through self-supervised pre-training on large-scale, unannotated histopathology image datasets. However, their diverse yet opaque pretraining contexts, shaped by both data-related and structural/training factors, introduce latent biases that hinder generalisability and transparency in downstream applications. In this paper, we propose AdaFusion, a novel prompt-guided inference framework that, to our knowledge, is among the very first to dynamically integrate complementary knowledge from multiple PFMs. Our method compresses and aligns tile-level features from diverse models and employs a lightweight attention mechanism to adaptively fuse them based on tissue phenotype context. We evaluate AdaFusion on three real-world benchmarks spanning treatment response prediction, tumour grading, and spatial gene expression inference. Our approach consistently surpasses individual PFMs across both classification and regression tasks, while offering interpretable insights into each model's biosemantic specialisation. These results highlight AdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced performance and interpretability of model-specific inductive biases.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation</title>
<link>https://arxiv.org/abs/2508.08765</link>
<guid>https://arxiv.org/abs/2508.08765</guid>
<content:encoded><![CDATA[
arXiv:2508.08765v2 Announce Type: replace 
Abstract: The growing presence of AI-generated videos on social networks poses new challenges for deepfake detection, as detectors trained under controlled conditions often fail to generalize to real-world scenarios. A key factor behind this gap is the aggressive, proprietary compression applied by platforms like YouTube and Facebook, which launder low-level forensic cues. However, replicating these transformations at scale is difficult due to API limitations and data-sharing constraints. For these reasons, we propose a first framework that emulates the video sharing pipelines of social networks by estimating compression and resizing parameters from a small set of uploaded videos. These parameters enable a local emulator capable of reproducing platform-specific artifacts on large datasets without direct API access. Experiments on FaceForensics++ videos shared via social networks demonstrate that our emulated data closely matches the degradation patterns of real uploads. Furthermore, detectors fine-tuned on emulated videos achieve comparable performance to those trained on actual shared media. Our approach offers a scalable and practical solution for bridging the gap between lab-based training and real-world deployment of deepfake detectors, particularly in the underexplored domain of compressed video content.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions</title>
<link>https://arxiv.org/abs/2508.16016</link>
<guid>https://arxiv.org/abs/2508.16016</guid>
<content:encoded><![CDATA[
arXiv:2508.16016v2 Announce Type: replace 
Abstract: Recent advancements in computer vision and deep learning have enhanced disaster-response capabilities, particularly in the rapid assessment of earthquake-affected urban environments. Timely identification of accessible entry points and structural obstacles is essential for effective search-and-rescue (SAR) operations. To address this need, we introduce DRespNeT, a high-resolution dataset specifically developed for aerial instance segmentation of post-earthquake structural environments. Unlike existing datasets, which rely heavily on satellite imagery or coarse semantic labeling, DRespNeT provides detailed polygon-level instance segmentation annotations derived from high-definition (1080p) aerial footage captured in disaster zones, including the 2023 Turkiye earthquake and other impacted regions. The dataset comprises 28 operationally critical classes, including structurally compromised buildings, access points such as doors, windows, and gaps, multiple debris levels, rescue personnel, vehicles, and civilian visibility. A distinctive feature of DRespNeT is its fine-grained annotation detail, enabling differentiation between accessible and obstructed areas, thereby improving operational planning and response efficiency. Performance evaluations using YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate significant gains in real-time situational awareness and decision-making. Our optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27 FPS on an RTX-4090 GPU for multi-target detection, meeting real-time operational requirements. The dataset and models support SAR teams and robotic systems, providing a foundation for enhancing human-robot collaboration, streamlining emergency response, and improving survivor outcomes.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection</title>
<link>https://arxiv.org/abs/2508.21135</link>
<guid>https://arxiv.org/abs/2508.21135</guid>
<content:encoded><![CDATA[
arXiv:2508.21135v2 Announce Type: replace 
Abstract: Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and na\"ive fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-Net: Progressive Learning Network for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2110.14484</link>
<guid>https://arxiv.org/abs/2110.14484</guid>
<content:encoded><![CDATA[
arXiv:2110.14484v3 Announce Type: replace-cross 
Abstract: In recent years, deep convolutional neural network-based segmentation methods have achieved state-of-the-art performance for many medical analysis tasks. However, most of these approaches rely on optimizing the U-Net structure or adding new functional modules, which overlooks the complementation and fusion of coarse-grained and fine-grained semantic information. To address these issues, we propose a 2D medical image segmentation framework called Progressive Learning Network (PL-Net), which comprises Internal Progressive Learning (IPL) and External Progressive Learning (EPL). PL-Net offers the following advantages: (1) IPL divides feature extraction into two steps, allowing for the mixing of different size receptive fields and capturing semantic information from coarse to fine granularity without introducing additional parameters; (2) EPL divides the training process into two stages to optimize parameters and facilitate the fusion of coarse-grained information in the first stage and fine-grained information in the second stage. We conducted comprehensive evaluations of our proposed method on five medical image segmentation datasets, and the experimental results demonstrate that PL-Net achieves competitive segmentation performance. It is worth noting that PL-Net does not introduce any additional learnable parameters compared to other U-Net variants.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrative Variational Autoencoders for Generative Modeling of an Image Outcome with Multiple Input Images</title>
<link>https://arxiv.org/abs/2402.02734</link>
<guid>https://arxiv.org/abs/2402.02734</guid>
<content:encoded><![CDATA[
arXiv:2402.02734v2 Announce Type: replace-cross 
Abstract: Understanding relationships across multiple imaging modalities is central to neuroimaging research. We introduce the Integrative Variational Autoencoder (InVA), the first hierarchical VAE framework for image-on-image regression in multimodal neuroimaging. Unlike standard VAEs, which are not designed for predictive integration across modalities, InVA models outcome images as functions of both shared and modality-specific features. This flexible, data-driven approach avoids rigid assumptions of classical tensor regression and outperforms conventional VAEs and nonlinear models such as BART. As a key application, InVA accurately predicts costly PET scans from structural MRI, offering an efficient and powerful tool for multimodal neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework</title>
<link>https://arxiv.org/abs/2408.01284</link>
<guid>https://arxiv.org/abs/2408.01284</guid>
<content:encoded><![CDATA[
arXiv:2408.01284v2 Announce Type: replace-cross 
Abstract: Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside classifiers for seen and unseen classes. This detector determines whether a test feature belongs to seen or unseen classes, followed by classification utilizing separate classifiers for each feature type. We test our framework on three popular audio-visual datasets and observe a significant improvement comparing to existing state-of-the-art works. Codes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation</title>
<link>https://arxiv.org/abs/2411.02992</link>
<guid>https://arxiv.org/abs/2411.02992</guid>
<content:encoded><![CDATA[
arXiv:2411.02992v2 Announce Type: replace-cross 
Abstract: Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the Microlens public benchmark. We release our code at https://github.com/GAIR-Lab/IISAN.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and How Does CLIP Enable Domain and Compositional Generalization?</title>
<link>https://arxiv.org/abs/2502.09507</link>
<guid>https://arxiv.org/abs/2502.09507</guid>
<content:encoded><![CDATA[
arXiv:2502.09507v3 Announce Type: replace-cross 
Abstract: The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orientation Scores should be a Piece of Cake</title>
<link>https://arxiv.org/abs/2504.00702</link>
<guid>https://arxiv.org/abs/2504.00702</guid>
<content:encoded><![CDATA[
arXiv:2504.00702v3 Announce Type: replace-cross 
Abstract: We axiomatically derive a family of wavelets for an orientation score, lifting from position space $\mathbb{R}^2$ to position and orientation space $\mathbb{R}^2\times S^1$, with fast reconstruction property, that minimise position-orientation uncertainty. We subsequently show that these minimum uncertainty states are well-approximated by cake wavelets: for standard parameters, the uncertainty gap of cake wavelets is less than 1.1, and in the limit, we prove the uncertainty gap tends to the minimum of 1. Next, we complete a previous theoretical argument that one does not have to train the lifting layer in (PDE-)G-CNNs, but can instead use cake wavelets. Finally, we show experimentally that in this way we can reduce the network complexity and improve the interpretability of (PDE-)G-CNNs, with only a slight impact on the model's performance.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation</title>
<link>https://arxiv.org/abs/2504.12908</link>
<guid>https://arxiv.org/abs/2504.12908</guid>
<content:encoded><![CDATA[
arXiv:2504.12908v2 Announce Type: replace-cross 
Abstract: Tactile sensing is crucial for achieving human-level robotic capabilities in manipulation tasks. As a promising solution, Vision-Based Tactile Sensors (VBTSs) offer high spatial resolution and cost-effectiveness, but present unique challenges in robotics for their complex physical characteristics and visual signal processing requirements. The lack of efficient and accurate simulation tools for VBTSs has significantly limited the scale and scope of tactile robotics research. We present Taccel, a high-performance simulation platform that integrates IPC and ABD to model robots, tactile sensors, and objects with both accuracy and unprecedented speed, achieving an 18-fold acceleration over real-time across thousands of parallel environments. Unlike previous simulators that operate at sub-real-time speeds with limited parallelization, Taccel provides precise physics simulation and realistic tactile signals while supporting flexible robot-sensor configurations through user-friendly APIs. Through extensive validation in object recognition, robotic grasping, and articulated object manipulation, we demonstrate precise simulation and successful sim-to-real transfer. These capabilities position Taccel as a powerful tool for scaling up tactile robotics research and development, potentially transforming how robots interact with and understand their physical environment.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2505.07879</link>
<guid>https://arxiv.org/abs/2505.07879</guid>
<content:encoded><![CDATA[
arXiv:2505.07879v3 Announce Type: replace-cross 
Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Neuroimaging Biomarkers of Brain Tumor Surgery with AI-Driven Methods</title>
<link>https://arxiv.org/abs/2507.04881</link>
<guid>https://arxiv.org/abs/2507.04881</guid>
<content:encoded><![CDATA[
arXiv:2507.04881v2 Announce Type: replace-cross 
Abstract: Brain tumor resection is a highly complex procedure with profound implications for survival and quality of life. Predicting patient outcomes is crucial to guide clinicians in balancing oncological control with preservation of neurological function. However, building reliable prediction models is severely limited by the rarity of curated datasets that include both pre- and post-surgery imaging, given the clinical, logistical and ethical challenges of collecting such data. In this study, we develop a novel framework that integrates explainable artificial intelligence (XAI) with neuroimaging-based feature engineering for survival assessment in brain tumor patients. We curated structural MRI data from 49 patients scanned pre- and post-surgery, providing a rare resource for identifying survival-related biomarkers. A key methodological contribution is the development of a global explanation optimizer, which refines survival-related feature attribution in deep learning models, thereby improving both the interpretability and reliability of predictions. From a clinical perspective, our findings provide important evidence that survival after oncological surgery is influenced by alterations in regions related to cognitive and sensory functions. These results highlight the importance of preserving areas involved in decision-making and emotional regulation to improve long-term outcomes. From a technical perspective, the proposed optimizer advances beyond state-of-the-art XAI methods by enhancing both the fidelity and comprehensibility of model explanations, thus reinforcing trust in the recognition patterns driving survival prediction. This work demonstrates the utility of XAI-driven neuroimaging analysis in identifying survival-related variability and underscores its potential to inform precision medicine strategies in brain tumor treatment.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework</title>
<link>https://arxiv.org/abs/2508.02521</link>
<guid>https://arxiv.org/abs/2508.02521</guid>
<content:encoded><![CDATA[
arXiv:2508.02521v3 Announce Type: replace-cross 
Abstract: The proliferation of audio deepfakes poses a growing threat to trust in digital communications. While detection methods have advanced, attributing audio deepfakes to their source models remains an underexplored yet crucial challenge. In this paper we introduce LAVA (Layered Architecture for Voice Attribution), a hierarchical framework for audio deepfake detection and model recognition that leverages attention-enhanced latent representations extracted by a convolutional autoencoder trained solely on fake audio. Two specialized classifiers operate on these features: Audio Deepfake Attribution (ADA), which identifies the generation technology, and Audio Deepfake Model Recognition (ADMR), which recognize the specific generative model instance. To improve robustness under open-set conditions, we incorporate confidence-based rejection thresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong performance: the ADA classifier achieves F1-scores over 95% across all datasets, and the ADMR module reaches 96.31% macro F1 across six classes. Additional tests on unseen attacks from ASVpoof2019 LA and error propagation analysis confirm LAVA's robustness and reliability. The framework advances the field by introducing a supervised approach to deepfake attribution and model recognition under open-set conditions, validated on public benchmarks and accompanied by publicly released models and code. Models and code are available at https://www.github.com/adipiz99/lava-framework.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrence Meets Transformers for Universal Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2509.08897</link>
<guid>https://arxiv.org/abs/2509.08897</guid>
<content:encoded><![CDATA[
<div> Proposed Keywords: multimodal retrieval, ReT-2, vision-language models, LSTM-inspired gating mechanisms, state-of-the-art performance

Summary: <br /><br /> In this paper, the researchers introduce ReT-2, a unified retrieval model that supports multimodal queries and searches across multimodal document collections containing both text and images. ReT-2 utilizes multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to integrate information across layers and modalities effectively. The model achieves state-of-the-art performance on challenging benchmarks like M2KR and M-BEIR, demonstrating faster inference and reduced memory usage compared to existing methods. When integrated into retrieval-augmented generation pipelines, ReT-2 enhances downstream performance on datasets like Encyclopedic-VQA and InfoSeek. Overall, ReT-2 offers a versatile solution for complex retrieval tasks in the realm of multimodal learning, showcasing its effectiveness across diverse settings and applications. Source code and trained models for ReT-2 are publicly accessible on GitHub for further exploration and utilization. <br /><br />Summary: <div>
arXiv:2509.08897v1 Announce Type: new 
Abstract: With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Action Recognition Generalizes to Untrained Domains</title>
<link>https://arxiv.org/abs/2509.08908</link>
<guid>https://arxiv.org/abs/2509.08908</guid>
<content:encoded><![CDATA[
<div> Keywords: action recognition, vision diffusion model, transformer, generalization, state-of-the-art <br />
<br />
Summary: 
The study introduces a method for action recognition that mimics human ability to recognize actions across different contexts, viewpoints, and species. By utilizing features generated by a Vision Diffusion Model (VDM) aggregated via a transformer, the model achieves robustness in recognizing actions. The model's conditioning on earlier diffusion process timesteps helps highlight semantic information over pixel-level details in the extracted features, improving generalization. Experimental results demonstrate the model's superior performance in classifying actions across animal species, different viewing angles, and recording contexts, setting a new state-of-the-art in all three benchmarks. The approach brings machine action recognition closer to human-like robustness, showcasing the potential of leveraging the VDM and transformer architecture for achieving advanced generalization capabilities. The project page and code repository are available for further exploration and implementation. <br /> <div>
arXiv:2509.08908v1 Announce Type: new 
Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$ Code: $\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability</title>
<link>https://arxiv.org/abs/2509.08910</link>
<guid>https://arxiv.org/abs/2509.08910</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Harm Prevention, Vulnerable Populations, PromptGuard Framework, Ethical Principles

Summary:
PromptGuard introduces a modular prompting framework called VulnGuard Prompt, which aims to prevent the generation of harmful, biased, or misleading information by Large Language Models that target vulnerable populations such as LGBTQ+ individuals and marginalized communities. The framework utilizes few-shot examples from GitHub repositories, ethical chain-of-thought reasoning, and adaptive role-prompting to create protective barriers tailored to specific populations. Through a multi-objective optimization approach, PromptGuard offers a 25-30% reduction in analytical harm by leveraging entropy bounds and Pareto optimality. The framework consists of six core modules including Input Classification, VulnGuard Prompting, Ethical Principles Integration, External Tool Interaction, Output Validation, and User-System Interaction, forming an intelligent expert system for real-time harm prevention. Mathematical formalization, convergence proofs, vulnerability analysis using information theory, and validation with GitHub-sourced datasets establish a strong foundation for further empirical research.<br /><br />Summary: <div>
arXiv:2509.08910v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) in real-world applications poses unprecedented risks of generating harmful, biased, or misleading information to vulnerable populations including LGBTQ+ individuals, single parents, and marginalized communities. While existing safety approaches rely on post-hoc filtering or generic alignment techniques, they fail to proactively prevent harmful outputs at the generation source. This paper introduces PromptGuard, a novel modular prompting framework with our breakthrough contribution: VulnGuard Prompt, a hybrid technique that prevents harmful information generation using real-world data-driven contrastive learning. VulnGuard integrates few-shot examples from curated GitHub repositories, ethical chain-of-thought reasoning, and adaptive role-prompting to create population-specific protective barriers. Our framework employs theoretical multi-objective optimization with formal proofs demonstrating 25-30% analytical harm reduction through entropy bounds and Pareto optimality. PromptGuard orchestrates six core modules: Input Classification, VulnGuard Prompting, Ethical Principles Integration, External Tool Interaction, Output Validation, and User-System Interaction, creating an intelligent expert system for real-time harm prevention. We provide comprehensive mathematical formalization including convergence proofs, vulnerability analysis using information theory, and theoretical validation framework using GitHub-sourced datasets, establishing mathematical foundations for systematic empirical research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures</title>
<link>https://arxiv.org/abs/2509.08926</link>
<guid>https://arxiv.org/abs/2509.08926</guid>
<content:encoded><![CDATA[
<div> Keywords: Object re-identification, label noise, Siamese network, statistical outlier detection, Beta-SOD <br />
Summary: <br />
Object re-identification methods often suffer from label noise, which hampers their performance. This study reframes Re-ID as a supervised image similarity task and utilizes a Siamese network architecture. A novel statistical outlier detection framework, Beta-SOD, is introduced to model cosine similarities between embedding pairs using a two-component Beta distribution mixture model. The approach combines binary cross-entropy, contrastive, and cosine embedding losses to optimize feature-level similarity learning. The identifiability of mixtures of two Beta distributions is established to ensure the learning task's well-posedness. Experimental results on person and vehicle re-identification datasets demonstrate that Beta-SOD outperforms existing methods, showcasing robustness and versatility in noisy Re-ID scenarios. The implementation of Beta-SOD is publicly available on GitHub. <br /> 
Summary: <div>
arXiv:2509.08926v1 Announce Type: new 
Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed.The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: https://github.com/waqar3411/Beta-SOD
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation</title>
<link>https://arxiv.org/abs/2509.08934</link>
<guid>https://arxiv.org/abs/2509.08934</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary artery disease, invasive coronary angiography, vessel segmentation, stenosis detection, image enhancement

Summary:
SFD-Mamba2Net is introduced as an end-to-end framework for improving the accuracy of coronary artery segmentation and stenosis detection in ICA images. The framework incorporates multi-scale structural priors, long-range dependency modeling, and frequency-domain detail enhancement strategies to address the challenges posed by low contrast, noise levels, and complex vascular structures in ICA images. The Curvature-Aware Structural Enhancement (CASE) module in the encoder highlights tubular vascular structures and suppresses background interference, while the Progressive High-Frequency Perception (PHFP) module in the decoder refines high-frequency details and integrates global structures. In testing, SFD-Mamba2Net outperformed existing methods in segmentation accuracy and achieved the highest true positive rate and positive predictive value in stenosis detection. This advancement in image analysis technology has the potential to enhance CAD diagnosis and improve patient outcomes. 

<br /><br />Summary: <div>
arXiv:2509.08934v1 Announce Type: new 
Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold standard for CAD diagnosis, necessitates precise vessel segmentation and stenosis detection. However, ICA images are typically characterized by low contrast, high noise levels, and complex, fine-grained vascular structures, which pose significant challenges to the clinical adoption of existing segmentation and detection methods. Objective: This study aims to improve the accuracy of coronary artery segmentation and stenosis detection in ICA images by integrating multi-scale structural priors, state-space-based long-range dependency modeling, and frequency-domain detail enhancement strategies. Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for ICA-based vascular segmentation and stenosis detection. In the encoder, a Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage multi-scale responses for highlighting slender tubular vascular structures, suppressing background interference, and directing attention toward vascular regions. In the decoder, we introduce a Progressive High-Frequency Perception (PHFP) module that employs multi-level wavelet decomposition to progressively refine high-frequency details while integrating low-frequency global structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed state-of-the-art methods across eight segmentation metrics, and achieved the highest true positive rate and positive predictive value in stenosis detection.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Live(r) Die: Predicting Survival in Colorectal Liver Metastasis</title>
<link>https://arxiv.org/abs/2509.08935</link>
<guid>https://arxiv.org/abs/2509.08935</guid>
<content:encoded><![CDATA[
<div> segmentation, radiomics, outcome prediction, colorectal cancer, liver metastasis

Summary:<br />
This study introduces a fully automated framework for predicting surgical outcomes in colorectal cancer patients with liver metastasis using pre- and post-contrast MRI images. The framework includes a segmentation pipeline that accurately identifies liver, tumor, and spleen regions using a novel zero-shot 3D prompt propagation algorithm. The segmented images are then processed through a radiomics pipeline that extracts features from each tumor and predicts survival using a novel autoencoder-based multiple instance neural network. The framework outperforms existing clinical and genomic biomarkers by improving the concordance index by over 10%. This integrated approach of automated segmentation and radiomics-based survival analysis shows promise in delivering accurate, annotation-efficient, and interpretable outcome predictions for patients with colorectal liver metastasis.<br />Summary: <div>
arXiv:2509.08935v1 Announce Type: new 
Abstract: Colorectal cancer frequently metastasizes to the liver, significantly reducing long-term survival. While surgical resection is the only potentially curative treatment for colorectal liver metastasis (CRLM), patient outcomes vary widely depending on tumor characteristics along with clinical and genomic factors. Current prognostic models, often based on limited clinical or molecular features, lack sufficient predictive power, especially in multifocal CRLM cases. We present a fully automated framework for surgical outcome prediction from pre- and post-contrast MRI acquired before surgery. Our framework consists of a segmentation pipeline and a radiomics pipeline. The segmentation pipeline learns to segment the liver, tumors, and spleen from partially annotated data by leveraging promptable foundation models to complete missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt propagation algorithm that leverages the Segment Anything Model to segment 3D regions of interest from a single point prompt, significantly improving our segmentation pipeline's accuracy and efficiency. The predicted pre- and post-contrast segmentations are then fed into our radiomics pipeline, which extracts features from each tumor and predicts survival using SurvAMINN, a novel autoencoder-based multiple instance neural network for survival analysis. SurvAMINN jointly learns dimensionality reduction and hazard prediction from right-censored survival data, focusing on the most aggressive tumors. Extensive evaluation on an institutional dataset comprising 227 patients demonstrates that our framework surpasses existing clinical and genomic biomarkers, delivering a C-index improvement exceeding 10%. Our results demonstrate the potential of integrating automated segmentation algorithms and radiomics-based survival analysis to deliver accurate, annotation-efficient, and interpretable outcome prediction in CRLM.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Divergent Representations between Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.08940</link>
<guid>https://arxiv.org/abs/2509.08940</guid>
<content:encoded><![CDATA[
<div> Keywords: visual representations, text-to-image models, evolutionary search algorithm, dataset, divergent representations

Summary:
CompCon is introduced in this paper as an evolutionary search algorithm that identifies visual attributes that differ between two text-to-image models. The goal is to uncover these differences and understand the types of prompts that trigger them. An automated data generation pipeline is used to create the ID2 dataset, consisting of 60 input-dependent differences. CompCon is evaluated against LLM- and VLM-powered baselines to determine its effectiveness in finding diverging representations. The algorithm is applied to compare popular text-to-image models, revealing distinctions like PixArt's depiction of loneliness with wet streets and Stable Diffusion 3.5's representation of African American individuals in media professions. The code for CompCon is available on GitHub for further exploration and research. 

<br /><br />Summary: <div>
arXiv:2509.08940v1 Announce Type: new 
Abstract: In this paper, we investigate when and how visual representations learned by two different generative models diverge. Given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, "flames" might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions. Code at: https://github.com/adobe-research/CompCon
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery</title>
<link>https://arxiv.org/abs/2509.08949</link>
<guid>https://arxiv.org/abs/2509.08949</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Unmanned Aerial Systems, Remote Sensing, Cloud Shadows, Sun Glint

Summary:
This study introduces a novel machine learning approach to address the challenges of cloud shadows and sun glint affecting imagery obtained from unmanned aerial systems (UAS) for remote sensing applications. By utilizing a U-Net based deep learning model, the system can accurately identify and separate regions with cloud shadows and sun glint from clear sky and unaffected regions. Through pixel-level data extraction and model training optimization, a high-quality image correction model is established to effectively recover the obstructed areas in the images. This approach enhances the accuracy and reliability of estimating water quality parameters from UAS imagery, offering a significant advancement in remote sensing technology. <div>
arXiv:2509.08949v1 Announce Type: new 
Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the current decade. They have significantly advanced remote sensing with the capability to deploy and image the terrain as per required spatial, spectral, temporal, and radiometric resolutions for various remote sensing applications. One of the major advantages of UAS imagery is that images can be acquired in cloudy conditions by flying the UAS under the clouds. The limitation to the technology is that the imagery is often sullied by cloud shadows. Images taken over water are additionally affected by sun glint. These are two pose serious issues for estimating water quality parameters from the UAS images. This study proposes a novel machine learning approach first to identify and extract regions with cloud shadows and sun glint and separate such regions from non-obstructed clear sky regions and sun-glint unaffected regions. The data was extracted from the images at pixel level to train an U-Net based deep learning model and best settings for model training was identified based on the various evaluation metrics from test cases. Using this evaluation, a high-quality image correction model was determined, which was used to recover the cloud shadow and sun glint areas in the images.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision</title>
<link>https://arxiv.org/abs/2509.08959</link>
<guid>https://arxiv.org/abs/2509.08959</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, CoSwin, attention, convolutional feature learning, image classification  
Summary:  
CoSwin is introduced as a novel feature-fusion architecture that combines hierarchical shifted window attention with localized convolutional feature learning. This approach enhances the model's ability to capture both fine-grained spatial details and global semantic structure. Evaluation on various image classification benchmarks demonstrates consistent performance improvements over existing convolutional and transformer-based models. CoSwin outperforms the baseline Swin Transformer on CIFAR-10, CIFAR-100, MNIST, SVHN, and Tiny ImageNet datasets, achieving significant enhancements in accuracy. These results highlight the effectiveness of local-global feature fusion in enhancing the generalization and robustness of transformers for small-scale vision tasks. The code and pretrained weights for CoSwin are available for further exploration.  
<br /><br />Summary: <div>
arXiv:2509.08959v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have achieved impressive results in computer vision by leveraging self-attention to model long-range dependencies. However, their emphasis on global context often comes at the expense of local feature extraction in small datasets, particularly due to the lack of key inductive biases such as locality and translation equivariance. To mitigate this, we propose CoSwin, a novel feature-fusion architecture that augments the hierarchical shifted window attention with localized convolutional feature learning. Specifically, CoSwin integrates a learnable local feature enhancement module into each attention block, enabling the model to simultaneously capture fine-grained spatial details and global semantic structure. We evaluate CoSwin on multiple image classification benchmarks including CIFAR-10, CIFAR-100, MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent performance gains over state-of-the-art convolutional and transformer-based models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the baseline Swin Transformer. These improvements underscore the effectiveness of local-global feature fusion in enhancing the generalization and robustness of transformers for small-scale vision. Code and pretrained weights available at https://github.com/puskal-khadka/coswin
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</title>
<link>https://arxiv.org/abs/2509.08982</link>
<guid>https://arxiv.org/abs/2509.08982</guid>
<content:encoded><![CDATA[
<div> feature matching, point cloud registration, geometric consistency, learned features, inlier ratios
Summary: 
iMatcher is introduced as a fully differentiable framework for feature matching in point cloud registration. It utilizes learned features to predict a confidence matrix that considers both local and global consistency. The method incorporates a local graph embedding module for initialization of the score matrix, followed by a repositioning step that refines the matrix through bilateral matching. Pairwise point features are then refined based on global geometric consistency learning to predict point-wise matching probabilities. Extensive experiments on various datasets demonstrate that iMatcher significantly enhances rigid registration performance, achieving high inlier ratios on KITTI, KITTI-360, and 3DMatch datasets. This highlights the method's robustness across diverse settings. <br /><br />Summary: <div>
arXiv:2509.08982v1 Announce Type: new 
Abstract: This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltrON: Ultrasound Occupancy Networks</title>
<link>https://arxiv.org/abs/2509.08991</link>
<guid>https://arxiv.org/abs/2509.08991</guid>
<content:encoded><![CDATA[
<div> Keywords: free-hand ultrasound imaging, shape reconstruction, implicit representation, acoustic features, weakly-supervised optimization

Summary:
In the field of free-hand ultrasound imaging, the choice of shape representation is crucial for accurate visualization and interpretation. This study introduces UltrON, a method that utilizes occupancy-based representation and acoustic features to enhance geometric consistency in shape reconstruction. By leveraging acoustic properties from B-mode ultrasound images, UltrON addresses the challenges posed by occlusions and sparse labeling. The approach mitigates the limitations of view-dependent nature and acoustic shadowing artifacts, leading to more accurate 3D reconstructions. An innovative loss function compensates for view-dependency in B-mode images, enabling UltrON to generalize to shapes of the same anatomy. Overall, UltrON provides a promising solution for improving shape reconstruction in ultrasound imaging, offering potential advancements in clinical practice and medical diagnostics. <br /><br />Summary: <div>
arXiv:2509.08991v1 Announce Type: new 
Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can assist clinicians in this process. Central to this task is the choice of shape representation, as it determines how accurately and efficiently the structure can be visualized, analyzed, and interpreted. Implicit representations, such as SDF and occupancy function, offer a powerful alternative to traditional voxel- or mesh-based methods by modeling continuous, smooth surfaces with compact storage, avoiding explicit discretization. Recent studies demonstrate that SDF can be effectively optimized using annotations derived from segmented B-mode ultrasound images. Yet, these approaches hinge on precise annotations, overlooking the rich acoustic information embedded in B-mode intensity. Moreover, implicit representation approaches struggle with the ultrasound's view-dependent nature and acoustic shadowing artifacts, which impair reconstruction. To address the problems resulting from occlusions and annotation dependency, we propose an occupancy-based representation and introduce \gls{UltrON} that leverages acoustic features to improve geometric consistency in weakly-supervised optimization regime. We show that these features can be obtained from B-mode images without additional annotation cost. Moreover, we propose a novel loss function that compensates for view-dependency in the B-mode images and facilitates occupancy optimization from multiview ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations of occlusions and sparse labeling and paves the way for more accurate 3D reconstruction. Code and dataset will be available at https://github.com/magdalena-wysocki/ultron.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Neural Representations of Intramyocardial Motion and Strain</title>
<link>https://arxiv.org/abs/2509.09004</link>
<guid>https://arxiv.org/abs/2509.09004</guid>
<content:encoded><![CDATA[
<div> latent codes, left ventricular displacement, tagging MRI, implicit neural representations, myocardial strain <br />
Summary:<br />
The study focuses on automating the quantification of intramyocardial motion and strain from tagging MRI, a challenging task. The researchers proposed a method using implicit neural representations (INRs) conditioned on learned latent codes to predict continuous left ventricular (LV) displacement. The method achieved the best tracking accuracy (2.14 mm RMSE) and lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to existing deep learning baselines. Additionally, the proposed method demonstrated a significant speed improvement, being approximately 380 times faster than the most accurate baseline. The findings indicate the efficacy of using INRs for accurate and scalable analysis of myocardial strain in large cardiovascular magnetic resonance (CMR) datasets.<br /><br />Summary: <div>
arXiv:2509.09004v1 Announce Type: new 
Abstract: Automatic quantification of intramyocardial motion and strain from tagging MRI remains an important but challenging task. We propose a method using implicit neural representations (INRs), conditioned on learned latent codes, to predict continuous left ventricular (LV) displacement -- without requiring inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to three deep learning baselines. In addition, our method is $\sim$380$\times$ faster than the most accurate baseline. These results highlight the suitability of INR-based models for accurate and scalable analysis of myocardial strain in large CMR datasets.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting</title>
<link>https://arxiv.org/abs/2509.09006</link>
<guid>https://arxiv.org/abs/2509.09006</guid>
<content:encoded><![CDATA[
<div> Keywords: Universal Domain Adaptation, Mutual Learning Network, Open-set Entropy Minimization, Enhanced Mutual Learning Network, Closed-set classifier

Summary: 
E-MLNet introduces a dynamic weighting strategy to Open-set Entropy Minimization in Universal Domain Adaptation. By leveraging closed-set classifier predictions, E-MLNet focuses on relevant class boundaries for target samples, distinguishing between known and unknown classes. Extensive experiments on Office-31, Office-Home, VisDA-2017, and ImageCLEF benchmarks show E-MLNet achieving the highest average H-scores on VisDA and ImageCLEF and outperforming MLNet in 22 out of 31 tasks in Open-Partial DA and 19 out of 31 tasks in Open-Set DA. The results confirm the benefits of the focused adaptation strategy of E-MLNet over its predecessor. 

<br /><br />Summary: <div>
arXiv:2509.09006v1 Announce Type: new 
Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a labeled source to an unlabeled target domain without assuming any relationship between their label sets, requiring models to classify known samples while rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet) use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization (OEM). However, this strategy treats all classifiers equally, diluting the learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet), which integrates a dynamic weighting strategy to OEM. By leveraging the closed-set classifier's predictions, E-MLNet focuses adaptation on the most relevant class boundaries for each target sample, sharpening the distinction between known and unknown classes. We conduct extensive experiments on four challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The results demonstrate that E-MLNet achieves the highest average H-scores on VisDA and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet outperforms the strong MLNet baseline in the majority of individual adaptation tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of 31 in the Open-Set DA setting -- confirming the benefits of our focused adaptation strategy.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation</title>
<link>https://arxiv.org/abs/2509.09014</link>
<guid>https://arxiv.org/abs/2509.09014</guid>
<content:encoded><![CDATA[
<div> Keywords: Urdu, image-caption dataset, vision-language research, COCO-Urdu, multimodal quality estimation

Summary: 
Urdu, a language spoken by over 250 million people, has been under-represented in vision-language research. To address this gap, the authors introduce COCO-Urdu, a large-scale image-caption dataset derived from MS COCO. This dataset contains 59,000 images and 319,000 Urdu captions, translated and validated through a comprehensive quality estimation framework. The captions were refined using large language models, and the dataset was benchmarked on various metrics, showing strong performance. COCO-Urdu is currently the largest publicly available Urdu captioning dataset. By releasing both the dataset and the quality estimation pipeline, the authors aim to reduce language bias in multimodal research and lay the foundation for more inclusive vision-language systems.<br /><br />Summary: <div>
arXiv:2509.09014v1 Announce Type: new 
Abstract: Urdu, spoken by over 250 million people, remains critically under-served in multimodal and vision-language research. The absence of large-scale, high-quality datasets has limited the development of Urdu-capable systems and reinforced biases in multilingual vision-language models trained primarily on high-resource languages. To address this gap, we present COCO-Urdu, a large-scale image-caption dataset derived from MS COCO, containing 59,000 images and 319,000 Urdu captions selected through stratified sampling to preserve the original distribution. Captions were translated using SeamlessM4T v2 and validated with a hybrid multimodal quality estimation framework that integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual grounding, and BERTScore with back-translation for semantic consistency; low-scoring captions were iteratively refined using open-source large language models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting consistently strong results. To the best of our knowledge, COCO-Urdu is the largest publicly available Urdu captioning dataset. By releasing both the dataset and the quality estimation pipeline, we aim to reduce language bias in multimodal research and establish a foundation for inclusive vision-language systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI</title>
<link>https://arxiv.org/abs/2509.09015</link>
<guid>https://arxiv.org/abs/2509.09015</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, fMRI visual decoding, multi-subject training, VoxelFormer, parameter-efficient

Summary:
VoxelFormer is a lightweight transformer architecture designed for visual decoding from fMRI data that allows for multi-subject training. It combines a Token Merging Transformer (ToMer) for efficient voxel compression and a query-driven Q-Former to generate fixed-size neural representations aligned with the CLIP image embedding space. The evaluation on the 7T Natural Scenes Dataset demonstrates that VoxelFormer achieves competitive retrieval performance on subjects included during training while using significantly fewer parameters compared to existing methods. This highlights the effectiveness of token merging and query-based transformers in achieving parameter efficiency in neural decoding tasks. <br /><br />Summary: <div>
arXiv:2509.09015v1 Announce Type: new 
Abstract: Recent advances in fMRI-based visual decoding have enabled compelling reconstructions of perceived images. However, most approaches rely on subject-specific training, limiting scalability and practical deployment. We introduce \textbf{VoxelFormer}, a lightweight transformer architecture that enables multi-subject training for visual decoding from fMRI. VoxelFormer integrates a Token Merging Transformer (ToMer) for efficient voxel compression and a query-driven Q-Former that produces fixed-size neural representations aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes Dataset, VoxelFormer achieves competitive retrieval performance on subjects included during training with significantly fewer parameters than existing methods. These results highlight token merging and query-based transformers as promising strategies for parameter-efficient neural decoding.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Anatomical Priors into a Causal Diffusion Model</title>
<link>https://arxiv.org/abs/2509.09054</link>
<guid>https://arxiv.org/abs/2509.09054</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D brain MRI, image synthesis, anatomical constraints, Probabilistic Causal Graph Model, counterfactual denoising UNet

Summary: 
The study focuses on improving image synthesis in 3D brain MRI studies, where subtle morphometric differences between cohorts are challenging to detect visually. The research introduces the Probabilistic Causal Graph Model (PCGM) to integrate anatomical constraints at a voxel-level and enhance the generation of anatomically plausible MRIs. By incorporating probabilistic graph modules and spatial binary masks, PCGM improves the quality of generated brain MRIs compared to baseline approaches. The study demonstrates the replication of subtle disease effects on cortical brain regions using synthetic MRIs generated by PCGM, showcasing its potential for investigating morphological differences. Overall, PCGM shows promise for enhancing imaging studies and understanding the impact of disease on brain structures. 

Summary:<br />Enhancing image synthesis in 3D brain MRI studies<br />Introducing Probabilistic Causal Graph Model (PCGM) for integrating anatomical constraints<br />Improving the quality of generated brain MRIs compared to baseline approaches<br />Demonstrating replication of subtle disease effects on cortical brain regions using synthetic MRIs<br />Potential for investigating morphological differences and enhancing imaging studies. <div>
arXiv:2509.09054v1 Announce Type: new 
Abstract: 3D brain MRI studies often examine subtle morphometric differences between cohorts that are hard to detect visually. Given the high cost of MRI acquisition, these studies could greatly benefit from image syntheses, particularly counterfactual image generation, as seen in other domains, such as computer vision. However, counterfactual models struggle to produce anatomically plausible MRIs due to the lack of explicit inductive biases to preserve fine-grained anatomical details. This shortcoming arises from the training of the models aiming to optimize for the overall appearance of the images (e.g., via cross-entropy) rather than preserving subtle, yet medically relevant, local variations across subjects. To preserve subtle variations, we propose to explicitly integrate anatomical constraints on a voxel-level as prior into a generative diffusion framework. Called Probabilistic Causal Graph Model (PCGM), the approach captures anatomical constraints via a probabilistic graph module and translates those constraints into spatial binary masks of regions where subtle variations occur. The masks (encoded by a 3D extension of ControlNet) constrain a novel counterfactual denoising UNet, whose encodings are then transferred into high-quality brain MRIs via our 3D diffusion decoder. Extensive experiments on multiple datasets demonstrate that PCGM generates structural brain MRIs of higher quality than several baseline approaches. Furthermore, we show for the first time that brain measurements extracted from counterfactuals (generated by PCGM) replicate the subtle effects of a disease on cortical brain regions previously reported in the neuroscience literature. This achievement is an important milestone in the use of synthetic MRIs in studies investigating subtle morphological differences.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.09064</link>
<guid>https://arxiv.org/abs/2509.09064</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D medical image volumes, self-supervised learning, multimodal large language models, Med3DInsight, segmentation and classification

Summary: 
Med3DInsight is a novel pretraining framework that integrates 3D medical image encoders with 2D MLLMs to enhance understanding of 3D medical image volumes. It includes a specially designed plane-slice-aware transformer module and employs partial optimal transport based alignment to improve tolerance to noise. The model does not require human annotations and aims to provide scalable multimodal 3D medical representation learning. Extensive experiments demonstrate state-of-the-art performance on segmentation and classification tasks using CT and MRI modalities across various public datasets. Med3DInsight can be seamlessly integrated into existing 3D medical image understanding networks, potentially enhancing their performance. The source code, generated datasets, and pre-trained models are available on GitHub at https://github.com/Qybc/Med3DInsight. 

<br /><br />Summary: <div>
arXiv:2509.09064v1 Announce Type: new 
Abstract: Understanding 3D medical image volumes is critical in the medical field, yet existing 3D medical convolution and transformer-based self-supervised learning (SSL) methods often lack deep semantic comprehension. Recent advancements in multimodal large language models (MLLMs) provide a promising approach to enhance image understanding through text descriptions. To leverage these 2D MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a novel pretraining framework that integrates 3D image encoders with 2D MLLMs via a specially designed plane-slice-aware transformer module. Additionally, our model employs a partial optimal transport based alignment, demonstrating greater tolerance to noise introduced by potential noises in LLM-generated content. Med3DInsight introduces a new paradigm for scalable multimodal 3D medical representation learning without requiring human annotations. Extensive experiments demonstrate our state-of-the-art performance on two downstream tasks, i.e., segmentation and classification, across various public datasets with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can be seamlessly integrated into existing 3D medical image understanding networks, potentially enhancing their performance. Our source code, generated datasets, and pre-trained models will be available at https://github.com/Qybc/Med3DInsight.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach</title>
<link>https://arxiv.org/abs/2509.09067</link>
<guid>https://arxiv.org/abs/2509.09067</guid>
<content:encoded><![CDATA[
<div> Keywords: graph convolutional neural networks, human action recognition, multi-task learning, human-object interaction, scene information

Summary: 
The article introduces a new methodology to enhance human action recognition using graph convolutional neural networks (GCNs) by including fixed object information and employing a multi-task learning approach. The current GCNs have shown strong performance in recognizing human action based on skeleton poses but struggle in detecting human-object interactions due to inadequate scene information representation. Real data from public environments was collected to create a dataset with interaction and non-interaction classes. By combining interaction area information and a multi-task learning approach, the proposed method achieved an accuracy of 99.25% in recognizing interaction and non-interaction actions, surpassing the base model's accuracy by 2.75%. This approach demonstrates the potential to improve human action recognition by incorporating fixed object information and utilizing a multi-task learning strategy. 

<br /><br />Summary: <div>
arXiv:2509.09067v1 Announce Type: new 
Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance in the field of human action recognition by using human skeleton poses. However, it fails to detect human-object interaction cases successfully due to the lack of effective representation of the scene information and appropriate learning architectures. In this context, we propose a methodology to utilize human action recognition performance by considering fixed object information in the environment and following a multi-task learning approach. In order to evaluate the proposed method, we collected real data from public environments and prepared our data set, which includes interaction classes of hands-on fixed objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and non-interaction classes of walking and standing. The multi-task learning approach, along with interaction area information, succeeds in recognizing the studied interaction and non-interaction actions with an accuracy of 99.25%, outperforming the accuracy of the base model using only human skeleton poses by 2.75%.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection</title>
<link>https://arxiv.org/abs/2509.09085</link>
<guid>https://arxiv.org/abs/2509.09085</guid>
<content:encoded><![CDATA[
<div> Keywords: multispectral object detection, feature fusion, cross-modal contrastive, differential feedback, IRDFusion<br />
<br />
Summary: 
The article presents a novel feature fusion framework for multispectral object detection that aims to improve perceptual performance by enhancing salient structures and suppressing background interference. The proposed method, called IRDFusion, consists of two modules: Mutual Feature Refinement Module (MFRM) and Differential Feature Feedback Module (DFFM). The MFRM improves feature representations by modeling intra- and inter-modal relationships, facilitating better alignment and discriminative power. The DFFM computes inter-modal differential features as guidance signals to adaptively fuse complementary information and suppress common-mode noise. Integrated into a unified framework, IRDFusion employs an Iterative Relation-Map Differential Guided Feature Fusion mechanism to progressively amplify salient relational signals while suppressing feature noise. Experimental results on FLIR, LLVIP, and M$^3$FD datasets demonstrate state-of-the-art performance and robustness across challenging scenarios. The code for IRDFusion will be made available on GitHub. <br /><br /> <div>
arXiv:2509.09085v1 Announce Type: new 
Abstract: Current multispectral object detection methods often retain extraneous background or noise during feature fusion, limiting perceptual performance.To address this, we propose an innovative feature fusion framework based on cross-modal feature contrastive and screening strategy, diverging from conventional approaches. The proposed method adaptively enhances salient structures by fusing object-aware complementary cross-modal features while suppressing shared background interference.Our solution centers on two novel, specially designed modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and inter-modal feature representations by modeling their relationships, thereby improving cross-modal alignment and discriminative power.Inspired by feedback differential amplifiers, the DFFM dynamically computes inter-modal differential features as guidance signals and feeds them back to the MFRM, enabling adaptive fusion of complementary information while suppressing common-mode noise across modalities. To enable robust feature learning, the MFRM and DFFM are integrated into a unified framework, which is formally formulated as an Iterative Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion. IRDFusion enables high-quality cross-modal fusion by progressively amplifying salient relational signals through iterative feedback, while suppressing feature noise, leading to significant performance gains.In extensive experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves state-of-the-art performance and consistently outperforms existing methods across diverse challenging scenarios, demonstrating its robustness and effectiveness. Code will be available at https://github.com/61s61min/IRDFusion.git.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2509.09090</link>
<guid>https://arxiv.org/abs/2509.09090</guid>
<content:encoded><![CDATA[
<div> compression, acceleration, VLA models, quantization, token pruning

Summary:
The paper introduces SQAP-VLA, a novel framework for accelerating Vision-Language-Action (VLA) models without the need for structured training. By co-designing quantization and token pruning, the framework overcomes compatibility issues and achieves significant gains in computational efficiency and inference speed. New quantization-aware token pruning criteria are proposed to enhance pruning effectiveness on aggressively quantized models. When applied to standard VLA models, SQAP-VLA achieves a 1.93x speedup and up to a 4.5% average success rate enhancement while preserving core model performance. This structured approach to VLA inference acceleration allows for simultaneous state-of-the-art quantization and token pruning, contributing to improved practical deployment of VLA models. <div>
arXiv:2509.09090v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a $\times$1.93 speedup and up to a 4.5\% average success rate enhancement compared to the original model.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization</title>
<link>https://arxiv.org/abs/2509.09110</link>
<guid>https://arxiv.org/abs/2509.09110</guid>
<content:encoded><![CDATA[
<div> LiDAR, Global Localization, Self-Supervised, Bird's-Eye View, CNN <br />
Summary: <br />
LiDAR-based global localization is crucial for simultaneous localization and mapping (SLAM). A new self-supervised framework, S-BEVLoc, based on bird's-eye view (BEV) eliminates the need for costly ground-truth poses. By creating training triplets from BEV images and utilizing known geographic distances, S-BEVLoc uses a CNN for local feature extraction and NetVLAD for global descriptor aggregation. The introduction of SoftCos loss improves learning from triplets. Results on KITTI and NCLT datasets demonstrate S-BEVLoc's state-of-the-art performance in place recognition, loop closure, and global localization, with scalability surpassing supervised approaches. <div>
arXiv:2509.09110v1 Announce Type: new 
Abstract: LiDAR-based global localization is an essential component of simultaneous localization and mapping (SLAM), which helps loop closure and re-localization. Current approaches rely on ground-truth poses obtained from GPS or SLAM odometry to supervise network training. Despite the great success of these supervised approaches, substantial cost and effort are required for high-precision ground-truth pose acquisition. In this work, we propose S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for LiDAR global localization, which eliminates the need for ground-truth poses and is highly scalable. We construct training triplets from single BEV images by leveraging the known geographic distances between keypoint-centered BEV patches. Convolutional neural network (CNN) is used to extract local features, and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce SoftCos loss to enhance learning from the generated triplets. Experimental results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves state-of-the-art performance in place recognition, loop closure, and global localization tasks, while offering scalability that would require extra effort for supervised approaches.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding</title>
<link>https://arxiv.org/abs/2509.09111</link>
<guid>https://arxiv.org/abs/2509.09111</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile devices, vision systems, human--device interactions, FPI-Det dataset, object detection

Summary:
The use of mobile devices has presented new challenges for vision systems, particularly in monitoring safety, assessing workplace productivity, and managing attention. Detecting phone usage by individuals requires a nuanced understanding of human-device interactions beyond simple object recognition. To address the limitations of existing benchmarks, the FPI-Det dataset has been introduced, featuring nearly 23,000 images with synchronized annotations for faces and phones in various scenarios. The dataset showcases extreme scale variations, frequent occlusions, and diverse capture conditions. Evaluating popular detectors like YOLO and DETR on the dataset provides baseline results and insights into performance across different object sizes, occlusion levels, and environments. The source code and dataset are publicly available for further research and development. <div>
arXiv:2509.09111v1 Announce Type: new 
Abstract: The widespread use of mobile devices has created new challenges for vision systems in safety monitoring, workplace productivity assessment, and attention management. Detecting whether a person is using a phone requires not only object recognition but also an understanding of behavioral context, which involves reasoning about the relationship between faces, hands, and devices under diverse conditions. Existing generic benchmarks do not fully capture such fine-grained human--device interactions. To address this gap, we introduce the FPI-Det, containing 22{,}879 images with synchronized annotations for faces and phones across workplace, education, transportation, and public scenarios. The dataset features extreme scale variation, frequent occlusions, and varied capture conditions. We evaluate representative YOLO and DETR detectors, providing baseline results and an analysis of performance across object sizes, occlusion levels, and environments. Source code and dataset is available at https://github.com/KvCgRv/FPI-Det.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention</title>
<link>https://arxiv.org/abs/2509.09116</link>
<guid>https://arxiv.org/abs/2509.09116</guid>
<content:encoded><![CDATA[
<div> Zero-shot segmentation, top-view crop images, plant individuals, hierarchical segmentation task, vision-language model

Summary:
ZeroPlantSeg introduces a zero-shot segmentation approach for rosette-shaped plant individuals from top-view images. Through the integration of a foundation segmentation model and a vision-language model, the system is able to extract plant individuals without the need for additional training. This innovative method surpasses existing zero-shot techniques and achieves superior cross-domain performance compared to supervised methods. The approach is evaluated on diverse datasets encompassing various plant species, growth stages, and shooting environments, demonstrating its robustness and versatility. ZeroPlantSeg presents a promising solution to the challenging hierarchical segmentation task, offering a more efficient and labor-saving alternative to species-specific annotated training datasets. The implementation of ZeroPlantSeg is publicly available on GitHub for further exploration and adoption. 

<br /><br />Summary: <div>
arXiv:2509.09116v1 Announce Type: new 
Abstract: Foundation segmentation models achieve reasonable leaf instance extraction from top-view crop images without training (i.e., zero-shot). However, segmenting entire plant individuals with each consisting of multiple overlapping leaves remains challenging. This problem is referred to as a hierarchical segmentation task, typically requiring annotated training datasets, which are often species-specific and require notable human labor. To address this, we introduce ZeroPlantSeg, a zero-shot segmentation for rosette-shaped plant individuals from top-view images. We integrate a foundation segmentation model, extracting leaf instances, and a vision-language model, reasoning about plants' structures to extract plant individuals without additional training. Evaluations on datasets with multiple plant species, growth stages, and shooting environments demonstrate that our method surpasses existing zero-shot methods and achieves better cross-domain performance than supervised methods. Implementations are available at https://github.com/JunhaoXing/ZeroPlantSeg.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval</title>
<link>https://arxiv.org/abs/2509.09118</link>
<guid>https://arxiv.org/abs/2509.09118</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, person representation learning, data curation, model architecture, GA-DMS<br />
Summary: 
This article introduces improvements to CLIP for person representation learning by addressing challenges related to data scarcity and global contrastive learning limitations. The authors present a noise-resistant data construction pipeline that leverages MLLMs to filter and caption web-sourced images, resulting in the creation of a large-scale dataset called WebPerson. They also propose the GA-DMS framework, which enhances cross-modal alignment by adaptively masking noisy textual tokens based on gradient-attention similarity scores. Additionally, masked token prediction objectives are incorporated to improve fine-grained semantic representation learning. Experimental results demonstrate that GA-DMS achieves state-of-the-art performance across various benchmarks. <br /><br />Summary: <div>
arXiv:2509.09118v1 Announce Type: new 
Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain</title>
<link>https://arxiv.org/abs/2509.09130</link>
<guid>https://arxiv.org/abs/2509.09130</guid>
<content:encoded><![CDATA[
<div> augmentation, diffusion model, attention mechanism, low-resource, PET imaging 

Summary: 
The article introduces ALL-PET, a low-resource PET foundation model operating directly in the projection domain. It utilizes a latent diffusion model with innovations such as Radon mask augmentation strategy to improve generalization with minimal data, positive/negative mask constraints for geometric consistency, and transparent medical attention for lesion-focused regions. The system supports clinician-defined ROI adjustments for task-adaptive emphasis aligned with PET acquisition physics. Experimental results demonstrate high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks such as low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB. <div>
arXiv:2509.09130v1 Announce Type: new 
Abstract: Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology</title>
<link>https://arxiv.org/abs/2509.09140</link>
<guid>https://arxiv.org/abs/2509.09140</guid>
<content:encoded><![CDATA[
<div> Persistent Homology, Artificial Neural Networks, noise robustness, Betti numbers, topological structure <br />
Summary: <br />
Persistent Homology (PH) and Artificial Neural Networks (ANNs) are contrasting methods for inferring topological structure from data. A study compared the noise robustness of an ANN trained to predict Betti numbers in 2D binary images against a PH pipeline using cubical complexes and the Signed Euclidean Distance Transform (SEDT). Results from one synthetic and two real-world datasets showed that ANNs outperformed the PH approach under noise, possibly due to their ability to learn contextual and geometric priors from training data. The use of ANNs for topology estimation provides a promising alternative to PH in the presence of structural noise. <div>
arXiv:2509.09140v1 Announce Type: new 
Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer contrasting approaches to inferring topological structure from data. In this study, we examine the noise robustness of a supervised neural network trained to predict Betti numbers in 2D binary images. We compare an ANN approach against a PH pipeline based on cubical complexes and the Signed Euclidean Distance Transform (SEDT), which is a widely adopted strategy for noise-robust topological analysis. Using one synthetic and two real-world datasets, we show that ANNs can outperform this PH approach under noise, likely due to their capacity to learn contextual and geometric priors from training data. Though still emerging, the use of ANNs for topology estimation offers a compelling alternative to PH under structural noise.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation</title>
<link>https://arxiv.org/abs/2509.09143</link>
<guid>https://arxiv.org/abs/2509.09143</guid>
<content:encoded><![CDATA[
<div> Objectness SIMilarity, 3D scenes, evaluation metric, object-centric evaluations, human perception <br />
Summary: <br />
This paper introduces Objectness SIMilarity (OSIM), a new evaluation metric for 3D scenes that focuses on individual objects to align more closely with human perception. Inspired by neuropsychological insights, OSIM uses an object detection model to quantify the "objectness" of each object in a scene. A user study confirms that OSIM better represents human perception than existing metrics. The metric is analyzed using different methods, and recent 3D reconstruction and generation models are re-evaluated to showcase advancements in the field. The code for OSIM is available on GitHub at https://github.com/Objectness-Similarity/OSIM. <div>
arXiv:2509.09143v1 Announce Type: new 
Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric for 3D scenes that explicitly focuses on "objects," which are fundamental units of human visual perception. Existing metrics assess overall image quality, leading to discrepancies with human perception. Inspired by neuropsychological insights, we hypothesize that human recognition of 3D scenes fundamentally involves attention to individual objects. OSIM enables object-centric evaluations by leveraging an object detection model and its feature representations to quantify the "objectness" of each object in the scene. Our user study demonstrates that OSIM aligns more closely with human perception compared to existing metrics. We also analyze the characteristics of OSIM using various approaches. Moreover, we re-evaluate recent 3D reconstruction and generation models under a standardized experimental setup to clarify advancements in this field. The code is available at https://github.com/Objectness-Similarity/OSIM.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Understanding by Design: How Datasets Shape Architectures and Insights</title>
<link>https://arxiv.org/abs/2509.09151</link>
<guid>https://arxiv.org/abs/2509.09151</guid>
<content:encoded><![CDATA[
<div> Keywords: video understanding, datasets, architectural evolution, inductive biases, model design

Summary:<br /><br />
This survey takes a dataset-driven perspective on video understanding, highlighting how motion complexity, temporal span, hierarchical composition, and multimodal richness influence architectural evolution. It explores how models, such as two-stream and 3D CNNs, sequential, transformer, and multimodal foundation models, have been developed in response to these dataset-driven pressures. The survey offers practical guidance for aligning model design with dataset invariances while considering scalability and task demands. By unifying datasets, inductive biases, and architectures, the survey provides a comprehensive retrospective and a prescriptive roadmap for advancing general-purpose video understanding. <div>
arXiv:2509.09151v1 Announce Type: new 
Abstract: Video understanding has advanced rapidly, fueled by increasingly complex datasets and powerful architectures. Yet existing surveys largely classify models by task or family, overlooking the structural pressures through which datasets guide architectural evolution. This survey is the first to adopt a dataset-driven perspective, showing how motion complexity, temporal span, hierarchical composition, and multimodal richness impose inductive biases that models should encode. We reinterpret milestones, from two-stream and 3D CNNs to sequential, transformer, and multimodal foundation models, as concrete responses to these dataset-driven pressures. Building on this synthesis, we offer practical guidance for aligning model design with dataset invariances while balancing scalability and task demands. By unifying datasets, inductive biases, and architectures into a coherent framework, this survey provides both a comprehensive retrospective and a prescriptive roadmap for advancing general-purpose video understanding.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge</title>
<link>https://arxiv.org/abs/2509.09153</link>
<guid>https://arxiv.org/abs/2509.09153</guid>
<content:encoded><![CDATA[
<div> Dataset, deep learning, cell-tissue interactions, OCELOT 2023 challenge, multi-scale semantics

Summary:
- Pathologists alternate between different magnifications when examining Whole-Slide Images to evaluate tissue morphology and cellular details for comprehensive diagnoses.
- Existing deep learning models struggle to capture the interdependent semantics between structures at different magnifications.
- The OCELOT 2023 challenge aimed to validate the importance of understanding cell-tissue interactions and accelerate research in the field.
- The challenge dataset includes multi-scale overlapping cell and tissue annotations from six organs.
- Top entries in the challenge significantly improved F1-score by incorporating cell-tissue relationships, showcasing the necessity of multi-scale semantics in models.

<br /><br />Summary: <div>
arXiv:2509.09153v1 Announce Type: new 
Abstract: Pathologists routinely alternate between different magnifications when examining Whole-Slide Images, allowing them to evaluate both broad tissue morphology and intricate cellular details to form comprehensive diagnoses. However, existing deep learning-based cell detection models struggle to replicate these behaviors and learn the interdependent semantics between structures at different magnifications. A key barrier in the field is the lack of datasets with multi-scale overlapping cell and tissue annotations. The OCELOT 2023 challenge was initiated to gather insights from the community to validate the hypothesis that understanding cell and tissue (cell-tissue) interactions is crucial for achieving human-level performance, and to accelerate the research in this field. The challenge dataset includes overlapping cell detection and tissue segmentation annotations from six organs, comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA) Whole-Slide Images with hematoxylin and eosin staining, divided into training, validation, and test subsets. Participants presented models that significantly enhanced the understanding of cell-tissue relationships. Top entries achieved up to a 7.99 increase in F1-score on the test set compared to the baseline cell-only model that did not incorporate cell-tissue relationships. This is a substantial improvement in performance over traditional cell-only detection methods, demonstrating the need for incorporating multi-scale semantics into the models. This paper provides a comparative analysis of the methods used by participants, highlighting innovative strategies implemented in the OCELOT 2023 challenge.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-DETR++ for UAV Object Detection</title>
<link>https://arxiv.org/abs/2509.09157</link>
<guid>https://arxiv.org/abs/2509.09157</guid>
<content:encoded><![CDATA[
<div> Keywords: Object detection, unmanned aerial vehicle imagery, RT-DETR++, channel-gated attention, CSP-PAC

Summary: 
The paper introduces RT-DETR++, an enhanced version of the RT-DETR model for object detection in UAV imagery. This improvement focuses on two main aspects. Firstly, a channel-gated attention-based upsampling/downsampling mechanism is introduced to minimize errors and preserve details during feature layer propagation. Secondly, the incorporation of CSP-PAC during feature fusion allows for the integration of multi-scale features by processing local and contextual information within the same layer. Evaluation shows that the novel neck design of RT-DETR++ achieves superior performance in detecting small and densely packed objects while maintaining real-time detection speed without increasing computational complexity. This study presents an effective approach for feature encoding design in real-time detection systems.<br /><br />Summary: <div>
arXiv:2509.09157v1 Announce Type: new 
Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents significant challenges. Issues such as densely packed small objects, scale variations, and occlusion are commonplace. This paper introduces RT-DETR++, which enhances the encoder component of the RT-DETR model. Our improvements focus on two key aspects. First, we introduce a channel-gated attention-based upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes errors and preserves details during feature layer propagation. Second, we incorporate CSP-PAC during feature fusion. This technique employs parallel hollow convolutions to process local and contextual information within the same layer, facilitating the integration of multi-scale features. Evaluation demonstrates that our novel neck design achieves superior performance in detecting small and densely packed objects. The model maintains sufficient speed for real-time detection without increasing computational complexity. This study provides an effective approach for feature encoding design in real-time detection systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.09159</link>
<guid>https://arxiv.org/abs/2509.09159</guid>
<content:encoded><![CDATA[
<div> retrieval, knowledge focusing, noise reduction, selective integration, KB-VQA <br />
<br />
The article introduces a training-free framework for Knowledge-based Visual Question Answering (KB-VQA) that aims to improve answer accuracy by reducing noise and redundancy in retrieved knowledge. The framework first selects essential parts from image-question pairs for knowledge retrieval, creating low-noise queries to enhance relevance. To address redundancy in retrieved knowledge, large models are prompted to extract answer-beneficial segments. A selective knowledge integration strategy allows the model to incorporate knowledge only when lacking confidence, reducing redundant information's impact. Through experiments, the framework demonstrates superior performance compared to existing methods. <br />
Summary: <div>
arXiv:2509.09159v1 Announce Type: new 
Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to understand images and utilize external knowledge to provide accurate answers. Existing approaches often directly augment models with retrieved information from knowledge sources while ignoring substantial knowledge redundancy, which introduces noise into the answering process. To address this, we propose a training-free framework with knowledge focusing for KB-VQA, that mitigates the impact of noise by enhancing knowledge relevance and reducing redundancy. First, for knowledge retrieval, our framework concludes essential parts from the image-question pairs, creating low-noise queries that enhance the retrieval of highly relevant knowledge. Considering that redundancy still persists in the retrieved knowledge, we then prompt large models to identify and extract answer-beneficial segments from knowledge. In addition, we introduce a selective knowledge integration strategy, allowing the model to incorporate knowledge only when it lacks confidence in answering the question, thereby mitigating the influence of redundant information. Our framework enables the acquisition of accurate and critical knowledge, and extensive experiments demonstrate that it outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution</title>
<link>https://arxiv.org/abs/2509.09163</link>
<guid>https://arxiv.org/abs/2509.09163</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral remote sensing, ground object classification, CWSSNet, spectral-spatial features, wavelet convolution

Summary: 
Hyperspectral remote sensing technology is valuable for forestry and agriculture applications, requiring precise ground object classification. This study utilized ZY1F satellite hyperspectral images in Yugan County, Jiangxi Province, proposing the CWSSNet classification framework. This framework integrates 3D spectral-spatial features and wavelet convolution to address feature redundancy and improve recognition accuracy. CWSSNet achieved high performance metrics in mean Intersection over Union, mean Accuracy, and mean F1-score in ground object classification. It demonstrated robustness in classifying water bodies, vegetation, and bare land, showcasing its effectiveness. Also, even with a 70% training set proportion, the model maintained reliable performance under small-sample training conditions. The integration of multimodal information and wavelet convolution in CWSSNet enhances the classification performance, providing a promising approach for hyperspectral image analysis. 

<br /><br />Summary: <div>
arXiv:2509.09163v1 Announce Type: new 
Abstract: Hyperspectral remote sensing technology has significant application value in fields such as forestry ecology and precision agriculture, while also putting forward higher requirements for fine ground object classification. However, although hyperspectral images are rich in spectral information and can improve recognition accuracy, they tend to cause prominent feature redundancy due to their numerous bands, high dimensionality, and spectral mixing characteristics. To address this, this study used hyperspectral images from the ZY1F satellite as a data source and selected Yugan County, Shangrao City, Jiangxi Province as the research area to perform ground object classification research. A classification framework named CWSSNet was proposed, which integrates 3D spectral-spatial features and wavelet convolution. This framework integrates multimodal information us-ing a multiscale convolutional attention module and breaks through the classification performance bottleneck of traditional methods by introducing multi-band decomposition and convolution operations in the wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%, and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and mean F1-score (mF1) respectively in Yugan County. It also obtained the highest Intersection over Union (IoU) in the classifica-tion of water bodies, vegetation, and bare land, demonstrating good robustness. Additionally, when the training set proportion was 70\%, the increase in training time was limited, and the classification effect was close to the optimal level, indicating that the model maintains reliable performance under small-sample training conditions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios</title>
<link>https://arxiv.org/abs/2509.09172</link>
<guid>https://arxiv.org/abs/2509.09172</guid>
<content:encoded><![CDATA[
<div> Dataset, Generative models, Image detection, Real-world conditions, AI-generated images<br />
Summary:<br />
The paper introduces the Real-World Robustness Dataset (RRDataset) for evaluating detection models in complex real-world conditions. The dataset includes images from seven major scenarios and assesses detector performance on images shared on social media platforms and altered through re-digitization methods. Benchmarking 17 detectors and 10 vision-language models on RRDataset highlights the limitations of current AI detection methods. A large-scale human study involving 192 participants investigates human few-shot learning capabilities in detecting AI-generated images. The results emphasize the need to develop more robust detection algorithms by leveraging human adaptability.<br /> <div>
arXiv:2509.09172v1 Announce Type: new 
Abstract: With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection</title>
<link>https://arxiv.org/abs/2509.09183</link>
<guid>https://arxiv.org/abs/2509.09183</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-light object detection, Image Signal Processing (ISP), RAW images, End-to-end training, Self-adaptive

Summary: 
- The study focuses on low-light object detection using RAW images, offering superior potential over RGB images
- The proposed Dark-ISP plugin is lightweight and self-adaptive, directly processing Bayer RAW images in dark environments
- The ISP pipeline is deconstructed into linear and nonlinear sub-modules, optimized through task-driven losses for RAW-to-RGB conversion aligned with detection objectives
- A Self-Boost mechanism is devised to facilitate cooperation between sub-modules, exploiting the ISP pipeline's intrinsic cascade structure
- Extensive experiments on three RAW image datasets show that the method outperforms state-of-the-art RGB- and RAW-based detection approaches in challenging low-light environments with minimal parameters. 

<br /><br />Summary: <div>
arXiv:2509.09183v1 Announce Type: new 
Abstract: Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results</title>
<link>https://arxiv.org/abs/2509.09190</link>
<guid>https://arxiv.org/abs/2509.09190</guid>
<content:encoded><![CDATA[
<div> Challenge, Visual Quality Comparison, Large Multimodal Models, Benchmark, Evaluation

Summary:
The VQualA 2025 Challenge, hosted at the ICCV 2025 Workshop, focused on assessing and improving the ability of state-of-the-art Large Multimodal Models (LMMs) to reason about visual quality differences across multiple images. The competition featured a diverse benchmark with various visual quality tasks, including single images, pairs, and groups. Participants were required to provide accurate quality judgments using holistic evaluation methods like binary preference and multi-choice questions. Over 100 entries were submitted, showcasing the capabilities of instruction-tuned LMMs in quality assessment. This challenge serves as a crucial step towards advancing open-domain visual quality reasoning and comparison, fostering research in interpretable and human-aligned quality evaluation systems.<br /><br />Summary: <div>
arXiv:2509.09190v1 Announce Type: new 
Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network</title>
<link>https://arxiv.org/abs/2509.09200</link>
<guid>https://arxiv.org/abs/2509.09200</guid>
<content:encoded><![CDATA[
<div> Keywords: human trajectory prediction, goal guidance, multi-granularity modeling, transformer-based recursive refinement network, state-of-the-art performance <br />
Summary: <br />
Accurate human trajectory prediction is vital for robotics and autonomous driving. Existing goal-guided approaches focus on coarse-grained goal prediction and fine-grained trajectory completion, but ignore intermediate temporal granularity. The MGTraj model addresses this gap by recursively encoding trajectory proposals at multiple granularities using a transformer-based recursive refinement network. Features across granularities are integrated using a weight-sharing strategy, with velocity prediction as an auxiliary task. Experimental results on EHT/UCY and Stanford Drone Dataset show that MGTraj outperforms baseline methods and achieves state-of-the-art performance among goal-guided models. <div>
arXiv:2509.09200v1 Announce Type: new 
Abstract: Accurate human trajectory prediction is crucial for robotics navigation and autonomous driving. Recent research has demonstrated that incorporating goal guidance significantly enhances prediction accuracy by reducing uncertainty and leveraging prior knowledge. Most goal-guided approaches decouple the prediction task into two stages: goal prediction and subsequent trajectory completion based on the predicted goal, which operate at extreme granularities: coarse-grained goal prediction forecasts the overall intention, while fine-grained trajectory completion needs to generate the positions for all future timesteps. The potential utility of intermediate temporal granularity remains largely unexplored, which motivates multi-granularity trajectory modeling. While prior work has shown that multi-granularity representations capture diverse scales of human dynamics and motion patterns, effectively integrating this concept into goal-guided frameworks remains challenging. In this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for human Trajectory prediction. MGTraj recursively encodes trajectory proposals from coarse to fine granularity levels. At each level, a transformer-based recursive refinement network (RRN) captures features and predicts progressive refinements. Features across different granularities are integrated using a weight-sharing strategy, and velocity prediction is employed as an auxiliary task to further enhance performance. Comprehensive experimental results in EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline methods and achieves state-of-the-art performance among goal-guided methods.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement</title>
<link>https://arxiv.org/abs/2509.09232</link>
<guid>https://arxiv.org/abs/2509.09232</guid>
<content:encoded><![CDATA[
<div> Universal medical image analysis, In-context learning, Medverse, 3D medical imaging, Autoregressive framework<br />
<br />
Summary: <br />
In this paper, the authors introduce Medverse, a universal in-context learning model for 3D medical imaging. Medverse is trained on 22 diverse datasets to perform tasks such as segmentation, transformation, and enhancement across various organs, imaging modalities, and clinical centers. The model employs a next-scale autoregressive framework to refine predictions from coarse to fine, allowing for high-fidelity outputs and multi-scale anatomical awareness. Additionally, a blockwise cross-attention module is introduced to facilitate long-range interactions while maintaining computational efficiency. Extensive evaluation on a range of datasets demonstrates that Medverse significantly outperforms existing in-context learning baselines. The model represents a novel approach in medical image analysis, offering a unified solution for diverse tasks and anatomical regions. The code and model weights are publicly available on GitHub for further research and development. <div>
arXiv:2509.09232v1 Announce Type: new 
Abstract: In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at https://github.com/jiesihu/Medverse.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification</title>
<link>https://arxiv.org/abs/2509.09242</link>
<guid>https://arxiv.org/abs/2509.09242</guid>
<content:encoded><![CDATA[
<div> classification, gastric tissue images, CoAtNeXt, ConvNeXtV2 blocks, CBAM

Summary:
CoAtNeXt, a novel hybrid model, was proposed for automated classification of gastric tissue images. It integrates ConvNeXtV2 blocks into the CoAtNet architecture and incorporates the Convolutional Block Attention Module (CBAM) for enhanced local feature extraction. The model achieved high accuracy, precision, recall, F1 score, and AUC on two publicly available datasets, outperforming both Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) models. CoAtNeXt demonstrated robust performance in binary and multiclass classification tasks, showcasing its potential to aid pathologists in improving diagnostic accuracy and reducing workload. <div>
arXiv:2509.09242v1 Announce Type: new 
Abstract: Background and objective Early diagnosis of gastric diseases is crucial to prevent fatal outcomes. Although histopathologic examination remains the diagnostic gold standard, it is performed entirely manually, making evaluations labor-intensive and prone to variability among pathologists. Critical findings may be missed, and lack of standard procedures reduces consistency. These limitations highlight the need for automated, reliable, and efficient methods for gastric tissue analysis. Methods In this study, a novel hybrid model named CoAtNeXt was proposed for the classification of gastric tissue images. The model is built upon the CoAtNet architecture by replacing its MBConv layers with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block Attention Module (CBAM) is integrated to improve local feature extraction through channel and spatial attention mechanisms. The architecture was scaled to achieve a balance between computational efficiency and classification performance. CoAtNeXt was evaluated on two publicly available datasets, HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary classification, and was compared against 10 Convolutional Neural Networks (CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved 96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89% AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07% precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all CNN and ViT models tested and surpassed previous studies in the literature. Conclusion Experimental results show that CoAtNeXt is a robust architecture for histopathological classification of gastric tissue images, providing performance on binary and multiclass. Its highlights its potential to assist pathologists by enhancing diagnostic accuracy and reducing workload.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis</title>
<link>https://arxiv.org/abs/2509.09254</link>
<guid>https://arxiv.org/abs/2509.09254</guid>
<content:encoded><![CDATA[
<div> dentistry, panoramic X-rays, LVLMs, MMOral, OralGPT <br />
Summary: <br />
Recent advancements in large vision-language models (LVLMs) have shown promise in general medical tasks, but their effectiveness in dentistry, particularly in interpreting panoramic X-rays, is still unexplored. To address this gap, researchers introduce MMOral, a comprehensive multimodal instruction dataset and benchmark tailored for panoramic X-ray analysis. The dataset includes annotated images and instruction-following instances, covering various tasks related to dentistry. Evaluation on MMOral-Bench reveals limitations in current LVLMs, prompting the development of OralGPT through supervised fine-tuning. OralGPT, trained on the MMOral dataset, shows significant performance improvements, indicating its potential for intelligent dentistry applications. Both MMOral and OralGPT offer a strong foundation for multimodal AI systems in the dental domain. <div>
arXiv:2509.09254v1 Announce Type: new 
Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATE: Dynamic Absolute Time Enhancement for Long Video Understanding</title>
<link>https://arxiv.org/abs/2509.09263</link>
<guid>https://arxiv.org/abs/2509.09263</guid>
<content:encoded><![CDATA[
<div> Keywords: Long video understanding, Multimodal large language models, Temporal reasoning, Event localization, Timestamp Injection Mechanism

Summary: 
Dynamic Absolute Time Enhancement (DATE) proposes a method to improve temporal awareness in multimodal large language models for long video understanding tasks. The approach utilizes the Timestamp Injection Mechanism (TIM) and a Temporal-Aware Similarity Sampling (TASS) strategy to enhance temporal comprehension. By integrating textual timestamp tokens with video frame embeddings, a continuous temporal reference system is established. The method addresses the challenge of long-range dependencies in video understanding by rephrasing the video sampling problem as a vision-language retrieval task. A two-stage algorithm is introduced to ensure semantic relevance and temporal coverage, resulting in improved absolute time understanding and key event localization. The proposed method achieves state-of-the-art performance among large language models on hour-long video benchmarks, surpassing even larger models in some instances. <br /><br />Summary: <div>
arXiv:2509.09263v1 Announce Type: new 
Abstract: Long video understanding remains a fundamental challenge for multimodal large language models (MLLMs), particularly in tasks requiring precise temporal reasoning and event localization. Existing approaches typically adopt uniform frame sampling and rely on implicit position encodings to model temporal order. However, these methods struggle with long-range dependencies, leading to critical information loss and degraded temporal comprehension. In this paper, we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a semantically guided Temporal-Aware Similarity Sampling (TASS) strategy. Specifically, we interleave video frame embeddings with textual timestamp tokens to construct a continuous temporal reference system. We further reformulate the video sampling problem as a vision-language retrieval task and introduce a two-stage algorithm to ensure both semantic relevance and temporal coverage: enriching each query into a descriptive caption to better align with the vision feature, and sampling key event with a similarity-driven temporally regularized greedy strategy. Our method achieves remarkable improvements w.r.t. absolute time understanding and key event localization, resulting in state-of-the-art performance among 7B and 72B models on hour-long video benchmarks. Particularly, our 7B model even exceeds many 72B models on some benchmarks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.09267</link>
<guid>https://arxiv.org/abs/2509.09267</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D medical image segmentation, efficiency, progressive pruning, dynamic, clinical application

Summary:
PSP-Seg is introduced as a dynamic and efficient 3D medical image segmentation framework that utilizes progressive pruning. This approach allows for the iterative removal of redundant modules, enhancing adaptability and resource efficiency. Evaluations on multiple datasets show that the lightweight variant, PSP-Seg-S, achieves comparable performance to nnU-Net while significantly reducing GPU memory usage, training time, and parameter numbers. Specifically, PSP-Seg-S reduces GPU memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87% across various datasets. These results highlight the cost-effectiveness and high performance of PSP-Seg, positioning it as a promising solution for widespread clinical application. 

<br /><br />Summary: <div>
arXiv:2509.09267v1 Announce Type: new 
Abstract: 3D medical image segmentation often faces heavy resource and time consumption, limiting its scalability and rapid deployment in clinical environments. Existing efficient segmentation models are typically static and manually designed prior to training, which restricts their adaptability across diverse tasks and makes it difficult to balance performance with resource efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a redundant model and iteratively prunes redundant modules through a combination of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on five public datasets, benchmarking it against seven state-of-the-art models and six efficient segmentation models. Results demonstrate that the lightweight variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87% across all datasets. These findings underscore PSP-Seg's potential as a cost-effective yet high-performing alternative for widespread clinical application.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Programmability: A Guide for Code-as-Thought in Chart Understanding</title>
<link>https://arxiv.org/abs/2509.09286</link>
<guid>https://arxiv.org/abs/2509.09286</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Chart understanding, Code-as-Thought, Visual Programmability, Reinforcement learning

Summary: 
Vision-Language Models (VLMs) are tested on their reasoning capabilities in chart understanding tasks. Existing approaches have limitations such as reliance on external tools or single reasoning strategies. A new Code-as-Thought (CaT) approach represents visual information in a verifiable, symbolic format. Visual Programmability is introduced as a learnable property to determine the optimal reasoning pathway for each task. A dual-reward system combines data-accuracy and decision rewards to train the model to choose between the CaT pathway and direct visual reasoning. By dynamically selecting the optimal reasoning pathway, VLMs can achieve strong and robust performance across diverse chart-understanding benchmarks. This work demonstrates that VLMs can be trained not only to reason but also how to reason effectively. 

<br /><br />Summary: <div>
arXiv:2509.09286v1 Announce Type: new 
Abstract: Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training</title>
<link>https://arxiv.org/abs/2509.09290</link>
<guid>https://arxiv.org/abs/2509.09290</guid>
<content:encoded><![CDATA[
<div> Segmentation models, brain MRI, multimodal, modality-agnostic, image augmentation <br />
<br />
Summary: <br />
Segmentation models for brain MRI play a crucial role in lesion detection, but most are limited to fixed image modalities. This study introduces a model capable of processing both seen and unseen modalities by incorporating a modality-agnostic input channel in the U-net architecture. To train the modality-agnostic component, an image augmentation scheme is developed to generate artificial MRI modalities. Tests conducted on 8 databases with 5 types of pathologies and 8 modalities show that the model can effectively process both familiar and new modalities to enhance segmentation accuracy. The approach maintains the ability to handle known MRI modalities while also accommodating new modalities encountered during inference, improving overall segmentation performance. <div>
arXiv:2509.09290v1 Announce Type: new 
Abstract: Segmentation models are important tools for the detection and analysis of lesions in brain MRI. Depending on the type of brain pathology that is imaged, MRI scanners can acquire multiple, different image modalities (contrasts). Most segmentation models for multimodal brain MRI are restricted to fixed modalities and cannot effectively process new ones at inference. Some models generalize to unseen modalities but may lose discriminative modality-specific information. This work aims to develop a model that can perform inference on data that contain image modalities unseen during training, previously seen modalities, and heterogeneous combinations of both, thus allowing a user to utilize any available imaging modalities. We demonstrate this is possible with a simple, thus practical alteration to the U-net architecture, by integrating a modality-agnostic input channel or pathway, alongside modality-specific input channels. To train this modality-agnostic component, we develop an image augmentation scheme that synthesizes artificial MRI modalities. Augmentations differentially alter the appearance of pathological and healthy brain tissue to create artificial contrasts between them while maintaining realistic anatomical integrity. We evaluate the method using 8 MRI databases that include 5 types of pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI, DWI, ADC and FLAIR). The results demonstrate that the approach preserves the ability to effectively process MRI modalities encountered during training, while being able to process new, unseen modalities to improve its segmentation. Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception</title>
<link>https://arxiv.org/abs/2509.09297</link>
<guid>https://arxiv.org/abs/2509.09297</guid>
<content:encoded><![CDATA[
<div> Keywords: open-set detection, UAV autonomy, embedding-based detectors, uncertainty estimation, spectral normalization

Summary: 
The article introduces a novel open-set detection framework to improve UAV autonomy in air-to-air object detection. Traditional closed-set detectors are not robust against domain shifts and flight data corruption, which can pose risks in safety-critical applications. The proposed framework is model-agnostic and specifically designed for embedding-based detectors. It handles unknown object rejection and maintains robustness against corrupt flight data by estimating semantic uncertainty through entropy modeling in the embedding space. The framework utilizes spectral normalization and temperature scaling to enhance open-set discrimination, achieving up to a 10% relative AUROC gain compared to standard YOLO-based detectors. Background rejection further enhances robustness without compromising detection accuracy, making the solution well-suited for reliable UAV perception in dynamic air-to-air environments. 

<br /><br />Summary: <div>
arXiv:2509.09297v1 Announce Type: new 
Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object detection under real-world conditions. Traditional closed-set detectors degrade significantly under domain shifts and flight data corruption, posing risks to safety-critical applications. We propose a novel, model-agnostic open-set detection framework designed specifically for embedding-based detectors. The method explicitly handles unknown object rejection while maintaining robustness against corrupted flight data. It estimates semantic uncertainty via entropy modeling in the embedding space and incorporates spectral normalization and temperature scaling to enhance open-set discrimination. We validate our approach on the challenging AOT aerial benchmark and through extensive real-world flight tests. Comprehensive ablation studies demonstrate consistent improvements over baseline methods, achieving up to a 10\% relative AUROC gain compared to standard YOLO-based detectors. Additionally, we show that background rejection further strengthens robustness without compromising detection accuracy, making our solution particularly well-suited for reliable UAV perception in dynamic air-to-air environments.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion</title>
<link>https://arxiv.org/abs/2509.09298</link>
<guid>https://arxiv.org/abs/2509.09298</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic aperture radar, object-centric learning, SlotSAR, target representation, clutter disentanglement

Summary:
SlotSAR is a novel object-centric learning framework designed to address the challenge of extracting target representations from cluttered synthetic aperture radar (SAR) images. The framework utilizes high-level semantic features from SARATR-X and low-level scattering features from a wavelet scattering network to create multi-level representations for robust target characterization. A multi-level slot attention module is introduced to enhance representation distinctiveness and facilitate object-centric learning. SlotSAR surpasses existing methods by preserving structural details in SAR imagery, leading to state-of-the-art performance. This disentanglement of target representations from background clutter is achieved without the need for mask annotations, making SlotSAR a powerful tool for enhancing the interpretation and analysis of SAR images. <div>
arXiv:2509.09298v1 Announce Type: new 
Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest but also complex background clutter, including terrain reflections and speckle noise. In many cases, such clutter exhibits intensity and patterns that resemble targets, leading models to extract entangled or spurious features. Such behavior undermines the ability to form clear target representations, regardless of the classifier. To address this challenge, we propose a novel object-centric learning (OCL) framework, named SlotSAR, that disentangles target representations from background clutter in SAR images without mask annotations. SlotSAR first extracts high-level semantic features from SARATR-X and low-level scattering features from the wavelet scattering network in order to obtain complementary multi-level representations for robust target characterization. We further present a multi-level slot attention module that integrates these low- and high-level features to enhance slot-wise representation distinctiveness, enabling effective OCL. Experimental results demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery by preserving structural details compared to existing OCL methods.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization</title>
<link>https://arxiv.org/abs/2509.09307</link>
<guid>https://arxiv.org/abs/2509.09307</guid>
<content:encoded><![CDATA[
<div> characterization, materials science, multimodal large language models, benchmark, MatCha <br />
Summary:<br />
Materials characterization plays a crucial role in understanding material properties and guiding design. The article introduces MatCha, a benchmark for evaluating multimodal large language models (MLLMs) in materials characterization image understanding. MatCha includes 1,500 expert-level questions across 21 tasks, reflecting real-world challenges in materials science. Evaluation of MLLMs on MatCha indicates a performance gap compared to human experts, especially in tasks requiring high-level expertise and visual perception. Few-shot and chain-of-thought prompting methods struggle to improve model performance. The study underscores the limited adaptability of current MLLMs to complex materials characterization scenarios. MatCha aims to stimulate research in new material discovery and the development of autonomous scientific agents. <div>
arXiv:2509.09307v1 Announce Type: new 
Abstract: Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception</title>
<link>https://arxiv.org/abs/2509.09310</link>
<guid>https://arxiv.org/abs/2509.09310</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative perception, heterogeneous models, few-shot unsupervised domain adaptation, self-training, OPV2V dataset

Summary:
Progressive Heterogeneous Collaborative Perception (PHCP) addresses the challenge of heterogeneous models in collaborative perception without the need for joint training. By dynamically aligning features through self-training an adapter during inference, PHCP eliminates the need for labeled data and achieves strong performance across diverse scenarios. The framework formulates the problem as few-shot unsupervised domain adaptation, demonstrating comparable performance to state-of-the-art methods while using only a small amount of unlabeled data. PHCP offers a practical solution for real-world applications by enabling vehicles to share information and overcome individual perception limitations effectively. The experiments on the OPV2V dataset showcase the effectiveness of PHCP in achieving high performance in heterogeneous collaborative perception scenarios. <br /><br />Summary: <div>
arXiv:2509.09310v1 Announce Type: new 
Abstract: Collaborative perception enables vehicles to overcome individual perception limitations by sharing information, allowing them to see further and through occlusions. In real-world scenarios, models on different vehicles are often heterogeneous due to manufacturer variations. Existing methods for heterogeneous collaborative perception address this challenge by fine-tuning adapters or the entire network to bridge the domain gap. However, these methods are impractical in real-world applications, as each new collaborator must undergo joint training with the ego vehicle on a dataset before inference, or the ego vehicle stores models for all potential collaborators in advance. Therefore, we pose a new question: Can we tackle this challenge directly during inference, eliminating the need for joint training? To answer this, we introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel framework that formulates the problem as few-shot unsupervised domain adaptation. Unlike previous work, PHCP dynamically aligns features by self-training an adapter during inference, eliminating the need for labeled data and joint training. Extensive experiments on the OPV2V dataset demonstrate that PHCP achieves strong performance across diverse heterogeneous scenarios. Notably, PHCP achieves performance comparable to SOTA methods trained on the entire dataset while using only a small amount of unlabeled data.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Recognition with Vision and Language Embeddings of VLMs</title>
<link>https://arxiv.org/abs/2509.09311</link>
<guid>https://arxiv.org/abs/2509.09311</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, image-text alignment, image classification, prompt design, fusion method 

Summary: 
Vision-language models (VLMs) have shown success in zero-shot classification by aligning images with text. This study evaluates the performance of language-guided and vision-only image classification using various VLMs, including established models like SigLIP 2 and recent ones like RADIOv2.5. The evaluation is conducted on the ImageNet-1k validation set and a label-corrected variant to analyze factors influencing accuracy such as prompt design, class diversity, k-NN neighbors, and reference set size. Results reveal that language and vision have complementary strengths, with certain classes benefiting from textual prompts while others from visual similarity. A learning-free fusion method based on per-class precision is introduced to enhance classification performance. The findings highlight the potential of leveraging both modalities for more effective image recognition tasks. The code for this study is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2509.09311v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification through image-text alignment. Yet, their purely visual inference capabilities remain under-explored. In this work, we conduct a comprehensive evaluation of both language-guided and vision-only image classification with a diverse set of dual-encoder VLMs, including both well-established and recent models such as SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the ImageNet-1k validation set and its label-corrected variant. The key factors affecting accuracy are analysed, including prompt design, class diversity, the number of neighbours in k-NN, and reference set size. We show that language and vision offer complementary strengths, with some classes favouring textual prompts and others better handled by visual similarity. To exploit this complementarity, we introduce a simple, learning-free fusion method based on per-class precision that improves classification performance. The code is available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM</title>
<link>https://arxiv.org/abs/2509.09324</link>
<guid>https://arxiv.org/abs/2509.09324</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, fashion design, Better Understanding Generation (BUG), LMM, FashionEdit dataset

Summary:
Generative AI is revolutionizing the fashion industry by empowering designers with large multimodal models that can create unique designs. However, fine-grained customization still faces challenges due to text uncertainty among end-users lacking professional background knowledge. To address this issue, the Better Understanding Generation (BUG) workflow with Language Model for Multimodal (LMM) is proposed. This framework allows for automatic creation and customization of clothing designs based on user chat and image prompts, unleashing creative potential and simplifying clothing design processes. The effectiveness of the model is demonstrated using the FashionEdit dataset, which evaluates generation similarity, user satisfaction, and design quality. The proposed framework aims to enhance the clothing design workflow by providing a user-friendly and automated solution. 

<br /><br />Summary: <div>
arXiv:2509.09324v1 Announce Type: new 
Abstract: Generative AI evolves the execution of complex workflows in industry, where the large multimodal model empowers fashion design in the garment industry. Current generation AI models magically transform brainstorming into fancy designs easily, but the fine-grained customization still suffers from text uncertainty without professional background knowledge from end-users. Thus, we propose the Better Understanding Generation (BUG) workflow with LMM to automatically create and fine-grain customize the cloth designs from chat with image-into-prompt. Our framework unleashes users' creative potential beyond words and also lowers the barriers of clothing design/editing without further human involvement. To prove the effectiveness of our model, we propose a new FashionEdit dataset that simulates the real-world clothing design workflow, evaluated from generation similarity, user satisfaction, and quality. The code and dataset: https://github.com/detectiveli/FashionEdit.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment</title>
<link>https://arxiv.org/abs/2509.09327</link>
<guid>https://arxiv.org/abs/2509.09327</guid>
<content:encoded><![CDATA[
<div> Automated surgical skill assessment, few-shot learning, pre-training strategies, domain similarity, procedure-specific data<br />
<br />
Summary:<br />
Automated surgical skill assessment (SSA) is a crucial task in surgical computer vision but faces challenges due to limited skill annotations. This study explores few-shot learning (FSL) for SSA and evaluates different pre-training sources. The research formulates SSA as a few-shot task and examines the impact of self-supervised pre-training on downstream performance. Small, domain-relevant datasets outperform larger, less aligned ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in 1-, 2-, and 5-shot settings, respectively. Incorporating procedure-specific data into pre-training improves performance, with an average gain of +1.22% in accuracy and +2.28% in F1-score. However, using less similar but large-scale sources can lead to performance degradation. The results highlight the importance of effective pre-training strategies for enhancing surgical skill assessment models. <div>
arXiv:2509.09327v1 Announce Type: new 
Abstract: Automated surgical skill assessment (SSA) is a central task in surgical computer vision. Developing robust SSA models is challenging due to the scarcity of skill annotations, which are time-consuming to produce and require expert consensus. Few-shot learning (FSL) offers a scalable alternative enabling model development with minimal supervision, though its success critically depends on effective pre-training. While widely studied for several surgical downstream tasks, pre-training has remained largely unexplored in SSA. In this work, we formulate SSA as a few-shot task and investigate how self-supervised pre-training strategies affect downstream few-shot SSA performance. We annotate a publicly available robotic surgery dataset with Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate various pre-training sources across three few-shot settings. We quantify domain similarity and analyze how domain gap and the inclusion of procedure-specific data into pre-training influence transferability. Our results show that small but domain-relevant datasets can outperform large scale, less aligned ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot settings, respectively. Moreover, incorporating procedure-specific data into pre-training with a domain-relevant external dataset significantly boosts downstream performance, with an average gain of +1.22% in accuracy and +2.28% in F1-score; however, applying the same strategy with less similar but large-scale sources can instead lead to performance degradation. Code and models are available at https://github.com/anastadimi/ssa-fsl.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.09349</link>
<guid>https://arxiv.org/abs/2509.09349</guid>
<content:encoded><![CDATA[
arXiv:2509.09349v1 Announce Type: new 
Abstract: Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors</title>
<link>https://arxiv.org/abs/2509.09352</link>
<guid>https://arxiv.org/abs/2509.09352</guid>
<content:encoded><![CDATA[
arXiv:2509.09352v1 Announce Type: new 
Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer given a single image. Though this intrinsic image decomposition problem has been studied for decades, it remains a significant challenge in cases of complex scenes, i.e. spatially-varying lighting effect and rich textures. In this paper, we propose a novel method for handling severe lighting and rich textures in intrinsic image decomposition, which enables to produce high-quality intrinsic images for real-world images. Specifically, we observe that previous learning-based methods tend to produce texture-less and over-smoothing intrinsic images, which can be used to infer the lighting and texture information given a RGB image. In this way, we design a texture-guided regularization term and formulate the decomposition problem into an optimization framework, to separate the material textures and lighting effect. We demonstrate that combining the novel texture-aware prior can produce superior results to existing approaches.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection</title>
<link>https://arxiv.org/abs/2509.09365</link>
<guid>https://arxiv.org/abs/2509.09365</guid>
<content:encoded><![CDATA[
arXiv:2509.09365v1 Announce Type: new 
Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data</title>
<link>https://arxiv.org/abs/2509.09368</link>
<guid>https://arxiv.org/abs/2509.09368</guid>
<content:encoded><![CDATA[
arXiv:2509.09368v1 Announce Type: new 
Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral function, thus necessitating monitoring for timely intervention. While lumbar puncture is the gold standard for ICP measurement, its invasiveness and associated risks drive the need for non-invasive alternatives. Optic nerve sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP directly correlates with increased ONSD. However, current clinical practices for ONSD measurement suffer from inconsistency in manual operation, subjectivity in optimal view selection, and variability in thresholding, limiting their reliability. To address these challenges, we introduce a fully automatic two-stage framework for ICP grading, integrating keyframe identification, ONSD measurement and clinical data. Specifically, the fundus ultrasound video processing stage performs frame-level anatomical segmentation, rule-based keyframe identification guided by an international consensus statement, and precise ONSD measurement. The intracranial pressure grading stage then fuses ONSD metrics with clinical features to enable the prediction of ICP grades, thereby demonstrating an innovative blend of interpretable ultrasound analysis and multi-source data integration for objective clinical evaluation. Experimental results demonstrate that our method achieves a validation accuracy of $0.845 \pm 0.071$ (with standard deviation from five-fold cross-validation) and an independent test accuracy of 0.786, significantly outperforming conventional threshold-based method ($0.637 \pm 0.111$ validation accuracy, $0.429$ test accuracy). Through effectively reducing operator variability and integrating multi-source information, our framework establishes a reliable non-invasive approach for clinical ICP evaluation, holding promise for improving patient management in acute neurological conditions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality</title>
<link>https://arxiv.org/abs/2509.09375</link>
<guid>https://arxiv.org/abs/2509.09375</guid>
<content:encoded><![CDATA[
arXiv:2509.09375v1 Announce Type: new 
Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained defects that depress yield and reliability. Most industrial defect segmentation compares a test image against an external normal set, a strategy that is brittle for IC imagery where layouts vary across products and accurate alignment is difficult. We observe that defects are predominantly local, while each image still contains rich, repeatable normal patterns. We therefore propose an unsupervised IC defect segmentation framework that requires no external normal support. A learnable normal-information extractor aggregates representative normal features from the test image, and a coherence loss enforces their association with normal regions. Guided by these features, a decoder reconstructs only normal content; the reconstruction residual then segments defects. Pseudo-anomaly augmentation further stabilizes training. Experiments on datasets from three IC process stages show consistent improvements over existing approaches and strong robustness to product variability.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift</title>
<link>https://arxiv.org/abs/2509.09397</link>
<guid>https://arxiv.org/abs/2509.09397</guid>
<content:encoded><![CDATA[
arXiv:2509.09397v1 Announce Type: new 
Abstract: Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</title>
<link>https://arxiv.org/abs/2509.09427</link>
<guid>https://arxiv.org/abs/2509.09427</guid>
<content:encoded><![CDATA[
arXiv:2509.09427v1 Announce Type: new 
Abstract: As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Concentration for Self-Supervised Dense Representations Learning</title>
<link>https://arxiv.org/abs/2509.09429</link>
<guid>https://arxiv.org/abs/2509.09429</guid>
<content:encoded><![CDATA[
arXiv:2509.09429v1 Announce Type: new 
Abstract: Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model</title>
<link>https://arxiv.org/abs/2509.09456</link>
<guid>https://arxiv.org/abs/2509.09456</guid>
<content:encoded><![CDATA[
arXiv:2509.09456v1 Announce Type: new 
Abstract: Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Efficient Glioma Segmentation on Sub-Saharan MRI</title>
<link>https://arxiv.org/abs/2509.09469</link>
<guid>https://arxiv.org/abs/2509.09469</guid>
<content:encoded><![CDATA[
arXiv:2509.09469v1 Announce Type: new 
Abstract: Gliomas are the most prevalent type of primary brain tumors, and their accurate segmentation from MRI is critical for diagnosis, treatment planning, and longitudinal monitoring. However, the scarcity of high-quality annotated imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for deploying advanced segmentation models in clinical workflows. This study introduces a robust and computationally efficient deep learning framework tailored for resource-constrained settings. We leveraged a 3D Attention UNet architecture augmented with residual blocks and enhanced through transfer learning from pre-trained weights on the BraTS 2021 dataset. Our model was evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma segmentation in SSA MRI data. Despite the limited data quality and quantity, our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80 for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding Non-Functional Hemisphere (SNFH). These results demonstrate the generalizability of the proposed model and its potential to support clinical decision making in low-resource settings. The compact architecture, approximately 90 MB, and sub-minute per-volume inference time on consumer-grade hardware further underscore its practicality for deployment in SSA health systems. This work contributes toward closing the gap in equitable AI for global health by empowering underserved regions with high-performing and accessible medical imaging solutions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.09495</link>
<guid>https://arxiv.org/abs/2509.09495</guid>
<content:encoded><![CDATA[
arXiv:2509.09495v1 Announce Type: new 
Abstract: Deepfakes, synthetic media created using advanced AI techniques, have intensified the spread of misinformation, particularly in politically sensitive contexts. Existing deepfake detection datasets are often limited, relying on outdated generation methods, low realism, or single-face imagery, restricting the effectiveness for general synthetic image detection. By analyzing social media posts, we identify multiple modalities through which deepfakes propagate misinformation. Furthermore, our human perception study demonstrates that recently developed proprietary models produce synthetic images increasingly indistinguishable from real ones, complicating accurate identification by the general public. Consequently, we present a comprehensive, politically-focused dataset specifically crafted for benchmarking detection against modern generative models. This dataset contains three million real images paired with descriptive captions, which are used for generating 963k corresponding high-quality synthetic images from a mix of proprietary and open-source models. Recognizing the continual evolution of generative techniques, we introduce an innovative crowdsourced adversarial platform, where participants are incentivized to generate and submit challenging synthetic images. This ongoing community-driven initiative ensures that deepfake detection methods remain robust and adaptive, proactively safeguarding public discourse from sophisticated misinformation threats.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human Motion Plausibility with Body Momentum</title>
<link>https://arxiv.org/abs/2509.09496</link>
<guid>https://arxiv.org/abs/2509.09496</guid>
<content:encoded><![CDATA[
arXiv:2509.09496v1 Announce Type: new 
Abstract: Many studies decompose human motion into local motion in a frame attached to the root joint and global motion of the root joint in the world frame, treating them separately. However, these two components are not independent. Global movement arises from interactions with the environment, which are, in turn, driven by changes in the body configuration. Motion models often fail to precisely capture this physical coupling between local and global dynamics, while deriving global trajectories from joint torques and external forces is computationally expensive and complex. To address these challenges, we propose using whole-body linear and angular momentum as a constraint to link local motion with global movement. Since momentum reflects the aggregate effect of joint-level dynamics on the body's movement through space, it provides a physically grounded way to relate local joint behavior to global displacement. Building on this insight, we introduce a new loss term that enforces consistency between the generated momentum profiles and those observed in ground-truth data. Incorporating our loss reduces foot sliding and jitter, improves balance, and preserves the accuracy of the recovered motion. Code and data are available at the project page https://hlinhn.github.io/momentum_bmvc.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Region-Wise Correspondence Prediction between Manga Line Art Images</title>
<link>https://arxiv.org/abs/2509.09501</link>
<guid>https://arxiv.org/abs/2509.09501</guid>
<content:encoded><![CDATA[
arXiv:2509.09501v1 Announce Type: new 
Abstract: Understanding region-wise correspondence between manga line art images is a fundamental task in manga processing, enabling downstream applications such as automatic line art colorization and in-between frame generation. However, this task remains largely unexplored, especially in realistic scenarios without pre-existing segmentation or annotations. In this paper, we introduce a novel and practical task: predicting region-wise correspondence between raw manga line art images without any pre-existing labels or masks. To tackle this problem, we divide each line art image into a set of patches and propose a Transformer-based framework that learns patch-level similarities within and across images. We then apply edge-aware clustering and a region matching algorithm to convert patch-level predictions into coherent region-level correspondences. To support training and evaluation, we develop an automatic annotation pipeline and manually refine a subset of the data to construct benchmark datasets. Experiments on multiple datasets demonstrate that our method achieves high patch-level accuracy (e.g., 96.34%) and generates consistent region-level correspondences, highlighting its potential for real-world manga applications.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Diffusion Contrastive Network for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2509.09527</link>
<guid>https://arxiv.org/abs/2509.09527</guid>
<content:encoded><![CDATA[
arXiv:2509.09527v1 Announce Type: new 
Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced under the influence of deep learning. By integrating heterogeneous data from multiple views, MVC enhances clustering analysis, making multi-view fusion critical to clustering performance. However, there is a problem of low-quality data in multi-view fusion. This problem primarily arises from two reasons: 1) Certain views are contaminated by noisy data. 2) Some views suffer from missing data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF) method to address this problem. SGDF leverages a multiple generative mechanism for the multi-view feature of each sample. It is robust to low-quality data. Building on SGDF, we further present the Generative Diffusion Contrastive Network (GDCN). Extensive experiments show that GDCN achieves the state-of-the-art results in deep MVC tasks. The source code is publicly available at https://github.com/HackerHyper/GDCN.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualTrack: Sensorless 3D Ultrasound needs Local and Global Context</title>
<link>https://arxiv.org/abs/2509.09530</link>
<guid>https://arxiv.org/abs/2509.09530</guid>
<content:encoded><![CDATA[
arXiv:2509.09530v1 Announce Type: new 
Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over conventional 2D imaging, yet its widespread adoption is limited by the cost and complexity of traditional 3D systems. Sensorless 3D US, which uses deep learning to estimate a 3D probe trajectory from a sequence of 2D US images, is a promising alternative. Local features, such as speckle patterns, can help predict frame-to-frame motion, while global features, such as coarse shapes and anatomical structures, can situate the scan relative to anatomy and help predict its general shape. In prior approaches, global features are either ignored or tightly coupled with local feature extraction, restricting the ability to robustly model these two complementary aspects. We propose DualTrack, a novel dual-encoder architecture that leverages decoupled local and global encoders specialized for their respective scales of feature extraction. The local encoder uses dense spatiotemporal convolutions to capture fine-grained features, while the global encoder utilizes an image backbone (e.g., a 2D CNN or foundation model) and temporal attention layers to embed high-level anatomical features and long-range dependencies. A lightweight fusion module then combines these features to estimate the trajectory. Experimental results on a large public benchmark show that DualTrack achieves state-of-the-art accuracy and globally consistent 3D reconstructions, outperforming previous methods and yielding an average reconstruction error below 5 mm.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders</title>
<link>https://arxiv.org/abs/2509.09547</link>
<guid>https://arxiv.org/abs/2509.09547</guid>
<content:encoded><![CDATA[
arXiv:2509.09547v1 Announce Type: new 
Abstract: Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</title>
<link>https://arxiv.org/abs/2509.09555</link>
<guid>https://arxiv.org/abs/2509.09555</guid>
<content:encoded><![CDATA[
arXiv:2509.09555v1 Announce Type: new 
Abstract: While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2509.09558</link>
<guid>https://arxiv.org/abs/2509.09558</guid>
<content:encoded><![CDATA[
arXiv:2509.09558v1 Announce Type: new 
Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2509.09572</link>
<guid>https://arxiv.org/abs/2509.09572</guid>
<content:encoded><![CDATA[
arXiv:2509.09572v1 Announce Type: new 
Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Grounding from Event Cameras</title>
<link>https://arxiv.org/abs/2509.09584</link>
<guid>https://arxiv.org/abs/2509.09584</guid>
<content:encoded><![CDATA[
arXiv:2509.09584v1 Announce Type: new 
Abstract: Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</title>
<link>https://arxiv.org/abs/2509.09595</link>
<guid>https://arxiv.org/abs/2509.09595</guid>
<content:encoded><![CDATA[
arXiv:2509.09595v1 Announce Type: new 
Abstract: Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth</title>
<link>https://arxiv.org/abs/2509.09610</link>
<guid>https://arxiv.org/abs/2509.09610</guid>
<content:encoded><![CDATA[
arXiv:2509.09610v1 Announce Type: new 
Abstract: Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Epistemic Humility in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.09658</link>
<guid>https://arxiv.org/abs/2509.09658</guid>
<content:encoded><![CDATA[
arXiv:2509.09658v1 Announce Type: new 
Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Understanding and Generation Truly Benefit Together -- or Just Coexist?</title>
<link>https://arxiv.org/abs/2509.09666</link>
<guid>https://arxiv.org/abs/2509.09666</guid>
<content:encoded><![CDATA[
arXiv:2509.09666v1 Announce Type: new 
Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising "aha moment" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Neural Distance Fields for Learning Human Motion Priors</title>
<link>https://arxiv.org/abs/2509.09667</link>
<guid>https://arxiv.org/abs/2509.09667</guid>
<content:encoded><![CDATA[
arXiv:2509.09667v1 Announce Type: new 
Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locality in Image Diffusion Models Emerges from Data Statistics</title>
<link>https://arxiv.org/abs/2509.09672</link>
<guid>https://arxiv.org/abs/2509.09672</guid>
<content:encoded><![CDATA[
arXiv:2509.09672v1 Announce Type: new 
Abstract: Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</title>
<link>https://arxiv.org/abs/2509.09676</link>
<guid>https://arxiv.org/abs/2509.09676</guid>
<content:encoded><![CDATA[
arXiv:2509.09676v1 Announce Type: new 
Abstract: Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</title>
<link>https://arxiv.org/abs/2509.09680</link>
<guid>https://arxiv.org/abs/2509.09680</guid>
<content:encoded><![CDATA[
arXiv:2509.09680v1 Announce Type: new 
Abstract: The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTV: A neural network approach for total variation minimization</title>
<link>https://arxiv.org/abs/2409.05569</link>
<guid>https://arxiv.org/abs/2409.05569</guid>
<content:encoded><![CDATA[
arXiv:2409.05569v2 Announce Type: cross 
Abstract: Neural network approaches have been demonstrated to work quite well to solve partial differential equations in practice. In this context approaches like physics-informed neural networks and the Deep Ritz method have become popular. In this paper, we propose a similar approach to solve an infinite-dimensional total variation minimization problem using neural networks. We illustrate that the resulting neural network problem does not have a solution in general. To circumvent this theoretic issue, we consider an auxiliary neural network problem, which indeed has a solution, and show that it converges in the sense of $\Gamma$-convergence to the original problem. For computing a numerical solution we further propose a discrete version of the auxiliary neural network problem and again show its $\Gamma$-convergence to the original infinite-dimensional problem. In particular, the $\Gamma$-convergence proof suggests a particular discretization of the total variation. Moreover, we connect the discrete neural network problem to a finite difference discretization of the infinite-dimensional total variation minimization problem. Numerical experiments are presented supporting our theoretical findings.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction</title>
<link>https://arxiv.org/abs/2509.08947</link>
<guid>https://arxiv.org/abs/2509.08947</guid>
<content:encoded><![CDATA[
arXiv:2509.08947v1 Announce Type: cross 
Abstract: Accurate measurement of images produced by electronic displays is critical for the evaluation of both traditional and computational displays. Traditional display measurement methods based on sparse radiometric sampling and fitting a model are inadequate for capturing spatially varying display artifacts, as they fail to capture high-frequency and pixel-level distortions. While cameras offer sufficient spatial resolution, they introduce optical, sampling, and photometric distortions. Furthermore, the physical measurement must be combined with a model of a visual system to assess whether the distortions are going to be visible. To enable perceptual assessment of displays, we propose a combination of a camera-based reconstruction pipeline with a visual difference predictor, which account for both the inaccuracy of camera measurements and visual difference prediction. The reconstruction pipeline combines HDR image stacking, MTF inversion, vignetting correction, geometric undistortion, homography transformation, and color correction, enabling cameras to function as precise display measurement instruments. By incorporating a Visual Difference Predictor (VDP), our system models the visibility of various stimuli under different viewing conditions for the human visual system. We validate the proposed CameraVDP framework through three applications: defective pixel detection, color fringing awareness, and display non-uniformity evaluation. Our uncertainty analysis framework enables the estimation of the theoretical upper bound for defect pixel detection performance and provides confidence intervals for VDP quality scores.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value bounds and Convergence Analysis for Averages of LRP attributions</title>
<link>https://arxiv.org/abs/2509.08963</link>
<guid>https://arxiv.org/abs/2509.08963</guid>
<content:encoded><![CDATA[
arXiv:2509.08963v1 Announce Type: cross 
Abstract: We analyze numerical properties of Layer-wise relevance propagation (LRP)-type attribution methods by representing them as a product of modified gradient matrices. This representation creates an analogy to matrix multiplications of Jacobi-matrices which arise from the chain rule of differentiation. In order to shed light on the distribution of attribution values, we derive upper bounds for singular values. Furthermore we derive component-wise bounds for attribution map values. As a main result, we apply these component-wise bounds to obtain multiplicative constants. These constants govern the convergence of empirical means of attributions to expectations of attribution maps. This finding has important implications for scenarios where multiple non-geometric data augmentations are applied to individual test samples, as well as for Smoothgrad-type attribution methods. In particular, our analysis reveals that the constants for LRP-beta remain independent of weight norms, a significant distinction from both gradient-based methods and LRP-epsilon.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography</title>
<link>https://arxiv.org/abs/2509.08973</link>
<guid>https://arxiv.org/abs/2509.08973</guid>
<content:encoded><![CDATA[
arXiv:2509.08973v1 Announce Type: cross 
Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam computed tomography (CBCT) scans. Although deep learning-based methods show promise in estimating scatter from CBCT measurements, their deployment in mobile CBCT systems or edge devices is still limited due to the large memory footprint of the networks. This study addresses the issue by applying networks at varying resolutions and suggesting an optimal one, based on speed and accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter signal was examined at six resolutions by comparing four interpolation methods. Next, a recent state-of-the-art method was trained across five image resolutions and evaluated for the reductions in floating-point operations (FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold reduction in FLOPs compared to the baseline method, while maintaining comarable performance in terms of mean-absolute-percentage-error (MAPE) and mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to 4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times 10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and 12, respectively. Further experiments comparing scatter-corrected reconstructions on a large, simulated dataset and real CBCT scans from water and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling in deep learning-based scatter estimation. The substantial reduction in FLOPs and GPU memory requirements achieved by our method enables scatter correction in resource-constrained environments, such as mobile CBCT and edge devices.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Vision-Language Models Solve Visual Math Equations?</title>
<link>https://arxiv.org/abs/2509.09013</link>
<guid>https://arxiv.org/abs/2509.09013</guid>
<content:encoded><![CDATA[
arXiv:2509.09013v1 Announce Type: cross 
Abstract: Despite strong performance in visual understanding and language-based reasoning, Vision-Language Models (VLMs) struggle with tasks requiring integrated perception and symbolic computation. We study this limitation through visual equation solving, where mathematical equations are embedded in images, variables are represented by object icons, and coefficients must be inferred by counting. While VLMs perform well on textual equations, they fail on visually grounded counterparts. To understand this gap, we decompose the task into coefficient counting and variable recognition, and find that counting is the primary bottleneck, even when recognition is accurate. We also observe that composing recognition and reasoning introduces additional errors, highlighting challenges in multi-step visual reasoning. Finally, as equation complexity increases, symbolic reasoning itself becomes a limiting factor. These findings reveal key weaknesses in current VLMs and point toward future improvements in visually grounded mathematical reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective</title>
<link>https://arxiv.org/abs/2509.09154</link>
<guid>https://arxiv.org/abs/2509.09154</guid>
<content:encoded><![CDATA[
arXiv:2509.09154v1 Announce Type: cross 
Abstract: Recent advances in agentic AI have led to systems capable of autonomous task execution and language-based reasoning, yet their spatial reasoning abilities remain limited and underexplored, largely constrained to symbolic and sequential processing. In contrast, human spatial intelligence, rooted in integrated multisensory perception, spatial memory, and cognitive maps, enables flexible, context-aware decision-making in unstructured environments. Therefore, bridging this gap is critical for advancing Agentic Spatial Intelligence toward better interaction with the physical 3D world. To this end, we first start from scrutinizing the spatial neural models as studied in computational neuroscience, and accordingly introduce a novel computational framework grounded in neuroscience principles. This framework maps core biological functions to six essential computation modules: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, an artificial cognitive map, spatial memory, and spatial reasoning. Together, these modules form a perspective landscape for agentic spatial reasoning capability across both virtual and physical environments. On top, we conduct a framework-guided analysis of recent methods, evaluating their relevance to each module and identifying critical gaps that hinder the development of more neuroscience-grounded spatial reasoning modules. We further examine emerging benchmarks and datasets and explore potential application domains ranging from virtual to embodied systems, such as robotics. Finally, we outline potential research directions, emphasizing the promising roadmap that can generalize spatial reasoning across dynamic or unstructured environments. We hope this work will benefit the research community with a neuroscience-grounded perspective and a structured pathway. Our project page can be found at Github.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication</title>
<link>https://arxiv.org/abs/2509.09168</link>
<guid>https://arxiv.org/abs/2509.09168</guid>
<content:encoded><![CDATA[
arXiv:2509.09168v1 Announce Type: cross 
Abstract: Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deployment in resource-constrained 6G networks. In this paper, we present a training-free framework for adaptive token merging in pretrained vision transformers to jointly reduce inference time and transmission resource usage. We formulate the selection of per-layer merging proportions as a multi-objective optimization problem to balance accuracy and computational cost. We employ Gaussian process-based Bayesian optimization to construct a Pareto frontier of optimal configurations, enabling flexible runtime adaptation to dynamic application requirements and channel conditions. Extensive experiments demonstrate that our method consistently outperforms other baselines and achieves significant reductions in floating-point operations while maintaining competitive accuracy across a wide range of signal-to-noise ratio (SNR) conditions. Additional results highlight the effectiveness of adaptive policies that adjust merging aggressiveness in response to channel quality, providing a practical mechanism to trade off latency and semantic fidelity on demand. These findings establish a scalable and efficient approach for deploying transformer-based semantic communication in future edge intelligence systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Statistical Similarity Trap in Extreme Convection Detection</title>
<link>https://arxiv.org/abs/2509.09195</link>
<guid>https://arxiv.org/abs/2509.09195</guid>
<content:encoded><![CDATA[
arXiv:2509.09195v1 Announce Type: cross 
Abstract: Current evaluation metrics for deep learning weather models create a "Statistical Similarity Trap", rewarding blurry predictions while missing rare, high-impact events. We provide quantitative evidence of this trap, showing sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous convection detection. We introduce DART (Dual Architecture for Regression Tasks), a framework addressing the challenge of transforming coarse atmospheric forecasts into high-resolution satellite brightness temperature fields optimized for extreme convection detection (below 220 K). DART employs dual-decoder architecture with explicit background/extreme decomposition, physically motivated oversampling, and task-specific loss functions. We present four key findings: (1) empirical validation of the Statistical Similarity Trap across multiple sophisticated baselines; (2) the "IVT Paradox", removing Integrated Water Vapor Transport, widely regarded as essential for atmospheric river analysis, improves extreme convection detection by 270%; (3) architectural necessity demonstrated through operational flexibility (DART achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent CSI), and (4) real-world validation with the August 2023 Chittagong flooding disaster as a case study. To our knowledge, this is the first work to systematically address this hybrid conversion-segmentation-downscaling task, with no direct prior benchmarks identified in existing literature. Our validation against diverse statistical and deep learning baselines sufficiently demonstrates DART's specialized design. The framework enables precise operational calibration through beta-tuning, trains in under 10 minutes on standard hardware, and integrates seamlessly with existing meteorological workflows, demonstrating a pathway toward trustworthy AI for extreme weather preparedness.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery</title>
<link>https://arxiv.org/abs/2509.09227</link>
<guid>https://arxiv.org/abs/2509.09227</guid>
<content:encoded><![CDATA[
arXiv:2509.09227v1 Announce Type: cross 
Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their integration within a multimodal deep learning (DL) framework for predicting postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH) patients. Methods: We utilized a publicly available longitudinal OCT dataset at five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage specific segmentation model delineated related structures, and an automated pipeline extracted quantitative, composite, qualitative, and dynamic features. Binary logistic regression models, constructed with and without dynamic parameters, assessed their incremental predictive value for best-corrected visual acuity (BCVA). A multimodal DL model combining clinical variables, OCT-derived features, and raw OCT images was developed and benchmarked against regression models. Results: The segmentation model achieved high accuracy across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses identified base diameter, ellipsoid zone integrity, and macular hole area as significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates consistently improved logistic regression AUC, especially at the 3-month follow-up. The multimodal DL model outperformed logistic regression, yielding higher AUCs and overall accuracy at each stage. The difference is as high as 0.12, demonstrating the complementary value of raw image volume and dynamic parameters. Conclusions: Integrating dynamic parameters into the multimodal DL model significantly enhances the accuracy of predictions. This fully automated process therefore represents a promising clinical decision support tool for personalized postoperative management in macular hole surgery.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual staining for 3D X-ray histology of bone implants</title>
<link>https://arxiv.org/abs/2509.09235</link>
<guid>https://arxiv.org/abs/2509.09235</guid>
<content:encoded><![CDATA[
arXiv:2509.09235v1 Announce Type: cross 
Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative to conventional 2D histology, enabling volumetric imaging of biological tissues without the need for physical sectioning or chemical staining. However, the inherent greyscale image contrast of X-ray tomography limits its biochemical specificity compared to traditional histological stains. Within digital pathology, deep learning-based virtual staining has demonstrated utility in simulating stained appearances from label-free optical images. In this study, we extend virtual staining to the X-ray domain by applying cross-modality image translation to generate artificially stained slices from synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image pairs of micro-CT and toluidine blue-stained histology from bone-implant samples, we trained a modified CycleGAN network tailored for limited paired data. Whole slide histology images were downsampled to match the voxel size of the CT data, with on-the-fly data augmentation for patch-based training. The model incorporates pixelwise supervision and greyscale consistency terms, producing histologically realistic colour outputs while preserving high-resolution structural detail. Our method outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the model can be applied to full CT volumes to generate virtually stained 3D datasets, enhancing interpretability without additional sample preparation. While features such as new bone formation were able to be reproduced, some variability in the depiction of implant degradation layers highlights the need for further training data and refinement. This work introduces virtual staining to 3D X-ray imaging and offers a scalable route for chemically informative, label-free tissue characterisation in biomedical research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</title>
<link>https://arxiv.org/abs/2509.09332</link>
<guid>https://arxiv.org/abs/2509.09332</guid>
<content:encoded><![CDATA[
arXiv:2509.09332v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Loop Filtering Using Learned Look-Up Tables for Video Coding</title>
<link>https://arxiv.org/abs/2509.09494</link>
<guid>https://arxiv.org/abs/2509.09494</guid>
<content:encoded><![CDATA[
arXiv:2509.09494v1 Announce Type: cross 
Abstract: In-loop filtering (ILF) is a key technology in video coding standards to reduce artifacts and enhance visual quality. Recently, neural network-based ILF schemes have achieved remarkable coding gains, emerging as a powerful candidate for next-generation video coding standards. However, the use of deep neural networks (DNN) brings significant computational and time complexity or high demands for dedicated hardware, making it challenging for general use. To address this limitation, we study a practical ILF solution by adopting look-up tables (LUTs). After training a DNN with a restricted reference range for ILF, all possible inputs are traversed, and the output values of the DNN are cached into LUTs. During the coding process, the filtering process is performed by simply retrieving the filtered pixel through locating the input pixels and interpolating between the cached values, instead of relying on heavy inference computations. In this paper, we propose a universal LUT-based ILF framework, termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of filtering LUTs and propose a series of customized indexing mechanisms to enable better filtering reference perception with limited storage consumption. Second, we propose the cross-component indexing mechanism to enable the filtering of different color components jointly. Third, in order to make our solution practical for coding uses, we propose the LUT compaction scheme to enable the LUT pruning, achieving a lower storage cost of the entire solution. The proposed framework is implemented in the VVC reference software. Experimental results show that the proposed framework achieves on average 0.82%/2.97%/1.63% and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI and RA configurations, respectively. Compared to DNN-based solutions, our proposed solution has much lower time complexity and storage cost.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner</title>
<link>https://arxiv.org/abs/2509.09513</link>
<guid>https://arxiv.org/abs/2509.09513</guid>
<content:encoded><![CDATA[
arXiv:2509.09513v1 Announce Type: cross 
Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjectReact: Learning Object-Relative Control for Visual Navigation</title>
<link>https://arxiv.org/abs/2509.09594</link>
<guid>https://arxiv.org/abs/2509.09594</guid>
<content:encoded><![CDATA[
arXiv:2509.09594v1 Announce Type: cross 
Abstract: Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication</title>
<link>https://arxiv.org/abs/2509.09597</link>
<guid>https://arxiv.org/abs/2509.09597</guid>
<content:encoded><![CDATA[
arXiv:2509.09597v1 Announce Type: cross 
Abstract: Graph alignment-the problem of identifying corresponding nodes across multiple graphs-is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech</title>
<link>https://arxiv.org/abs/2509.09631</link>
<guid>https://arxiv.org/abs/2509.09631</guid>
<content:encoded><![CDATA[
arXiv:2509.09631v1 Announce Type: cross 
Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration</title>
<link>https://arxiv.org/abs/2509.09671</link>
<guid>https://arxiv.org/abs/2509.09671</guid>
<content:encoded><![CDATA[
arXiv:2509.09671v1 Announce Type: cross 
Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale, contact-rich demonstrations and hold promise for scaling dexterous robotic manipulation. Yet demonstration inaccuracies and embodiment gaps between human and robot hands limit the straightforward use of these data. Existing methods adopt a three-stage workflow, including retargeting, tracking, and residual correction, which often leaves demonstrations underused and compound errors across stages. We introduce Dexplore, a unified single-loop optimization that jointly performs retargeting and tracking to learn robot control policies directly from MoCap at scale. Rather than treating demonstrations as ground truth, we use them as soft guidance. From raw trajectories, we derive adaptive spatial scopes, and train with reinforcement learning to keep the policy in-scope while minimizing control effort and accomplishing the task. This unified formulation preserves demonstration intent, enables robot-specific strategies to emerge, improves robustness to noise, and scales to large demonstration corpora. We distill the scaled tracking policy into a vision-based, skill-conditioned generative controller that encodes diverse manipulation skills in a rich latent representation, supporting generalization across objects and real-world deployment. Taken together, these contributions position Dexplore as a principled bridge that transforms imperfect demonstrations into effective training signals for dexterous manipulation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Total Disentanglement of Font Images into Style and Character Class Features</title>
<link>https://arxiv.org/abs/2403.12784</link>
<guid>https://arxiv.org/abs/2403.12784</guid>
<content:encoded><![CDATA[
arXiv:2403.12784v2 Announce Type: replace 
Abstract: In this paper, we demonstrate a total disentanglement of font images. Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features. It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts. These disentangled features guarantee the reconstruction of the original font image. Various experiments have been conducted to understand the performance of total disentanglement. First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?'' Hofstadter (1985). Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation. Code is available here: https://github.com/uchidalab/total_disentanglement
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Augmentation in Images using Language</title>
<link>https://arxiv.org/abs/2404.02353</link>
<guid>https://arxiv.org/abs/2404.02353</guid>
<content:encoded><![CDATA[
arXiv:2404.02353v3 Announce Type: replace 
Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReceiptSense: Beyond Traditional OCR -- A Dataset for Receipt Understanding</title>
<link>https://arxiv.org/abs/2406.04493</link>
<guid>https://arxiv.org/abs/2406.04493</guid>
<content:encoded><![CDATA[
arXiv:2406.04493v2 Announce Type: replace 
Abstract: Multilingual OCR and information extraction from receipts remains challenging, particularly for complex scripts like Arabic. We introduce \dataset, a comprehensive dataset designed for Arabic-English receipt understanding comprising 20,000 annotated receipts from diverse retail settings, 30,000 OCR-annotated images, and 10,000 item-level annotations, and a new Receipt QA subset with 1265 receipt images paired with 40 question-answer pairs each to support LLM evaluation for receipt understanding. The dataset captures merchant names, item descriptions, prices, receipt numbers, and dates to support object detection, OCR, and information extraction tasks. We establish baseline performance using traditional methods (Tesseract OCR) and advanced neural networks, demonstrating the dataset's effectiveness for processing complex, noisy real-world receipt layouts. Our publicly accessible dataset advances automated multilingual document processing research (see https://github.com/Update-For-Integrated-Business-AI/CORU ).
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic infant 2D pose estimation from videos: comparing seven deep neural network methods</title>
<link>https://arxiv.org/abs/2406.17382</link>
<guid>https://arxiv.org/abs/2406.17382</guid>
<content:encoded><![CDATA[
arXiv:2406.17382v4 Announce Type: replace 
Abstract: Automatic markerless estimation of infant posture and motion from ordinary videos carries great potential for movement studies "in the wild", facilitating understanding of motor development and massively increasing the chances of early diagnosis of disorders. There is rapid development of human pose estimation methods in computer vision thanks to advances in deep learning and machine learning. However, these methods are trained on datasets that feature adults in different contexts. This work tests and compares seven popular methods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet, MediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine position and in more complex settings. Surprisingly, all methods except DeepLabCut and MediaPipe have competitive performance without additional finetuning, with ViTPose performing best. Next to standard performance metrics (average precision and recall), we introduce errors expressed in the neck-mid-hip (torso length) ratio and additionally study missed and redundant detections, and the reliability of the internal confidence ratings of the different methods, which are relevant for downstream tasks. Among the networks with competitive performance, only AlphaPose could run close to real time (27 fps) on our machine. We provide documented Docker containers or instructions for all the methods we used, our analysis scripts, and the processed data at https://hub.docker.com/u/humanoidsctu and https://osf.io/x465b/.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Guided Multi-scale Interaction Network for Face Super-Resolution</title>
<link>https://arxiv.org/abs/2409.00591</link>
<guid>https://arxiv.org/abs/2409.00591</guid>
<content:encoded><![CDATA[
arXiv:2409.00591v4 Announce Type: replace 
Abstract: Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multiscale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multiscale interaction network (AMINet), which incorporates local and global feature interactions, as well as encoder-decoder phase feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote the fusion of global features and the local features extracted from different receptive fields by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within the LGFI and encoder-decoder phases. Our above design allows the free flow of multiscale features from within modules and between the encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</title>
<link>https://arxiv.org/abs/2411.00827</link>
<guid>https://arxiv.org/abs/2411.00827</guid>
<content:encoded><![CDATA[
arXiv:2411.00827v5 Announce Type: replace 
Abstract: As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATOR's high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong transferability and automated process, we introduce the VLJailbreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses.VLJailbreakBench is publicly available at https://roywang021.github.io/VLJailbreakBench.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods</title>
<link>https://arxiv.org/abs/2411.10546</link>
<guid>https://arxiv.org/abs/2411.10546</guid>
<content:encoded><![CDATA[
arXiv:2411.10546v2 Announce Type: replace 
Abstract: This paper introduces a large-scale multi-modal dataset captured in and around well-known landmarks in Oxford using a custom-built multi-sensor perception unit as well as a millimetre-accurate map from a Terrestrial LiDAR Scanner (TLS). The perception unit includes three synchronised global shutter colour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all precisely calibrated. We also establish benchmarks for tasks involving localisation, reconstruction, and novel-view synthesis, which enable the evaluation of Simultaneous Localisation and Mapping (SLAM) methods, Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as radiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting. To evaluate 3D reconstruction the TLS 3D models are used as ground truth. Localisation ground truth is computed by registering the mobile LiDAR scans to the TLS 3D models. Radiance field methods are evaluated not only with poses sampled from the input trajectory, but also from viewpoints that are from trajectories which are distant from the training poses. Our evaluation demonstrates a key limitation of state-of-the-art radiance field methods: we show that they tend to overfit to the training poses/images and do not generalise well to out-of-sequence poses. They also underperform in 3D reconstruction compared to MVS systems using the same visual inputs. Our dataset and benchmarks are intended to facilitate better integration of radiance field methods and SLAM systems. The raw and processed data, along with software for parsing and evaluation, can be accessed at https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</title>
<link>https://arxiv.org/abs/2502.05857</link>
<guid>https://arxiv.org/abs/2502.05857</guid>
<content:encoded><![CDATA[
arXiv:2502.05857v3 Announce Type: replace 
Abstract: Learning an agent model that behaves like humans-capable of jointly perceiving the environment, predicting the future, and taking actions from a first-person perspective-is a fundamental challenge in computer vision. Existing methods typically train separate models for these abilities, which fail to capture their intrinsic relationships and prevent them from learning from each other. Inspired by how humans learn through the perception-action loop, we propose EgoAgent, a unified agent model that simultaneously learns to represent, predict, and act within a single transformer. EgoAgent explicitly models the causal and temporal dependencies among these abilities by formulating the task as an interleaved sequence of states and actions. It further introduces a joint embedding-action-prediction architecture with temporally asymmetric predictor and observer branches, enabling synergistic optimization across all three capabilities. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction demonstrate the superiority of our method. The code and trained models will be publicly available at https://github.com/zju3dv/EgoAgent.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForestSplats: Deformable transient field for Gaussian Splatting in the Wild</title>
<link>https://arxiv.org/abs/2503.06179</link>
<guid>https://arxiv.org/abs/2503.06179</guid>
<content:encoded><![CDATA[
arXiv:2503.06179v3 Announce Type: replace 
Abstract: Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time rendering speeds and high-quality results in static scenes. Although 3D-GS shows effectiveness in static scenes, their performance significantly degrades in real-world environments due to transient objects, lighting variations, and diverse levels of occlusion. To tackle this, existing methods estimate occluders or transient elements by leveraging pre-trained models or integrating additional transient field pipelines. However, these methods still suffer from two defects: 1) Using semantic features from the Vision Foundation model (VFM) causes additional computational costs. 2) The transient field requires significant memory to handle transient elements with per-view Gaussians and struggles to define clear boundaries for occluders, solely relying on photometric errors. To address these problems, we propose ForestSplats, a novel approach that leverages the deformable transient field and a superpixel-aware mask to efficiently represent transient elements in the 2D scene across unconstrained image collections and effectively decompose static scenes from transient distractors without VFM. We designed the transient field to be deformable, capturing per-view transient elements. Furthermore, we introduce a superpixel-aware mask that clearly defines the boundaries of occluders by considering photometric errors and superpixels. Additionally, we propose uncertainty-aware densification to avoid generating Gaussians within the boundaries of occluders during densification. Through extensive experiments across several benchmark datasets, we demonstrate that ForestSplats outperforms existing methods without VFM and shows significant memory efficiency in representing transient elements.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models</title>
<link>https://arxiv.org/abs/2504.05815</link>
<guid>https://arxiv.org/abs/2504.05815</guid>
<content:encoded><![CDATA[
arXiv:2504.05815v3 Announce Type: replace 
Abstract: Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called "Parasite" for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. "Parasite" as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, "Parasite" achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at https://anonymous.4open.science/r/Parasite-1715/.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization</title>
<link>https://arxiv.org/abs/2504.21831</link>
<guid>https://arxiv.org/abs/2504.21831</guid>
<content:encoded><![CDATA[
arXiv:2504.21831v2 Announce Type: replace 
Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for Summarization), a lightweight, efficient, and scalable vision language model designed for segment wise video summarization. Leveraging multi modal prompts that combine textual and audio derived signals, DEEVISum incorporates Multi Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement over baseline distillation (0.5%), while EE reduces inference time by approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset, our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing the performance of significantly larger models, all while maintaining a lower computational footprint. We publicly release our code and processed dataset to support further research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Falsification of Speech Videos with Live Optical Signatures (Extended Version)</title>
<link>https://arxiv.org/abs/2504.21846</link>
<guid>https://arxiv.org/abs/2504.21846</guid>
<content:encoded><![CDATA[
arXiv:2504.21846v2 Announce Type: replace 
Abstract: High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes VeriLight, a low-overhead and unobtrusive system for protecting speech videos from visual manipulations of speaker identity and lip and facial motion. Unlike the predominant purely digital falsification detection methods, VeriLight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of VeriLight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds $>$200 bps into video while remaining imperceptible both in video and live. Experiments on extensive video datasets show VeriLight achieves AUCs $\geq$ 0.99 and a true positive rate of 100% in detecting falsified videos. Further, VeriLight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its feature extraction methods. A demonstration of VeriLight is available at https://mobilex.cs.columbia.edu/verilight.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
<link>https://arxiv.org/abs/2505.04119</link>
<guid>https://arxiv.org/abs/2505.04119</guid>
<content:encoded><![CDATA[
arXiv:2505.04119v3 Announce Type: replace 
Abstract: Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-GAPrompt.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdvReal: Physical Adversarial Patch Generation Framework for Security Evaluation of Object Detection Systems</title>
<link>https://arxiv.org/abs/2505.16402</link>
<guid>https://arxiv.org/abs/2505.16402</guid>
<content:encoded><![CDATA[
arXiv:2505.16402v2 Announce Type: replace 
Abstract: Autonomous vehicles are typical complex intelligent systems with artificial intelligence at their core. However, perception methods based on deep learning are extremely vulnerable to adversarial samples, resulting in security accidents. How to generate effective adversarial examples in the physical world and evaluate object detection systems is a huge challenge. In this study, we propose a unified joint adversarial training framework for both 2D and 3D domains, which simultaneously optimizes texture maps in 2D image and 3D mesh spaces to better address intra-class diversity and real-world environmental variations. The framework includes a novel realistic enhanced adversarial module, with time-space and relighting mapping pipeline that adjusts illumination consistency between adversarial patches and target garments under varied viewpoints. Building upon this, we develop a realism enhancement mechanism that incorporates non-rigid deformation modeling and texture remapping to ensure alignment with the human body's non-rigid surfaces in 3D scenes. Extensive experiment results in digital and physical environments demonstrate that the adversarial textures generated by our method can effectively mislead the target detection model. Specifically, our method achieves an average attack success rate (ASR) of 70.13% on YOLOv12 in physical scenarios, significantly outperforming existing methods such as T-SEA (21.65%) and AdvTexture (19.70%). Moreover, the proposed method maintains stable ASR across multiple viewpoints and distances, with an average attack success rate exceeding 90% under both frontal and oblique views at a distance of 4 meters. This confirms the method's strong robustness and transferability under multi-angle attacks, varying lighting conditions, and real-world distances. The demo video and code can be obtained at https://github.com/Huangyh98/AdvReal.git.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.19455</link>
<guid>https://arxiv.org/abs/2505.19455</guid>
<content:encoded><![CDATA[
arXiv:2505.19455v2 Announce Type: replace 
Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs) has achieved promising progress by leveraging prompt tuning to enable continual multi-modal learning. However, most existing methods adopt cross-modal prompt isolation, constructing visual and textual prompts separately, which exacerbates modality imbalance and leads to degraded performance over time. To tackle this issue, we propose MM-Prompt, a novel framework incorporating cross-modal prompt query and cross-modal prompt recovery. The former enables balanced prompt selection by incorporating cross-modal signals during query formation, while the latter promotes joint prompt reconstruction through iterative cross-modal interactions, guided by an alignment loss to prevent representational drift. Extensive experiments show that MM-Prompt surpasses prior approaches in accuracy and knowledge retention, while maintaining balanced modality engagement throughout continual learning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization</title>
<link>https://arxiv.org/abs/2505.19613</link>
<guid>https://arxiv.org/abs/2505.19613</guid>
<content:encoded><![CDATA[
arXiv:2505.19613v2 Announce Type: replace 
Abstract: Adversarial transferability remains a critical challenge in evaluating the robustness of deep neural networks. In security-critical applications, transferability enables black-box attacks without access to model internals, making it a key concern for real-world adversarial threat assessment. While Vision Transformers (ViTs) have demonstrated strong adversarial performance, existing attacks often fail to transfer effectively across architectures, especially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models. In this paper, we introduce \textbf{TESSER} -- a novel adversarial attack framework that enhances transferability via two key strategies: (1) \textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients based on token-wise importance derived from intermediate feature activations, and (2) \textit{Spectral Smoothness Regularization (SSR)}, which suppresses high-frequency noise in perturbations using a differentiable Gaussian prior. These components work in tandem to generate perturbations that are both semantically meaningful and spectrally smooth. Extensive experiments on ImageNet across 12 diverse architectures demonstrate that TESSER achieves +10.9\% higher attack succes rate (ASR) on CNNs and +7.2\% on ViTs compared to the state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER significantly improves robustness against defended models, achieving 53.55\% ASR on adversarially trained CNNs. Qualitative analysis shows strong alignment between TESSER's perturbations and salient visual regions identified via Grad-CAM, while frequency-domain analysis reveals a 12\% reduction in high-frequency energy, confirming the effectiveness of spectral regularization.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound</title>
<link>https://arxiv.org/abs/2506.23538</link>
<guid>https://arxiv.org/abs/2506.23538</guid>
<content:encoded><![CDATA[
arXiv:2506.23538v2 Announce Type: replace 
Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Improved U-Net Model for Offline handwriting signature denoising</title>
<link>https://arxiv.org/abs/2507.00365</link>
<guid>https://arxiv.org/abs/2507.00365</guid>
<content:encoded><![CDATA[
arXiv:2507.00365v2 Announce Type: replace 
Abstract: Handwriting signatures, as an important means of identity recognition, are widely used in multiple fields such as financial transactions, commercial contracts and personal affairs due to their legal effect and uniqueness. In forensic science appraisals, the analysis of offline handwriting signatures requires the appraiser to provide a certain number of signature samples, which are usually derived from various historical contracts or archival materials. However, the provided handwriting samples are often mixed with a large amount of interfering information, which brings severe challenges to handwriting identification work. This study proposes a signature handwriting denoising model based on the improved U-net structure, aiming to enhance the robustness of the signature recognition system. By introducing discrete wavelet transform and PCA transform, the model's ability to suppress noise has been enhanced. The experimental results show that this modelis significantly superior to the traditional methods in denoising effect, can effectively improve the clarity and readability of the signed images, and provide more reliable technical support for signature analysis and recognition.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</title>
<link>https://arxiv.org/abs/2507.00792</link>
<guid>https://arxiv.org/abs/2507.00792</guid>
<content:encoded><![CDATA[
arXiv:2507.00792v3 Announce Type: replace 
Abstract: Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.14456</link>
<guid>https://arxiv.org/abs/2507.14456</guid>
<content:encoded><![CDATA[
arXiv:2507.14456v4 Announce Type: replace 
Abstract: End-to-end autonomous driving requires adaptive and robust handling of complex and diverse traffic environments. However, prevalent single-mode planning methods attempt to learn an overall policy while struggling to acquire diversified driving skills to handle diverse scenarios. Therefore, this paper proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework featuring a Global Expert and a Scene-Adaptive Experts Group, equipped with a Dual-aware Router. Specifically, the Global Expert is trained on the overall dataset, possessing robust performance. The Scene-Adaptive Experts are trained on corresponding scene subsets, achieving adaptive performance. The Dual-aware Router simultaneously considers scenario-level features and routing uncertainty to dynamically activate expert modules. Through the effective coupling of the Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router, GEMINUS achieves both adaptability and robustness across diverse scenarios. GEMINUS outperforms existing methods in the Bench2Drive closed-loop benchmark and achieves state-of-the-art performance in Driving Score and Success Rate, even with only monocular vision input. The code is available at https://github.com/newbrains1/GEMINUS.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization</title>
<link>https://arxiv.org/abs/2508.05211</link>
<guid>https://arxiv.org/abs/2508.05211</guid>
<content:encoded><![CDATA[
arXiv:2508.05211v2 Announce Type: replace 
Abstract: Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging numerous visual tokens for fine-grained visual information, but this token redundancy results in significant computational costs. Previous research aimed at reducing visual tokens during inference typically leverages importance maps derived from attention scores among vision-only tokens or vision-language tokens to prune tokens across one or multiple pruning stages. Despite this progress, pruning frameworks and strategies remain simplistic and insufficiently explored, often resulting in substantial performance degradation. In this paper, we propose VFlowOpt, a token pruning framework that introduces an importance map derivation process and a progressive pruning module with a recycling mechanism. The hyperparameters of its pruning strategy are further optimized by a visual information flow-guided method. Specifically, we compute an importance map for image tokens based on their attention-derived context relevance and patch-level information entropy. We then decide which tokens to retain or prune and aggregate the pruned ones as recycled tokens to avoid potential information loss. Finally, we apply a visual information flow-guided method that regards the last token in the LMM as the most representative signal of text-visual interactions. This method minimizes the discrepancy between token representations in LMMs with and without pruning, thereby enabling superior pruning strategies tailored to different LMMs. Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while maintaining comparable performance, leading to an 89% reduction in KV-Cache memory and 3.8 times faster inference.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Training for Handwritten Mathematical Expression Recognition</title>
<link>https://arxiv.org/abs/2508.09220</link>
<guid>https://arxiv.org/abs/2508.09220</guid>
<content:encoded><![CDATA[
arXiv:2508.09220v3 Announce Type: replace 
Abstract: Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression \textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily due to the arduous and costly process of manual annotation. To bridge this gap, we propose a novel method integrating limited handwritten formulas with large-scale LaTeX-rendered formulas by developing a scalable data engine to generate complex and consistent LaTeX sequences. With this engine, we built the largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80 million high-quality training instances. Then we propose \texttt{TexTeller}, the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a relatively small HME dataset. The expansive training dataset and our refined pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA) performance across nearly all benchmarks. To advance the field, we will openly release our complete model, entire dataset, and full codebase, enabling further research building upon our contributions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glo-UMF: A Unified Multi-model Framework for Automated Morphometry of Glomerular Ultrastructural Characterization</title>
<link>https://arxiv.org/abs/2508.10351</link>
<guid>https://arxiv.org/abs/2508.10351</guid>
<content:encoded><![CDATA[
arXiv:2508.10351v2 Announce Type: replace 
Abstract: Background and Objective: To address the inability of single-model architectures to perform simultaneous analysis of complex glomerular ultrastructures, we developed Glo-UMF, a unified multi-model framework integrating segmentation, classification, and detection to systematically quantify key ultrastructural features. Methods: Glo-UMF decouples quantification tasks by constructing three dedicated deep models: an ultrastructure segmentation model, a glomerular filtration barrier (GFB) region classification model, and an electron-dense deposits (EDD) detection model. Their outputs are integrated through a post-processing workflow with adaptive GFB cropping and measurement location screening, enhancing measurement reliability and providing comprehensive quantitative results that overcome the limitations of traditional grading. Results: Trained on 372 electron microscopy images, Glo-UMF enables simultaneous quantification of glomerular basement membrane (GBM) thickness, the degree of foot process effacement (FPE), and EDD location. In 115 test cases spanning 9 renal pathological types, the automated quantification results showed strong agreement with pathological reports, with an average processing time of 4.23$\pm$0.48 seconds per case on a CPU environment. Conclusions: The modular design of Glo-UMF allows for flexible extensibility, supporting the joint quantification of multiple features. This framework ensures robust generalization and clinical applicability, demonstrating significant potential as an efficient auxiliary tool in glomerular pathological analysis.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^2$-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models</title>
<link>https://arxiv.org/abs/2508.12880</link>
<guid>https://arxiv.org/abs/2508.12880</guid>
<content:encoded><![CDATA[
arXiv:2508.12880v2 Announce Type: replace 
Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism</title>
<link>https://arxiv.org/abs/2508.16884</link>
<guid>https://arxiv.org/abs/2508.16884</guid>
<content:encoded><![CDATA[
arXiv:2508.16884v2 Announce Type: replace 
Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its strong long-range dependency modelling ability. \textcolor{blue}{However, its large model size and weak local feature modeling ability hinder its application in real scenarios. To balance computation efficiency and performance in downstream vision tasks, we propose an efficient ViT model with sparse attention (dubbed SAEViT) and convolution blocks. Specifically, a Sparsely Aggregated Attention (SAA) module has been proposed to perform adaptive sparse sampling and recover the feature map via deconvolution operation,} which significantly reduces the computational complexity of attention operations. In addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed to enhance inter-channel information exchange through feature decomposition and redistribution, which mitigates the redundancy in traditional feed-forward networks (FFN). Finally, a hierarchical pyramid structure with embedded depth-wise separable convolutional blocks (DWSConv) is devised to further strengthen convolutional features. Extensive experiments on mainstream datasets show that SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively, demonstrating a lightweight solution for fundamental vision tasks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels</title>
<link>https://arxiv.org/abs/2508.20193</link>
<guid>https://arxiv.org/abs/2508.20193</guid>
<content:encoded><![CDATA[
arXiv:2508.20193v2 Announce Type: replace 
Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio, spectrum monitoring, and secure wireless communication. However, existing solutions often rely on large labeled datasets or multi-stage training pipelines, which limit scalability and generalization in practice. We propose a unified Vision Transformer (ViT) framework that integrates supervised, self-supervised, and reconstruction objectives. The model combines a ViT encoder, a lightweight convolutional decoder, and a linear classifier; the reconstruction branch maps augmented signals back to their originals, anchoring the encoder to fine-grained I/Q structure. This strategy promotes robust, discriminative feature learning during pretraining, while partial label supervision in fine-tuning enables effective classification with limited labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and ViT baselines in low-label regimes, approaches ResNet-level accuracy with only 15-20% labeled data, and maintains strong performance across varying SNR levels. Overall, the framework provides a simple, generalizable, and label-efficient solution for AMR.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Alignment in LVLMs with Debiased Self-Judgment</title>
<link>https://arxiv.org/abs/2508.20655</link>
<guid>https://arxiv.org/abs/2508.20655</guid>
<content:encoded><![CDATA[
arXiv:2508.20655v2 Announce Type: replace 
Abstract: The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis</title>
<link>https://arxiv.org/abs/2508.20877</link>
<guid>https://arxiv.org/abs/2508.20877</guid>
<content:encoded><![CDATA[
arXiv:2508.20877v2 Announce Type: replace 
Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt</title>
<link>https://arxiv.org/abs/2509.01704</link>
<guid>https://arxiv.org/abs/2509.01704</guid>
<content:encoded><![CDATA[
arXiv:2509.01704v2 Announce Type: replace 
Abstract: Accurate classification of rock sizes is a vital component in geotechnical engineering, mining, and resource management, where precise estimation influences operational efficiency and safety. In this paper, we propose an enhanced deep learning model based on the ConvNeXt architecture, augmented with both self-attention and channel attention mechanisms. Building upon the foundation of ConvNext, our proposed model, termed CNSCA, introduces self-attention to capture long-range spatial dependencies and channel attention to emphasize informative feature channels. This hybrid design enables the model to effectively capture both fine-grained local patterns and broader contextual relationships within rock imagery, leading to improved classification accuracy and robustness. We evaluate our model on a rock size classification dataset and compare it against three strong baseline. The results demonstrate that the incorporation of attention mechanisms significantly enhances the models capability for fine-grained classification tasks involving natural textures like rocks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty</title>
<link>https://arxiv.org/abs/2301.00349</link>
<guid>https://arxiv.org/abs/2301.00349</guid>
<content:encoded><![CDATA[
arXiv:2301.00349v4 Announce Type: replace-cross 
Abstract: Medical image segmentation is critical for disease diagnosis and treatment assessment. However, concerns regarding the reliability of segmentation regions persist among clinicians, mainly attributed to the absence of confidence assessment, robustness, and calibration to accuracy. To address this, we introduce DEviS, an easily implementable foundational model that seamlessly integrates into various medical image segmentation networks. DEviS not only enhances the calibration and robustness of baseline segmentation accuracy but also provides high-efficiency uncertainty estimation for reliable predictions. By leveraging subjective logic theory, we explicitly model probability and uncertainty for medical image segmentation. Here, the Dirichlet distribution parameterizes the distribution of probabilities for different classes of the segmentation results. To generate calibrated predictions and uncertainty, we develop a trainable calibrated uncertainty penalty. Furthermore, DEviS incorporates an uncertainty-aware filtering module, which designs the metric of uncertainty-calibrated error to filter out-of-distribution data. We conducted validation studies on publicly available datasets, including ISIC2018, KiTS2021, LiTS2017, and BraTS2019, to assess the accuracy and robustness of different backbone segmentation models enhanced by DEviS, as well as the efficiency and reliability of uncertainty estimation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images</title>
<link>https://arxiv.org/abs/2405.03486</link>
<guid>https://arxiv.org/abs/2405.03486</guid>
<content:encoded><![CDATA[
arXiv:2405.03486v3 Announce Type: replace-cross 
Abstract: With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which improves the effectiveness and robustness of existing classifiers, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sigma Flows for Image and Data Labeling and Learning Structured Prediction</title>
<link>https://arxiv.org/abs/2408.15946</link>
<guid>https://arxiv.org/abs/2408.15946</guid>
<content:encoded><![CDATA[
arXiv:2408.15946v2 Announce Type: replace-cross 
Abstract: This paper introduces the sigma flow model for the prediction of structured labelings of data observed on Riemannian manifolds, including Euclidean image domains as special case. The approach combines the Laplace-Beltrami framework for image denoising and enhancement, introduced by Sochen, Kimmel and Malladi about 25 years ago, and the assignment flow approach introduced and studied by the authors.
  The sigma flow arises as Riemannian gradient flow of generalized harmonic energies and thus is governed by a nonlinear geometric PDE which determines a harmonic map from a closed Riemannian domain manifold to a statistical manifold, equipped with the Fisher-Rao metric from information geometry. A specific ingredient of the sigma flow is the mutual dependency of the Riemannian metric of the domain manifold on the evolving state. This makes the approach amenable to machine learning in a specific way, by realizing this dependency through a mapping with compact time-variant parametrization that can be learned from data. Proof of concept experiments demonstrate the expressivity of the sigma flow model and prediction performance.
  Structural similarities to transformer network architectures and networks generated by the geometric integration of sigma flows are pointed out, which highlights the connection to deep learning and, conversely, may stimulate the use of geometric design principles for structured prediction in other areas of scientific machine learning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction</title>
<link>https://arxiv.org/abs/2501.01087</link>
<guid>https://arxiv.org/abs/2501.01087</guid>
<content:encoded><![CDATA[
arXiv:2501.01087v4 Announce Type: replace-cross 
Abstract: Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, \textit{Gaussian-activated Linear model (GLinear)}, for multivariate TSF that exploits periodic patterns to provide better accuracy. It achieves higher prediction accuracy while requiring less historical data than other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictors (Autoformer) shows that the GLinear, despite being data efficient, outperforms the existing architectures in most cases of multivariate TSF while being competitive in others. We hope that the proposed GLinear model opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2502.02657</link>
<guid>https://arxiv.org/abs/2502.02657</guid>
<content:encoded><![CDATA[
arXiv:2502.02657v2 Announce Type: replace-cross 
Abstract: We present a neural radiance field (NeRF) based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photorealistic texture. Our system adopts the state-of-the-art NeRF representation to incorporate lidar. Adding lidar data adds strong geometric constraints on the depth and surface normals, which is particularly useful when modelling uniform texture surfaces which contain ambiguous visual reconstruction cues. A key contribution of this work is a novel method to quantify the epistemic uncertainty of the lidar-visual NeRF reconstruction by estimating the spatial variance of each point location in the radiance field given the sensor observations from the cameras and lidar. This provides a principled approach to evaluate the contribution of each sensor modality to the final reconstruction. In this way, reconstructions that are uncertain (due to e.g. uniform visual texture, limited observation viewpoints, or little lidar coverage) can be identified and removed. Our system is integrated with a real-time lidar SLAM system which is used to bootstrap a Structure-from-Motion (SfM) reconstruction procedure. It also helps to properly constrain the overall metric scale which is essential for the lidar depth loss. The refined SLAM trajectory can then be divided into submaps using Spectral Clustering to group sets of co-visible images together. This submapping approach is more suitable for visual reconstruction than distance-based partitioning. Our uncertainty estimation is particularly effective when merging submaps as their boundaries often contain artefacts due to limited observations. We demonstrate the reconstruction system using a multi-camera, lidar sensor suite in experiments involving both robot-mounted and handheld scanning. Our test datasets cover a total area of more than 20,000 square metres.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-HOP: Visuo-Haptic 6D Object Pose Tracking</title>
<link>https://arxiv.org/abs/2502.17434</link>
<guid>https://arxiv.org/abs/2502.17434</guid>
<content:encoded><![CDATA[
arXiv:2502.17434v2 Announce Type: replace-cross 
Abstract: Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Project website: https://ivl.cs.brown.edu/research/v-hop
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses in Human-Robot Bartending Tasks</title>
<link>https://arxiv.org/abs/2503.04308</link>
<guid>https://arxiv.org/abs/2503.04308</guid>
<content:encoded><![CDATA[
arXiv:2503.04308v3 Announce Type: replace-cross 
Abstract: Datasets for object detection often do not account for enough variety of glasses, due to their transparent and reflective properties. Specifically, open-vocabulary object detectors, widely used in embodied robotic agents, fail to distinguish subclasses of glasses. This scientific gap poses an issue for robotic applications that suffer from accumulating errors between detection, planning, and action execution. This paper introduces a novel method for acquiring real-world data from RGB-D sensors that minimizes human effort. We propose an auto-labeling pipeline that generates labels for all the acquired frames based on the depth measurements. We provide a novel real-world glass object dataset GlassNICOLDataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a humanoid robot platform. The dataset consists of 7850 images recorded from five different cameras. We show that our trained baseline model outperforms state-of-the-art open-vocabulary approaches. In addition, we deploy our baseline model in an embodied agent approach to the NICOL platform, on which it achieves a success rate of 81% in a human-robot bartending scenario.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation</title>
<link>https://arxiv.org/abs/2505.07687</link>
<guid>https://arxiv.org/abs/2505.07687</guid>
<content:encoded><![CDATA[
arXiv:2505.07687v2 Announce Type: replace-cross 
Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at https://github.com/gatina-yone/ABS-Mamba
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07815</link>
<guid>https://arxiv.org/abs/2505.07815</guid>
<content:encoded><![CDATA[
arXiv:2505.07815v3 Announce Type: replace-cross 
Abstract: Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net</title>
<link>https://arxiv.org/abs/2505.09521</link>
<guid>https://arxiv.org/abs/2505.09521</guid>
<content:encoded><![CDATA[
arXiv:2505.09521v2 Announce Type: replace-cross 
Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain Convolutional Neural Networks (CNNs) that fail to capture cross-channel time-frequency cues or on heavy transformer/Generative Adversarial Network (GAN) decoders that strain memory and stability. To address these limitations, we propose Spec2VolCAMU-Net, a lightweight architecture featuring a Multi-directional Time-Frequency Convolutional Attention Encoder for rich feature extraction and a Vision-Mamba U-Net decoder that uses linear-time state-space blocks for efficient long-range spatial modelling. We frame the goal of this work as establishing a new state of the art in the spatial fidelity of single-volume reconstruction, a foundational prerequisite for the ultimate aim of generating temporally coherent fMRI time series. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording Structural Similarity Index (SSIM) of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive Signal-to-Noise Ratio (PSNR) scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism</title>
<link>https://arxiv.org/abs/2506.24074</link>
<guid>https://arxiv.org/abs/2506.24074</guid>
<content:encoded><![CDATA[
arXiv:2506.24074v2 Announce Type: replace-cross 
Abstract: Spatial computer vision techniques have the potential to improve the diagnostic performance of colonoscopy. However, the lack of 3D colonoscopy datasets for training and validation hinders their development. This paper introduces C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video Dataset, featuring enhanced realism designed to facilitate the quantitative evaluation of 3D colon reconstruction algorithms. 192 video sequences totaling 169,371 frames were captured by imaging 60 unique, high-fidelity silicone colon phantom segments. Ground truth depth, surface normals, optical flow, occlusion, diffuse maps, six-degree-of-freedom pose, coverage map, and 3D models are provided for 169 colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a gastroenterologist are provided with ground truth poses. Lastly, the dataset includes 15 videos with colon deformations for qualitative assessment. C3VDv2 emulates diverse and challenging scenarios for 3D reconstruction algorithms, including fecal debris, mucous pools, blood, debris obscuring the colonoscope lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2 will allow for more robust and representative development and evaluation of 3D reconstruction algorithms. Project Page - https://durrlab.github.io/C3VDv2/
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</title>
<link>https://arxiv.org/abs/2507.05148</link>
<guid>https://arxiv.org/abs/2507.05148</guid>
<content:encoded><![CDATA[
arXiv:2507.05148v2 Announce Type: replace-cross 
Abstract: X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Organized Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)</title>
<link>https://arxiv.org/abs/2508.03762</link>
<guid>https://arxiv.org/abs/2508.03762</guid>
<content:encoded><![CDATA[
arXiv:2508.03762v2 Announce Type: replace-cross 
Abstract: In this intercontinental, confirmatory study, we include a retrospective cohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries) to train and externally validate the PI-CAI-2B model, i.e., an efficient, next-generation iteration of the state-of-the-art AI system that was developed for detecting Gleason grade group $\geq$2 prostate cancer on MRI during the PI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities in 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12 independent centers based in Europe, North America, Asia and Africa, are used for training and internal testing. Additionally, 2010 cases (2010 patients; 20 external cities in 12 countries) from population-based screening (STHLM3-MRI, IP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in Europe, North and South Americas, Asia and Australia, are used for external testing. Primary endpoint is the proportion of AI-based assessments in agreement with the standard of care diagnoses (i.e., clinical assessments made by expert uropathologists on histopathology, if available, or at least two expert urogenital radiologists in consensus; with access to patient history and peer consultation) in the detection of Gleason grade group $\geq$2 prostate cancer within the external testing cohorts. Our statistical analysis plan is prespecified with a hypothesis of diagnostic interchangeability to the standard of care at the PI-RADS $\geq$3 (primary diagnosis) or $\geq$4 (screening) cut-off, considering an absolute margin of 0.05 and reader estimates derived from the PI-CAI observer study (62 radiologists reading 400 cases). Secondary measures comprise the area under the receiver operating characteristic curve (AUROC) of the AI system stratified by imaging quality, patient age and patient ethnicity to identify underlying biases (if any).
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preprocessing Algorithm Leveraging Geometric Modeling for Scale Correction in Hyperspectral Images for Improved Unmixing Performance</title>
<link>https://arxiv.org/abs/2508.08431</link>
<guid>https://arxiv.org/abs/2508.08431</guid>
<content:encoded><![CDATA[
arXiv:2508.08431v2 Announce Type: replace-cross 
Abstract: Spectral variability significantly impacts the accuracy and convergence of hyperspectral unmixing algorithms. Many methods address complex spectral variability; yet large-scale distortions to the scale of the observed pixel signatures due to topography, illumination, and shadowing remain a major challenge. These variations often degrade unmixing performance and complicate model fitting. Because of this, correcting these variations can offer significant advantages in real-world GIS applications. In this paper, we propose a novel preprocessing algorithm that corrects scale-induced spectral variability prior to unmixing. By estimating and correcting these distortions to the scale of the pixel signatures, the algorithm produces pixel signatures with minimal distortions in scale. Since these distortions in scale (which hinder the performance of many unmixing methods) are greatly minimized in the output provided by the proposed method, the abundance estimation of the unmixing algorithms is significantly improved. We present a rigorous mathematical framework to describe and correct for scale variability and provide extensive experimental validation of the proposed algorithm. Furthermore, the algorithm's impact is evaluated across a wide range of state-of-the-art unmixing methods on two synthetic and two real hyperspectral datasets. The proposed preprocessing step consistently improves the performance of these algorithms, achieving error reductions of around 50%, even for algorithms specifically designed to handle spectral variability. This demonstrates that scale correction acts as a complementary step, facilitating more accurate unmixing with existing methods. The algorithm's generality, consistent impact, and significant influence highlight its potential as a key component in practical hyperspectral unmixing pipelines. The implementation code will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
<link>https://arxiv.org/abs/2509.01106</link>
<guid>https://arxiv.org/abs/2509.01106</guid>
<content:encoded><![CDATA[
arXiv:2509.01106v2 Announce Type: replace-cross 
Abstract: We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Sparse Attention for Faster Video Diffusion Training</title>
<link>https://arxiv.org/abs/2509.01085</link>
<guid>https://arxiv.org/abs/2509.01085</guid>
<content:encoded><![CDATA[
<div> Keywords: Video diffusion Transformer, Bidirectional Sparse Attention, training efficiency, generative quality, computational complexity

Summary:
The article addresses the challenges faced by Video diffusion Transformer (DiT) models in producing high-resolution, long-duration videos due to computational bottlenecks. The proposed Bidirectional Sparse Attention (BSA) framework aims to improve training and inference efficiency by dynamically sparsifying Queries and Key-Value pairs within 3D full attention. This approach optimizes query sparsity through semantic similarity and dynamic spatial-time training, while achieving Key-Value sparsity by retaining only the most salient blocks for computation. Experiments show that BSA accelerates DiT training by reducing FLOPs by up to 20x and achieving 17.79x faster attention training. Despite these efficiency improvements, the generative quality of the models is preserved or even improved when compared to full attention models. <div>
arXiv:2509.01085v2 Announce Type: replace 
Abstract: Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery Aligned With Canopy Height Maps</title>
<link>https://arxiv.org/abs/2509.01202</link>
<guid>https://arxiv.org/abs/2509.01202</guid>
<content:encoded><![CDATA[
<div> LiDAR, canopy height maps, deep learning, forest monitoring, PrediTree<br />
Summary:<br />
The PrediTree dataset is introduced for training and evaluating tree height prediction models at sub-meter resolution. It combines high-resolution LiDAR-derived canopy height maps with multi-temporal and multi-spectral imagery from diverse forest ecosystems in France. This dataset enables the training of deep learning methods to predict tree growth based on past observations, filling a critical monitoring gap. An encoder-decoder framework is proposed to utilize the dataset, considering the relative time differences between the canopy height map and image acquisition dates. Experiments show that a U-Net architecture trained on PrediTree achieves the lowest error rate among tested models, outperforming ResNet-50 by 12% and reducing error by 30% compared to experiments with fewer bands. The dataset is publicly available, along with processing and training codebases for further research and applications. <br /><br />Summary: <div>
arXiv:2509.01202v2 Announce Type: replace 
Abstract: We present PrediTree, the first comprehensive open-source dataset designed for training and evaluating tree height prediction models at sub-meter resolution. This dataset combines very high-resolution (0.5m) LiDAR-derived canopy height maps, spatially aligned with multi-temporal and multi-spectral imagery, across diverse forest ecosystems in France, totaling 3,141,568 images. PrediTree addresses a critical gap in forest monitoring capabilities by enabling the training of deep learning methods that can predict tree growth based on multiple past observations. To make use of this PrediTree dataset, we propose an encoder-decoder framework that requires the multi-temporal multi-spectral imagery and the relative time differences in years between the canopy height map timestamp (target) and each image acquisition date for which this framework predicts the canopy height. The conducted experiments demonstrate that a U-Net architecture trained on the PrediTree dataset provides the highest masked mean squared error of $11.78\%$, outperforming the next-best architecture, ResNet-50, by around $12\%$, and cutting the error of the same experiments but on fewer bands (red, green, blue only), by around $30\%$. This dataset is publicly available on https://huggingface.co/datasets/hiyam-d/PrediTree, and both processing and training codebases are available on {GitHub}.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</title>
<link>https://arxiv.org/abs/2509.01907</link>
<guid>https://arxiv.org/abs/2509.01907</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing, Disaster Monitoring, Image Pairs, Textual Annotations, Vision-Language Models

Summary:
The article introduces the Remote Sensing Change Caption (RSCC) dataset, a comprehensive benchmark consisting of over 62,000 pre- and post-disaster image pairs with detailed change captions. This dataset addresses the lack of temporal image pairs and textual annotations in existing remote sensing datasets, enabling a more accurate understanding of disaster impacts over time. By bridging the gap between temporal and semantic information in remote sensing data, RSCC facilitates the training and evaluation of vision-language models for disaster-aware bi-temporal analysis. The results demonstrate the dataset's effectiveness in supporting detailed disaster-related analysis and pave the way for enhanced vision-language applications in remote sensing. The code and dataset for RSCC are readily available for further research and development at https://github.com/Bili-Sakura/RSCC. 

<br /><br />Summary: 
- RSCC dataset comprises pre- and post-disaster image pairs with detailed change captions. 
- It enables robust training of vision-language models for disaster monitoring. 
- Addresses the lack of temporal image pairs and textual annotations in existing datasets. 
- Facilitates accurate and interpretable disaster-related analysis in remote sensing. 
- Dataset and code are accessible for further research and development. <div>
arXiv:2509.01907v3 Announce Type: replace 
Abstract: Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D and 4D World Modeling: A Survey</title>
<link>https://arxiv.org/abs/2509.07996</link>
<guid>https://arxiv.org/abs/2509.07996</guid>
<content:encoded><![CDATA[
<div> Keywords: world modeling, 3D representation, generative methods, structured taxonomy, evaluation metrics 

Summary: 
World modeling in the field of artificial intelligence has been a crucial research area for agents to understand and predict dynamic environments. This survey focuses on 3D and 4D world modeling, filling gaps in existing literature that often overlook these representations. It introduces a structured taxonomy categorizing approaches into VideoGen, OccGen, and LiDARGen, based on different representations. The survey also defines precise terms and presents datasets and evaluation metrics specifically designed for 3D/4D settings. Practical applications of world modeling are discussed, along with open challenges in the field. The survey aims to provide a foundational reference for researchers working on 3D and 4D world modeling, highlighting promising research directions for further advancements in the field.<br /><br />Summary: <div>
arXiv:2509.07996v1 Announce Type: new 
Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities</title>
<link>https://arxiv.org/abs/2509.08003</link>
<guid>https://arxiv.org/abs/2509.08003</guid>
<content:encoded><![CDATA[
<div> Hierarchical Cross-Modal Gated Attention, Heterogeneous Convolutional Adaptive Multi-Scale Attention, Cascading Convolutional Transformer Feature Refinement, urban flooding, deep-learning<br />
Summary:<br />
In response to the challenges posed by traditional flood detection methods in the face of escalating urban flooding due to climate change, XFloodNet, a novel framework, is introduced. XFloodNet incorporates advanced deep-learning techniques through three key components: a Hierarchical Cross-Modal Gated Attention mechanism for precise feature alignment, a Heterogeneous Convolutional Adaptive Multi-Scale Attention module for feature extraction, and a Cascading Convolutional Transformer Feature Refinement technique for noise-resistant flood detection. Evaluation on benchmark datasets showcases XFloodNet's state-of-the-art performance in flood classification, surpassing existing methods with significantly higher F1-scores of 93.33%, 82.24%, and 88.60% for the Chennai Floods, Rhine18 Floods, and Harz17 Floods datasets respectively. <div>
arXiv:2509.08003v1 Announce Type: new 
Abstract: In an era of escalating climate change, urban flooding has emerged as a critical challenge for sustainable cities, threatening lives, infrastructure, and ecosystems. Traditional flood detection methods are constrained by their reliance on unimodal data and static rule-based systems, which fail to capture the dynamic, non-linear relationships inherent in flood events. Furthermore, existing attention mechanisms and ensemble learning approaches exhibit limitations in hierarchical refinement, cross-modal feature integration, and adaptability to noisy or unstructured environments, resulting in suboptimal flood classification performance. To address these challenges, we present XFloodNet, a novel framework that redefines urban flood classification through advanced deep-learning techniques. XFloodNet integrates three novel components: (1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically aligns visual and textual features, enabling precise multi-granularity interactions and resolving contextual ambiguities; (2) a Heterogeneous Convolutional Adaptive Multi-Scale Attention module, which leverages frequency-enhanced channel attention and frequency-modulated spatial attention to extract and prioritize discriminative flood-related features across spectral and spatial domains; and (3) a Cascading Convolutional Transformer Feature Refinement technique that harmonizes hierarchical features through adaptive scaling and cascading operations, ensuring robust and noise-resistant flood detection. We evaluate our proposed method on three benchmark datasets, such as Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively, surpassing existing methods by significant margins.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
<link>https://arxiv.org/abs/2509.08016</link>
<guid>https://arxiv.org/abs/2509.08016</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, Video Parallel Scaling, temporal detail, computational costs, performance improvement

Summary: 
Video Large Language Models (VideoLLMs) face challenges in capturing fine-grained temporal detail while minimizing computational costs. The proposed solution, Video Parallel Scaling (VPS), expands a model's perceptual bandwidth without increasing its context window by running multiple parallel inference streams. By aggregating the output probabilities from these streams, VPS effectively leverages uncorrelated visual evidence to improve performance without additional training. The approach contracts the Chinchilla scaling law, enhancing temporal reasoning capabilities across various model architectures and scales. VPS outperforms other parallel alternatives and complements other decoding strategies, offering a memory-efficient and robust framework for VideoLLMs. Extensive experiments demonstrate the consistent and significant performance improvements enabled by VPS on benchmarks such as Video-MME and EventHallusion.  

<br /><br />Summary: <div>
arXiv:2509.08016v1 Announce Type: new 
Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change</title>
<link>https://arxiv.org/abs/2509.08024</link>
<guid>https://arxiv.org/abs/2509.08024</guid>
<content:encoded><![CDATA[
<div> keywords: stance detection, multimodal, hierarchical fusion, Large Language Model, transformer module<br />
Summary:<br />
The article introduces a new multimodal stance detection framework that integrates textual and visual information through a hierarchical fusion approach. It utilizes a Large Language Model to retrieve stance-relevant summaries from text and a domain-aware image caption generator to interpret visual content. A specialized transformer module then captures interactions between the texts and images for joint modeling. The proposed framework achieves an accuracy of 76.2%, precision of 76.3%, recall of 76.2%, and F1-score of 76.2% on the MultiClimate dataset, surpassing existing state-of-the-art methods. This approach addresses the challenge posed by social media content combining text and visual elements, providing a robust solution for stance classification.<br /> 
Summary: <div>
arXiv:2509.08024v1 Announce Type: new 
Abstract: With the rapid proliferation of information across digital platforms, stance detection has emerged as a pivotal challenge in social media analysis. While most of the existing approaches focus solely on textual data, real-world social media content increasingly combines text with visual elements creating a need for advanced multimodal methods. To address this gap, we propose a multimodal stance detection framework that integrates textual and visual information through a hierarchical fusion approach. Our method first employs a Large Language Model to retrieve stance-relevant summaries from source text, while a domain-aware image caption generator interprets visual content in the context of the target topic. These modalities are then jointly modeled along with the reply text, through a specialized transformer module that captures interactions between the texts and images. The proposed modality fusion framework integrates diverse modalities to facilitate robust stance classification. We evaluate our approach on the MultiClimate dataset, a benchmark for climate change-related stance detection containing aligned video frames and transcripts. We achieve accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%, respectively, outperforming existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2509.08026</link>
<guid>https://arxiv.org/abs/2509.08026</guid>
<content:encoded><![CDATA[
<div> Keywords: SI-EDTL, swarm intelligence, ensemble, transfer learning, UAV images 

Summary: 
SI-EDTL is a two-stage swarm intelligence ensemble deep transfer learning model designed for detecting multiple vehicles in UAV images. It utilizes three pre-trained feature extractor models and five transfer classifiers to create 15 base learners, which are then aggregated through weighted averaging to classify regions as Car, Van, Truck, Bus, or background. The model optimizes hyperparameters using the whale optimization algorithm to ensure a balance between accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing, SI-EDTL demonstrates superior performance compared to existing methods on the AU-AIR UAV dataset. <div>
arXiv:2509.08026v1 Announce Type: new 
Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep transfer learning model for detecting multiple vehicles in UAV images. It combines three pre-trained Faster R-CNN feature extractor models (InceptionV3, ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5, Na\"ive Bayes), resulting in 15 different base learners. These are aggregated via weighted averaging to classify regions as Car, Van, Truck, Bus, or background. Hyperparameters are optimized with the whale optimization algorithm to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV dataset.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
<link>https://arxiv.org/abs/2509.08027</link>
<guid>https://arxiv.org/abs/2509.08027</guid>
<content:encoded><![CDATA[
<div> dataset, Martian, digital elevation model, machine learning, MCTED<br />
<br />
Summary: <br />
This work introduces the MCTED dataset for predicting Martian digital elevation models using machine learning. The dataset comprises 80,898 data samples generated from high-resolution Mars orthoimages and DEM pairs from the Mars Reconnaissance Orbiter. A comprehensive processing pipeline was employed to address artefacts and missing data points. The dataset includes optical image patches, DEM patches, and mask patches to handle altered elevation regions. Training and validation splits were carefully created to prevent data leakage. Statistical insights on sample distribution and elevation values are provided. A small U-Net model trained on the MCTED dataset outperformed a depth estimation foundation model, showcasing the dataset's effectiveness. The dataset and code for its generation are openly available for use in public repositories. <br /> <div>
arXiv:2509.08027v1 Announce Type: new 
Abstract: This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
<link>https://arxiv.org/abs/2509.08104</link>
<guid>https://arxiv.org/abs/2509.08104</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud prediction, loss functions, deep learning, adaptive probabilistic matching, ShapeNet benchmarks<br />
Summary:<br />
The article introduces the Adaptive Probabilistic Matching Loss (APML) as a new approach for training deep learning models in point cloud prediction tasks. Existing loss functions like Chamfer Distance have limitations such as point congestion and non-differentiable operations. APML addresses these issues by using Sinkhorn iterations on a similarity matrix to approximate one-to-one matching. By analytically determining the temperature parameter, APML ensures minimum assignment probability without manual tuning. The proposed loss function offers near-quadratic runtime and superior spatial distribution in both dense and sparse regions, leading to faster convergence and improved performance on ShapeNet benchmarks and a spatiotemporal Transformer model. The code for APML is available for public use on GitHub, offering a practical solution for enhancing deep learning models in point cloud prediction tasks. <br /><br />Summary: <div>
arXiv:2509.08104v1 Announce Type: new 
Abstract: Training deep learning models for point cloud prediction tasks such as shape completion and generation depends critically on loss functions that measure discrepancies between predicted and ground-truth point sets. Commonly used functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on nearest-neighbor assignments, which often induce many-to-one correspondences, leading to point congestion in dense regions and poor coverage in sparse regions. These losses also involve non-differentiable operations due to index selection, which may affect gradient-based optimization. Earth Mover Distance (EMD) enforces one-to-one correspondences and captures structural similarity more effectively, but its cubic computational complexity limits its practical use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully differentiable approximation of one-to-one matching that leverages Sinkhorn iterations on a temperature-scaled similarity matrix derived from pairwise distances. We analytically compute the temperature to guarantee a minimum assignment probability, eliminating manual tuning. APML achieves near-quadratic runtime, comparable to Chamfer-based losses, and avoids non-differentiable operations. When integrated into state-of-the-art architectures (PoinTr, PCN, FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC) that generates 3D human point clouds from WiFi CSI measurements, APM loss yields faster convergence, superior spatial distribution, especially in low-density regions, and improved or on-par quantitative performance without additional hyperparameter search. The code is available at: https://github.com/apm-loss/apml.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2509.08205</link>
<guid>https://arxiv.org/abs/2509.08205</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, deep unfolding networks, lightweight framework, robust principal component analysis, channel attention mechanism 

Summary: 
The article introduces a new framework called L-RPCANet for infrared small target detection (ISTD) that addresses challenges in parameter lightweightness and noise robustness. L-RPCANet utilizes a hierarchical bottleneck structure to refine channel-wise features and reduce network parameter complexity. A noise reduction module is incorporated to enhance noise robustness, while the integration of squeeze-and-excitation networks (SENets) as a channel attention mechanism allows for focusing on important features across channels. Extensive experiments on ISTD datasets demonstrate the superior performance of L-RPCANet compared to existing methods such as RPCANet, DRPCANet, and RPCANet++. The code for L-RPCANet will be made available on GitHub at the specified link. <div>
arXiv:2509.08205v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) is one of the key techniques in image processing. Although deep unfolding networks (DUNs) have demonstrated promising performance in ISTD due to their model interpretability and data adaptability, existing methods still face significant challenges in parameter lightweightness and noise robustness. In this regard, we propose a highly lightweight framework based on robust principal component analysis (RPCA) called L-RPCANet. Technically, a hierarchical bottleneck structure is constructed to reduce and increase the channel dimension in the single-channel input infrared image to achieve channel-wise feature refinement, with bottleneck layers designed in each module to extract features. This reduces the number of channels in feature extraction and improves the lightweightness of network parameters. Furthermore, a noise reduction module is embedded to enhance the robustness against complex noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a channel attention mechanism to focus on the varying importance of different features across channels, thereby achieving excellent performance while maintaining both lightweightness and robustness. Extensive experiments on the ISTD datasets validate the superiority of our proposed method compared with state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code will be available at https://github.com/xianchaoxiu/L-RPCANet.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing</title>
<link>https://arxiv.org/abs/2509.08228</link>
<guid>https://arxiv.org/abs/2509.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: digital cameras, compressive measurement, Video Snapshot Compressive Imaging, Ultra-Sparse Sampling, Digital Micro-mirror Device

Summary:
Digital cameras consume a significant amount of power to capture and encode video, leading to concerns about sustainability with high-speed cameras. Compressive measurement techniques, such as Video Snapshot Compressive Imaging (SCI) and Ultra-Sparse Sampling (USS), have been proposed to reduce power consumption per pixel significantly. USS introduces a new sampling strategy where only one sub-frame is set to 1 at each spatial location, allowing for efficient image recovery. A Digital Micro-mirror Device (DMD) encoding system validates the effectiveness of USS. The proposed BSTFormer algorithm utilizes different attention mechanisms to exploit the sparsity of USS measurements successfully. USS offers a higher dynamic range than traditional random sampling methods and is suitable for implementing a complete video SCI system on a chip due to its fixed exposure time. This research presents a promising direction for sustainable high-speed imaging applications. 

<br /><br />Summary: <div>
arXiv:2509.08228v1 Announce Type: new 
Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps. Imagining gigapixel cameras operating at 100-1000 fps, the current processing model is unsustainable. To address this, physical layer compressive measurement has been proposed to reduce power consumption per pixel by 10-100X. Video Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the optical sensor layer to increase effective frame rate. A commonly used sampling strategy of video SCI is Random Sampling (RS) where each mask element value is randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated that images can be recovered from a fraction of the image pixels. Inspired by I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial location, only one sub-frame is set to 1 and all others are set to 0. We then build a Digital Micro-mirror Device (DMD) encoding system to verify the effectiveness of our USS strategy. Ideally, we can decompose the USS measurement into sub-measurements for which we can utilize I2P algorithms to recover high-speed frames. However, due to the mismatch between the DMD and CCD, the USS measurement cannot be perfectly decomposed. To this end, we propose BSTFormer, a sparse TransFormer that utilizes local Block attention, global Sparse attention, and global Temporal attention to exploit the sparsity of the USS measurement. Extensive results on both simulated and real-world data show that our method significantly outperforms all previous state-of-the-art algorithms. Additionally, an essential advantage of the USS strategy is its higher dynamic range than that of the RS strategy. Finally, from the application perspective, the USS strategy is a good choice to implement a complete video SCI system on chip due to its fixed exposure time.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation</title>
<link>https://arxiv.org/abs/2509.08232</link>
<guid>https://arxiv.org/abs/2509.08232</guid>
<content:encoded><![CDATA[
<div> Keywords: video anomaly detection, fatal incidents, GTA-Crime dataset, data generation framework, domain adaptation strategy

Summary:
Recent advancements in video anomaly detection have enabled the identification of criminal activities in surveillance videos, but the detection of fatal incidents such as shootings and stabbings remains challenging due to their rarity and ethical issues. To address this limitation, the authors introduce GTA-Crime, a dataset and generation framework using Grand Theft Auto 5 (GTA5) containing fatal situations captured from CCTV multiview perspectives under various conditions. They also release a framework for generating these types of videos. By incorporating GTA-Crime with a snippet-level domain adaptation strategy using Wasserstein adversarial training, they aim to bridge the gap between synthetic and real-world features, such as UCF-Crime, for enhanced fatal violence detection accuracy. Experimental results validate the effectiveness of the GTA-Crime dataset and demonstrate the improvement in real-world violence detection accuracy through the proposed domain adaptation strategy. The dataset and data generation framework are publicly available for research purposes. 

<br /><br />Summary: <div>
arXiv:2509.08232v1 Announce Type: new 
Abstract: Recent advancements in video anomaly detection (VAD) have enabled identification of various criminal activities in surveillance videos, but detecting fatal incidents such as shootings and stabbings remains difficult due to their rarity and ethical issues in data collection. Recognizing this limitation, we introduce GTA-Crime, a fatal video anomaly dataset and generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains fatal situations such as shootings and stabbings, captured from CCTV multiview perspectives under diverse conditions including action types, weather, time of day, and viewpoints. To address the rarity of such scenarios, we also release a framework for generating these types of videos. Additionally, we propose a snippet-level domain adaptation strategy using Wasserstein adversarial training to bridge the gap between synthetic GTA-Crime features and real-world features like UCF-Crime. Experimental results validate our GTA-Crime dataset and demonstrate that incorporating GTA-Crime with our domain adaptation strategy consistently enhances real world fatal violence detection accuracy. Our dataset and the data generation framework are publicly available at https://github.com/ta-ho/GTA-Crime.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification</title>
<link>https://arxiv.org/abs/2509.08234</link>
<guid>https://arxiv.org/abs/2509.08234</guid>
<content:encoded><![CDATA[
<div> ViT; CXR; medical imaging; channel replication; pneumonia<br />
<br />
Summary:<br />
A new channel replication strategy, RepViT-CXR, has been developed to adapt single-channel grayscale chest X-ray images for ViTs, showcasing high performance in TB and pneumonia detection. On the TB-CXR dataset, RepViT-CXR achieved a remarkable accuracy of 99.9% and AUC of 99.9%, surpassing previous state-of-the-art methods like Topo-CXR. The Pediatric Pneumonia dataset also saw impressive results, with RepViT-CXR obtaining 99.0% accuracy, superior to DCNN and VGG16 baselines. Additionally, on the Shenzhen TB dataset, the approach achieved 91.1% accuracy and an AUC of 91.2%, outperforming CNN-based methods. This research highlights the efficacy of simple yet effective techniques like channel replication in enabling ViTs to excel in grayscale medical imaging tasks, showcasing their potential for real-world clinical screening applications.<br /><br /> <div>
arXiv:2509.08234v1 Announce Type: new 
Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia. Recent advances in deep learning, particularly Vision Transformers (ViTs), have shown strong potential for automated medical image analysis. However, most ViT architectures are pretrained on natural images and require three-channel inputs, while CXR scans are inherently grayscale. To address this gap, we propose RepViT-CXR, a channel replication strategy that adapts single-channel CXR images into a ViT-compatible format without introducing additional information loss. We evaluate RepViT-CXR on three benchmark datasets. On the TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%, surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy, 99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0% accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%, outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a performance improvement over previously reported CNN-based methods. These results demonstrate that a simple yet effective channel replication strategy allows ViTs to fully leverage their representational power on grayscale medical imaging tasks. RepViT-CXR establishes a new state of the art for TB and pneumonia detection from chest X-rays, showing strong potential for deployment in real-world clinical screening systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI</title>
<link>https://arxiv.org/abs/2509.08243</link>
<guid>https://arxiv.org/abs/2509.08243</guid>
<content:encoded><![CDATA[
<div> Keywords: sMRI, deep learning, Alzheimer's disease, asymmetry, diagnostic accuracy 

Summary: 
This study introduces a novel end-to-end network for detecting Alzheimer's disease based on brain asymmetry induced by atrophy. The network combines a 3D CNN Encoder with a Symmetry Interactive Transformer (SIT) to analyze left and right hemisphere features. By focusing on asymmetrical regions caused by structural changes, the network achieves a higher diagnostic accuracy of 92.5% compared to other CNN-based methods. Visualization results demonstrate that the network effectively identifies regions of brain atrophy, especially the asymmetric pathological characteristics associated with Alzheimer's disease. This approach improves interpretability and diagnostic performance, highlighting the importance of considering brain asymmetry in the detection of AD. <br /><br />Summary: <div>
arXiv:2509.08243v1 Announce Type: new 
Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has achieved remarkable progress in the prediction and diagnosis of Alzheimer's disease (AD). Existing studies have used CNN and transformer to build a well-performing network, but most of them are based on pretraining or ignoring the asymmetrical character caused by brain disorders. We propose an end-to-end network for the detection of disease-based asymmetric induced by left and right brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive Transformer (SIT). Following the inter-equal grid block fetch operation, the corresponding left and right hemisphere features are aligned and subsequently fed into the SIT for diagnostic analysis. SIT can help the model focus more on the regions of asymmetry caused by structural changes, thus improving diagnostic performance. We evaluated our method based on the ADNI dataset, and the results show that the method achieves better diagnostic accuracy (92.5\%) compared to several CNN methods and CNNs combined with a general transformer. The visualization results show that our network pays more attention in regions of brain atrophy, especially for the asymmetric pathological characteristics induced by AD, demonstrating the interpretability and effectiveness of the method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.08260</link>
<guid>https://arxiv.org/abs/2509.08260</guid>
<content:encoded><![CDATA[
<div> Keywords: event-based cameras, video deblurring, frame interpolation, self-supervised learning, dataset construction

Summary: 
The paper introduces a unified self-supervised framework, EVDI++, for Event-based Video Deblurring and Interpolation using frame-based cameras with extended exposure times. The Learnable Double Integral (LDI) network is utilized to estimate the mapping relation between reference frames and sharp latent images. A learning-based division reconstruction module refines coarse results and optimizes overall training efficiency. An adaptive fusion strategy is employed for final results by utilizing confidence embedded in LDI outputs of concurrent events. A self-supervised learning framework enables network training with real-world blurry videos and events. The proposed method achieves state-of-the-art performance in video deblurring and interpolation tasks on both synthetic and real-world datasets, including a dataset constructed with a DAVIS346c camera. <div>
arXiv:2509.08260v1 Announce Type: new 
Abstract: Frame-based cameras with extended exposure times often produce perceptible visual blurring and information loss between frames, significantly degrading video quality. To address this challenge, we introduce EVDI++, a unified self-supervised framework for Event-based Video Deblurring and Interpolation that leverages the high temporal resolution of event cameras to mitigate motion blur and enable intermediate frame prediction. Specifically, the Learnable Double Integral (LDI) network is designed to estimate the mapping relation between reference frames and sharp latent images. Then, we refine the coarse results and optimize overall training efficiency by introducing a learning-based division reconstruction module, enabling images to be converted with varying exposure intervals. We devise an adaptive parameter-free fusion strategy to obtain the final results, utilizing the confidence embedded in the LDI outputs of concurrent events. A self-supervised learning framework is proposed to enable network training with real-world blurry videos and events by exploring the mutual constraints among blurry frames, latent images, and event streams. We further construct a dataset with real-world blurry images and events using a DAVIS346c camera, demonstrating the generalizability of the proposed EVDI++ in real-world scenarios. Extensive experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art performance in video deblurring and interpolation tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral Mamba for Hyperspectral Object Tracking</title>
<link>https://arxiv.org/abs/2509.08265</link>
<guid>https://arxiv.org/abs/2509.08265</guid>
<content:encoded><![CDATA[
<div> Spectral information, Cross-depth interactions, Temporal dependencies, Object tracking, Hyperspectral imaging <br />
Summary: <br />
The article introduces a new hyperspectral object tracking network called HyMamba, which addresses the limitations of existing hyperspectral trackers by unifying spectral, cross-depth, and temporal modeling through state space modules. The Spectral State Integration module enables the refinement and propagation of spectral features with cross-depth and temporal spectral information. The Hyperspectral Mamba module learns spatial and spectral information simultaneously through three directional scanning SSMs. HyMamba combines features from false-color and hyperspectral inputs, enhancing them with original spectral features from raw hyperspectral images. Experimental results show that HyMamba achieves state-of-the-art performance on seven benchmark datasets, with impressive AUC and DP@20 scores on the HOTC2020 dataset. The code for HyMamba will be made available on GitHub for further research and development. <br /> <div>
arXiv:2509.08265v1 Announce Type: new 
Abstract: Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features</title>
<link>https://arxiv.org/abs/2509.08266</link>
<guid>https://arxiv.org/abs/2509.08266</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, biases, attention values, image characteristics, prompt specificity

Summary: 
This study explores how Vision Language Models (VLMs) rely on biases during training to answer questions about visual properties, particularly when asked specific questions. The researchers developed a framework to analyze the impact of different input data characteristics on VLM performance. By examining attention values in VLMs with varying input parameters, they found that even minor changes in image and prompt details can significantly affect how the model generates answers. Specifically, modifications in image size, objects in the image, background color, and prompt specificity can lead to variations in VLM performance. This research highlights the importance of understanding how VLM behavior changes based on input data characteristics and suggests the need for methods to characterize and address these changes effectively. <br /><br />Summary: <div>
arXiv:2509.08266v1 Announce Type: new 
Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on inherent biases learned during training to respond to questions about visual properties of an image. These biases are exacerbated when VLMs are asked highly specific questions that require focusing on specific areas of the image. For example, a VLM tasked with counting stars on a modified American flag (e.g., with more than 50 stars) will often disregard the visual evidence and fail to answer accurately. We build upon this research and develop a multi-dimensional examination framework to systematically determine which characteristics of the input data, including both the image and the accompanying prompt, lead to such differences in performance. Using open-source VLMs, we further examine how attention values fluctuate with varying input parameters (e.g., image size, number of objects in the image, background color, prompt specificity). This research aims to learn how the behavior of vision language models changes and to explore methods for characterizing such changes. Our results suggest, among other things, that even minor modifications in image characteristics and prompt specificity can lead to large changes in how a VLM formulates its answer and, subsequently, its overall performance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title>
<link>https://arxiv.org/abs/2509.08280</link>
<guid>https://arxiv.org/abs/2509.08280</guid>
<content:encoded><![CDATA[
<div> uncertainty estimator, semantic segmentation, 3D point clouds, zero-shot learning, ScanNet v2<br />
<br />
Summary:<br /> 
The paper introduces E3DPC-GZSL, a novel approach for generalized zero-shot semantic segmentation of 3D point clouds. One of the main challenges in this task is the tendency of models to make biased predictions towards seen classes. E3DPC-GZSL addresses this issue by integrating an evidence-based uncertainty estimator into the classifier, allowing for adjustment of prediction probabilities based on prediction uncertainty. Additionally, the method introduces a training strategy that refines the semantic space by merging learnable parameters with text-derived features, improving model optimization for unseen data. Experimental results show that E3DPC-GZSL achieves state-of-the-art performance on datasets such as ScanNet v2 and S3DIS. <div>
arXiv:2509.08280v1 Announce Type: new 
Abstract: Generalized zero-shot semantic segmentation of 3D point clouds aims to classify each point into both seen and unseen classes. A significant challenge with these models is their tendency to make biased predictions, often favoring the classes encountered during training. This problem is more pronounced in 3D applications, where the scale of the training data is typically smaller than in image-based tasks. To address this problem, we propose a novel method called E3DPC-GZSL, which reduces overconfident predictions towards seen classes without relying on separate classifiers for seen and unseen data. E3DPC-GZSL tackles the overconfidence problem by integrating an evidence-based uncertainty estimator into a classifier. This estimator is then used to adjust prediction probabilities using a dynamic calibrated stacking factor that accounts for pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel training strategy that improves uncertainty estimation by refining the semantic space. This is achieved by merging learnable parameters with text-derived features, thereby improving model optimization for unseen data. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on generalized zero-shot semantic segmentation datasets, including ScanNet v2 and S3DIS.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection</title>
<link>https://arxiv.org/abs/2509.08289</link>
<guid>https://arxiv.org/abs/2509.08289</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly supervised object detection, Heatmap-guided proposal selector, Background class representation, Semantic gap, Negative certainty supervision loss

Summary:
Weakly supervised object detection (WSOD) aims to detect objects without requiring box-level annotations. Current WSOD methods face limitations in generating accurate pseudo GT boxes, lacking background class representation, and slow convergence during optimization. To address these challenges, a heatmap-guided proposal selector (HGPS) algorithm is proposed to pre-select proposals and refine pseudo GT boxes. Additionally, a weakly supervised basic detection network (WSBDN) is introduced to incorporate background class representation and bridge the semantic gap between branches. Furthermore, a negative certainty supervision loss is implemented to speed up convergence by utilizing ignored proposals. Experimental results on PASCAL VOC datasets show significant performance improvements compared to state-of-the-art WSOD methods, achieving mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012. The framework code is available for public use at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2509.08289v1 Announce Type: new 
Abstract: Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia</title>
<link>https://arxiv.org/abs/2509.08303</link>
<guid>https://arxiv.org/abs/2509.08303</guid>
<content:encoded><![CDATA[
<div> oil palm plantations, deforestation, Indonesia, geospatial dataset, sustainability

Summary: 
The article introduces a new open-access geospatial dataset focusing on oil palm plantations in Indonesia, a major contributor to deforestation in the region. The dataset, created through expert labeling of high-resolution satellite imagery, includes detailed annotations of oil palm plantations and related land cover types from 2020 to 2024. It distinguishes different stages of oil palm planting and similar perennial crops, ensuring data quality through consensus and field validation. By providing a comprehensive dataset under a CC-BY license, it aims to support monitoring of oil palm expansion, aid sustainability efforts, and align with global deforestation reduction goals. The dataset, suitable for training neural networks, fills a crucial gap in remote sensing data and promotes transparency in land cover mapping. Following FAIR data principles, the resource contributes to enhancing accuracy in tracking and addressing the challenges posed by oil palm cultivation in Indonesia. <br /><br /> <div>
arXiv:2509.08303v1 Announce Type: new 
Abstract: Oil palm cultivation remains one of the leading causes of deforestation in Indonesia. To better track and address this challenge, detailed and reliable mapping is needed to support sustainability efforts and emerging regulatory frameworks. We present an open-access geospatial dataset of oil palm plantations and related land cover types in Indonesia, produced through expert labeling of high-resolution satellite imagery from 2020 to 2024. The dataset provides polygon-based, wall-to-wall annotations across a range of agro-ecological zones and includes a hierarchical typology that distinguishes oil palm planting stages as well as similar perennial crops. Quality was ensured through multi-interpreter consensus and field validation. The dataset was created using wall-to-wall digitization over large grids, making it suitable for training and benchmarking both conventional convolutional neural networks and newer geospatial foundation models. Released under a CC-BY license, it fills a key gap in training data for remote sensing and aims to improve the accuracy of land cover types mapping. By supporting transparent monitoring of oil palm expansion, the resource contributes to global deforestation reduction goals and follows FAIR data principles.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</title>
<link>https://arxiv.org/abs/2509.08311</link>
<guid>https://arxiv.org/abs/2509.08311</guid>
<content:encoded><![CDATA[
<div> Keywords: medical vision-language, pre-training, CT scans, radiograph interpretation, SimCroP

Summary:
SimCroP is a novel framework designed for chest CT scans that leverages similarity-driven alignment and cross-granularity fusion to enhance radiograph interpretation. By optimizing the encoder using multi-modal masked modeling, SimCroP can better understand low-level semantics from radiographs. The similarity-driven alignment module enables the model to select and align the correct patches corresponding to each sentence in reports, addressing the challenge of spatial sparsity in lesions. The cross-granularity fusion module integrates information across different levels, improving the model's ability to capture key pathology structures in sparse radiographs. Pre-trained on a large-scale paired CT-reports dataset, SimCroP outperforms state-of-the-art medical self-supervised learning and vision-language pre-training methods on image classification and segmentation tasks. This framework is valuable for enhancing the performance of multi-scale downstream tasks in medical imaging analysis.<br /><br />Summary: <div>
arXiv:2509.08311v1 Announce Type: new 
Abstract: Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Experimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at https://github.com/ToniChopp/SimCroP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosted Training of Lightweight Early Exits for Optimizing CNN Image Classification Inference</title>
<link>https://arxiv.org/abs/2509.08318</link>
<guid>https://arxiv.org/abs/2509.08318</guid>
<content:encoded><![CDATA[
<div> Boosted Training Scheme for Early Exits, real-time image classification, resource-constrained platforms, convolutional neural networks, embedded deployment

Summary:
Boosted Training Scheme for Early Exits (BTS-EE) addresses the challenge of real-time image classification on resource-constrained platforms by introducing a sequential training approach for early-exit strategies. This approach aligns branch training with inference-time data distributions, improving efficiency in balancing accuracy with latency and power budgets. A lightweight branch architecture based on 1D convolutions and a Class Precision Margin (CPM) calibration method enable reliable exit decisions by allowing per-class threshold tuning. Experiments on the CINIC-10 dataset with a ResNet18 backbone show that BTS-EE consistently outperforms non-boosted training, achieving up to a 45 percent reduction in computation with only a 2 percent accuracy degradation. These findings expand the design space for deploying convolutional neural networks in real-time image processing systems, offering practical efficiency gains for applications such as industrial inspection, embedded vision, and UAV-based monitoring. 

<br /><br />Summary: <div>
arXiv:2509.08318v1 Announce Type: new 
Abstract: Real-time image classification on resource-constrained platforms demands inference methods that balance accuracy with strict latency and power budgets. Early-exit strategies address this need by attaching auxiliary classifiers to intermediate layers of convolutional neural networks (CNNs), allowing "easy" samples to terminate inference early. However, conventional training of early exits introduces a covariance shift: downstream branches are trained on full datasets, while at inference they process only the harder, non-exited samples. This mismatch limits efficiency--accuracy trade-offs in practice. We introduce the Boosted Training Scheme for Early Exits (BTS-EE), a sequential training approach that aligns branch training with inference-time data distributions. Each branch is trained and calibrated before the next, ensuring robustness under selective inference conditions. To further support embedded deployment, we propose a lightweight branch architecture based on 1D convolutions and a Class Precision Margin (CPM) calibration method that enables per-class threshold tuning for reliable exit decisions. Experiments on the CINIC-10 dataset with a ResNet18 backbone demonstrate that BTS-EE consistently outperforms non-boosted training across 64 configurations, achieving up to 45 percent reduction in computation with only 2 percent accuracy degradation. These results expand the design space for deploying CNNs in real-time image processing systems, offering practical efficiency gains for applications such as industrial inspection, embedded vision, and UAV-based monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2509.08338</link>
<guid>https://arxiv.org/abs/2509.08338</guid>
<content:encoded><![CDATA[
<div> Keywords: malignant melanoma, convolutional neural networks, vision-language models, retrieval-augmented framework, clinical decision support <br />
Summary: <br />
Accurate and early diagnosis of malignant melanoma is crucial for patient outcomes. While CNNs have shown promise in dermoscopic image analysis, they often overlook clinical metadata and need substantial preprocessing. Vision-language models offer a multimodal approach but struggle with clinical specificity on general-domain data. A retrieval-augmented VLM framework is proposed, incorporating semantically similar patient cases for diagnostic prompts. This method enables informed predictions without fine-tuning and significantly enhances accuracy and error correction compared to traditional methods. Results show that retrieval-augmented prompting is a robust strategy for clinical decision support. <div>
arXiv:2509.08338v1 Announce Type: new 
Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving patient outcomes. While convolutional neural networks (CNNs) have shown promise in dermoscopic image analysis, they often neglect clinical metadata and require extensive preprocessing. Vision-language models (VLMs) offer a multimodal alternative but struggle to capture clinical specificity when trained on general-domain data. To address this, we propose a retrieval-augmented VLM framework that incorporates semantically similar patient cases into the diagnostic prompt. Our method enables informed predictions without fine-tuning and significantly improves classification accuracy and error correction over conventional baselines. These results demonstrate that retrieval-augmented prompting provides a robust strategy for clinical decision support.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.08374</link>
<guid>https://arxiv.org/abs/2509.08374</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Detection, Multi-view Cameras, LiDAR, InsFusion, Autonomous Driving 

Summary:
InsFusion is a new approach proposed for three-dimensional object detection in autonomous driving and smart transportation systems. It addresses the issue of accumulated errors in the feature extraction process by extracting proposals from both raw and fused features and utilizing attention mechanisms applied to the raw features. By querying the raw features with the extracted proposals, InsFusion effectively mitigates the impact of the accumulated errors. Experimental results on the nuScenes dataset show that InsFusion outperforms various advanced baseline methods and achieves new state-of-the-art performance in 3D object detection. This innovative method combines the strengths of multi-view cameras and LiDAR data, highlighting the importance of feature fusion and attention mechanisms in enhancing object detection accuracy. 

<br /><br />Summary: <div>
arXiv:2509.08374v1 Announce Type: new 
Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a crucial component for autonomous driving and smart transportation. However, in the process of basic feature extraction, perspective transformation, and feature fusion, noise and error will gradually accumulate. To address this issue, we propose InsFusion, which can extract proposals from both raw and fused features and utilizes these proposals to query the raw features, thereby mitigating the impact of accumulated errors. Additionally, by incorporating attention mechanisms applied to the raw features, it thereby mitigates the impact of accumulated errors. Experiments on the nuScenes dataset demonstrate that InsFusion is compatible with various advanced baseline methods and delivers new state-of-the-art performance for 3D object detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</title>
<link>https://arxiv.org/abs/2509.08376</link>
<guid>https://arxiv.org/abs/2509.08376</guid>
<content:encoded><![CDATA[
<div> Keywords: disentangle, video data, self-supervised learning, transformer-based architecture, motion transfer<br />
Summary:<br />
The article proposes a new framework for disentangling video data into dynamic motion and static content components. It introduces a self-supervised pipeline that utilizes a transformer-based architecture to generate flexible features for motion and content. By incorporating low-bitrate vector quantization as an information bottleneck, the framework promotes disentanglement and creates a meaningful discrete motion space. The latent motion and content are then used as conditional inputs to a denoising diffusion model for self-supervised representation learning. The framework is tested on real-world talking head videos for tasks like motion transfer and auto-regressive motion generation, showing its effectiveness. Moreover, it demonstrates generalizability to other types of video data such as pixel sprites of 2D cartoon characters. This work contributes to the field of video analysis and generation by presenting a novel approach to self-supervised learning of disentangled video representations. <br /><br />Summary: <div>
arXiv:2509.08376v1 Announce Type: new 
Abstract: We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Causality-Aware Vision-Based 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2509.08388</link>
<guid>https://arxiv.org/abs/2509.08388</guid>
<content:encoded><![CDATA[
<div> Keyword: 3D semantic occupancy prediction, 2D-to-3D transformation pipeline, causal loss, Occ3D benchmark, camera perturbations <br />
Summary: 
The paper introduces a novel causal loss for 3D semantic occupancy prediction that enables end-to-end supervision of the 2D-to-3D transformation pipeline. This loss ensures gradient flow regulation from 3D voxel representations back to 2D features, making the entire pipeline differentiable and trainable. The proposed Semantic Causality-Aware 2D-to-3D Transformation includes Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for robustness against camera perturbations, and Normalized Convolution for feature propagation. Experimental results on the Occ3D benchmark show that the method achieves state-of-the-art performance, demonstrating improved robustness to camera perturbations and enhanced 2D-to-3D semantic consistency. <br /><br />Summary: <div>
arXiv:2509.08388v1 Announce Type: new 
Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
<link>https://arxiv.org/abs/2509.08392</link>
<guid>https://arxiv.org/abs/2509.08392</guid>
<content:encoded><![CDATA[
<div> Vehicle image enhancement, Traffic surveillance, License plate recognition, Vertical Residual Autoencoder, Image restoration <br />
Summary: <br />
The study addresses the challenge of enhancing vehicle images in traffic surveillance systems affected by various degradations like noise and blur. It introduces a Vertical Residual Autoencoder (VRAE) architecture that incorporates an enhancement strategy utilizing input-aware features to improve representation learning. Experiments on a vehicle image dataset show that the VRAE outperforms conventional methods such as Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. The VRAE significantly enhances peak signal-to-noise ratio (PSNR) by 20%, reduces normalized mean squared error (NMSE) by 50%, and improves structural similarity index (SSIM) by 1% compared to AE at the same depth, with only a marginal increase in parameters. The results demonstrate the effectiveness of the VRAE in improving the accuracy of license plate recognition systems in challenging real-world conditions. <div>
arXiv:2509.08392v1 Announce Type: new 
Abstract: In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%, and enhances SSIM by 1\%, while requiring only a marginal increase of roughly 1\% in parameters.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</title>
<link>https://arxiv.org/abs/2509.08421</link>
<guid>https://arxiv.org/abs/2509.08421</guid>
<content:encoded><![CDATA[
<div> Sparse Transformation, Density-aware Weighting, Multi-view Consistency Loss, SCFusion, Multi-View Multi-Object Tracking (MVMOT) <br />
Summary: <br />
SCFusion addresses challenges in multi-view object tracking by implementing sparse transformation to avoid feature distortion, density-aware weighting for adaptive feature fusion, and multi-view consistency loss for discriminative feature learning. The framework outperforms TrackTacular with an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX. SCFusion effectively integrates features from multiple cameras in a Bird's-Eye-View space, enhancing object tracking accuracy in surveillance, autonomous driving, and sports analytics applications. <div>
arXiv:2509.08421v1 Announce Type: new 
Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.08422</link>
<guid>https://arxiv.org/abs/2509.08422</guid>
<content:encoded><![CDATA[
<div> explanation, video-based AI systems, Latent Diffusion, counterfactuals, interpretability
Summary:
- The article introduces a novel framework called Latent Diffusion for Video Counterfactual Explanations (LD-ViCE) to explain the behavior of video-based AI models.
- LD-ViCE operates in latent space using a state-of-the-art diffusion model to reduce computational costs and improve the generation of realistic and interpretable counterfactual explanations.
- Experiments show that LD-ViCE outperforms a recent state-of-the-art method, achieving a significant increase in R2 score of up to 68% while reducing inference time by half.
- The framework is tested on diverse video datasets including EchoNet-Dynamic, FERV39k, and Something-Something V2, demonstrating its effectiveness in providing valuable insights into the target model behavior.
- LD-ViCE offers semantically meaningful and temporally coherent explanations, addressing limitations in existing explanation techniques and advancing the trustworthy deployment of AI in safety-critical domains.

Summary: <br /><br />Explanation: LD-ViCE framework introduced for explaining video-based AI model behavior. <br /> Effectiveness: Outperforms state-of-the-art method with significant R2 score increase. <br /> Efficiency: Reduces computational costs and inference time. <br /> Diverse Testing: Tested on EchoNet-Dynamic, FERV39k, and Something-Something V2 datasets. <br /> Interpretability: Offers valuable insights and generates meaningful explanations for target model behavior. <div>
arXiv:2509.08422v1 Announce Type: new 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time</title>
<link>https://arxiv.org/abs/2509.08436</link>
<guid>https://arxiv.org/abs/2509.08436</guid>
<content:encoded><![CDATA[
<div> Dataset, Robustness, Classification, Hyperspectral image, Degradations  
Summary:  
The article introduces HyperTTA, a framework aimed at improving hyperspectral image classification models' robustness to various real-world degradations. By creating a multi-degradation hyperspectral dataset, nine types of degradations are systematically simulated to evaluate classification performance comprehensively. The framework includes a spectral-spatial transformer classifier (SSTC) with a multi-level receptive field mechanism and label smoothing regularization to capture spatial context effectively. Additionally, a lightweight test-time adaptation strategy, confidence-aware entropy-minimized LayerNorm adapter (CELA), is introduced to dynamically adapt without access to source data or target annotations. Notably, the proposed HyperTTA framework outperforms existing baselines across different degradation scenarios according to extensive experiments on benchmark datasets, highlighting the effectiveness of both the classification backbone and the test-time adaptation strategy.Code will be made publicly available.  
<br /><br /> <div>
arXiv:2509.08436v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification models are highly sensitive to distribution shifts caused by various real-world degradations such as noise, blur, compression, and atmospheric effects. To address this challenge, we propose HyperTTA, a unified framework designed to enhance model robustness under diverse degradation conditions. Specifically, we first construct a multi-degradation hyperspectral dataset that systematically simulates nine representative types of degradations, providing a comprehensive benchmark for robust classification evaluation. Based on this, we design a spectral-spatial transformer classifier (SSTC) enhanced with a multi-level receptive field mechanism and label smoothing regularization to jointly capture multi-scale spatial context and improve generalization. Furthermore, HyperTTA incorporates a lightweight test-time adaptation (TTA) strategy, the confidence-aware entropy-minimized LayerNorm adapter (CELA), which updates only the affine parameters of LayerNorm layers by minimizing prediction entropy on high-confidence unlabeled target samples. This confidence-aware adaptation prevents unreliable updates from noisy predictions, enabling robust and dynamic adaptation without access to source data or target annotations. Extensive experiments on two benchmark datasets demonstrate that HyperTTA outperforms existing baselines across a wide range of degradation scenarios, validating the effectiveness of both its classification backbone and the proposed TTA scheme. Code will be made available publicly.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting</title>
<link>https://arxiv.org/abs/2509.08442</link>
<guid>https://arxiv.org/abs/2509.08442</guid>
<content:encoded><![CDATA[
<div> Keywords: cortical thickness, forecasting, Spherical Brownian Bridge Diffusion Model, denoising model, neurodegenerative processes

Summary: 
The article introduces a novel approach, the Spherical Brownian Bridge Diffusion Model (SBDM), for accurate forecasting of individualized cortical thickness trajectories. The model addresses the challenges posed by the complex geometry of the cerebral cortex and the integration of multi-modal data. SBDM utilizes a bidirectional conditional Brownian bridge diffusion process to forecast cortical thickness at the vertex level of cortical surfaces. A new denoising model, the conditional spherical U-Net (CoS-UNet), is introduced to seamlessly integrate cortical surfaces and tabular conditions. Experimental results based on longitudinal datasets from the ADNI and OASIS demonstrate significantly reduced prediction errors compared to previous approaches. SBDM also enables the generation of individual factual and counterfactual cortical thickness trajectories, providing a framework for exploring hypothetical scenarios of cortical development. <br /><br />Summary: <div>
arXiv:2509.08442v1 Announce Type: new 
Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First-order State Space Model for Lightweight Image Super-resolution</title>
<link>https://arxiv.org/abs/2509.08458</link>
<guid>https://arxiv.org/abs/2509.08458</guid>
<content:encoded><![CDATA[
<div> calculation process, State Space Model, lightweight super-resolution, First-order State Space Model, MambaIR 

Summary:
State space models (SSMs) like Mamba have shown promise in NLP and vision tasks. However, existing vision models based on Mamba focus more on network architecture and scan paths than on the SSM module. To explore SSMs' potential, researchers modified the calculation process of SSM to enhance performance on lightweight super-resolution tasks. They introduced the First-order State Space Model (FSSM) to improve the original Mamba module by incorporating token correlations. By applying a first-order hold condition and deriving a new discretized form, they analyzed cumulative error. Extensive experimental results demonstrated that FSSM improves the performance of MambaIR without increasing parameters, surpassing current lightweight super-resolution methods and achieving state-of-the-art results. <br /><br />Summary: <div>
arXiv:2509.08458v1 Announce Type: new 
Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximally Useful and Minimally Redundant: The Key to Self Supervised Learning for Imbalanced Data</title>
<link>https://arxiv.org/abs/2509.08469</link>
<guid>https://arxiv.org/abs/2509.08469</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive self-supervised learning, imbalanced datasets, multi-view assumptions, mutual information, state-of-the-art accuracy

Summary: 
In this paper, the authors address the issue of imbalanced datasets in contrastive self-supervised learning. They propose a new framework that extends the traditional multi-view approach to incorporate more than two views, leveraging the concept of mutual information to enhance learning. By distinguishing between intra and inter discriminatory characteristics, the method aims to extract representative features of underrepresented classes. A novel loss function is introduced to filter out extreme features, leading to improved representation learning. Experimental results demonstrate the effectiveness of the more than two view objective, achieving state-of-the-art accuracy in self-supervised imbalanced dataset classification tasks. Specifically, significant performance gains are observed in Cifar10-LT, Cifar100-LT, and Imagenet-LT datasets using popular neural network architectures. The proposed approach shows promise in addressing the challenges posed by imbalanced datasets in self-supervised learning methodologies.<br /><br />Summary: <div>
arXiv:2509.08469v1 Announce Type: new 
Abstract: The robustness of contrastive self-supervised learning (CSSL) for imbalanced datasets is largely unexplored. CSSL usually makes use of \emph{multi-view} assumptions to learn discriminatory features via similar and dissimilar data samples. CSSL works well on balanced datasets, but does not generalize well for imbalanced datasets. In a very recent paper, as part of future work, Yann LeCun pointed out that the self-supervised multiview framework can be extended to cases involving \emph{more than two views}. Taking a cue from this insight we propose a theoretical justification based on the concept of \emph{mutual information} to support the \emph{more than two views} objective and apply it to the problem of dataset imbalance in self-supervised learning. The proposed method helps extract representative characteristics of the tail classes by segregating between \emph{intra} and \emph{inter} discriminatory characteristics. We introduce a loss function that helps us to learn better representations by filtering out extreme features. Experimental evaluation on a variety of self-supervised frameworks (both contrastive and non-contrastive) also prove that the \emph{more than two view} objective works well for imbalanced datasets. We achieve a new state-of-the-art accuracy in self-supervised imbalanced dataset classification (2\% improvement in Cifar10-LT using Resnet-18, 5\% improvement in Cifar100-LT using Resnet-18, 3\% improvement in Imagenet-LT (1k) using Resnet-50).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation</title>
<link>https://arxiv.org/abs/2509.08489</link>
<guid>https://arxiv.org/abs/2509.08489</guid>
<content:encoded><![CDATA[
<div> detection, segmentation, inpainting, vision-language description, prompt-driven image analysis

Summary:
Prompt-driven image analysis involves converting natural-language instructions into multiple steps such as detection, segmentation, editing, and description. This study showcases a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a seamless workflow. The system allows for end-to-end processing from a single prompt, offers transparent debugging with intermediate artifacts, and supports interaction through both a user interface and a command-line interface. Integration choices are highlighted to enhance robustness, including threshold adjustments, mask inspection techniques, and resource-aware defaults. The study reveals high accuracy in detection and segmentation tasks, with inpainting consuming a significant portion of the total runtime. Recommendations are provided for tuning parameters and ensuring reliability through version pinning, artifact logging, and seed control. Overall, this work offers a reliable framework for integrating modern vision and multimodal models for tasks like object replacement and scene augmentation. 

<br /><br />Summary: <div>
arXiv:2509.08489v1 Announce Type: new 
Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</title>
<link>https://arxiv.org/abs/2509.08490</link>
<guid>https://arxiv.org/abs/2509.08490</guid>
<content:encoded><![CDATA[
<div> Keywords: Underwater object detection, challenges, image quality degradation, large vision-language models, synthetic data generation <br />
Summary:<br />
- Current underwater object detection (UOD) methods face challenges including image quality degradation, small object detection, and computational constraints. Traditional techniques have limitations in capturing the complexities of underwater environments.<br />
- Large vision-language models (LVLMs) show potential in UOD by combining multi-modal capabilities. However, real-time application of LVLMs in UOD needs further exploration.<br />
- Synthetic data generation using LVLMs like DALL-E 3 and fine-tuning Florence-2 shows promise in augmenting datasets, but requires refinement for realism and applicability.<br />
- UOD needs advanced methods to overcome challenges and improve performance in dynamic underwater environments.<br />
- Further research is necessary to optimize LVLM application in real-time UOD scenarios. <br /> <div>
arXiv:2509.08490v1 Announce Type: new 
Abstract: Underwater object detection (UOD) is vital to diverse marine applications, including oceanographic research, underwater robotics, and marine conservation. However, UOD faces numerous challenges that compromise its performance. Over the years, various methods have been proposed to address these issues, but they often fail to fully capture the complexities of underwater environments. This review systematically categorizes UOD challenges into five key areas: Image quality degradation, target-related issues, data-related challenges, computational and processing constraints, and limitations in detection methodologies. To address these challenges, we analyze the progression from traditional image processing and object detection techniques to modern approaches. Additionally, we explore the potential of large vision-language models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated in other domains. We also present case studies, including synthetic dataset generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review identifies three key insights: (i) Current UOD methods are insufficient to fully address challenges like image degradation and small object detection in dynamic underwater environments. (ii) Synthetic data generation using LVLMs shows potential for augmenting datasets but requires further refinement to ensure realism and applicability. (iii) LVLMs hold significant promise for UOD, but their real-time application remains under-explored, requiring further research on optimization techniques.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</title>
<link>https://arxiv.org/abs/2509.08502</link>
<guid>https://arxiv.org/abs/2509.08502</guid>
<content:encoded><![CDATA[
<div> Action recognition, time sensitivity, video representations, chiral pairs, self-supervised adaptation <br />
Summary:<br /> 
The study aims to create compact video representations sensitive to visual changes over time by introducing the task of chiral action recognition. This task involves differentiating between pairs of temporally opposite actions like opening and closing a door. The proposed model uses a self-supervised adaptation recipe to inject time-sensitivity into frozen image features, based on an auto-encoder with a latent space inspired by perceptual straightening. The model demonstrates linear separability between chiral pairs on three datasets â Something-Something, EPIC-Kitchens, and Charade, outperforming larger pre-trained video models. It also enhances classification performance on standard benchmarks when combined with existing models. <div>
arXiv:2509.08502v1 Announce Type: new 
Abstract: Our objective is to develop compact video representations that are sensitive to visual change over time. To measure such time-sensitivity, we introduce a new task: chiral action recognition, where one needs to distinguish between a pair of temporally opposite actions, such as "opening vs. closing a door", "approaching vs. moving away from something", "folding vs. unfolding paper", etc. Such actions (i) occur frequently in everyday life, (ii) require understanding of simple visual change over time (in object state, size, spatial position, count . . . ), and (iii) are known to be poorly represented by many video embeddings. Our goal is to build time aware video representations which offer linear separability between these chiral pairs. To that end, we propose a self-supervised adaptation recipe to inject time-sensitivity into a sequence of frozen image features. Our model is based on an auto-encoder with a latent space with inductive bias inspired by perceptual straightening. We show that this results in a compact but time-sensitive video representation for the proposed task across three datasets: Something-Something, EPIC-Kitchens, and Charade. Our method (i) outperforms much larger video models pre-trained on large-scale video datasets, and (ii) leads to an improvement in classification performance on standard benchmarks when combined with these existing models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</title>
<link>https://arxiv.org/abs/2509.08519</link>
<guid>https://arxiv.org/abs/2509.08519</guid>
<content:encoded><![CDATA[
<div> Keywords: Human-Centric Video Generation, multimodal inputs, collaborative control, task-specific strategies, progressive training

Summary: 
HuMo is a unified framework for Human-Centric Video Generation that tackles the challenges of coordinating multimodal inputs. It addresses the scarcity of training data by creating a dataset with diverse paired text, reference images, and audio. HuMo employs a two-stage progressive training approach with task-specific strategies for subject preservation and audio-visual sync. The model uses minimal-invasive image injection for subject preservation and a focus-by-predicting strategy for audio-visual sync. HuMo incorporates joint learning of controllabilities across multimodal inputs by progressively integrating the audio-visual sync task. During inference, a time-adaptive Classifier-Free Guidance strategy allows for flexible multimodal control. Experimental results show that HuMo outperforms specialized methods, establishing itself as a collaborative multimodal-conditioned HCVG framework. Visit the project page for more information: https://phantom-video.github.io/HuMo. 

Summary:<br /><br />Keywords: Human-Centric Video Generation, multimodal inputs, collaborative control, task-specific strategies, progressive training <br />HuMo is a unified framework for Human-Centric Video Generation that tackles the challenges of coordinating multimodal inputs. It addresses the scarcity of training data by creating a dataset with diverse paired text, reference images, and audio. HuMo employs a two-stage progressive training approach with task-specific strategies for subject preservation and audio-visual sync. The model uses minimal-invasive image injection for subject preservation and a focus-by-predicting strategy for audio-visual sync. HuMo incorporates joint learning of controllabilities across multimodal inputs by progressively integrating the audio-visual sync task. During inference, a time-adaptive Classifier-Free Guidance strategy allows for flexible multimodal control. Experimental results show that HuMo outperforms specialized methods, establishing itself as a collaborative multimodal-conditioned HCVG framework. Visit the project page for more information: https://phantom-video.github.io/HuMo. <div>
arXiv:2509.08519v1 Announce Type: new 
Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
<link>https://arxiv.org/abs/2509.08538</link>
<guid>https://arxiv.org/abs/2509.08538</guid>
<content:encoded><![CDATA[
<div> benchmark, video models, hallucinations, MESH, evaluation <br />
<br />
Summary: 
The article introduces a new benchmark, MESH, to assess the performance of Large Video Models (LVMs) in understanding dynamic video content. LVMs, which integrate temporal information with vision modules and language models, are prone to producing inaccurate or irrelevant descriptions known as hallucinations. MESH uses a Question-Answering framework with binary and multi-choice formats to systematically evaluate hallucinations in LVMs. It follows a bottom-up approach by assessing basic objects, coarse-to-fine subject features, and subject-action pairs, mirroring human video understanding processes. The evaluations demonstrate that while LVMs perform well in recognizing basic objects and features, they struggle with fine details and aligning multiple actions involving various subjects in longer videos, leading to an increase in hallucinations. MESH provides an effective and comprehensive method for identifying and addressing these hallucinations in video models. <br /> <div>
arXiv:2509.08538v1 Announce Type: new 
Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large Language Models (LLMs) and vision modules by integrating temporal information to better understand dynamic video content. Despite their progress, LVMs are prone to hallucinations-producing inaccurate or irrelevant descriptions. Current benchmarks for video hallucination depend heavily on manual categorization of video content, neglecting the perception-based processes through which humans naturally interpret videos. We introduce MESH, a benchmark designed to evaluate hallucinations in LVMs systematically. MESH uses a Question-Answering framework with binary and multi-choice formats incorporating target and trap instances. It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding. We demonstrate that MESH offers an effective and comprehensive approach for identifying hallucinations in videos. Our evaluations show that while LVMs excel at recognizing basic objects and features, their susceptibility to hallucinations increases markedly when handling fine details or aligning multiple actions involving various subjects in longer videos.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping</title>
<link>https://arxiv.org/abs/2509.08550</link>
<guid>https://arxiv.org/abs/2509.08550</guid>
<content:encoded><![CDATA[
<div> Plant phenotyping, deep learning, multi-view data, Growth Modelling Grand Challenge, Plant Age Prediction, Leaf Count Estimation <br />
Summary: 
The article introduces the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia 2025, which focuses on plant phenotyping using multi-view data. Traditional single-view models for plant analysis often lack the necessary information for accurate trait estimation. The challenge dataset includes multiple plants photographed from various heights and angles, requiring view-invariant embeddings for optimal results. The ViewSparsifier approach, incorporating 24 views in random selections, outperformed in both Plant Age Prediction and Leaf Count Estimation tasks. Experimental results with randomized view selection using selection matrices show promise for future research and enhancement of plant phenotyping accuracy. The use of multi-view data allows for a more comprehensive understanding of plant growth, health, and development, leading to improved plant health assessment and harvest readiness prediction. <br /> <div>
arXiv:2509.08550v1 Announce Type: new 
Abstract: Plant phenotyping involves analyzing observable characteristics of plants to better understand their growth, health, and development. In the context of deep learning, this analysis is often approached through single-view classification or regression models. However, these methods often fail to capture all information required for accurate estimation of target phenotypic traits, which can adversely affect plant health assessment and harvest readiness prediction. To address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia 2025 provides a multi-view dataset featuring multiple plants and two tasks: Plant Age Prediction and Leaf Count Estimation. Each plant is photographed from multiple heights and angles, leading to significant overlap and redundancy in the captured information. To learn view-invariant embeddings, we incorporate 24 views, referred to as the selection vector, in a random selection. Our ViewSparsifier approach won both tasks. For further improvement and as a direction for future research, we also experimented with randomized view selection across all five height levels (120 views total), referred to as selection matrices.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.08570</link>
<guid>https://arxiv.org/abs/2509.08570</guid>
<content:encoded><![CDATA[
<div> fusion, semantic gap, feature dispersion, Expectation-Maximization Aggregation, Text-Guided Pixel Decoder
<br />
Summary: 
Multimodal models have excelled in natural image segmentation but face challenges in medical applications due to semantic gap and feature dispersion. To address these issues, an Expectation-Maximization Aggregation mechanism is proposed to cluster features into semantic centers for better correspondence. A Text-Guided Pixel Decoder leverages textual knowledge to bridge the semantic gap and guide visual representations. The combination of these mechanisms significantly enhances the model's generalization ability. Experimental results on cardiac and fundus datasets show superiority over state-of-the-art approaches in domain generalization benchmarks. <div>
arXiv:2509.08570v1 Announce Type: new 
Abstract: Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data</title>
<link>https://arxiv.org/abs/2509.08571</link>
<guid>https://arxiv.org/abs/2509.08571</guid>
<content:encoded><![CDATA[
<div> GraphTopoNet, Greenland, subglacial bed, uncertainty, spatial graphs <br />
<br />
GraphTopoNet is a new framework for accurately mapping Greenland's subglacial bed, crucial for sea-level projections. It fuses various data sources like elevation, velocity, and mass balance into spatial graphs, incorporating uncertainty through Monte Carlo dropout. By capturing local variability and broad structure, it outperforms existing methods, reducing error by up to 60% while preserving fine glacial features. The hybrid loss function combines radar supervision with regularization techniques to handle data gaps effectively. The resulting bed maps enhance reliability for operational modeling, benefitting climate forecasting and policy-making agencies. GraphTopoNet demonstrates how graph machine learning can transform sparse and uncertain geophysical observations into actionable knowledge on a continental scale.<br /><br />Summary: <div>
arXiv:2509.08571v1 Announce Type: new 
Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level projections, but radar observations are sparse and uneven. We introduce GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built from surface observables (elevation, velocity, mass balance) are augmented with gradient features and polynomial trends to capture both local variability and broad structure. To handle data gaps, we employ a hybrid loss that combines confidence-weighted radar supervision with dynamically balanced regularization. Applied to three Greenland subregions, GraphTopoNet outperforms interpolation, convolutional, and graph-based baselines, reducing error by up to 60 percent while preserving fine-scale glacial features. The resulting bed maps improve reliability for operational modeling, supporting agencies engaged in climate forecasting and policy. More broadly, GraphTopoNet shows how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</title>
<link>https://arxiv.org/abs/2509.08580</link>
<guid>https://arxiv.org/abs/2509.08580</guid>
<content:encoded><![CDATA[
<div> Keywords: medical segmentation, implicit shape prior, sparse slice annotations, informative slice selection, assisted segmentation

Summary:
This paper proposes a method to reduce the manual workload for complex 3D segmentation tasks in medical imaging. By introducing an implicit shape prior and a framework for selecting informative slices, the method aims to assist in the segmentation of organs at risk in radiotherapy planning and in diagnosing age-related degenerative diseases such as sarcopenia. The approach involves segmenting volumes from sparse slice annotations and automatically selecting the most informative slices to guide further interactions, thereby speeding up the segmentation process. Experimental validation demonstrates the effectiveness of the method in assisting segmentation for brain cancer patients and in accelerating the creation of a new database for patients with sarcopenia. The proposed method shows promise in alleviating the burden of manual segmentation for medical professionals and improving the efficiency of complex segmentation tasks in medical imaging. 

<br /><br />Summary: <div>
arXiv:2509.08580v1 Announce Type: new 
Abstract: The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientIML: Efficient High-Resolution Image Manipulation Localization</title>
<link>https://arxiv.org/abs/2509.08583</link>
<guid>https://arxiv.org/abs/2509.08583</guid>
<content:encoded><![CDATA[
<div> Keywords: high-resolution SIF dataset, diffusion-based forgery, EfficientIML model, lightweight backbone, real-time forensic applications

Summary:
The article introduces a new dataset called the high-resolution SIF dataset, containing over 1200 diffusion-generated manipulations with semantically extracted masks. As imaging devices deliver increasingly higher resolutions, traditional forgery detection methods may struggle to detect these new manipulation types. To address this challenge, the authors propose the EfficientIML model with a lightweight EfficientRWKV backbone. This model utilizes a hybrid state-space and attention network to capture global context and local details simultaneously. Furthermore, a multi-scale supervision strategy ensures consistency across hierarchical predictions. Evaluations on the dataset and standard benchmarks show that the EfficientIML model outperforms ViT-based and other state-of-the-art lightweight baselines in localization performance, FLOPs, and inference speed. This makes the model suitable for real-time forensic applications. 

<br /><br />Summary: <div>
arXiv:2509.08583v1 Announce Type: new 
Abstract: With imaging devices delivering ever-higher resolutions and the emerging diffusion-based forgery methods, current detectors trained only on traditional datasets (with splicing, copy-moving and object removal forgeries) lack exposure to this new manipulation type. To address this, we propose a novel high-resolution SIF dataset of 1200+ diffusion-generated manipulations with semantically extracted masks. However, this also imposes a challenge on existing methods, as they face significant computational resource constraints due to their prohibitive computational complexities. Therefore, we propose a novel EfficientIML model with a lightweight, three-stage EfficientRWKV backbone. EfficientRWKV's hybrid state-space and attention network captures global context and local details in parallel, while a multi-scale supervision strategy enforces consistency across hierarchical predictions. Extensive evaluations on our dataset and standard benchmarks demonstrate that our approach outperforms ViT-based and other SOTA lightweight baselines in localization performance, FLOPs and inference speed, underscoring its suitability for real-time forensic applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging</title>
<link>https://arxiv.org/abs/2509.08618</link>
<guid>https://arxiv.org/abs/2509.08618</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, medical image segmentation, retinal imaging, CLAPS, unified framework

Summary: 
CLAPS is a novel method for unified segmentation in retinal imaging that addresses challenges such as modality ambiguity in textual descriptions, reliance on manual prompting, and lack of a unified framework. The approach involves pre-training a CLIP-based image encoder on a large retinal dataset, using GroundingDINO to generate spatial prompts, and enhancing text prompts with modality signatures. By automating both textual and spatial prompts, CLAPS guides SAM to achieve precise segmentation in a fully automated pipeline. Extensive experiments on diverse datasets demonstrate that CLAPS outperforms existing benchmarks and specialized expert models across various segmentation categories, showcasing its broad generalizability as a foundation model. 

<br /><br />Summary: <div>
arXiv:2509.08618v1 Announce Type: new 
Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have significantly impacted medical image segmentation, especially in retinal imaging, where precise segmentation is vital for diagnosis. Despite this progress, current methods face critical challenges: 1) modality ambiguity in textual disease descriptions, 2) a continued reliance on manual prompting for SAM-based workflows, and 3) a lack of a unified framework, with most methods being modality- and task-specific. To overcome these hurdles, we propose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method for unified segmentation across diverse tasks and modalities in retinal imaging. Our approach begins by pre-training a CLIP-based image encoder on a large, multi-modal retinal dataset to handle data scarcity and distribution imbalance. We then leverage GroundingDINO to automatically generate spatial bounding box prompts by detecting local lesions. To unify tasks and resolve ambiguity, we use text prompts enhanced with a unique "modality signature" for each imaging modality. Ultimately, these automated textual and spatial prompts guide SAM to execute precise segmentation, creating a fully automated and unified pipeline. Extensive experiments on 12 diverse datasets across 11 critical segmentation categories show that CLAPS achieves performance on par with specialized expert models while surpassing existing benchmarks across most metrics, demonstrating its broad generalizability as a foundation model.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdsQA: Towards Advertisement Video Understanding</title>
<link>https://arxiv.org/abs/2509.08621</link>
<guid>https://arxiv.org/abs/2509.08621</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, advertisement videos, ad Video QA benchmark, Deepseek-R1, ReAd-R

Summary:
(1) The study focuses on utilizing advertisement videos as a test-bed to evaluate the ability of Large language models (LLMs) beyond objective content perception.
(2) The authors contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips for 5 tasks.
(3) They introduce ReAd-R, an RL model that generates answers through reward-driven optimization based on questions.
(4) The study benchmarks 14 top-tier LLMs on AdsQA, with ReAd-R outperforming competitors with long-chain reasoning capabilities by a significant margin, achieving state-of-the-art results.
<br /><br />Summary: <div>
arXiv:2509.08621v1 Announce Type: new 
Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos' traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
<link>https://arxiv.org/abs/2509.08624</link>
<guid>https://arxiv.org/abs/2509.08624</guid>
<content:encoded><![CDATA[
<div> framework, multimodal, ophthalmic images, OCT, fundus photography

Summary: 
The study introduces a novel unpaired multimodal framework, UOPSL, for enhancing disease recognition in ophthalmic images. Traditional approaches relying solely on fundus or textual features lack fine-grained spatial information captured by distinct cues in different imaging modalities. The framework utilizes extensive OCT-derived spatial priors to dynamically identify lesion predilection sites, enhancing fundus image-based disease recognition. Through contrastive learning on unpaired OCT and fundus images, the framework learns the predilection sites matrix in the OCT latent space. This matrix captures lesion localization patterns, aiding in fundus image classification without paired OCT data. Extensive experiments across diverse datasets demonstrate the framework's superiority over existing benchmarks. <div>
arXiv:2509.08624v1 Announce Type: new 
Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have led to substantial improvements in ophthalmic disease identification in recent years. However, acquiring paired multimodal ophthalmic images remains prohibitively expensive. While fundus photography is simple and cost-effective, the limited availability of OCT data and inherent modality imbalance hinder further progress. Conventional approaches that rely solely on fundus or textual features often fail to capture fine-grained spatial information, as each imaging modality provides distinct cues about lesion predilection sites. In this study, we propose a novel unpaired multimodal framework \UOPSL that utilizes extensive OCT-derived spatial priors to dynamically identify predilection sites, enhancing fundus image-based disease recognition. Our approach bridges unpaired fundus and OCTs via extended disease text descriptions. Initially, we employ contrastive learning on a large corpus of unpaired OCT and fundus images while simultaneously learning the predilection sites matrix in the OCT latent space. Through extensive optimization, this matrix captures lesion localization patterns within the OCT feature space. During the fine-tuning or inference phase of the downstream classification task based solely on fundus images, where paired OCT data is unavailable, we eliminate OCT input and utilize the predilection sites matrix to assist in fundus image classification learning. Extensive experiments conducted on 9 diverse datasets across 28 critical categories demonstrate that our framework outperforms existing benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation</title>
<link>https://arxiv.org/abs/2509.08628</link>
<guid>https://arxiv.org/abs/2509.08628</guid>
<content:encoded><![CDATA[
<div> semi-supervised, diffusion models, domain translation, latent space, sample-to-sample translation
Summary: 
Latent Aligned Diffusion Bridges (LADB) is introduced as a semi-supervised framework for domain translation, utilizing pretrained diffusion models and a Latent Aligned Diffusion Model (LADM) to bridge domain gaps. By aligning source and target distributions in a shared latent space, LADB enables deterministic domain mapping without full supervision. It strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired data. Performance is demonstrated in depth-to-image translation under partial supervision, and LADB is extended to handle multi-source and multi-target translation tasks. The framework proves versatile in handling diverse use cases and scenarios where data annotation is costly or incomplete. LADB presents a scalable and effective solution for real-world domain translation challenges. 
<br /><br />Summary: <div>
arXiv:2509.08628v1 Announce Type: new 
Abstract: Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network</title>
<link>https://arxiv.org/abs/2509.08661</link>
<guid>https://arxiv.org/abs/2509.08661</guid>
<content:encoded><![CDATA[
<div> Keywords: ISolated Sign Language Recognition, Dual-SignLanguageNet, gesture morphology, trajectory modeling, optimal transport fusion <br />
Summary: 
Dual-SignLanguageNet (DSLNet) is introduced to address the challenge of recognizing isolated sign language gestures that are morphologically similar yet semantically distinct. The architecture decouples and models gesture morphology and trajectory in separate coordinate systems using wrist-centric and facial-centric frames. Specialized networks are employed for shape analysis and trajectory modeling, which are integrated via an optimal transport fusion mechanism. DSLNet achieves state-of-the-art performance on datasets like WLASL-100, WLASL-300, and LSA64 with high accuracy while utilizing fewer parameters than competing models. The use of dual-reference frames and dual-stream processing allows DSLNet to effectively capture the complexity of sign language gestures, leading to improved recognition performance. <div>
arXiv:2509.08661v1 Announce Type: new 
Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization</title>
<link>https://arxiv.org/abs/2509.08670</link>
<guid>https://arxiv.org/abs/2509.08670</guid>
<content:encoded><![CDATA[
<div> FractalPINN-Flow, deep learning, optical flow estimation, unsupervised, Fractal Deformation Network (FDN)<br />
<br />
Summary:<br />
FractalPINN-Flow introduces an unsupervised deep learning framework for dense optical flow estimation that operates without the need for ground truth data. The architecture of FractalPINN-Flow is built around the Fractal Deformation Network (FDN), a recursive encoder-decoder structure inspired by fractal geometry and self-similarity. Unlike traditional CNNs, FDN utilizes repeated encoder-decoder nesting with skip connections to capture fine details and long-range motion patterns. The training objective involves minimizing an energy functional incorporating data fidelity terms and total variation regularization to ensure brightness constancy, spatial smoothness, and coherent flow fields. Experimental results on synthetic and benchmark datasets demonstrate that FractalPINN-Flow generates accurate, smooth, and edge-preserving optical flow fields. The model excels particularly in high-resolution data settings and scenarios with limited annotations.<br /> <div>
arXiv:2509.08670v1 Announce Type: new 
Abstract: We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework</title>
<link>https://arxiv.org/abs/2509.08694</link>
<guid>https://arxiv.org/abs/2509.08694</guid>
<content:encoded><![CDATA[
<div> Keywords: coastal water segmentation, satellite imagery, Robust U-Net, HSV color space, multi-modal constraints

Summary:
Coastal water segmentation from satellite images is challenging due to complex spectral characteristics and irregular boundaries. Traditional RGB-based approaches face issues with training instability and poor generalization. The Robust U-Net framework introduced in this paper addresses these challenges by incorporating HSV color space supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. HSV supervision was found to have the highest impact, and the complete framework demonstrated superior training stability and segmentation quality. The method showed consistent improvements across evaluation metrics while maintaining computational efficiency. The training configurations and code are available for reproducibility at the provided Github link.  <div>
arXiv:2509.08694v1 Announce Type: new 
Abstract: Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Imaging for Enhanced Computer Vision</title>
<link>https://arxiv.org/abs/2509.08712</link>
<guid>https://arxiv.org/abs/2509.08712</guid>
<content:encoded><![CDATA[
<div> Keywords: computational imaging, computer vision, object detection, depth estimation, autonomous navigation

Summary: 
This paper provides a comprehensive survey of computational imaging (CI) techniques and their impact on computer vision (CV applications. Conventional imaging methods often struggle in challenging conditions, such as low light or motion blur, limiting the performance of CV systems. CI techniques like light field imaging and HDR imaging enhance image acquisition and reconstruction processes. The survey examines how these techniques synergize with core CV tasks such as object detection and face recognition. The paper also discusses the potential for adaptive imaging pipelines to improve efficiency in real-world scenarios like autonomous navigation and robotics. Overall, the paper highlights the opportunities, challenges, and future research directions in the field of computational imaging and its application to computer vision.<br /><br />Summary: <div>
arXiv:2509.08712v1 Announce Type: new 
Abstract: This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruc- tion processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</title>
<link>https://arxiv.org/abs/2509.08715</link>
<guid>https://arxiv.org/abs/2509.08715</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, lightweight framework, visual question answering, efficiency, BcQLM

Summary:
The article introduces a lightweight multimodal large language model (MLLM) framework called BcQLM for visual question answering. BcQLM is optimized for efficient multimodal understanding with only 1.2 billion parameters, significantly reducing computational costs while maintaining performance levels similar to standard MLLMs. The model, based on BreezeCLIP, is designed to balance accuracy and efficiency, making it suitable for real-world applications in resource-constrained environments. Experiments on various datasets validate the effectiveness of BcQLM in achieving this balance. The modular and extensible design of BcQLM allows for its adaptation to a wide range of multimodal tasks. The framework offers a potential solution for deploying MLLMs on practical hardware while addressing the challenges of energy efficiency, computational scalability, and environmental sustainability.<br /><br />Summary: <div>
arXiv:2509.08715v1 Announce Type: new 
Abstract: As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</title>
<link>https://arxiv.org/abs/2509.08738</link>
<guid>https://arxiv.org/abs/2509.08738</guid>
<content:encoded><![CDATA[
arXiv:2509.08738v1 Announce Type: new 
Abstract: This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</title>
<link>https://arxiv.org/abs/2509.08764</link>
<guid>https://arxiv.org/abs/2509.08764</guid>
<content:encoded><![CDATA[
arXiv:2509.08764v1 Announce Type: new 
Abstract: Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</title>
<link>https://arxiv.org/abs/2509.08777</link>
<guid>https://arxiv.org/abs/2509.08777</guid>
<content:encoded><![CDATA[
arXiv:2509.08777v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images</title>
<link>https://arxiv.org/abs/2509.08780</link>
<guid>https://arxiv.org/abs/2509.08780</guid>
<content:encoded><![CDATA[
arXiv:2509.08780v1 Announce Type: new 
Abstract: Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation</title>
<link>https://arxiv.org/abs/2509.08794</link>
<guid>https://arxiv.org/abs/2509.08794</guid>
<content:encoded><![CDATA[
arXiv:2509.08794v1 Announce Type: new 
Abstract: Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching</title>
<link>https://arxiv.org/abs/2509.08805</link>
<guid>https://arxiv.org/abs/2509.08805</guid>
<content:encoded><![CDATA[
arXiv:2509.08805v1 Announce Type: new 
Abstract: Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts</title>
<link>https://arxiv.org/abs/2509.08818</link>
<guid>https://arxiv.org/abs/2509.08818</guid>
<content:encoded><![CDATA[
arXiv:2509.08818v1 Announce Type: new 
Abstract: Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RewardDance: Reward Scaling in Visual Generation</title>
<link>https://arxiv.org/abs/2509.08826</link>
<guid>https://arxiv.org/abs/2509.08826</guid>
<content:encoded><![CDATA[
arXiv:2509.08826v1 Announce Type: new 
Abstract: Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video</title>
<link>https://arxiv.org/abs/2509.08828</link>
<guid>https://arxiv.org/abs/2509.08828</guid>
<content:encoded><![CDATA[
arXiv:2509.08828v1 Announce Type: new 
Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization</title>
<link>https://arxiv.org/abs/2509.07993</link>
<guid>https://arxiv.org/abs/2509.07993</guid>
<content:encoded><![CDATA[
arXiv:2509.07993v1 Announce Type: cross 
Abstract: The rapid evolution of deepfake generation technologies poses critical challenges for detection systems, as non-continual learning methods demand frequent and expensive retraining. We reframe deepfake detection (DFD) as a Continual Learning (CL) problem, proposing an efficient framework that incrementally adapts to emerging visual manipulation techniques while retaining knowledge of past generators. Our framework, unlike prior approaches that rely on unreal simulation sequences, simulates the real-world chronological evolution of deepfake technologies in extended periods across 7 years. Simultaneously, our framework builds upon lightweight visual backbones to allow for the real-time performance of DFD systems. Additionally, we contribute two novel metrics: Continual AUC (C-AUC) for historical performance and Forward Transfer AUC (FWT-AUC) for future generalization. Through extensive experimentation (over 600 simulations), we empirically demonstrate that while efficient adaptation (+155 times faster than full retraining) and robust retention of historical knowledge is possible, the generalization of current approaches to future generators without additional training remains near-random (FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing generator. Such observations are the foundation of our newly proposed Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery</title>
<link>https://arxiv.org/abs/2509.07994</link>
<guid>https://arxiv.org/abs/2509.07994</guid>
<content:encoded><![CDATA[
arXiv:2509.07994v1 Announce Type: cross 
Abstract: Despite advancements in rehabilitation protocols, clinical assessment of upper extremity (UE) function after stroke largely remains subjective, relying heavily on therapist observation and coarse scoring systems. This subjectivity limits the sensitivity of assessments to detect subtle motor improvements, which are critical for personalized rehabilitation planning. Recent progress in computer vision offers promising avenues for enabling objective, quantitative, and scalable assessment of UE motor function. Among standardized tests, the Box and Block Test (BBT) is widely utilized for measuring gross manual dexterity and tracking stroke recovery, providing a structured setting that lends itself well to computational analysis. However, existing datasets targeting stroke rehabilitation primarily focus on daily living activities and often fail to capture clinically structured assessments such as block transfer tasks. Furthermore, many available datasets include a mixture of healthy and stroke-affected individuals, limiting their specificity and clinical utility. To address these critical gaps, we introduce StrokeVision-Bench, the first-ever dedicated dataset of stroke patients performing clinically structured block transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized into four clinically meaningful action classes, with each sample represented in two modalities: raw video frames and 2D skeletal keypoints. We benchmark several state-of-the-art video action recognition and skeleton-based action classification methods to establish performance baselines for this domain and facilitate future research in automated stroke rehabilitation assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</title>
<link>https://arxiv.org/abs/2509.08007</link>
<guid>https://arxiv.org/abs/2509.08007</guid>
<content:encoded><![CDATA[
arXiv:2509.08007v1 Announce Type: cross 
Abstract: Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions-of-interests (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of a CT-brain analysis tool for measuring global cortical atrophy in older patient cohorts</title>
<link>https://arxiv.org/abs/2509.08012</link>
<guid>https://arxiv.org/abs/2509.08012</guid>
<content:encoded><![CDATA[
arXiv:2509.08012v1 Announce Type: cross 
Abstract: Quantification of brain atrophy currently requires visual rating scales which are time consuming and automated brain image analysis is warranted. We validated our automated deep learning (DL) tool measuring the Global Cerebral Atrophy (GCA) score against trained human raters, and associations with age and cognitive impairment, in representative older (>65 years) patients. CT-brain scans were obtained from patients in acute medicine (ORCHARD-EPR), acute stroke (OCS studies) and a legacy sample. Scans were divided in a 60/20/20 ratio for training, optimisation and testing. CT-images were assessed by two trained raters (rater-1=864 scans, rater-2=20 scans). Agreement between DL tool-predicted GCA scores (range 0-39) and the visual ratings was evaluated using mean absolute error (MAE) and Cohen's weighted kappa. Among 864 scans (ORCHARD-EPR=578, OCS=200, legacy scans=86), MAE between the DL tool and rater-1 GCA scores was 3.2 overall, 3.1 for ORCHARD-EPR, 3.3 for OCS and 2.6 for the legacy scans and half had DL-predicted GCA error between -2 and 2. Inter-rater agreement was Kappa=0.45 between the DL-tool and rater-1, and 0.41 between the tool and rater- 2 whereas it was lower at 0.28 for rater-1 and rater-2. There was no difference in GCA scores from the DL-tool and the two raters (one-way ANOVA, p=0.35) or in mean GCA scores between the DL-tool and rater-1 (paired t-test, t=-0.43, p=0.66), the tool and rater-2 (t=1.35, p=0.18) or between rater-1 and rater-2 (t=0.99, p=0.32). DL-tool GCA scores correlated with age and cognitive scores (both p<0.001). Our DL CT-brain analysis tool measured GCA score accurately and without user input in real-world scans acquired from older patients. Our tool will enable extraction of standardised quantitative measures of atrophy at scale for use in health data research and will act as proof-of-concept towards a point-of-care clinically approved tool.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
arXiv:2509.08015v1 Announce Type: cross 
Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators, enable the study of structure-function relationships for clinical research and medical device design. However, current models face a trade-off between controllability and anatomical realism. We propose a programmable and compositional framework for guiding unconditional diffusion models of human anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our method involves the selection of certain tissues within multi-tissue segmentation maps, upon which we apply geometric moment losses to guide the reverse diffusion process. This framework supports the independent control over size, shape, and position, as well as the composition of multi-component constraints during inference.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
<link>https://arxiv.org/abs/2509.08018</link>
<guid>https://arxiv.org/abs/2509.08018</guid>
<content:encoded><![CDATA[
arXiv:2509.08018v1 Announce Type: cross 
Abstract: The application of Digital Twin (DT) technology and Federated Learning (FL) has great potential to change the field of biomedical image analysis, particularly for Computed Tomography (CT) scans. This paper presents Federated Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm. FTL uses pre-trained models and knowledge transfer between peer nodes to solve problems such as data privacy, limited computing resources, and data heterogeneity. The proposed framework allows real-time collaboration between cloud servers and Digital Twin-enabled CT scanners while protecting patient identity. We apply the FTL method to a heterogeneous CT scan dataset and assess model performance using convergence time, model accuracy, precision, recall, F1 score, and confusion matrix. It has been shown to perform better than conventional FL and Clustered Federated Learning (CFL) methods with better precision, accuracy, recall, and F1-score. The technique is beneficial in settings where the data is not independently and identically distributed (non-IID), and it offers reliable, efficient, and secure solutions for medical diagnosis. These findings highlight the possibility of using FTL to improve decision-making in digital twin-based CT scan analysis, secure and efficient medical image analysis, promote privacy, and open new possibilities for applying precision medicine and smart healthcare systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quadrotor Navigation using Reinforcement Learning with Privileged Information</title>
<link>https://arxiv.org/abs/2509.08177</link>
<guid>https://arxiv.org/abs/2509.08177</guid>
<content:encoded><![CDATA[
arXiv:2509.08177v1 Announce Type: cross 
Abstract: This paper presents a reinforcement learning-based quadrotor navigation method that leverages efficient differentiable simulation, novel loss functions, and privileged information to navigate around large obstacles. Prior learning-based methods perform well in scenes that exhibit narrow obstacles, but struggle when the goal location is blocked by large walls or terrain. In contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged information and a yaw alignment loss to guide the robot around large obstacles. The policy is evaluated in photo-realistic simulation environments containing large obstacles, sharp corners, and dead-ends. Our approach achieves an 86% success rate and outperforms baseline strategies by 34%. We deploy the policy onboard a custom quadrotor in outdoor cluttered environments both during the day and night. The policy is validated across 20 flights, covering 589 meters without collisions at speeds up to 4 m/s.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</title>
<link>https://arxiv.org/abs/2509.08302</link>
<guid>https://arxiv.org/abs/2509.08302</guid>
<content:encoded><![CDATA[
arXiv:2509.08302v1 Announce Type: cross 
Abstract: Foundation models are revolutionizing autonomous driving perception, transitioning the field from narrow, task-specific deep learning models to versatile, general-purpose architectures trained on vast, diverse datasets. This survey examines how these models address critical challenges in autonomous perception, including limitations in generalization, scalability, and robustness to distributional shifts. The survey introduces a novel taxonomy structured around four essential capabilities for robust performance in dynamic driving environments: generalized knowledge, spatial understanding, multi-sensor robustness, and temporal reasoning. For each capability, the survey elucidates its significance and comprehensively reviews cutting-edge approaches. Diverging from traditional method-centric surveys, our unique framework prioritizes conceptual design principles, providing a capability-driven guide for model development and clearer insights into foundational aspects. We conclude by discussing key challenges, particularly those associated with the integration of these capabilities into real-time, scalable systems, and broader deployment challenges related to computational demands and ensuring model reliability against issues like hallucinations and out-of-distribution failures. The survey also outlines crucial future research directions to enable the safe and effective deployment of foundation models in autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Rectified Flow for Low-light RAW Image Enhancement</title>
<link>https://arxiv.org/abs/2509.08330</link>
<guid>https://arxiv.org/abs/2509.08330</guid>
<content:encoded><![CDATA[
arXiv:2509.08330v1 Announce Type: cross 
Abstract: Enhancing RAW images captured under low light conditions is a challenging task. Recent deep learning based RAW enhancement methods have shifted from using real paired data to relying on synthetic datasets. These synthetic datasets are typically generated by physically modeling sensor noise, but existing approaches often consider only additive noise, ignore multiplicative components, and rely on global calibration that overlooks pixel level manufacturing variations. As a result, such methods struggle to accurately reproduce real sensor noise. To address these limitations, this paper derives a noise model from the physical noise generation mechanisms that occur under low illumination and proposes a novel composite model that integrates both additive and multiplicative noise. To solve the model, we introduce a physics based per pixel noise simulation and calibration scheme that estimates and synthesizes noise for each individual pixel, thereby overcoming the restrictions of traditional global calibration and capturing spatial noise variations induced by microscopic CMOS manufacturing differences. Motivated by the strong performance of rectified flow methods in image generation and processing, we further combine the physics-based noise synthesis with a rectified flow generative framework and present PGRF a physics-guided rectified flow framework for low light image enhancement. PGRF leverages the ability of rectified flows to model complex data distributions and uses physical guidance to steer the generation toward the desired clean image. To validate the effectiveness of the proposed model, we established the LLID dataset, an indoor low light benchmark captured with the Sony A7S II camera. Experimental results demonstrate that the proposed framework achieves significant improvements in low light RAW image enhancement.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry</title>
<link>https://arxiv.org/abs/2509.08333</link>
<guid>https://arxiv.org/abs/2509.08333</guid>
<content:encoded><![CDATA[
arXiv:2509.08333v1 Announce Type: cross 
Abstract: Visual-based localization has made significant progress, yet its performance often drops in large-scale, outdoor, and long-term settings due to factors like lighting changes, dynamic scenes, and low-texture areas. These challenges degrade feature extraction and tracking, which are critical for accurate motion estimation. While learning-based methods such as SuperPoint and SuperGlue show improved feature coverage and robustness, they still face generalization issues with out-of-distribution data. We address this by enhancing deep feature extraction and tracking through self-supervised learning with task specific feedback. Our method promotes stable and informative features, improving generalization and reliability in challenging environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics</title>
<link>https://arxiv.org/abs/2509.08461</link>
<guid>https://arxiv.org/abs/2509.08461</guid>
<content:encoded><![CDATA[
arXiv:2509.08461v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their remarkable capacity to process and reason over structured and unstructured data modalities beyond natural language. In this work, we explore the applications of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa 3.2, to the task of identifying neutrino interactions in pixelated detector data from high-energy physics (HEP) experiments. We benchmark this model against a state-of-the-art convolutional neural network (CNN) architecture, similar to those used in the NOvA and DUNE experiments, which have achieved high efficiency and purity in classifying electron and muon neutrino events. Our evaluation considers both the classification performance and interpretability of the model predictions. We find that VLMs can outperform CNNs, while also providing greater flexibility in integrating auxiliary textual or semantic information and offering more interpretable, reasoning-based predictions. This work highlights the potential of VLMs as a general-purpose backbone for physics event classification, due to their high performance, interpretability, and generalizability, which opens new avenues for integrating multimodal reasoning in experimental neutrino physics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-ViT Hybrid for Pneumonia Detection: Theory and Empiric on Limited Data without Pretraining</title>
<link>https://arxiv.org/abs/2509.08586</link>
<guid>https://arxiv.org/abs/2509.08586</guid>
<content:encoded><![CDATA[
arXiv:2509.08586v1 Announce Type: cross 
Abstract: This research explored the hybridization of CNN and ViT within a training dataset of limited size, and introduced a distinct class imbalance. The training was made from scratch with a mere focus on theoretically and experimentally exploring the architectural strengths of the proposed hybrid model. Experiments were conducted across varied data fractions with balanced and imbalanced training datasets. Comparatively, the hybrid model, complementing the strengths of CNN and ViT, achieved the highest recall of 0.9443 (50% data fraction in balanced) and consistency in F1 score around 0.85, suggesting reliability in diagnosis. Additionally, the model was successful in outperforming CNN and ViT in imbalanced datasets. Despite its complex architecture, it required comparable training time to the transformers in all data fractions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts</title>
<link>https://arxiv.org/abs/2509.08640</link>
<guid>https://arxiv.org/abs/2509.08640</guid>
<content:encoded><![CDATA[
arXiv:2509.08640v1 Announce Type: cross 
Abstract: Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\% of cases, correctly incorporated the specified finding in 89-99\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\% AUC in internal validation and by 1-11\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Part: high fidelity and structure coherent shape decomposition</title>
<link>https://arxiv.org/abs/2509.08643</link>
<guid>https://arxiv.org/abs/2509.08643</guid>
<content:encoded><![CDATA[
arXiv:2509.08643v1 Announce Type: cross 
Abstract: Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
<link>https://arxiv.org/abs/2509.08699</link>
<guid>https://arxiv.org/abs/2509.08699</guid>
<content:encoded><![CDATA[
arXiv:2509.08699v1 Announce Type: cross 
Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</title>
<link>https://arxiv.org/abs/2509.08757</link>
<guid>https://arxiv.org/abs/2509.08757</guid>
<content:encoded><![CDATA[
arXiv:2509.08757v1 Announce Type: cross 
Abstract: Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PianoVAM: A Multimodal Piano Performance Dataset</title>
<link>https://arxiv.org/abs/2509.08800</link>
<guid>https://arxiv.org/abs/2509.08800</guid>
<content:encoded><![CDATA[
arXiv:2509.08800v1 Announce Type: cross 
Abstract: The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Information in Domain-Invariant Representation Improves Transfer Learning</title>
<link>https://arxiv.org/abs/2306.00262</link>
<guid>https://arxiv.org/abs/2306.00262</guid>
<content:encoded><![CDATA[
arXiv:2306.00262v5 Announce Type: replace 
Abstract: We propose MaxDIRep, a domain adaptation method that improves the decomposition of data representations into domain-independent and domain-dependent components. Existing methods, such as Domain-Separation Networks (DSN), use a weak orthogonality constraint between these components, which can lead to label-relevant features being partially encoded in the domain-dependent representation (DDRep) rather than the domain-independent representation (DIRep). As a result, information crucial for target-domain classification may be missing from the DIRep. MaxDIRep addresses this issue by applying a Kullback-Leibler (KL) divergence constraint to minimize the information content of the DDRep, thereby encouraging the DIRep to retain features that are both domain-invariant and predictive of target labels. Through geometric analysis and an ablation study on synthetic datasets, we show why DSN's weaker constraint can lead to suboptimal adaptation. Experiments on standard image benchmarks and a network intrusion detection task demonstrate that MaxDIRep achieves strong performance, works with pretrained models, and generalizes to non-image classification tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Channel Bias to Feature Redundancy: Uncovering the "Less is More" Principle in Few-Shot Learning</title>
<link>https://arxiv.org/abs/2310.03843</link>
<guid>https://arxiv.org/abs/2310.03843</guid>
<content:encoded><![CDATA[
arXiv:2310.03843v2 Announce Type: replace 
Abstract: Deep neural networks often fail to adapt representations to novel tasks under distribution shifts, especially when only a few examples are available. This paper identifies a core obstacle behind this failure: channel bias, where networks develop a rigid emphasis on feature dimensions that were discriminative for the source task, but this emphasis is misaligned and fails to adapt to the distinct needs of a novel task. This bias leads to a striking and detrimental consequence: feature redundancy. We demonstrate that for few-shot tasks, classification accuracy is significantly improved by using as few as 1-5% of the most discriminative feature dimensions, revealing that the vast majority are actively harmful. Our theoretical analysis confirms that this redundancy originates from confounding feature dimensions-those with high intra-class variance but low inter-class separability-which are especially problematic in low-data regimes. This "less is more" phenomenon is a defining characteristic of the few-shot setting, diminishing as more samples become available. To address this, we propose a simple yet effective soft-masking method, Augmented Feature Importance Adjustment (AFIA), which estimates feature importance from augmented data to mitigate the issue. By establishing the cohesive link from channel bias to its consequence of extreme feature redundancy, this work provides a foundational principle for few-shot representation transfer and a practical method for developing more robust few-shot learning algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Robust Representations via Bidirectional Transition for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2312.01915</link>
<guid>https://arxiv.org/abs/2312.01915</guid>
<content:encoded><![CDATA[
arXiv:2312.01915v2 Announce Type: replace 
Abstract: Visual reinforcement learning has proven effective in solving control tasks with high-dimensional observations. However, extracting reliable and generalizable representations from vision-based observations remains a central challenge. Inspired by the human thought process, when the representation extracted from the observation can predict the future and trace history, the representation is reliable and accurate in comprehending the environment. Based on this concept, we introduce a Bidirectional Transition (BiT) model, which leverages the ability to bidirectionally predict environmental transitions both forward and backward to extract reliable representations. Our model demonstrates competitive generalization performance and sample efficiency on two settings of the DeepMind Control suite. Additionally, we utilize robotic manipulation and CARLA simulators to demonstrate the wide applicability of our method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2404.04256</link>
<guid>https://arxiv.org/abs/2404.04256</guid>
<content:encoded><![CDATA[
arXiv:2404.04256v3 Announce Type: replace 
Abstract: Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable prediction. In this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic segmentation utilizing the advanced Mamba. Unlike conventional methods that rely on CNNs, with their limited local receptive fields, or Vision Transformers (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields with linear complexity. By employing a Siamese encoder and innovating a Mamba-based fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our proposed method is rigorously evaluated on both RGB-Thermal and RGB-Depth semantic segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks. Code is available at https://github.com/zifuwan/Sigma.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorCLIP: Visual Prior Guided Vision-Language Model for Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2405.10160</link>
<guid>https://arxiv.org/abs/2405.10160</guid>
<content:encoded><![CDATA[
arXiv:2405.10160v3 Announce Type: replace 
Abstract: Remote sensing image-text retrieval plays a crucial role in remote sensing interpretation, yet remains challenging under both closed-domain and open-domain scenarios due to semantic noise and domain shifts. To address these issues, we propose a visual prior-guided vision-language model, PriorCLIP, which leverages visual priors for unbiased representation learning and adaptive vision-language alignment. In the closed-domain setting, PriorCLIP introduces two Progressive Attention Encoder (PAE) structures: Spatial-PAE constructs a belief matrix with instruction embeddings to filter key features and mitigate semantic bias. At the same time, Temporal-PAE exploits cyclic activation across time steps to enhance text representation. For the open-domain setting, we design a two-stage prior representation learning strategy, consisting of large-scale pre-training on coarse-grained image-text pairs, followed by fine-tuning on fine-grained pairs using vision-instruction, which enables robust retrieval across long-tail concepts and vocabulary shifts. Furthermore, a cluster-based symmetric contrastive Attribution Loss is proposed to constrain inter-class relations and alleviate semantic confusion in the shared embedding space. Extensive experiments on RSICD and RSITMD benchmarks demonstrate that PriorCLIP achieves substantial improvements, outperforming existing methods by 4.9% and 4.0% in closed-domain retrieval, and by 7.3% and 9.4% in open-domain retrieval, respectively.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformer with Sparse Scan Prior</title>
<link>https://arxiv.org/abs/2405.13335</link>
<guid>https://arxiv.org/abs/2405.13335</guid>
<content:encoded><![CDATA[
arXiv:2405.13335v2 Announce Type: replace 
Abstract: In recent years, Transformers have achieved remarkable progress in computer vision tasks. However, their global modeling often comes with substantial computational overhead, in stark contrast to the human eye's efficient information processing. Inspired by the human eye's sparse scanning mechanism, we propose a \textbf{S}parse \textbf{S}can \textbf{S}elf-\textbf{A}ttention mechanism ($\rm{S}^3\rm{A}$). This mechanism predefines a series of Anchors of Interest for each token and employs local attention to efficiently model the spatial information around these anchors, avoiding redundant global modeling and excessive focus on local information. This approach mirrors the human eye's functionality and significantly reduces the computational load of vision models. Building on $\rm{S}^3\rm{A}$, we introduce the \textbf{S}parse \textbf{S}can \textbf{Vi}sion \textbf{T}ransformer (SSViT). Extensive experiments demonstrate the outstanding performance of SSViT across a variety of tasks. Specifically, on ImageNet classification, without additional supervision or training data, SSViT achieves top-1 accuracies of \textbf{84.4\%/85.7\%} with \textbf{4.4G/18.2G} FLOPs. SSViT also excels in downstream tasks such as object detection, instance segmentation, and semantic segmentation. Its robustness is further validated across diverse datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Have Large Vision-Language Models Mastered Art History?</title>
<link>https://arxiv.org/abs/2409.03521</link>
<guid>https://arxiv.org/abs/2409.03521</guid>
<content:encoded><![CDATA[
arXiv:2409.03521v2 Announce Type: replace 
Abstract: The emergence of large Vision-Language Models (VLMs) has established new baselines in image classification across multiple domains. We examine whether their multimodal reasoning can also address a challenge mastered by human experts. Specifically, we test whether VLMs can classify the style, author and creation date of paintings, a domain traditionally mastered by art historians. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. This requires a contextual and stylistic interpretation rather than straightforward object recognition. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively reason about the historical and stylistic attributes of paintings. We present the first study of its kind, conducting an in-depth analysis of three VLMs, namely CLIP, LLaVA, and GPT-4o, evaluating their zero-shot classification of art style, author and time period. Using two image benchmarks of artworks, we assess the models' ability to interpret style, evaluate their sensitivity to prompts, and examine failure cases. Additionally, we focus on how these models compare to human art historical expertise by analyzing misclassifications, providing insights into their reasoning and classification patterns.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Chinese Continuous Sign Language Dataset Based on Complex Environments</title>
<link>https://arxiv.org/abs/2409.11960</link>
<guid>https://arxiv.org/abs/2409.11960</guid>
<content:encoded><![CDATA[
arXiv:2409.11960v2 Announce Type: replace 
Abstract: The current bottleneck in continuous sign language recognition (CSLR) research lies in the fact that most publicly available datasets are limited to laboratory environments or television program recordings, resulting in a single background environment with uniform lighting, which significantly deviates from the diversity and complexity found in real-life scenarios. To address this challenge, we have constructed a new, large-scale dataset for Chinese continuous sign language (CSL) based on complex environments, termed the complex environment - chinese sign language dataset (CE-CSL). This dataset encompasses 5,988 continuous CSL video clips collected from daily life scenes, featuring more than 70 different complex backgrounds to ensure representativeness and generalization capability. To tackle the impact of complex backgrounds on CSLR performance, we propose a time-frequency network (TFNet) model for continuous sign language recognition. This model extracts frame-level features and then utilizes both temporal and spectral information to separately derive sequence features before fusion, aiming to achieve efficient and accurate CSLR. Experimental results demonstrate that our approach achieves significant performance improvements on the CE-CSL, validating its effectiveness under complex background conditions. Additionally, our proposed method has also yielded highly competitive results when applied to three publicly available CSL datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALOcc: Adaptive Lifting-Based 3D Semantic Occupancy and Cost Volume-Based Flow Predictions</title>
<link>https://arxiv.org/abs/2411.07725</link>
<guid>https://arxiv.org/abs/2411.07725</guid>
<content:encoded><![CDATA[
arXiv:2411.07725v2 Announce Type: replace 
Abstract: 3D semantic occupancy and flow prediction are fundamental to spatiotemporal scene understanding. This paper proposes a vision-based framework with three targeted improvements. First, we introduce an occlusion-aware adaptive lifting mechanism incorporating depth denoising. This enhances the robustness of 2D-to-3D feature transformation while mitigating reliance on depth priors. Second, we enforce 3D-2D semantic consistency via jointly optimized prototypes, using confidence- and category-aware sampling to address the long-tail classes problem. Third, to streamline joint prediction, we devise a BEV-centric cost volume to explicitly correlate semantic and flow features, supervised by a hybrid classification-regression scheme that handles diverse motion scales. Our purely convolutional architecture establishes new SOTA performance on multiple benchmarks for both semantic occupancy and joint occupancy semantic-flow prediction. We also present a family of models offering a spectrum of efficiency-performance trade-offs. Our real-time version exceeds all existing real-time methods in speed and accuracy, ensuring its practical viability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GloFinder: AI-empowered QuPath Plugin for WSI-level Glomerular Detection, Visualization, and Curation</title>
<link>https://arxiv.org/abs/2411.18795</link>
<guid>https://arxiv.org/abs/2411.18795</guid>
<content:encoded><![CDATA[
arXiv:2411.18795v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has demonstrated significant success in automating the detection of glomeruli, the key functional units of the kidney, from whole slide images (WSIs) in kidney pathology. However, existing open-source tools are often distributed as source code or Docker containers, requiring advanced programming skills that hinder accessibility for non-programmers, such as clinicians. Additionally, current models are typically trained on a single dataset and lack flexibility in adjusting confidence levels for predictions. To overcome these challenges, we introduce GloFinder, a QuPath plugin designed for single-click automated glomeruli detection across entire WSIs with online editing through the graphical user interface (GUI). GloFinder employs CircleNet, an anchor-free detection framework utilizing circle representations for precise object localization, with models trained on approximately 160,000 manually annotated glomeruli. To further enhance accuracy, the plugin incorporates Weighted Circle Fusion (WCF), an ensemble method that combines confidence scores from multiple CircleNet models to produce refined predictions, achieving superior performance in glomerular detection. GloFinder enables direct visualization and editing of results in QuPath, facilitating seamless interaction for clinicians and providing a powerful tool for nephropathology research and clinical practice. Code and the QuPath plugin are available at https://github.com/hrlblab/GloFinder
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition</title>
<link>https://arxiv.org/abs/2412.01137</link>
<guid>https://arxiv.org/abs/2412.01137</guid>
<content:encoded><![CDATA[
arXiv:2412.01137v2 Announce Type: replace 
Abstract: Scene text recognition (STR) suffers from challenges of either less realistic synthetic training data or the difficulty of collecting sufficient high-quality real-world data, limiting the effectiveness of trained models. Meanwhile, despite producing holistically appealing text images, diffusion-based visual text generation methods struggle to synthesize accurate and realistic instance-level text at scale. To tackle this, we introduce TextSSR: a novel pipeline for Synthesizing Scene Text Recognition training data. TextSSR targets three key synthesizing characteristics: accuracy, realism, and scalability. It achieves accuracy through a proposed region-centric text generation with position-glyph enhancement, ensuring proper character placement. It maintains realism by guiding style and appearance generation using contextual hints from surrounding text or background. This character-aware diffusion architecture enjoys precise character-level control and semantic coherence preservation, without relying on natural language prompts. Therefore, TextSSR supports large-scale generation through combinatorial text permutations. Based on these, we present TextSSR-F, a dataset of 3.55 million quality-screened text instances. Extensive experiments show that STR models trained on TextSSR-F outperform those trained on existing synthetic datasets by clear margins on common benchmarks, and further improvements are observed when mixed with real-world training data. Code is available at https://github.com/YesianRohn/TextSSR.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</title>
<link>https://arxiv.org/abs/2412.13155</link>
<guid>https://arxiv.org/abs/2412.13155</guid>
<content:encoded><![CDATA[
arXiv:2412.13155v2 Announce Type: replace 
Abstract: Artificial intelligence generative models exhibit remarkable capabilities in content creation, particularly in face image generation, customization, and restoration. However, current AI-generated faces (AIGFs) often fall short of human preferences due to unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation framework for AIGFs. To address this need, we introduce FaceQ, a large-scale, comprehensive database of AI-generated Face images with fine-grained Quality annotations reflecting human preferences. The FaceQ database comprises 12,255 images generated by 29 models across three tasks: (1) face generation, (2) face customization, and (3) face restoration. It includes 32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple dimensions: quality, authenticity, identity (ID) fidelity, and text-image correspondence. Using the FaceQ database, we establish F-Bench, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA), face quality assessment (FQA), AI-generated content image quality assessment (AIGCIQA), and preference evaluation metrics, manifesting that these standard metrics are relatively ineffective in evaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ database will be publicly available upon publication.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.06130</link>
<guid>https://arxiv.org/abs/2502.06130</guid>
<content:encoded><![CDATA[
arXiv:2502.06130v2 Announce Type: replace 
Abstract: While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, inspired by the observation that the text-to-image generation process is the inverse of image-conditioned response generation in LVLMs, we explore the potential of leveraging text-to-image generative models to assist in mitigating hallucinations in LVLMs. We discover that generative models can offer valuable self-feedback for mitigating hallucinations at both the response and token levels. Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free algorithm that incorporates feedback from text-to-image generative models into the decoding process to effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference and provides self-feedback to verify and correct the initial response through complementary or contrastive decoding. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations, consistently surpassing state-of-the-art methods across six benchmarks. Code is available at https://github.com/zhangce01/DeGF.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression</title>
<link>https://arxiv.org/abs/2503.02733</link>
<guid>https://arxiv.org/abs/2503.02733</guid>
<content:encoded><![CDATA[
arXiv:2503.02733v2 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference increases substantially, posing challenges in resource-constrained scenarios. Inspired by the success of traditional video compression frameworks, which process video frame by frame and can efficiently compress long videos, we adopt this modeling strategy for INRs to decrease memory consumption, while aiming to unify the frameworks from the perspective of timeline-based autoregressive modeling. In this work, we present a novel understanding of INR models from an autoregressive (AR) perspective and introduce a Unified AutoRegressive Framework for memory-efficient Neural Video Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural video compression under a unified autoregressive paradigm. It partitions videos into several clips and processes each clip using a different INR model instance, leveraging the advantages of both compression frameworks while allowing seamless adaptation to either in form. To further reduce temporal redundancy between clips, we design two modules to optimize the initialization, training, and compression of these model parameters. UAR-NVC supports adjustable latencies by varying the clip length. Extensive experimental results demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to resource-constrained environments and significantly improve performance compared to different baseline models. The project page: "https://wj-inf.github.io/UAR-NVC-page/".
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNF: Gaussian Neural Fields for Multidimensional Signal Representation and Reconstruction</title>
<link>https://arxiv.org/abs/2503.06762</link>
<guid>https://arxiv.org/abs/2503.06762</guid>
<content:encoded><![CDATA[
arXiv:2503.06762v2 Announce Type: replace 
Abstract: Neural fields have emerged as a powerful framework for representing continuous multidimensional signals such as images and videos, 3D and 4D objects and scenes, and radiance fields. While efficient, achieving high-quality representation requires the use of wide and deep neural networks. These, however, are slow to train and evaluate. Although several acceleration techniques have been proposed, they either trade memory for faster training and/or inference, rely on thousands of fitted primitives with considerable optimization time, or compromise the smooth, continuous nature of neural fields. In this paper, we introduce Gaussian Neural Fields (GNF), a novel compact neural decoder that maps learned feature grids into continuous non-linear signals, such as RGB images, Signed Distance Functions (SDFs), and radiance fields, using a single compact layer of Gaussian kernels defined in a high-dimensional feature space. Our key observation is that neurons in traditional MLPs perform simple computations, usually a dot product followed by an activation function, necessitating wide and deep MLPs or high-resolution feature grids to model complex functions. In this paper, we show that replacing MLP-based decoders with Gaussian kernels whose centers are learned features yields highly accurate representations of 2D (RGB), 3D (geometry), and 5D (radiance fields) signals with just a single layer of such kernels. This representation is highly parallelizable, operates on low-resolution grids, and trains in under $15$ seconds for 3D geometry and under $11$ minutes for view synthesis. GNF matches the accuracy of deep MLP-based decoders with far fewer parameters and significantly higher inference throughput.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
arXiv:2503.09151v3 Announce Type: replace 
Abstract: We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation</title>
<link>https://arxiv.org/abs/2503.13794</link>
<guid>https://arxiv.org/abs/2503.13794</guid>
<content:encoded><![CDATA[
arXiv:2503.13794v5 Announce Type: replace 
Abstract: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards properties of adversarial image perturbations</title>
<link>https://arxiv.org/abs/2503.14111</link>
<guid>https://arxiv.org/abs/2503.14111</guid>
<content:encoded><![CDATA[
arXiv:2503.14111v2 Announce Type: replace 
Abstract: Using stochastic gradient approach we study the properties of adversarial perturbations resulting in noticeable growth of VMAF image quality metric. The structure of the perturbations is investigated depending on the acceptable PSNR values and based on the Fourier power spectrum computations for the perturbations. It is demonstrated that moderate variation of image brightness ($\sim 10$ pixel units in a restricted region of an image can result in VMAF growth by $\sim 60\%$). Unlike some other methods demonstrating similar VMAF growth, the subjective quality of an image remains almost unchanged. It is also shown that the adversarial perturbations may demonstrate approximately linear dependence of perturbation amplitudes on the image brightness. The perturbations are studied based on the direct VMAF optimization in PyTorch. The significant discrepancies between the metric values and subjective judgements are also demonstrated when image restoration from noise is carried out using the same direct VMAF optimization.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CamC2V: Context-aware Controllable Video Generation</title>
<link>https://arxiv.org/abs/2504.06022</link>
<guid>https://arxiv.org/abs/2504.06022</guid>
<content:encoded><![CDATA[
arXiv:2504.06022v2 Announce Type: replace 
Abstract: Recently, image-to-video (I2V) diffusion models have demonstrated impressive scene understanding and generative quality, incorporating image conditions to guide generation. However, these models primarily animate static images without extending beyond their provided context. Introducing additional constraints, such as camera trajectories, can enhance diversity but often degrade visual quality, limiting their applicability for tasks requiring faithful scene representation. We propose CamC2V, a context-to-video (C2V) model that integrates multiple image conditions as context with 3D constraints alongside camera control to enrich both global semantics and fine-grained visual details. This enables more coherent and context-aware video generation. Moreover, we motivate the necessity of temporal awareness for an effective context representation. Our comprehensive study on the RealEstate10K dataset demonstrates improvements in visual quality and camera controllability. We will publish our code upon acceptance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TerraMind: Large-Scale Generative Multimodality for Earth Observation</title>
<link>https://arxiv.org/abs/2504.11171</link>
<guid>https://arxiv.org/abs/2504.11171</guid>
<content:encoded><![CDATA[
arXiv:2504.11171v4 Announce Type: replace 
Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces "Thinking-in-Modalities" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification</title>
<link>https://arxiv.org/abs/2504.11500</link>
<guid>https://arxiv.org/abs/2504.11500</guid>
<content:encoded><![CDATA[
arXiv:2504.11500v2 Announce Type: replace 
Abstract: Transit Origin-Destination (OD) data are fundamental for optimizing public transit services, yet current collection methods, such as manual surveys, Bluetooth and WiFi tracking, or Automated Passenger Counters, are either costly, device-dependent, or incapable of individual-level matching. Meanwhile, onboard surveillance cameras already deployed on most transit vehicles provide an underutilized opportunity for automated OD data collection. Leveraging this, we present TransitReID, a novel framework for individual-level and occlusion-resistant passenger re-identification tailored to transit environments. Our approach introduces three key innovations: (1) an occlusion-robust ReID algorithm that integrates a variational autoencoder-guided region-attention mechanism and selective quality feature averaging to dynamically emphasize visible and discriminative body regions under severe occlusions and viewpoint variations; (2) a Hierarchical Storage and Dynamic Matching HSDM mechanism that transforms static gallery matching into a dynamic process for robustness, accuracy, and speed in real-world bus operations; and (3) a multi-threaded edge implementation that enables near real-time OD estimation while ensuring privacy by processing all data locally. To support research in this domain, we also construct a new TransitReID dataset with over 17,000 images captured from bus front and rear cameras under diverse occlusion and viewpoint conditions. Experimental results demonstrate that TransitReID achieves state-of-the-art performance, with R-1 accuracy of 88.3 percent and mAP of 92.5 percent, and further sustains 90 percent OD estimation accuracy in bus route simulations on NVIDIA Jetson edge devices. This work advances both the algorithmic and system-level foundations of automated transit OD collection, paving the way for scalable, privacy-preserving deployment in intelligent transportation systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning</title>
<link>https://arxiv.org/abs/2505.13812</link>
<guid>https://arxiv.org/abs/2505.13812</guid>
<content:encoded><![CDATA[
arXiv:2505.13812v2 Announce Type: replace 
Abstract: Existing point cloud representation learning methods primarily rely on data-driven strategies to extract geometric information from large amounts of scattered data. However, most methods focus solely on the spatial distribution features of point clouds while overlooking the relationship between local information and the whole structure, which limits the accuracy of point cloud representation. Local information reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object's shape. In real-world, objects undergo deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object's geometric features. Therefore, the appropriate introduction of physics-driven mechanism can effectively compensate for the limitations of data-driven methods in structural modeling and significantly enhance the generalization and interpretability of point cloud representations in downstream tasks such as understanding and recognition. Inspired by this, we incorporate a physics-driven mechanism into the data-driven method to learn fine-grained features in point clouds and model the structural relationship between local regions and the whole shape. Specifically, we design a dual-task encoder-decoder framework that combines the geometric modeling capability of data-driven implicit fields with physics-driven elastic deformation. Through the integration of physics-based loss functions, the framework is guided to predict localized deformation and explicitly capture the correspondence between local structural changes and whole shape variations. Experimental results show that our method outperforms existing approaches in object classification and segmentation, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenFlow: Interactive Modular System for Image Generation</title>
<link>https://arxiv.org/abs/2506.21369</link>
<guid>https://arxiv.org/abs/2506.21369</guid>
<content:encoded><![CDATA[
arXiv:2506.21369v2 Announce Type: replace 
Abstract: Generative art unlocks boundless creative possibilities, yet its full potential remains untapped due to the technical expertise required for advanced architectural concepts and computational workflows. To bridge this gap, we present GenFlow, a novel modular framework that empowers users of all skill levels to generate images with precision and ease. Featuring a node-based editor for seamless customization and an intelligent assistant powered by natural language processing, GenFlow transforms the complexity of workflow creation into an intuitive and accessible experience. By automating deployment processes and minimizing technical barriers, our framework makes cutting-edge generative art tools available to everyone. A user study demonstrated GenFlow's ability to optimize workflows, reduce task completion times, and enhance user understanding through its intuitive interface and adaptive features. These results position GenFlow as a groundbreaking solution that redefines accessibility and efficiency in the realm of generative art.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</title>
<link>https://arxiv.org/abs/2508.16654</link>
<guid>https://arxiv.org/abs/2508.16654</guid>
<content:encoded><![CDATA[
arXiv:2508.16654v3 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a "black-box" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability</title>
<link>https://arxiv.org/abs/2508.21197</link>
<guid>https://arxiv.org/abs/2508.21197</guid>
<content:encoded><![CDATA[
arXiv:2508.21197v2 Announce Type: replace 
Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for interpreting deep neural networks by quantifying their sensitivity to human-defined concepts. However, when computed independently at different layers, CAVs often exhibit inconsistencies, making cross-layer comparisons unreliable. To address this issue, we propose the Global Concept Activation Vector (GCAV), a novel framework that unifies CAVs into a single, semantically consistent representation. Our method leverages contrastive learning to align concept representations across layers and employs an attention-based fusion mechanism to construct a globally integrated CAV. By doing so, our method significantly reduces the variance in TCAV scores while preserving concept relevance, ensuring more stable and reliable concept attributions. To evaluate the effectiveness of GCAV, we introduce Testing with Global Concept Activation Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We conduct extensive experiments on multiple deep neural networks, demonstrating that our method effectively mitigates concept inconsistency across layers, enhances concept localization, and improves robustness against adversarial perturbations. By integrating cross-layer information into a coherent framework, our method offers a more comprehensive and interpretable understanding of how deep learning models encode human-defined concepts. Code and models are available at https://github.com/Zhenghao-He/GCAV.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternating Minimization Schemes for Computing Rate-Distortion-Perception Functions with $f$-Divergence Perception Constraints</title>
<link>https://arxiv.org/abs/2408.15015</link>
<guid>https://arxiv.org/abs/2408.15015</guid>
<content:encoded><![CDATA[
arXiv:2408.15015v2 Announce Type: replace-cross 
Abstract: We study the computation of the rate-distortion-perception function (RDPF) for discrete memoryless sources subject to a single-letter average distortion constraint and a perception constraint belonging to the family of $f$-divergences. In this setting, the RDPF forms a convex programming problem for which we characterize optimal parametric solutions. We employ the developed solutions in an alternating minimization scheme, namely Optimal Alternating Minimization (OAM), for which we provide convergence guarantees. Nevertheless, the OAM scheme does not lead to a direct implementation of a generalized Blahut-Arimoto (BA) type of algorithm due to implicit equations in the iteration's structure. To overcome this difficulty, we propose two alternative minimization approaches whose applicability depends on the smoothness of the used perception metric: a Newton-based Alternating Minimization (NAM) scheme, relying on Newton's root-finding method for the approximation of the optimal solution of the iteration, and a Relaxed Alternating Minimization (RAM) scheme, based on relaxing the OAM iterates. We show, by deriving necessary and sufficient conditions, that both schemes guarantee convergence to a globally optimal solution. We also provide sufficient conditions on the distortion and perception constraints, which guarantee that the proposed algorithms converge exponentially fast in the number of iteration steps. We corroborate our theoretical results with numerical simulations and establish connections with existing results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of World Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.11260</link>
<guid>https://arxiv.org/abs/2501.11260</guid>
<content:encoded><![CDATA[
arXiv:2501.11260v4 Announce Type: replace-cross 
Abstract: Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</title>
<link>https://arxiv.org/abs/2503.22943</link>
<guid>https://arxiv.org/abs/2503.22943</guid>
<content:encoded><![CDATA[
arXiv:2503.22943v3 Announce Type: replace-cross 
Abstract: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-based Loss Functions in Computer Vision: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2504.04242</link>
<guid>https://arxiv.org/abs/2504.04242</guid>
<content:encoded><![CDATA[
arXiv:2504.04242v2 Announce Type: replace-cross 
Abstract: Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse</title>
<link>https://arxiv.org/abs/2506.14107</link>
<guid>https://arxiv.org/abs/2506.14107</guid>
<content:encoded><![CDATA[
arXiv:2506.14107v2 Announce Type: replace-cross 
Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces D\'ej\`a Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Free Analytical Quantization Scheme for Deep Learning Models</title>
<link>https://arxiv.org/abs/2412.07391</link>
<guid>https://arxiv.org/abs/2412.07391</guid>
<content:encoded><![CDATA[
<div> quantization, CNN models, Image classification, model optimization, computational efficiency
Summary:
- The paper addresses the challenges faced by CNN models due to their computational and storage demands.
- A novel post-training quantization method for model weights is introduced, aiming to reduce model size and computational requirements.
- The method optimizes clipping thresholds and scaling factors while minimizing quantization noise.
- Empirical results on real-world datasets showcase the effectiveness of the quantization scheme in preserving model accuracy.
- The research contributes towards making CNN models more feasible for deployment on resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2412.07391v3 Announce Type: replace 
Abstract: Despite the success of CNN models on a variety of Image classification and segmentation tasks, their extensive computational and storage demands pose considerable challenges for real-world deployment on resource-constrained devices. Quantization is one technique that aims to alleviate these large storage requirements and speed up the inference process by reducing the precision of model parameters to lower-bit representations. In this paper, we introduce a novel post-training quantization method for model weights. Our method finds optimal clipping thresholds and scaling factors along with mathematical guarantees that our method minimizes quantization noise. Empirical results on real-world datasets demonstrate that our quantization scheme significantly reduces model size and computational requirements while preserving model accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
<div> eigenvalue decomposition, Transformer architectures, mesh structure, Hodge Laplacian operator, deep learning layer

Summary:
This paper introduces a new approach for encoding mesh structures in Transformer architectures used for shape analysis tasks. Traditional methods rely on spectral features and eigenvalue decomposition operations, making them computationally expensive. The proposed method is inspired by the construction of the Hodge Laplacian operator in Discrete Exterior Calculus and utilizes discrete Hodge operators and exterior derivatives. By incorporating multi-head attention mechanisms to approximate Hodge matrices and learn discrete operators, the new architecture achieves comparable performance in mesh segmentation and classification tasks without the need for costly eigenvalue decomposition operations. This approach results in a more computationally efficient system and eliminates the complexity of preprocessing operations. <div>
arXiv:2509.01839v3 Announce Type: replace-cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis</title>
<link>https://arxiv.org/abs/2509.06986</link>
<guid>https://arxiv.org/abs/2509.06986</guid>
<content:encoded><![CDATA[
<div> Transformer-based architecture, cellular morphology, batch effects, out-of-distribution generalization, biological signal preservation <br /> 
Summary: 
CellPainTR is a new Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods, CellPainTR's design allows for effective out-of-distribution generalization to entirely unseen datasets without the need for fine-tuning. In validation on the JUMP dataset, CellPainTR outperformed established methods like ComBat and Harmony in batch integration and biological signal preservation. It demonstrated robustness through an out-of-distribution task on the Bray et al. dataset, maintaining high performance despite significant domain and feature shifts. This work represents a significant advancement in creating foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis. <br /> <div>
arXiv:2509.06986v1 Announce Type: new 
Abstract: Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection</title>
<link>https://arxiv.org/abs/2509.06987</link>
<guid>https://arxiv.org/abs/2509.06987</guid>
<content:encoded><![CDATA[
<div> multimodal fusion, defect detection, rail structure, YOLOv8n, Vision Transformer

Summary:<br />
- This paper introduces a new multimodal fusion architecture for defect detection in rail structures.
- The fusion combines YOLO and Vision Transformer backbones to enhance object detection accuracy.
- Two defect classes, rail Rupture and Surface defect, are targeted in the experimental evaluation.
- Results show a 0.2-point improvement in precision and overall accuracy compared to vision-only approaches.
- Statistical analysis confirms the significance of the accuracy differences between multimodal and single modality methods.

<br /><br />Summary: <div>
arXiv:2509.06987v1 Announce Type: new 
Abstract: Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.06988</link>
<guid>https://arxiv.org/abs/2509.06988</guid>
<content:encoded><![CDATA[
<div> Keywords: out-of-distribution detection, post-hoc method, data privacy protection, subspace projection, feature reconstruction<br />
<br />
Summary: 
The paper introduces a new post-hoc method called Classifier-based Feature Reconstruction (ClaFR) for out-of-distribution (OOD) detection, crucial for security applications. Unlike existing methods, ClaFR does not require access to training data, making it suitable for scenarios where data privacy protection is a concern. The method works by performing an orthogonal decomposition of classifier weights to extract the class-known subspace, mapping original data features into this subspace, and calculating the feature reconstruction error to determine the OOD score. Despite its simplicity, ClaFR outperforms other algorithms on various OOD benchmarks. The code for ClaFR is available on GitHub, allowing for easy implementation and testing. <div>
arXiv:2509.06988v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at https://github.com/Aie0923/ClaFR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining</title>
<link>https://arxiv.org/abs/2509.06990</link>
<guid>https://arxiv.org/abs/2509.06990</guid>
<content:encoded><![CDATA[
<div> Continued pretraining, specialized domains, SSL methods, DIET-CP, performance boost <br />
Summary: <br />
The article introduces DIET-CP, a continued pretraining strategy that aims to adapt foundation models to new target domains, especially in specialized domains with small datasets. DIET-CP requires no labels and introduces minimal hyperparameters, making it suitable for a wide range of data modalities and backbone choices. It can significantly improve the performance of state-of-the-art models like DINOv3 with just 1000 images. This strategy bridges the gap between available pretrained models and the need for continued pretraining, providing a simple and effective solution for adapting foundation models to new data distributions. <div>
arXiv:2509.06990v1 Announce Type: new 
Abstract: Continued pretraining offers a promising solution for adapting foundation models to a new target domain. However, in specialized domains, available datasets are often very small, limiting the applicability of SSL methods developed for large-scale pretraining and making hyperparameter search infeasible. In addition, pretrained models are usually released as backbone-weights only, lacking important information to continue pretraining. We propose to bridge this gap with DIET-CP, a simple continued pretraining strategy, where any strong foundation model can be steered towards the new data distribution of interest. DIET-CP relies on a very simple objective, requires no labels, and introduces no more hyperparameters than supervised finetuning. It is stable across data modalities and backbone choices, while providing a significant performance boost for state-of-the-art models such as DINOv3 using only 1000 images.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06992</link>
<guid>https://arxiv.org/abs/2509.06992</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Prompt Tuning, Vision-Language Models, Adversarial Robustness, Global Label Embedding, Cross-Layer Generator Sharing

Summary:
Federated Adversarial Prompt Tuning (FedAPT) is introduced to enhance adversarial robustness in large Vision-Language Models tuned using Federated Prompt Tuning (FPT). When faced with non-independent and identically distributed settings, FedAPT addresses the "class information gap" by implementing a class-aware prompt generator guided by a Global Label Embedding. This generator creates more globally-aligned visual prompts, improving robustness against adversarial attacks. Additionally, a cross-layer generator sharing strategy enhances prompt coupling across different layers, further strengthening defense mechanisms. Through extensive experiments on various image classification datasets, FedAPT outperforms existing methods, demonstrating superior adversarial robustness and generalization capabilities in cross-domain and cross-dataset scenarios. FedAPT proves effective for real-world applications requiring enhanced model security. 

<br /><br />Summary: <div>
arXiv:2509.06992v1 Announce Type: new 
Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client collaborative fine-tuning of large Vision-Language Models (VLMs). However, models tuned using FPT are vulnerable to adversarial attacks, leading to misclassification in downstream tasks. In this work, we introduce Federated Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance the adversarial robustness of FPT. We identify a key issue in FedAPT under non-independent and identically distributed (non-IID) settings: a \textit{class information gap} between clients and the global model. Clients rely solely on limited local label information to generate adversarial samples for training, while the global model must defend against adversarial attacks from global labels. To address this issue, we propose a \textbf{class-aware prompt generator} that generates visual prompts from text prompts. This generator is guided by a \emph{Global Label Embedding} (serving as a ``beacon") which encodes cross-client label information to create more globally-aligned visual prompts. Additionally, we propose a \textbf{cross-layer generator sharing} strategy to enhance prompt coupling across different layers of the model, further boosting adversarial robustness. Extensive experiments on multiple image classification datasets demonstrate the superiority of FedAPT in improving adversarial robustness, outperforming existing methods by a large margin. FedAPT also exhibits exceptional generalization in cross-domain and cross-dataset scenarios, indicating its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)</title>
<link>https://arxiv.org/abs/2509.06993</link>
<guid>https://arxiv.org/abs/2509.06993</guid>
<content:encoded><![CDATA[
<div> Embed2Scale Challenge, EarthVision, geospatial models, hyperspectral data cubes, classification<br />
<br />
Summary:
The EarthVision Embed2Scale Challenge at CVPR 2025 focuses on developing geospatial models to embed SSL4EO-S12 hyperspectral data cubes into vectors for various tasks like classification and regression. The technical report presents the Top-1 winning solution for this challenge. The proposed method aims to efficiently transform the hyperspectral geospatial data cubes into embedding vectors to enhance downstream tasks. By integrating innovative techniques, the solution demonstrates effectiveness in handling complex geospatial data and achieving high performance in the challenge. The method showcases advancements in geospatial modeling and sets a benchmark for future research in utilizing hyperspectral data for diverse applications. <div>
arXiv:2509.06993v1 Announce Type: new 
Abstract: EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into embedding vectors that faciliatetes various downstream tasks, e.g., classification, regression, etc. In this technical report, we introduce our proposed method for the Top-1 winning solution on the Embed2Scale Challenge.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality</title>
<link>https://arxiv.org/abs/2509.06994</link>
<guid>https://arxiv.org/abs/2509.06994</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, enterprise applications, ViLD framework, benchmark dataset, operational requirements

Summary: 
The paper introduces the ViLD framework, designed to evaluate Vision-Language Models (VLMs) based on operational enterprise requirements. Ten business-critical tasks are defined, such as logo detection, OCR, object detection, and demographic analysis, among others. The BlockWeaver Algorithm is introduced to compare OCR outputs from VLMs effectively. A benchmark dataset of 7,500 diverse samples is constructed from real-world images and videos. ViLD combines semantic matching, traditional metrics, and novel methods to assess VLM capabilities. Leading open-source VLMs are benchmarked against a proprietary baseline using the ViLD framework, providing insights for their deployment in enterprise environments.

<br /><br />Summary: <div>
arXiv:2509.06994v1 Announce Type: new 
Abstract: Open-source Vision-Language Models show immense promise for enterprise applications, yet a critical disconnect exists between academic evaluation and enterprise deployment requirements. Current benchmarks rely heavily on multiple-choice questions and synthetic data, failing to capture the complexity of real-world business applications like social media content analysis. This paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge this gap by evaluating VLMs on operational enterprise requirements. We define ten business-critical tasks: logo detection, OCR, object detection, human presence and demographic analysis, human activity and appearance analysis, scene detection, camera perspective and media quality assessment, dominant colors, comprehensive description, and NSFW detection. To this framework, we bring an innovative BlockWeaver Algorithm that solves the challenging problem of comparing unordered, variably-grouped OCR outputs from VLMs without relying on embeddings or LLMs, achieving remarkable speed and reliability. To demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500 diverse samples, carefully stratified from a corpus of one million real-world images and videos. ViLD provides actionable insights by combining semantic matching (both embedding-based and LLM-as-a-judge approaches), traditional metrics, and novel methods to measure the completeness and faithfulness of descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and InternVL) against a powerful proprietary baseline as per ViLD framework, we provide one of the first industry-grounded, task-driven assessment of VLMs capabilities, offering actionable insights for their deployment in enterprise environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
<link>https://arxiv.org/abs/2509.06995</link>
<guid>https://arxiv.org/abs/2509.06995</guid>
<content:encoded><![CDATA[
<div> DICOM headers, self-supervised learning, Protocol Genome, clinical imaging, calibration improvements <br />
<br />Summary: 
Protocol Genome is a self-supervised learning system that utilizes DICOM headers to improve the performance of clinical imaging tasks such as chest CT triage for PE, brain MRI glioma grading, and chest radiograph cardiomegaly detection. By leveraging structured DICOM headers as labels, Protocol Genome achieves higher AUROC scores and improved calibration and robustness across different imaging modalities and vendors. The system learns protocol-aware image representations through protocol-image contrastive learning, masked protocol prediction, and protocol-protocol translation. It significantly outperforms strong SSL baselines and ImageNet transfer methods, with notable calibration improvements. The technique reduces false positives at protocol borders and is deployable in a clinical setting through PACS integration. A model card and deployment guide are provided, including de-identification and bias audits. <div>
arXiv:2509.06995v1 Announce Type: new 
Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</title>
<link>https://arxiv.org/abs/2509.06996</link>
<guid>https://arxiv.org/abs/2509.06996</guid>
<content:encoded><![CDATA[
<div> universal cultural technology, vision, symbolic communication, resilience, language models  
Writing is a universal cultural technology that allows for symbolic communication using vision. Humans have a remarkable ability to recognize words even when they are fragmented or occluded. This study investigates whether advanced vision language models (VLMs) possess the same resilience. Two benchmarks were created using Chinese logographs and English alphabetic words to test VLMs' ability to recognize "visible but unreadable" stimuli. Despite performing well on clean text, current VLMs struggle with these perturbations, often producing irrelevant outputs. This indicates a limitation in the models' reliance on visual invariances rather than compositional priors essential for strong literacy skills. The study suggests the need for architectures and training strategies that incorporate symbol segmentation and composition in cross-script contexts. These findings highlight challenges in deploying multimodal systems in education, accessibility, cultural heritage, and security.<br /><br />Summary: Writing is a universal cultural technology that allows for symbolic communication through vision, and humans have a remarkable ability to recognize words despite visual challenges. The study explores the resilience of advanced vision language models (VLMs) by testing their performance on "visible but unreadable" stimuli in different writing systems. While VLMs excel with clean text, they struggle with perturbations, indicating a need to incorporate symbol segmentation and composition in training architectures for improved literacy. This has implications for the development of multimodal systems in various fields, emphasizing the importance of addressing challenges in education, accessibility, cultural heritage, and security. <div>
arXiv:2509.06996v1 Announce Type: new 
Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Syn: K-space Data Synthesis in Ultra Low-data Regimes</title>
<link>https://arxiv.org/abs/2509.06997</link>
<guid>https://arxiv.org/abs/2509.06997</guid>
<content:encoded><![CDATA[
<div> Feature-level learning, cardiac magnetic resonance imaging, k-space data, frequency domain, dynamic MRI reconstruction <br />
Summary: <br />
This study addresses the challenge of robust reconstruction of dynamic cardiac MRI due to limited and diverse k-space data by performing feature-level learning in the frequency domain. By leveraging the global feature space of the Fourier transform, stable and rich generation of k-space data is achieved even in low-data scenarios. The integration of k-space data across time frames with multiple fusion strategies further optimizes the generative trajectory. Experimental results demonstrate the method's strong generative ability in low-data regimes, showing promise in alleviating data scarcity in dynamic MRI reconstruction. <div>
arXiv:2509.06997v1 Announce Type: new 
Abstract: Owing to the inherently dynamic and complex characteristics of cardiac magnetic resonance (CMR) imaging, high-quality and diverse k-space data are rarely available in practice, which in turn hampers robust reconstruction of dynamic cardiac MRI. To address this challenge, we perform feature-level learning directly in the frequency domain and employ a temporal-fusion strategy as the generative guidance to synthesize k-space data. Specifically, leveraging the global representation capacity of the Fourier transform, the frequency domain can be considered a natural global feature space. Therefore, unlike traditional methods that use pixel-level convolution for feature learning and modeling in the image domain, this letter focuses on feature-level modeling in the frequency domain, enabling stable and rich generation even with ultra low-data regimes. Moreover, leveraging the advantages of feature-level modeling in the frequency domain, we integrate k-space data across time frames with multiple fusion strategies to steer and further optimize the generative trajectory. Experimental results demonstrate that the proposed method possesses strong generative ability in low-data regimes, indicating practical potential to alleviate data scarcity in dynamic MRI reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
<div> attribute prediction, generalization, model evaluation, semantic grouping, clustering

Summary:
This work explores the ability of models to generalize attribute knowledge across conceptually distant categories. By introducing novel train-test split strategies, such as semantic grouping and clustering, the study evaluates the robustness of attribute prediction under conditions of reduced correlation between training and test sets. Results indicate a significant decrease in performance as the correlation decreases, highlighting the sensitivity of models to split design. Among the methods evaluated, clustering shows the most effective trade-off, reducing hidden correlations while maintaining learnability. These findings shed light on the limitations of current representations and offer valuable insights for the development of future benchmarks in attribute reasoning. 

<br /><br />Summary: <div>
arXiv:2509.06998v1 Announce Type: new 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models</title>
<link>https://arxiv.org/abs/2509.07010</link>
<guid>https://arxiv.org/abs/2509.07010</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, 3D shape generation, CAD design, quantitative evaluation, generative models<br />
<br />
Summary: 
This paper introduces a human-in-the-loop framework for evaluating 3D models generated by Large Language Models (LLMs). It proposes a range of metrics to assess the geometric fidelity of LLM-generated models compared to ground truth CAD references. The study compares LLM performance using different input modalities, showing that models generated with code-based correction prompts achieve the highest fidelity. The research highlights the importance of semantic richness in improving generation accuracy. The quantitative evaluation approach introduced in the study allows for faster convergence towards ground truth compared to traditional qualitative methods. This work contributes to advancing AI-assisted shape synthesis and provides a scalable methodology for validating and refining generative models for various CAD applications.<br /> 
Summary: <div>
arXiv:2509.07010v1 Announce Type: new 
Abstract: Large Language Models are increasingly capable of interpreting multimodal inputs to generate complex 3D shapes, yet robust methods to evaluate geometric and structural fidelity remain underdeveloped. This paper introduces a human in the loop framework for the quantitative evaluation of LLM generated 3D models, supporting applications such as democratization of CAD design, reverse engineering of legacy designs, and rapid prototyping. We propose a comprehensive suite of similarity and complexity metrics, including volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy, to benchmark generated models against ground truth CAD references. Using an L bracket component as a case study, we systematically compare LLM performance across four input modalities: 2D orthographic views, isometric sketches, geometric structure trees, and code based correction prompts. Our findings demonstrate improved generation fidelity with increased semantic richness, with code level prompts achieving perfect reconstruction across all metrics. A key contribution of this work is demonstrating that our proposed quantitative evaluation approach enables significantly faster convergence toward the ground truth, especially compared to traditional qualitative methods based solely on visual inspection and human intuition. This work not only advances the understanding of AI assisted shape synthesis but also provides a scalable methodology to validate and refine generative models for diverse CAD applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</title>
<link>https://arxiv.org/abs/2509.07021</link>
<guid>https://arxiv.org/abs/2509.07021</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, new-view synthesis, edge devices, memory compression, rendering memory<br />
Summary:<br />
MEGS$^{2}$ is a novel framework designed to optimize memory usage in 3D Gaussian Splatting (3DGS) for new-view synthesis. By focusing on reducing the total primitive number and parameters per primitive, MEGS$^{2}$ successfully achieves significant memory compression. It replaces memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes for color representations. Additionally, MEGS$^{2}$ introduces a unified soft pruning framework that addresses both primitive and lobe pruning in a single optimization problem. Experimental results demonstrate a 50% reduction in static VRAM and a 40% reduction in rendering VRAM compared to existing methods, while maintaining high rendering quality. <br /><br /> <div>
arXiv:2509.07021v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.07027</link>
<guid>https://arxiv.org/abs/2509.07027</guid>
<content:encoded><![CDATA[
<div> Gaussianity regularization, text-to-image models, moment-based regularization, power spectrum-based regularization, permutation invariance 

Summary: 
Gaussianity regularization is proposed to enforce alignment with a standard Gaussian distribution in the latent space of text-to-image models. The regularization combines moment-based and power spectrum-based approaches to ensure conformity to Gaussian properties. By applying the regularization to randomly permuted inputs, permutation invariance is maintained. Existing Gaussianity regularizations are encompassed within this unified framework, with some corresponding to specific moment losses. The proposed regularization outperforms previous methods by enhancing aesthetics, preventing reward hacking, and accelerating convergence in generative modeling for text-to-image models. The analytical knowledge of expected values in moments and power spectrum distributions allows for effective regularization and facilitates improved performance in downstream tasks.<br /><br />Summary: <div>
arXiv:2509.07027v1 Announce Type: new 
Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
<link>https://arxiv.org/abs/2509.07047</link>
<guid>https://arxiv.org/abs/2509.07047</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, microscopy, reward function optimization, SAM framework, real-time streaming data

Summary:
Image segmentation is crucial in microscopy for analyzing complex visual data. Different approaches like custom models, transfer learning, and foundational models can be used for this task. However, foundational models often have many tuning parameters that require manual optimization, hindering real-time data analysis. In this study, a reward function-based optimization approach was introduced to fine-tune the SAM framework by Meta for image segmentation tasks. By constructing reward functions that represent the physics of the imaged system, including particle size distributions and geometries, the SAM model was enhanced to align better with diverse segmentation requirements, leading to an optimized variant, SAM$^{*}$, suitable for real-time streaming data segmentation. This approach was demonstrated to be effective in microscopy imaging, where precise segmentation is essential for analyzing cellular structures, material interfaces, and nanoscale features. 

<br /><br />Summary: Image segmentation in microscopy is critical and can be done using different models. However, foundational models often require manual optimization. A reward function-based optimization approach was introduced in this study for fine-tuning the SAM framework. By integrating reward functions representing the physics of the imaged system, an optimized variant, SAM$^{*}$, was developed for diverse segmentation tasks, including real-time streaming data segmentation. This approach was effective in accurately analyzing cellular structures, material interfaces, and nanoscale features in microscopy imaging. <div>
arXiv:2509.07047v1 Announce Type: new 
Abstract: Image segmentation is a critical task in microscopy, essential for accurately analyzing and interpreting complex visual data. This task can be performed using custom models trained on domain-specific datasets, transfer learning from pre-trained models, or foundational models that offer broad applicability. However, foundational models often present a considerable number of non-transparent tuning parameters that require extensive manual optimization, limiting their usability for real-time streaming data analysis. Here, we introduce a reward function-based optimization to fine-tune foundational models and illustrate this approach for SAM (Segment Anything Model) framework by Meta. The reward functions can be constructed to represent the physics of the imaged system, including particle size distributions, geometries, and other criteria. By integrating a reward-driven optimization framework, we enhance SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$, that better aligns with the requirements of diverse segmentation tasks and particularly allows for real-time streaming data segmentation. We demonstrate the effectiveness of this approach in microscopy imaging, where precise segmentation is crucial for analyzing cellular structures, material interfaces, and nanoscale features.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Classification of Streaming Data with Image Distillation</title>
<link>https://arxiv.org/abs/2509.07049</link>
<guid>https://arxiv.org/abs/2509.07049</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming data classification, data distillation, limited resources, image data, computational efficiency

Summary:
The study explores a novel approach to efficiently classify streaming image data in resource-constrained environments. It introduces a method called Distillation Based Classification (DBC) that distills essential features from data streams to enhance classification accuracy while reducing computational demands. Comparing DBC with traditional algorithms like Hoeffding Trees and Adaptive Random Forest, DBC outperformed them and Reservoir Sampling Based Classification (RBC) with a 73.1% accuracy rate. This demonstrates the effectiveness of the DBC method in processing complex data streams and improving classification accuracy and efficiency. Overall, this research represents a significant advancement in streaming data classification, offering a new standard for handling streaming data with limited memory and computational resources. 

<br /><br />Summary: <div>
arXiv:2509.07049v1 Announce Type: new 
Abstract: This study tackles the challenge of efficiently classifying streaming data in envi-ronments with limited memory and computational resources. It delves into the application of data distillation as an innovative approach to improve the precision of streaming image data classification. By focusing on distilling essential features from data streams, our method aims to minimize computational demands while preserving crucial information for accurate classification. Our investigation com-pares this approach against traditional algorithms like Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for image data. The Distillation Based Classification (DBC) demonstrated superior performance, achieving a 73.1% accuracy rate, surpassing both traditional methods and Reservoir Sam-pling Based Classification (RBC) technique. This marks a significant advance-ment in streaming data classification, showcasing the effectiveness of our method in processing complex data streams and setting a new standard for accuracy and efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Evaluation of Gender Bias Across 13 Large Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07050</link>
<guid>https://arxiv.org/abs/2509.07050</guid>
<content:encoded><![CDATA[
<div> gender bias, multimodal models, AI-generated images, occupational stereotypes, fairness evaluation

Summary:
- Large multimodal models (LMMs) used for text-to-image generation have been found to perpetuate gender bias in AI-generated images.
- The Aymara Image Fairness Evaluation benchmark was introduced to assess social bias in AI-generated images.
- 13 commercially available LMMs were tested using gender-neutral prompts to generate images of people in different professions.
- The results showed that LMMs systematically amplified occupational gender stereotypes, with a default bias towards male representation.
- Variation in bias across models suggests that high bias is not inevitable and can be influenced by design choices.
- The study highlights the importance of standardized, automated evaluation tools for ensuring accountability and fairness in AI development.<br /><br />Summary: <div>
arXiv:2509.07050v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster VGGT with Block-Sparse Global Attention</title>
<link>https://arxiv.org/abs/2509.07120</link>
<guid>https://arxiv.org/abs/2509.07120</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, such as VGGT and $\pi^3$, for multi-view reconstruction in computer vision face a runtime bottleneck due to the quadratic complexity of global attention layers. The global attention matrix of these models shows concentration on cross-view geometric matches. This paper proposes a replacement for dense global attention using block-sparse kernels, resulting in up to 4x faster inference with comparable performance. The approach does not require retraining of the backbone, works for both VGGT and $\pi^3$, and supports large image collections. Evaluations on various multi-view benchmarks demonstrate the effectiveness of the proposed method.

Keywords: multi-view reconstruction, computer vision, transformer-based models, global attention, block-sparse kernels<br /><br />Summary: 
- Transformer-based models like VGGT and $\pi^3$ face a runtime bottleneck due to the quadratic complexity of global attention layers.
- The global attention matrix of these models indicates a concentration on cross-view geometric matches.
- A replacement for dense global attention using block-sparse kernels is proposed, resulting in up to 4x faster inference with similar performance.
- The approach does not necessitate retraining of the backbone and is applicable to both VGGT and $\pi^3.
- Evaluations on various multi-view benchmarks confirm the efficacy of the proposed method. <div>
arXiv:2509.07120v1 Announce Type: new 
Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been an important task in computer vision. Recent transformer-based models like VGGT and $\pi^3$ have achieved impressive results with simple architectures, yet they face an inherent runtime bottleneck, due to the quadratic complexity of the global attention layers, that limits the scalability to large image sets. In this paper, we empirically analyze the global attention matrix of these models and observe that probability mass concentrates on a small subset of patch-patch interactions that correspond to cross-view geometric matches. Motivated by the structured attention and inspired by recent advancement in large language models, we propose a replacement for the dense global attention operation based on highly optimized block-sparse kernels, yielding up to $4\times$ faster inference with comparable task performance. Our retrofit requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and supports large image collections. Evaluations on a comprehensive suite of multi-view benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</title>
<link>https://arxiv.org/abs/2509.07130</link>
<guid>https://arxiv.org/abs/2509.07130</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual-Inertial Odometry, Virtual Reality, pose spoofing, edge servers, unsupervised detection

Summary:
Visual-Inertial Odometry (VIO) fusion of camera and IMU data for real-time pose estimation in VR is vulnerable to pose spoofing when offloaded to edge servers. This paper introduces an unsupervised detection and recovery mechanism trained on attack-free sessions to detect runtime deviations and restore pose consistency. The model learns temporal regularities of motion to identify pose spoofing and initiates recovery measures. Evaluation in an offloaded-VIO environment using the ILLIXR testbed demonstrates significant reductions in trajectory and pose error compared to a no-defense baseline. This approach addresses the server-side threat surface posed by subtle pose spoofing, providing enhanced security and reliability for immersive VR experiences. 

<br /><br />Summary: <div>
arXiv:2509.07130v1 Announce Type: new 
Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by fusing camera and Inertial Measurement Unit (IMU) data for real-time pose. However, current trend of offloading VIO to edge servers can lead server-side threat surface where subtle pose spoofing can accumulate into substantial drift, while evading heuristic checks. In this paper, we study this threat and present an unsupervised, label-free detection and recovery mechanism. The proposed model is trained on attack-free sessions to learn temporal regularities of motion to detect runtime deviations and initiate recovery to restore pose consistency. We evaluate the approach in a realistic offloaded-VIO environment using ILLIXR testbed across multiple spoofing intensities. Experimental results in terms of well-known performance metrics show substantial reductions in trajectory and pose error compared to a no-defense baseline.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement</title>
<link>https://arxiv.org/abs/2509.07178</link>
<guid>https://arxiv.org/abs/2509.07178</guid>
<content:encoded><![CDATA[
<div> face enhancement techniques, deepfake detectors, anti-forensic, GAN-based enhancements, detection accuracy <br />
Summary: 

This study explores the impact of face enhancement techniques on the accuracy of deepfake detectors. The research suggests that while these enhancement methods improve facial appearance, they can unintentionally distort biometric features and hinder deepfake detection. By analyzing traditional image processing and GAN-based enhancements, the study evaluates the effectiveness of these techniques on various detection methods. Results show that even basic enhancement filters can significantly decrease detection accuracy, with ASR reaching up to 64.63%. Moreover, GAN-based techniques exploit these vulnerabilities further, achieving ASR up to 75.12%. The study also investigates the potential use of face enhancement methods as anti-forensic tools, highlighting the importance of developing more resilient forensic methods in response to these advancements. <div>
arXiv:2509.07178v1 Announce Type: new 
Abstract: Face enhancement techniques are widely used to enhance facial appearance. However, they can inadvertently distort biometric features, leading to significant decrease in the accuracy of deepfake detectors. This study hypothesizes that these techniques, while improving perceptual quality, can degrade the performance of deepfake detectors. To investigate this, we systematically evaluate whether commonly used face enhancement methods can serve an anti-forensic role by reducing detection accuracy. We use both traditional image processing methods and advanced GAN-based enhancements to evaluate the robustness of deepfake detectors. We provide a comprehensive analysis of the effectiveness of these enhancement techniques, focusing on their impact on Na\"ive, Spatial, and Frequency-based detection methods. Furthermore, we conduct adversarial training experiments to assess whether exposure to face enhancement transformations improves model robustness. Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2 datasets indicate that even basic enhancement filters can significantly reduce detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%. Our results demonstrate that face enhancement methods can effectively function as anti-forensic tools, emphasizing the need for more resilient and adaptive forensic methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensionally Reduced Open-World Clustering: DROWCULA</title>
<link>https://arxiv.org/abs/2509.07184</link>
<guid>https://arxiv.org/abs/2509.07184</guid>
<content:encoded><![CDATA[
<div> Keywords: annotated data, supervised learning, open-world, Vision Transformers, clustering

Summary:
In the field of supervised learning, providing labels to instances is a labor-intensive task, particularly in open-world scenarios where new classes may appear unpredictably. This study presents a fully unsupervised approach to identifying novel categories in image datasets. By leveraging Vision Transformers and attention mechanisms to generate embeddings, and incorporating manifold learning techniques to enhance clustering performance through data geometry, the proposed method achieves State-of-the-Art results on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet datasets. The approach is effective regardless of whether the number of clusters is known in advance or not. The code for implementing the method is publicly available on GitHub. This work not only showcases the potential of unsupervised techniques but also contributes to advancing the field of image classification in challenging open-world settings.<br /><br />Summary: <div>
arXiv:2509.07184v1 Announce Type: new 
Abstract: Working with annotated data is the cornerstone of supervised learning. Nevertheless, providing labels to instances is a task that requires significant human effort. Several critical real-world applications make things more complicated because no matter how many labels may have been identified in a task of interest, it could be the case that examples corresponding to novel classes may appear in the future. Not unsurprisingly, prior work in this, so-called, `open-world' context has focused a lot on semi-supervised approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully unsupervised approach to the problem of determining the novel categories in a particular dataset. Our approach relies on estimating the number of clusters using Vision Transformers, which utilize attention mechanisms to generate vector embeddings. Furthermore, we incorporate manifold learning techniques to refine these embeddings by exploiting the intrinsic geometry of the data, thereby enhancing the overall image clustering performance. Overall, we establish new State-of-the-Art results on single-modal clustering and Novel Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do so, both when the number of clusters is known or unknown ahead of time. The code is available at: https://github.com/DROWCULA/DROWCULA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning</title>
<link>https://arxiv.org/abs/2509.07213</link>
<guid>https://arxiv.org/abs/2509.07213</guid>
<content:encoded><![CDATA[
<div> Keywords: breast ultrasound segmentation, multimodal model, global context, local precision, lesion size

Summary: 
Breast ultrasound segmentation is essential for reliable measurement and analysis, but can be challenging for small or low-contrast lesions. The XBusNet model, a dual-prompt, dual-branch multimodal approach, combines image features with clinically relevant text prompts to improve segmentation accuracy. The model includes a global pathway for whole-image semantics and a local pathway for precise boundary modeling, enhanced by prompts describing lesion shape, margin, and BI-RADS terms. By automatically assembling prompts from structured metadata, manual intervention is minimized. Evaluation on the BLU dataset through cross-validation shows that XBusNet outperforms six strong baselines, achieving state-of-the-art performance with a mean Dice of 0.8765 and IoU of 0.8149. Small lesions benefit the most from the model, with improved segmentation accuracy and fewer errors. Ablation studies confirm the complementary roles of global context, local precision, and prompt-based modulation in enhancing segmentation results. <br /><br />Summary: <div>
arXiv:2509.07213v1 Announce Type: new 
Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion</title>
<link>https://arxiv.org/abs/2509.07277</link>
<guid>https://arxiv.org/abs/2509.07277</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, medical imaging, breast cancer classification, data augmentation, generative models

Summary:
This article introduces a framework for breast cancer classification in thermograms that addresses data scarcity in medical imaging. The framework utilizes a Diffusion Probabilistic Model (DPM) for data augmentation, showing superiority over traditional methods and a ProGAN baseline. By combining deep features from a pre-trained ResNet-50 with handcrafted nonlinear features like Fractal Dimension derived from U-Net segmented tumors, an XGBoost classifier achieves high accuracy (98.0%) and sensitivity (98.1%). Ablation studies and statistical tests confirm the importance of both DPM augmentation and nonlinear feature fusion in achieving this success. The study validates the effectiveness of advanced generative models and interpretable features in creating highly accurate medical diagnostic tools.<br /><br />Summary: <div>
arXiv:2509.07277v1 Announce Type: new 
Abstract: Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction Alignment Improves Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified multimodal models, Reconstruction Alignment, visual understanding, generation, post-training

Summary:
Reconstruction Alignment (RecA) is introduced as a post-training method for Unified Multimodal Models (UMMs) that leverages visual understanding encoder embeddings as dense "text prompts" to provide rich supervision without captions. By conditioning a UMM on its own visual understanding embeddings and optimizing it to reconstruct the input image with a self-supervised reconstruction loss, RecA realigns understanding and generation, improving generation and editing fidelity across different UMM architectures. With just 27 GPU-hours, RecA significantly enhances image generation performance on various benchmarks while surpassing larger open-source models. This resource-efficient method boosts editing benchmarks and establishes itself as an efficient and general post-training alignment strategy for UMMs. <div>
arXiv:2509.07295v1 Announce Type: new 
Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion</title>
<link>https://arxiv.org/abs/2509.07327</link>
<guid>https://arxiv.org/abs/2509.07327</guid>
<content:encoded><![CDATA[
<div> Keywords: multispectral remote sensing, unmanned aerial vehicle, object detection, dual-domain enhancement, mamba fusion

Summary: 
DEPF is proposed to address challenges in multispectral remote sensing object detection using UAVs. It introduces Dual-Domain Enhancement Module (DDE) for enhancing low-light images with Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). The Priority-Guided Mamba Fusion Module (PGMF) enhances local target modeling by prioritizing features based on modality difference. The method outperforms state-of-the-art techniques on DroneVehicle and VEDAI datasets. The code is available in the supplementary material. <div>
arXiv:2509.07327v1 Announce Type: new 
Abstract: Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition</title>
<link>https://arxiv.org/abs/2509.07335</link>
<guid>https://arxiv.org/abs/2509.07335</guid>
<content:encoded><![CDATA[
<div> GCNs, skeleton-based action recognition, Gaussian Topology Refinement, Gated Graph Convolution, ambiguous actions <br />
Summary: <br />
The article introduces a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. By incorporating a Gaussian filter to refine the skeleton topology graph, G$^{3}$CN improves the representation of ambiguous actions. Additionally, the integration of Gated Recurrent Units (GRUs) in the GCN framework enhances information propagation between skeleton points. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples. The method shows strong generalization across various GCN backbones, highlighting its potential for enhancing the performance of skeleton-based action recognition systems. <br /> <div>
arXiv:2509.07335v1 Announce Type: new 
Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for skeleton-based action recognition, primarily due to their ability to leverage graph topology for feature aggregation, a key factor in extracting meaningful representations. However, despite their success, GCNs often struggle to effectively distinguish between ambiguous actions, revealing limitations in the representation of learned topological and spatial features. To address this challenge, we propose a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates a Gaussian filter to refine the skeleton topology graph, improving the representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs) are integrated into the GCN framework to enhance information propagation between skeleton points. Our method shows strong generalization across various GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parse Graph-Based Visual-Language Interaction for Human Pose Estimation</title>
<link>https://arxiv.org/abs/2509.07385</link>
<guid>https://arxiv.org/abs/2509.07385</guid>
<content:encoded><![CDATA[
<div> Parse Graphs, Human Pose Estimation, Multimodal Fusion, Visual-Language Interaction, Guided Module 

Summary: 
This article introduces Parse Graph-based Visual-Language interaction (PGVL) for improving human pose estimation (HPE) by integrating visual and language modalities. Existing methods often struggle with occluded scenes, leading to alignment and location failures. PGVL addresses this issue by incorporating a novel Guided Module (GM) that allows for effective fusion of diverse information. The approach involves top-down decomposition and bottom-up composition, constructing modality-specific parse graphs and utilizing recursive bidirectional cross-attention guided by the GM. By focusing on local features for occluded areas and integrating global features for inferencing invisible parts, PGVL enhances the performance of HPE models. The proposed network design based on PGVL shows promising results on major pose estimation datasets, emphasizing the importance of multimodal fusion in enhancing context and hierarchies for accurate human pose estimation. <div>
arXiv:2509.07385v1 Announce Type: new 
Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and hierarchies, yet prior work mostly focuses on single modality modeling, ignoring the potential of multimodal fusion. Notably, language offers rich HPE priors like spatial relations for occluded scenes, but existing visual-language fusion via global feature integration weakens occluded region responses and causes alignment and location failures. To address this issue, we propose Parse Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module (GM). In PGVL, low-level nodes focus on local features, maximizing the maintenance of responses in occluded areas and high-level nodes integrate global features to infer occluded or invisible parts. GM enables high semantic nodes to guide the feature update of low semantic nodes that have undergone cross attention. It ensuring effective fusion of diverse information. PGVL includes top-down decomposition and bottom-up composition. In the first stage, modality specific parse graphs are constructed. Next stage. recursive bidirectional cross-attention is used, purified by GM. We also design network based on PGVL. The PGVL and our network is validated on major pose estimation datasets. We will release the code soon.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</title>
<link>https://arxiv.org/abs/2509.07435</link>
<guid>https://arxiv.org/abs/2509.07435</guid>
<content:encoded><![CDATA[
<div> modular design, PBR materials, 3D asset generation, Gaussian Asset Adapter, multi-view diffusion priors  
Summary:  
The article introduces the Lightweight Gaussian Asset Adapter (LGAA), a framework designed to simplify the creation of 3D assets with physically based rendering (PBR) materials. The LGAA unifies geometry modeling and PBR material synthesis by utilizing multi-view (MV) diffusion priors. Its modular design includes the LGAA Wrapper, LGAA Switcher, and LGAA Decoder components, enabling the incorporation of multiple diffusion priors for efficient convergence. Through the use of text-and image-conditioned MV diffusion models, the LGAA demonstrates superior performance in generating high-quality, relightable mesh assets. The framework is trained on a limited dataset of 69k multi-view instances, showcasing its efficiency in 3D asset creation. The code, pre-trained weights, and dataset will be made publicly available, allowing for further research and implementation in autonomous 3D asset generation.  
<br /><br />Summary: <div>
arXiv:2509.07435v1 Announce Type: new 
Abstract: The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting</title>
<link>https://arxiv.org/abs/2509.07447</link>
<guid>https://arxiv.org/abs/2509.07447</guid>
<content:encoded><![CDATA[
<div> multimodal large language models, egocentric videos, gaze information, personalized AI user experiences, EgoGazeVQA
Summary:
multimodal large language models have improved AI assistants' capabilities, egocentric videos capture user focus and context, gaze information is crucial for user intent indication, EgoGazeVQA benchmark leverages gaze for better video understanding, existing models struggle with user intentions, gaze-guided intent prompting enhances performance, spatial, temporal, and intent-related cues improve interpretation, gaze-related fine-tuning impacts prompting effectiveness, value of gaze in personalized AI assistants in egocentric settings<br /><br />Summary: <div>
arXiv:2509.07447v1 Announce Type: new 
Abstract: The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</title>
<link>https://arxiv.org/abs/2509.07450</link>
<guid>https://arxiv.org/abs/2509.07450</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-View Geo-Localization, Multi-modal, Multi-view alignment, Explainable reasoning, GLEAM-C<br />
Summary:<br />
The article introduces a new Cross-View Geo-Localization (CVGL) model, GLEAM-C, that integrates multiple views and modalities for accurate location identification. The model aligns different perspectives like UAV imagery, street maps, and ground photographs with satellite imagery. It uses a two-phase training strategy to enhance efficiency and accuracy. To improve interpretability, the authors introduce GLEAM-X, a task that combines cross-view correspondence prediction with explainable reasoning using multimodal large language models. They create a bilingual benchmark for training and testing data using GPT-4o and Doubao-1.5-Thinking-Vision-Pro. Human revision of the test set ensures detailed evaluation of explainable cross-view reasoning. This comprehensive CVGL pipeline aims to advance transparency and scalability in Geo-Localization by combining accurate matching with interpretable reasoning. The code and datasets used in the study will be publicly available on GitHub at https://github.com/Lucky-Lance/GLEAM. <br /> <div>
arXiv:2509.07450v1 Announce Type: new 
Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they merely predict whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/GLEAM.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning</title>
<link>https://arxiv.org/abs/2509.07455</link>
<guid>https://arxiv.org/abs/2509.07455</guid>
<content:encoded><![CDATA[
<div> framework, deep learning, OCTA, retinal vasculature, vascular reconstruction <br />
Summary:
The article introduces XOCT, a novel deep learning framework designed to improve the reconstruction of retinal vasculature using OCTA. XOCT utilizes Cross-Dimensional Supervision (CDS) along with a Multi-Scale Feature Fusion (MSFF) network to enhance the accuracy and resolution of vascular details in OCTA images. The CDS module provides targeted guidance by leveraging layer-wise en-face projections, allowing the network to learn distinct representations for each retinal layer. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction and channel reweighting strategies. Experimental results on the OCTA-500 dataset show significant improvements in vascular reconstruction, particularly in en-face projections critical for clinical evaluation of retinal diseases. XOCT has the potential to enhance OCTA accessibility, reliability, and diagnostic value in ophthalmic disease detection and monitoring. The code for XOCT is available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2509.07455v1 Announce Type: new 
Abstract: Optical Coherence Tomography Angiography (OCTA) and its derived en-face projections provide high-resolution visualization of the retinal and choroidal vasculature, which is critical for the rapid and accurate diagnosis of retinal diseases. However, acquiring high-quality OCTA images is challenging due to motion sensitivity and the high costs associated with software modifications for conventional OCT devices. Moreover, current deep learning methods for OCT-to-OCTA translation often overlook the vascular differences across retinal layers and struggle to reconstruct the intricate, dense vascular details necessary for reliable diagnosis. To overcome these limitations, we propose XOCT, a novel deep learning framework that integrates Cross-Dimensional Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise en-face projections, generated via segmentation-weighted z-axis averaging, as supervisory signals to compel the network to learn distinct representations for each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction combined with a channel reweighting strategy, effectively capturing vascular details at multiple spatial scales. Our experiments on the OCTA-500 dataset demonstrate XOCT's improvements, especially for the en-face projections which are significant for clinical evaluation of retinal pathologies, underscoring its potential to enhance OCTA accessibility, reliability, and diagnostic value for ophthalmic disease detection and monitoring. The code is available at https://github.com/uci-cbcl/XOCT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting</title>
<link>https://arxiv.org/abs/2509.07456</link>
<guid>https://arxiv.org/abs/2509.07456</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, bias mitigation, machine unlearning, vision models, fairness

Summary:
Deep neural networks often exhibit bias due to spurious correlations in training data, leading to unfair predictions in critical fields like medicine and autonomous driving. Traditional bias mitigation methods require retraining or data pipeline redesign, but Bias-Aware Machine Unlearning offers a post-hoc solution. By selectively removing biased samples or features, this approach can significantly reduce subgroup disparities in vision models without the need for full retraining. Through experiments on various datasets, improvements in demographic parity were observed, with gains of up to 94.86% on CUB-200, 30.28% on CIFAR-10, and 97.37% on CelebA. Achieving these results with minimal accuracy loss, machine unlearning methods scored favorably across utility, fairness, quality, and privacy evaluations. This research demonstrates the practicality of machine unlearning in enhancing fairness in deployed vision systems. 

<br /><br />Summary: <div>
arXiv:2509.07456v1 Announce Type: new 
Abstract: Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANYPORTAL: Zero-Shot Consistent Video Background Replacement</title>
<link>https://arxiv.org/abs/2509.07472</link>
<guid>https://arxiv.org/abs/2509.07472</guid>
<content:encoded><![CDATA[
<div> diffusion models, video generation technology, foreground consistency, video content creation, image diffusion models
<br />
Summary: 
ANYPORTAL is a new zero-shot framework for video background replacement that combines temporal and relighting features of diffusion models. It addresses the challenge of foreground consistency by introducing a Refinement Projection Algorithm for detailed manipulation. The framework does not require training and ensures precise foreground preservation and temporally coherent relighting. Experimental results show high-quality video results can be achieved efficiently on consumer-grade GPUs, making it a practical solution for video editing and content creation. <div>
arXiv:2509.07472v1 Announce Type: new 
Abstract: Despite the rapid advancements in video generation technology, creating high-quality videos that precisely align with user intentions remains a significant challenge. Existing methods often fail to achieve fine-grained control over video details, limiting their practical applicability. We introduce ANYPORTAL, a novel zero-shot framework for video background replacement that leverages pre-trained diffusion models. Our framework collaboratively integrates the temporal prior of video diffusion models with the relighting capabilities of image diffusion models in a zero-shot setting. To address the critical challenge of foreground consistency, we propose a Refinement Projection Algorithm, which enables pixel-level detail manipulation to ensure precise foreground preservation. ANYPORTAL is training-free and overcomes the challenges of achieving foreground consistency and temporally coherent relighting. Experimental results demonstrate that ANYPORTAL achieves high-quality results on consumer-grade GPUs, offering a practical and efficient solution for video content creation and editing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification</title>
<link>https://arxiv.org/abs/2509.07477</link>
<guid>https://arxiv.org/abs/2509.07477</guid>
<content:encoded><![CDATA[
<div> Interpretable deep learning, MedicalPatchNet, chest X-ray classification, image segmentation, CheXpert dataset<br />
<br />
Summary: 
MedicalPatchNet is a novel approach for chest X-ray classification that enhances interpretability in deep neural networks. By splitting images into patches and classifying them independently before aggregating predictions, the model provides transparent attribution of decisions to distinct regions in the image. Trained on the CheXpert dataset, MedicalPatchNet achieves comparable classification performance to EfficientNet-B0 while significantly improving interpretability. It outperforms existing methods in pathology localization accuracy, making it more reliable and accessible for non-AI experts. By mitigating risks associated with shortcut learning, the model enhances clinical trust in AI-assisted diagnostics. The publicly available code for MedicalPatchNet contributes to safer and explainable AI in medical imaging applications. <br /><br /> <div>
arXiv:2509.07477v1 Announce Type: new 
Abstract: Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors</title>
<link>https://arxiv.org/abs/2509.07484</link>
<guid>https://arxiv.org/abs/2509.07484</guid>
<content:encoded><![CDATA[
<div> Keywords: Vector graphics, implicit neural representations, text-to-video diffusion models, animation, automation

Summary: 
This paper introduces a new method for automating the animation of vector graphics by combining implicit neural representations with text-to-video diffusion models. The use of layered implicit neural representations helps reconstruct vector graphics while preserving their unique properties like infinite resolution and precise color and shape constraints. By leveraging motion priors from pretrained text-to-video diffusion models, the neural representations are optimized using video score distillation sampling to generate smooth animations. The proposed method demonstrates improved flexibility and animation quality compared to existing techniques, providing vivid and natural vector graphic animations. <div>
arXiv:2509.07484v1 Announce Type: new 
Abstract: Vector graphics, known for their scalability and user-friendliness, provide a unique approach to visual content compared to traditional pixel-based images. Animation of these graphics, driven by the motion of their elements, offers enhanced comprehensibility and controllability but often requires substantial manual effort. To automate this process, we propose a novel method that integrates implicit neural representations with text-to-video diffusion models for vector graphic animation. Our approach employs layered implicit neural representations to reconstruct vector graphics, preserving their inherent properties such as infinite resolution and precise color and shape constraints, which effectively bridges the large domain gap between vector graphics and diffusion models. The neural representations are then optimized using video score distillation sampling, which leverages motion priors from pretrained text-to-video diffusion models. Finally, the vector graphics are warped to match the representations resulting in smooth animation. Experimental results validate the effectiveness of our method in generating vivid and natural vector graphic animations, demonstrating significant improvement over existing techniques that suffer from limitations in flexibility and animation quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Visual Navigation Assistance</title>
<link>https://arxiv.org/abs/2509.07488</link>
<guid>https://arxiv.org/abs/2509.07488</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language, indoor navigation, visually impaired, BLIP-2 model, Low Rank Adaptation (LoRA)

Summary: 
The study focuses on vision-language-driven indoor navigation for the visually impaired, utilizing a combination of images and natural language guidance. Traditional navigation systems struggle indoors due to a lack of precise location data, prompting the integration of vision and language models to offer step-by-step navigational instructions. By fine-tuning the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset, the researchers were able to significantly enhance the generation of directional instructions. Additionally, a novel evaluation metric was proposed to assess navigational performance, placing emphasis on directional and sequential elements for a more comprehensive analysis. Overall, the study highlights the effectiveness of the integrated vision-language approach in improving accessibility and independence for individuals with visual impairments when navigating indoor spaces. 

<br /><br />Summary: <div>
arXiv:2509.07488v1 Announce Type: new 
Abstract: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning</title>
<link>https://arxiv.org/abs/2509.07493</link>
<guid>https://arxiv.org/abs/2509.07493</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, Signed Distance Field, 3D surface reconstruction, DiGS framework, multi-scale hierarchy <br />
Summary:<br />
The article introduces a new framework called DiGS that integrates Signed Distance Field (SDF) learning into the 3D Gaussian Splatting (3DGS) pipeline for improved surface reconstruction. By assigning each Gaussian primitive a learnable SDF value, DiGS enhances the alignment of primitives with underlying geometry, resulting in increased accuracy and consistency across views. A novel geometry-guided grid growth strategy is implemented to ensure dense and coherent distribution of Gaussians along geometry-consistent regions in a multi-scale hierarchy. Experimental results on various benchmarks, including DTU, Mip-NeRF 360, and Tanks&amp;Temples, demonstrate that the DiGS framework significantly enhances reconstruction quality and completeness while maintaining high rendering fidelity. <div>
arXiv:2509.07493v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition</title>
<link>https://arxiv.org/abs/2509.07495</link>
<guid>https://arxiv.org/abs/2509.07495</guid>
<content:encoded><![CDATA[
<div> vulnerable, adversarial attacks, deep neural networks, remote sensing applications, robustness <br />
Summary:<br />
- The article discusses the vulnerability of Deep Neural Networks (DNNs) to adversarial attacks in remote sensing applications.
- Current mixing-based strategies to increase transferability of adversarial examples may destroy global semantic features.
- The proposed framework focuses on non-targeted attacks and introduces a local mixing strategy to generate diverse yet semantically consistent inputs.
- Logit loss is adapted from targeted attacks to non-targeted scenarios to address gradient vanishing issues of cross-entropy loss.
- A perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability.
- Extensive experiments on FGSCR-42 and MTARSI datasets show superior performance compared to 12 state-of-the-art methods across 6 surrogate models, with a significant improvement in black-box attack success rate with ResNet as the surrogate on MTARSI.<br /> 
Summary: <div>
arXiv:2509.07495v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.07507</link>
<guid>https://arxiv.org/abs/2509.07507</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised annotation, 3D object detection, temporal multi-view, Teacher-Student distillation, multi-view projection loss

Summary: <br /><br /> Annotation of 3D data for object detection is a costly process, leading to the development of weakly supervised methods using 2D box annotations. However, relying solely on 2D boxes can introduce ambiguities due to multiple valid 3D poses. The MVAT framework addresses this by leveraging temporal multi-view data to create dense 3D object representations. A Teacher-Student distillation approach is used, with the Teacher network learning from multiple viewpoints and generating high-quality pseudo-labels for the Student network to predict 3D boxes accurately. A multi-view 2D projection loss ensures consistency between predicted 3D boxes and 2D annotations. Experimental results on nuScenes and Waymo Open datasets show that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, narrowing the gap with fully supervised methods without requiring 3D box annotations. The code for MVAT is available in the public repository. <div>
arXiv:2509.07507v1 Announce Type: new 
Abstract: Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \footnote{Code available upon acceptance} Our code is available in our public repository (\href{https://github.com/CEA-LIST/MVAT}{code}).
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHWGesture -- A dataset for multimodal understanding of clinical gestures</title>
<link>https://arxiv.org/abs/2509.07525</link>
<guid>https://arxiv.org/abs/2509.07525</guid>
<content:encoded><![CDATA[
<div> Gesture understanding, deep learning, multimodal dataset, clinical assessment, hand dexterity

Summary: 
The paper introduces EHWGesture, a multimodal video dataset for gesture understanding that addresses challenges in dynamic gesture recognition for applications such as automatic clinical assessment of hand dexterity. The dataset features five clinically relevant gestures captured from 25 healthy subjects using high-resolution RGB-Depth cameras and an event camera. Precise ground-truth hand landmark tracking is provided by a motion capture system, and all devices are spatially calibrated and synchronized for cross-modal alignment. The dataset includes over 1,100 recordings organized based on execution speed to embed an action quality task within gesture understanding. Baseline experiments demonstrate the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment, positioning EHWGesture as a comprehensive benchmark for advancing multimodal clinical gesture understanding. <br /><br />Summary: <div>
arXiv:2509.07525v1 Announce Type: new 
Abstract: Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Few-Shot Spatial Control for Diffusion Models</title>
<link>https://arxiv.org/abs/2509.07530</link>
<guid>https://arxiv.org/abs/2509.07530</guid>
<content:encoded><![CDATA[
<div> Few-Shot Control, Pretrained Models, Image Generation, Spatial Conditioning, Fine-Grained Control<br />
<br />
Summary: Universal Few-Shot Control (UFC) improves fine-grained control in pretrained text-to-image diffusion models by adapting to novel spatial conditions. UFC constructs task-specific control features using a matching mechanism and parameter updates from a small set. With just 30 annotated examples, UFC achieves precise control in six novel spatial tasks. Even with only 0.1% of training data, UFC performs competitively with fully supervised methods. It is applicable to various diffusion backbones like UNet and DiT, demonstrating effectiveness across architectures. The code for UFC is available on GitHub for further exploration and implementation. <div>
arXiv:2509.07530v1 Announce Type: new 
Abstract: Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
<link>https://arxiv.org/abs/2509.07534</link>
<guid>https://arxiv.org/abs/2509.07534</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked Image Modeling, 3D medical imaging, Hounsfield Unit, Foreground Masking, Segmentation.

Summary:
Masked Image Modeling (MIM) has made significant advances in computer vision, but its application in 3D medical image processing has been limited due to random masking techniques. To address this, a new approach using Hounsfield Unit (HU) measurements for Foreground Masking was developed, focusing on anatomical structures and excluding non-tissue regions like air and fluid. Experiments on multiple medical imaging datasets showed improved segmentation quality and Dice scores, reflecting the effectiveness of domain-centric MIM. This innovative strategy presents a promising direction for representation learning in medical image segmentation.

<br /><br />Summary: <div>
arXiv:2509.07534v1 Announce Type: new 
Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%, BraTS:~78.55\%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at github.com/AISeedHub/SubFore/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextlessRAG: End-to-End Visual Document RAG by Speech Without Text</title>
<link>https://arxiv.org/abs/2509.07538</link>
<guid>https://arxiv.org/abs/2509.07538</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-based question answering, document images, TextlessRAG, layout-aware reranking, bilingual dataset

Summary: 
TextlessRAG is introduced as an innovative framework for speech-based question answering over document images, eliminating the need for ASR, TTS, and OCR by directly interpreting speech. The framework retrieves relevant visual knowledge and generates answers in a fully textless pipeline. A layout-aware reranking mechanism is integrated to enhance retrieval performance. Experiments show significant improvements in efficiency and accuracy. The release of a bilingual speech-document RAG dataset, including Chinese and English voice queries paired with document content, aims to advance research in the field. The dataset and the TextlessRAG pipeline are accessible on the GitHub repository for further exploration and development.<br /><br />Summary: <div>
arXiv:2509.07538v1 Announce Type: new 
Abstract: Document images encapsulate a wealth of knowledge, while the portability of spoken queries enables broader and flexible application scenarios. Yet, no prior work has explored knowledge base question answering over visual document images with queries provided directly in speech. We propose TextlessRAG, the first end-to-end framework for speech-based question answering over large-scale document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR, directly interpreting speech, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. To further boost performance, we integrate a layout-aware reranking mechanism to refine retrieval. Experiments demonstrate substantial improvements in both efficiency and accuracy. To advance research in this direction, we also release the first bilingual speech--document RAG dataset, featuring Chinese and English voice queries paired with multimodal document content. Both the dataset and our pipeline will be made available at repository:https://github.com/xiepeijinhit-hue/textlessrag
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</title>
<link>https://arxiv.org/abs/2509.07552</link>
<guid>https://arxiv.org/abs/2509.07552</guid>
<content:encoded><![CDATA[
<div> Framework, Gaussian full-head synthesis, single unposed image, 3D GANs, large-scale synthetic dataset

Summary:
Our proposed feed-forward framework allows for efficient Gaussian full-head synthesis from a single unposed image in a single forward pass. By utilizing a large-scale synthetic dataset created from trained 3D GANs, we are able to train our framework effectively. A coarse-to-fine Gaussian head generation pipeline enables high-fidelity reconstruction, with sparse points from the FLAME model interacting with image features for feature extraction and shape reconstruction. A dual-branch framework integrates structured spherical triplane and unstructured point-based features for improved reconstruction. The results of our experiments demonstrate the effectiveness of our framework compared to existing methods.<br /><br />Summary: <div>
arXiv:2509.07552v1 Announce Type: new 
Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks</title>
<link>https://arxiv.org/abs/2509.07581</link>
<guid>https://arxiv.org/abs/2509.07581</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, 3D shape recognition, Graph Attention Network, Attention mechanism, Transparency<br />
Summary:<br />
- This paper introduces the Class Node Graph Attention Network (CGAT) architecture for transparent 3D shape recognition tasks, addressing the black-box nature of deep learning models.
- CGAT utilizes graph attention convolutions and an attention mechanism to provide explanations of its decision-making process, enhancing trust and accountability.
- By analyzing local mean curvature and distance to centroid node features, as well as model depth, optimal settings for the CGAT model were proposed.
- Models incorporating directed edges to a global CLS node produced more intuitive attention maps and desirable classification performance.
- The combination of local mean curvature and distance to centroid as node features yielded increased performance with a weighted F1 score of 0.76, along with more comprehensive attention visualizations. <div>
arXiv:2509.07581v1 Announce Type: new 
Abstract: Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Image Forensics: A Review and Critical Evaluation</title>
<link>https://arxiv.org/abs/2509.07591</link>
<guid>https://arxiv.org/abs/2509.07591</guid>
<content:encoded><![CDATA[
<div> Age traces, temporal image forensics, in-field sensor defects, sensor dust, eXplainable Artificial Intelligence<br />
Summary:<br />Temporal image forensics focuses on estimating the age of digital images using time-dependent traces from the image acquisition pipeline. This review provides insights into known age traces such as in-field sensor defects and sensor dust, along with various techniques used in temporal image forensics. The review also highlights the issue of content bias and emphasizes the importance of eXplainable Artificial Intelligence in validating forensic techniques. Additionally, a new forensic setting is proposed, the properties of in-field sensor defects are confirmed, and it is demonstrated that some methods may rely on content bias rather than age traces. The features learned by a neural network for dating palmprint images are analyzed, revealing potential distractions in learning age traces. The study includes a thorough review, re-implementation of previous work, and experimental verification of findings. <div>
arXiv:2509.07591v1 Announce Type: new 
Abstract: Temporal image forensics is the science of estimating the age of a digital image. Usually, time-dependent traces (age traces) introduced by the image acquisition pipeline are exploited for this purpose. In this review, a comprehensive overview of the field of temporal image forensics based on time-dependent traces from the image acquisition pipeline is given. This includes a detailed insight into the properties of known age traces (i.e., in-field sensor defects and sensor dust) and temporal image forensics techniques. Another key aspect of this work is to highlight the problem of content bias and to illustrate how important eXplainable Artificial Intelligence methods are to verify the reliability of temporal image forensics techniques. Apart from reviewing material presented in previous works, in this review: (i) a new (probably more realistic) forensic setting is proposed; (ii) the main properties (growth rate and spatial distribution) of in-field sensor defects are verified; (iii) it is shown that a method proposed to utilize in-field sensor defects for image age approximation actually exploits other traces (most likely content bias); (iv) the features learned by a neural network dating palmprint images are further investigated; (v) it is shown how easily a neural network can be distracted from learning age traces. For this purpose, previous work is analyzed, re-implemented if required and experiments are conducted.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</title>
<link>https://arxiv.org/abs/2509.07596</link>
<guid>https://arxiv.org/abs/2509.07596</guid>
<content:encoded><![CDATA[
<div> gender bias, vision-language foundation models, benchmarks, evaluation, spurious features

Summary: Gender bias in vision-language foundation models (VLMs) is a concern, often evaluated using benchmarks with gender annotations. However, spurious correlations between gender and non-gender features in these benchmarks can skew bias evaluation. To address this issue, the impact of perturbing non-gender features on bias evaluation was systematically studied across four benchmarks and various VLMs. Minimal perturbations, such as masking objects or blurring backgrounds, were found to significantly alter bias scores, indicating that current bias evaluations may be influenced by spurious features rather than true gender bias. Creating spurious feature-free benchmarks is difficult, so it is recommended to report bias metrics alongside feature-sensitivity measurements for a more reliable bias assessment. <div>
arXiv:2509.07596v1 Announce Type: new 
Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about their safe deployment and is typically evaluated using benchmarks with gender annotations on real-world images. However, as these benchmarks often contain spurious correlations between gender and non-gender features, such as objects and backgrounds, we identify a critical oversight in gender bias evaluation: Do spurious features distort gender bias evaluation? To address this question, we systematically perturb non-gender features across four widely used benchmarks (COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact on bias evaluation. Our findings reveal that even minimal perturbations, such as masking just 10% of objects or weakly blurring backgrounds, can dramatically alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in CLIP variants. This suggests that current bias evaluations often reflect model responses to spurious features rather than gender bias, undermining their reliability. Since creating spurious feature-free benchmarks is fundamentally challenging, we recommend reporting bias metrics alongside feature-sensitivity measurements to enable a more reliable bias assessment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2509.07613</link>
<guid>https://arxiv.org/abs/2509.07613</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical vision-language models, 3D imaging, Alzheimer's disease, fine-tuning pipeline, cognitive function<br />
<br />
Summary: 
This article introduces a data-efficient fine-tuning pipeline for 3D medical imaging models to improve Alzheimer's disease (AD) diagnosis. The proposed system addresses limitations in current models by leveraging patient metadata and integrating clinical diagnostic knowledge. By converting structured metadata into synthetic reports and incorporating an auxiliary token to predict cognitive function scores, the model enhances image-text alignment and provides additional supervision for fine-tuning. Through lightweight prompt tuning on both image and text modalities, the approach achieves state-of-the-art performance on AD datasets with only 1,500 training images, surpassing existing methods that require 10,000 images. This innovative methodology demonstrates the potential for more effective and efficient diagnosis of AD using 3D imaging modalities. Code for the system will be made available upon publication. <br /><br />Summary: <div>
arXiv:2509.07613v1 Announce Type: new 
Abstract: Medical vision-language models (Med-VLMs) have shown impressive results in tasks such as report generation and visual question answering, but they still face several limitations. Most notably, they underutilize patient metadata and lack integration of clinical diagnostic knowledge. Moreover, most existing models are typically trained from scratch or fine-tuned on large-scale 2D image-text pairs, requiring extensive computational resources, and their effectiveness on 3D medical imaging is often limited due to the absence of structural information. To address these gaps, we propose a data-efficient fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate its application in Alzheimer's disease (AD) diagnosis. Our system introduces two key innovations. First, we convert structured metadata into synthetic reports, enriching textual input for improved image-text alignment. Second, we add an auxiliary token trained to predict the mini-mental state examination (MMSE) score, a widely used clinical measure of cognitive function that correlates with AD severity. This provides additional supervision for fine-tuning. Applying lightweight prompt tuning to both image and text modalities, our approach achieves state-of-the-art performance on two AD datasets using 1,500 training images, outperforming existing methods fine-tuned on 10,000 images. Code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</title>
<link>https://arxiv.org/abs/2509.07623</link>
<guid>https://arxiv.org/abs/2509.07623</guid>
<content:encoded><![CDATA[
<div> framework, self-supervised, deep learning, neurodegenerative diseases, MRI data  
Summary:  
- The article introduces a self-supervised cross-encoder framework for diagnosing neurodegenerative diseases from MRI data.  
- It leverages the temporal continuity in longitudinal MRI scans for supervision, disentangling learned representations into static and dynamic components.
- The static representation captures stable anatomical features through contrastive learning. 
- The dynamic representation reflects temporal changes and can be fine-tuned for downstream classification tasks with input-gradient regularization.
- Experimental results on the ADNI dataset show superior classification accuracy and interpretability, with strong zero-shot generalization on the OASIS dataset and cross-task generalization on the PPMI dataset.  
<br /><br />Summary: <div>
arXiv:2509.07623v1 Announce Type: new 
Abstract: Deep learning has shown significant potential in diagnosing neurodegenerative diseases from MRI data. However, most existing methods rely heavily on large volumes of labeled data and often yield representations that lack interpretability. To address both challenges, we propose a novel self-supervised cross-encoder framework that leverages the temporal continuity in longitudinal MRI scans for supervision. This framework disentangles learned representations into two components: a static representation, constrained by contrastive learning, which captures stable anatomical features; and a dynamic representation, guided by input-gradient regularization, which reflects temporal changes and can be effectively fine-tuned for downstream classification tasks. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves superior classification accuracy and improved interpretability. Furthermore, the learned representations exhibit strong zero-shot generalization on the Open Access Series of Imaging Studies (OASIS) dataset and cross-task generalization on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the proposed method will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</title>
<link>https://arxiv.org/abs/2509.07647</link>
<guid>https://arxiv.org/abs/2509.07647</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic watermarking, Latent diffusion models, Hermitian Symmetric Fourier Watermarking, Center-aware embedding, Robustness.

Summary: 
Hermitian Symmetric Fourier Watermarking (SFW) is proposed as a novel embedding method for semantic watermarking techniques in latent diffusion models (LDMs). This method enforces Hermitian symmetry to maintain frequency integrity and introduces a center-aware embedding strategy to enhance robustness against cropping attacks. By applying these techniques to existing watermarking schemes, the frequency-domain structures are improved, leading to better robustness and retrieval accuracy. Experimental results demonstrate that SFW achieves state-of-the-art verification and identification performance across various attack scenarios. Ablation studies confirm the positive impact of SFW on detection capabilities, the effectiveness of center-aware embedding against cropping attacks, and the influence of message capacity on identification accuracy. SFW achieves the highest detection accuracy while maintaining superior image fidelity, as validated by FID and CLIP scores. Overall, SFW provides an effective framework for balancing robustness and image fidelity in semantic watermarking. 

<br /><br />Summary: <div>
arXiv:2509.07647v1 Announce Type: new 
Abstract: Semantic watermarking techniques for latent diffusion models (LDMs) are robust against regeneration attacks, but often suffer from detection performance degradation due to the loss of frequency integrity. To tackle this problem, we propose a novel embedding method called Hermitian Symmetric Fourier Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian symmetry. Additionally, we introduce a center-aware embedding strategy that reduces the vulnerability of semantic watermarking due to cropping attacks by ensuring robust information retention. To validate our approach, we apply these techniques to existing semantic watermarking schemes, enhancing their frequency-domain structures for better robustness and retrieval accuracy. Extensive experiments demonstrate that our methods achieve state-of-the-art verification and identification performance, surpassing previous approaches across various attack scenarios. Ablation studies confirm the impact of SFW on detection capabilities, the effectiveness of the center-aware embedding against cropping, and how message capacity influences identification accuracy. Notably, our method achieves the highest detection accuracy while maintaining superior image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed SFW is shown to be an effective framework for balancing robustness and image fidelity, addressing the inherent trade-offs in semantic watermarking. Code available at https://github.com/thomas11809/SFWMark
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection</title>
<link>https://arxiv.org/abs/2509.07654</link>
<guid>https://arxiv.org/abs/2509.07654</guid>
<content:encoded><![CDATA[
arXiv:2509.07654v1 Announce Type: new 
Abstract: Small moving target detection is crucial for many defense applications but remains highly challenging due to low signal-to-noise ratios, ambiguous visual cues, and cluttered backgrounds. In this work, we propose a novel deep learning framework that differs fundamentally from existing approaches, which often rely on target-specific features or motion cues and tend to lack robustness in complex environments. Our key insight is that small target detection and background discrimination are inherently coupled, even cluttered video backgrounds often exhibit strong low-rank structures that can serve as stable priors for detection. We reformulate the task as a tensor-based low-rank and sparse decomposition problem and conduct a theoretical analysis of the background, target, and noise components to guide model design. Building on these insights, we introduce TenRPCANet, a deep neural network that requires minimal assumptions about target characteristics. Specifically, we propose a tokenization strategy that implicitly enforces multi-order tensor low-rank priors through a self-attention mechanism. This mechanism captures both local and non-local self-similarity to model the low-rank background without relying on explicit iterative optimization. In addition, inspired by the sparse component update in tensor RPCA, we design a feature refinement module to enhance target saliency. The proposed method achieves state-of-the-art performance on two highly distinct and challenging tasks: multi-frame infrared small target detection and space object detection. These results demonstrate both the effectiveness and the generalizability of our approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration</title>
<link>https://arxiv.org/abs/2509.07662</link>
<guid>https://arxiv.org/abs/2509.07662</guid>
<content:encoded><![CDATA[
arXiv:2509.07662v1 Announce Type: new 
Abstract: Previous deep image registration methods that employ single homography, multi-grid homography, or thin-plate spline often struggle with real scenes containing depth disparities due to their inherent limitations. To address this, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet), which employs free-form deformation with an exponential-decay basis function. This design achieves higher efficiency and performs well in scenes with depth disparities, benefiting from its inherent locality. We also introduce an Adaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion aggregator used in previous methods. By transforming dense interactions into sparse ones, ASMA reduces parameters and improves accuracy. Additionally, we propose a progressive correlation refinement strategy that leverages global-local correlation patterns for coarse-to-fine motion estimation, further enhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet reduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%, respectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art method. With an additional local refinement stage,EDFFDNet-2 further improves PSNR by 1.06 dB while maintaining lower computational costs. Our method also demonstrates strong generalization ability across datasets, outperforming previous deep learning methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor Projection Removal Adversarial Training</title>
<link>https://arxiv.org/abs/2509.07673</link>
<guid>https://arxiv.org/abs/2509.07673</guid>
<content:encoded><![CDATA[
arXiv:2509.07673v1 Announce Type: new 
Abstract: Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
<link>https://arxiv.org/abs/2509.07680</link>
<guid>https://arxiv.org/abs/2509.07680</guid>
<content:encoded><![CDATA[
arXiv:2509.07680v1 Announce Type: new 
Abstract: Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</title>
<link>https://arxiv.org/abs/2509.07704</link>
<guid>https://arxiv.org/abs/2509.07704</guid>
<content:encoded><![CDATA[
arXiv:2509.07704v1 Announce Type: new 
Abstract: Recently, learned image compression has attracted considerable attention due to its superior performance over traditional methods. However, most existing approaches employ a single entropy model to estimate the probability distribution of pixel values across the entire image, which limits their ability to capture the diverse statistical characteristics of different semantic regions. To overcome this limitation, we propose Segmentation-Assisted Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework utilizes semantic segmentation to guide the selection and adaptation of multiple entropy models, enabling more accurate probability distribution estimation for distinct semantic regions. Specifically, SEEC first extracts image features and then applies semantic segmentation to identify different regions, each assigned a specialized entropy model to better capture its unique statistical properties. Finally, a multi-channel discrete logistic mixture likelihood is employed to model the pixel value distributions effectively. Experimental results on benchmark datasets demonstrate that SEEC achieves state-of-the-art compression ratios while introducing only minimal encoding and decoding latency. With superior performance, the proposed model also supports Regions of Interest (ROIs) coding condition on the provided segmentation mask. Our code is available at https://github.com/chunbaobao/SEEC.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XSRD-Net: EXplainable Stroke Relapse Detection</title>
<link>https://arxiv.org/abs/2509.07772</link>
<guid>https://arxiv.org/abs/2509.07772</guid>
<content:encoded><![CDATA[
arXiv:2509.07772v1 Announce Type: new 
Abstract: Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.07774</link>
<guid>https://arxiv.org/abs/2509.07774</guid>
<content:encoded><![CDATA[
arXiv:2509.07774v1 Announce Type: new 
Abstract: Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.
  While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</title>
<link>https://arxiv.org/abs/2509.07782</link>
<guid>https://arxiv.org/abs/2509.07782</guid>
<content:encoded><![CDATA[
arXiv:2509.07782v1 Announce Type: new 
Abstract: RayGauss has achieved state-of-the-art rendering quality for novel-view synthesis on synthetic and indoor scenes by representing radiance and density fields with irregularly distributed elliptical basis functions, rendered via volume ray casting using a Bounding Volume Hierarchy (BVH). However, its computational cost prevents real-time rendering on real-world scenes. Our approach, RayGaussX, builds on RayGauss by introducing key contributions that accelerate both training and inference. Specifically, we incorporate volumetric rendering acceleration strategies such as empty-space skipping and adaptive sampling, enhance ray coherence, and introduce scale regularization to reduce false-positive intersections. Additionally, we propose a new densification criterion that improves density distribution in distant regions, leading to enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x to 12x faster training and 50x to 80x higher rendering speeds (FPS) on real-world datasets while improving visual quality by up to +0.56 dB in PSNR. Project page with videos and code: https://raygaussx.github.io/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss</title>
<link>https://arxiv.org/abs/2509.07798</link>
<guid>https://arxiv.org/abs/2509.07798</guid>
<content:encoded><![CDATA[
arXiv:2509.07798v1 Announce Type: new 
Abstract: Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose a novel approach for fusing two orthogonal anisotropic LR MR images to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinate-based loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patient-agnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at https://github.com/MajaSchle/tripleSR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.07809</link>
<guid>https://arxiv.org/abs/2509.07809</guid>
<content:encoded><![CDATA[
arXiv:2509.07809v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>