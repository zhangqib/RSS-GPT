<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>Bidirectional Sparse Attention for Faster Video Diffusion Training</title>
<link>https://arxiv.org/abs/2509.01085</link>
<guid>https://arxiv.org/abs/2509.01085</guid>
<content:encoded><![CDATA[
<div> Keywords: Video diffusion Transformer, Bidirectional Sparse Attention, training efficiency, generative quality, computational complexity

Summary:
The article addresses the challenges faced by Video diffusion Transformer (DiT) models in producing high-resolution, long-duration videos due to computational bottlenecks. The proposed Bidirectional Sparse Attention (BSA) framework aims to improve training and inference efficiency by dynamically sparsifying Queries and Key-Value pairs within 3D full attention. This approach optimizes query sparsity through semantic similarity and dynamic spatial-time training, while achieving Key-Value sparsity by retaining only the most salient blocks for computation. Experiments show that BSA accelerates DiT training by reducing FLOPs by up to 20x and achieving 17.79x faster attention training. Despite these efficiency improvements, the generative quality of the models is preserved or even improved when compared to full attention models. <div>
arXiv:2509.01085v2 Announce Type: replace 
Abstract: Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery Aligned With Canopy Height Maps</title>
<link>https://arxiv.org/abs/2509.01202</link>
<guid>https://arxiv.org/abs/2509.01202</guid>
<content:encoded><![CDATA[
<div> LiDAR, canopy height maps, deep learning, forest monitoring, PrediTree<br>
Summary:<br>
The PrediTree dataset is introduced for training and evaluating tree height prediction models at sub-meter resolution. It combines high-resolution LiDAR-derived canopy height maps with multi-temporal and multi-spectral imagery from diverse forest ecosystems in France. This dataset enables the training of deep learning methods to predict tree growth based on past observations, filling a critical monitoring gap. An encoder-decoder framework is proposed to utilize the dataset, considering the relative time differences between the canopy height map and image acquisition dates. Experiments show that a U-Net architecture trained on PrediTree achieves the lowest error rate among tested models, outperforming ResNet-50 by 12% and reducing error by 30% compared to experiments with fewer bands. The dataset is publicly available, along with processing and training codebases for further research and applications. <br><br>Summary: <div>
arXiv:2509.01202v2 Announce Type: replace 
Abstract: We present PrediTree, the first comprehensive open-source dataset designed for training and evaluating tree height prediction models at sub-meter resolution. This dataset combines very high-resolution (0.5m) LiDAR-derived canopy height maps, spatially aligned with multi-temporal and multi-spectral imagery, across diverse forest ecosystems in France, totaling 3,141,568 images. PrediTree addresses a critical gap in forest monitoring capabilities by enabling the training of deep learning methods that can predict tree growth based on multiple past observations. To make use of this PrediTree dataset, we propose an encoder-decoder framework that requires the multi-temporal multi-spectral imagery and the relative time differences in years between the canopy height map timestamp (target) and each image acquisition date for which this framework predicts the canopy height. The conducted experiments demonstrate that a U-Net architecture trained on the PrediTree dataset provides the highest masked mean squared error of $11.78\%$, outperforming the next-best architecture, ResNet-50, by around $12\%$, and cutting the error of the same experiments but on fewer bands (red, green, blue only), by around $30\%$. This dataset is publicly available on https://huggingface.co/datasets/hiyam-d/PrediTree, and both processing and training codebases are available on {GitHub}.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</title>
<link>https://arxiv.org/abs/2509.01907</link>
<guid>https://arxiv.org/abs/2509.01907</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing, Disaster Monitoring, Image Pairs, Textual Annotations, Vision-Language Models

Summary:
The article introduces the Remote Sensing Change Caption (RSCC) dataset, a comprehensive benchmark consisting of over 62,000 pre- and post-disaster image pairs with detailed change captions. This dataset addresses the lack of temporal image pairs and textual annotations in existing remote sensing datasets, enabling a more accurate understanding of disaster impacts over time. By bridging the gap between temporal and semantic information in remote sensing data, RSCC facilitates the training and evaluation of vision-language models for disaster-aware bi-temporal analysis. The results demonstrate the dataset's effectiveness in supporting detailed disaster-related analysis and pave the way for enhanced vision-language applications in remote sensing. The code and dataset for RSCC are readily available for further research and development at https://github.com/Bili-Sakura/RSCC. 

<br><br>Summary: 
- RSCC dataset comprises pre- and post-disaster image pairs with detailed change captions. 
- It enables robust training of vision-language models for disaster monitoring. 
- Addresses the lack of temporal image pairs and textual annotations in existing datasets. 
- Facilitates accurate and interpretable disaster-related analysis in remote sensing. 
- Dataset and code are accessible for further research and development. <div>
arXiv:2509.01907v3 Announce Type: replace 
Abstract: Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>3D and 4D World Modeling: A Survey</title>
<link>https://arxiv.org/abs/2509.07996</link>
<guid>https://arxiv.org/abs/2509.07996</guid>
<content:encoded><![CDATA[
<div> Keywords: world modeling, 3D representation, generative methods, structured taxonomy, evaluation metrics 

Summary: 
World modeling in the field of artificial intelligence has been a crucial research area for agents to understand and predict dynamic environments. This survey focuses on 3D and 4D world modeling, filling gaps in existing literature that often overlook these representations. It introduces a structured taxonomy categorizing approaches into VideoGen, OccGen, and LiDARGen, based on different representations. The survey also defines precise terms and presents datasets and evaluation metrics specifically designed for 3D/4D settings. Practical applications of world modeling are discussed, along with open challenges in the field. The survey aims to provide a foundational reference for researchers working on 3D and 4D world modeling, highlighting promising research directions for further advancements in the field.<br /><br />Summary: <div>
arXiv:2509.07996v1 Announce Type: new 
Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities</title>
<link>https://arxiv.org/abs/2509.08003</link>
<guid>https://arxiv.org/abs/2509.08003</guid>
<content:encoded><![CDATA[
<div> Hierarchical Cross-Modal Gated Attention, Heterogeneous Convolutional Adaptive Multi-Scale Attention, Cascading Convolutional Transformer Feature Refinement, urban flooding, deep-learning<br />
Summary:<br />
In response to the challenges posed by traditional flood detection methods in the face of escalating urban flooding due to climate change, XFloodNet, a novel framework, is introduced. XFloodNet incorporates advanced deep-learning techniques through three key components: a Hierarchical Cross-Modal Gated Attention mechanism for precise feature alignment, a Heterogeneous Convolutional Adaptive Multi-Scale Attention module for feature extraction, and a Cascading Convolutional Transformer Feature Refinement technique for noise-resistant flood detection. Evaluation on benchmark datasets showcases XFloodNet's state-of-the-art performance in flood classification, surpassing existing methods with significantly higher F1-scores of 93.33%, 82.24%, and 88.60% for the Chennai Floods, Rhine18 Floods, and Harz17 Floods datasets respectively. <div>
arXiv:2509.08003v1 Announce Type: new 
Abstract: In an era of escalating climate change, urban flooding has emerged as a critical challenge for sustainable cities, threatening lives, infrastructure, and ecosystems. Traditional flood detection methods are constrained by their reliance on unimodal data and static rule-based systems, which fail to capture the dynamic, non-linear relationships inherent in flood events. Furthermore, existing attention mechanisms and ensemble learning approaches exhibit limitations in hierarchical refinement, cross-modal feature integration, and adaptability to noisy or unstructured environments, resulting in suboptimal flood classification performance. To address these challenges, we present XFloodNet, a novel framework that redefines urban flood classification through advanced deep-learning techniques. XFloodNet integrates three novel components: (1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically aligns visual and textual features, enabling precise multi-granularity interactions and resolving contextual ambiguities; (2) a Heterogeneous Convolutional Adaptive Multi-Scale Attention module, which leverages frequency-enhanced channel attention and frequency-modulated spatial attention to extract and prioritize discriminative flood-related features across spectral and spatial domains; and (3) a Cascading Convolutional Transformer Feature Refinement technique that harmonizes hierarchical features through adaptive scaling and cascading operations, ensuring robust and noise-resistant flood detection. We evaluate our proposed method on three benchmark datasets, such as Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively, surpassing existing methods by significant margins.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
<link>https://arxiv.org/abs/2509.08016</link>
<guid>https://arxiv.org/abs/2509.08016</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, Video Parallel Scaling, temporal detail, computational costs, performance improvement

Summary: 
Video Large Language Models (VideoLLMs) face challenges in capturing fine-grained temporal detail while minimizing computational costs. The proposed solution, Video Parallel Scaling (VPS), expands a model's perceptual bandwidth without increasing its context window by running multiple parallel inference streams. By aggregating the output probabilities from these streams, VPS effectively leverages uncorrelated visual evidence to improve performance without additional training. The approach contracts the Chinchilla scaling law, enhancing temporal reasoning capabilities across various model architectures and scales. VPS outperforms other parallel alternatives and complements other decoding strategies, offering a memory-efficient and robust framework for VideoLLMs. Extensive experiments demonstrate the consistent and significant performance improvements enabled by VPS on benchmarks such as Video-MME and EventHallusion.  

<br /><br />Summary: <div>
arXiv:2509.08016v1 Announce Type: new 
Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change</title>
<link>https://arxiv.org/abs/2509.08024</link>
<guid>https://arxiv.org/abs/2509.08024</guid>
<content:encoded><![CDATA[
<div> keywords: stance detection, multimodal, hierarchical fusion, Large Language Model, transformer module<br />
Summary:<br />
The article introduces a new multimodal stance detection framework that integrates textual and visual information through a hierarchical fusion approach. It utilizes a Large Language Model to retrieve stance-relevant summaries from text and a domain-aware image caption generator to interpret visual content. A specialized transformer module then captures interactions between the texts and images for joint modeling. The proposed framework achieves an accuracy of 76.2%, precision of 76.3%, recall of 76.2%, and F1-score of 76.2% on the MultiClimate dataset, surpassing existing state-of-the-art methods. This approach addresses the challenge posed by social media content combining text and visual elements, providing a robust solution for stance classification.<br /> 
Summary: <div>
arXiv:2509.08024v1 Announce Type: new 
Abstract: With the rapid proliferation of information across digital platforms, stance detection has emerged as a pivotal challenge in social media analysis. While most of the existing approaches focus solely on textual data, real-world social media content increasingly combines text with visual elements creating a need for advanced multimodal methods. To address this gap, we propose a multimodal stance detection framework that integrates textual and visual information through a hierarchical fusion approach. Our method first employs a Large Language Model to retrieve stance-relevant summaries from source text, while a domain-aware image caption generator interprets visual content in the context of the target topic. These modalities are then jointly modeled along with the reply text, through a specialized transformer module that captures interactions between the texts and images. The proposed modality fusion framework integrates diverse modalities to facilitate robust stance classification. We evaluate our approach on the MultiClimate dataset, a benchmark for climate change-related stance detection containing aligned video frames and transcripts. We achieve accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%, respectively, outperforming existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2509.08026</link>
<guid>https://arxiv.org/abs/2509.08026</guid>
<content:encoded><![CDATA[
<div> Keywords: SI-EDTL, swarm intelligence, ensemble, transfer learning, UAV images 

Summary: 
SI-EDTL is a two-stage swarm intelligence ensemble deep transfer learning model designed for detecting multiple vehicles in UAV images. It utilizes three pre-trained feature extractor models and five transfer classifiers to create 15 base learners, which are then aggregated through weighted averaging to classify regions as Car, Van, Truck, Bus, or background. The model optimizes hyperparameters using the whale optimization algorithm to ensure a balance between accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing, SI-EDTL demonstrates superior performance compared to existing methods on the AU-AIR UAV dataset. <div>
arXiv:2509.08026v1 Announce Type: new 
Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep transfer learning model for detecting multiple vehicles in UAV images. It combines three pre-trained Faster R-CNN feature extractor models (InceptionV3, ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5, Na\"ive Bayes), resulting in 15 different base learners. These are aggregated via weighted averaging to classify regions as Car, Van, Truck, Bus, or background. Hyperparameters are optimized with the whale optimization algorithm to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV dataset.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
<link>https://arxiv.org/abs/2509.08027</link>
<guid>https://arxiv.org/abs/2509.08027</guid>
<content:encoded><![CDATA[
<div> dataset, Martian, digital elevation model, machine learning, MCTED<br />
<br />
Summary: <br />
This work introduces the MCTED dataset for predicting Martian digital elevation models using machine learning. The dataset comprises 80,898 data samples generated from high-resolution Mars orthoimages and DEM pairs from the Mars Reconnaissance Orbiter. A comprehensive processing pipeline was employed to address artefacts and missing data points. The dataset includes optical image patches, DEM patches, and mask patches to handle altered elevation regions. Training and validation splits were carefully created to prevent data leakage. Statistical insights on sample distribution and elevation values are provided. A small U-Net model trained on the MCTED dataset outperformed a depth estimation foundation model, showcasing the dataset's effectiveness. The dataset and code for its generation are openly available for use in public repositories. <br /> <div>
arXiv:2509.08027v1 Announce Type: new 
Abstract: This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
<link>https://arxiv.org/abs/2509.08104</link>
<guid>https://arxiv.org/abs/2509.08104</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud prediction, loss functions, deep learning, adaptive probabilistic matching, ShapeNet benchmarks<br />
Summary:<br />
The article introduces the Adaptive Probabilistic Matching Loss (APML) as a new approach for training deep learning models in point cloud prediction tasks. Existing loss functions like Chamfer Distance have limitations such as point congestion and non-differentiable operations. APML addresses these issues by using Sinkhorn iterations on a similarity matrix to approximate one-to-one matching. By analytically determining the temperature parameter, APML ensures minimum assignment probability without manual tuning. The proposed loss function offers near-quadratic runtime and superior spatial distribution in both dense and sparse regions, leading to faster convergence and improved performance on ShapeNet benchmarks and a spatiotemporal Transformer model. The code for APML is available for public use on GitHub, offering a practical solution for enhancing deep learning models in point cloud prediction tasks. <br /><br />Summary: <div>
arXiv:2509.08104v1 Announce Type: new 
Abstract: Training deep learning models for point cloud prediction tasks such as shape completion and generation depends critically on loss functions that measure discrepancies between predicted and ground-truth point sets. Commonly used functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on nearest-neighbor assignments, which often induce many-to-one correspondences, leading to point congestion in dense regions and poor coverage in sparse regions. These losses also involve non-differentiable operations due to index selection, which may affect gradient-based optimization. Earth Mover Distance (EMD) enforces one-to-one correspondences and captures structural similarity more effectively, but its cubic computational complexity limits its practical use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully differentiable approximation of one-to-one matching that leverages Sinkhorn iterations on a temperature-scaled similarity matrix derived from pairwise distances. We analytically compute the temperature to guarantee a minimum assignment probability, eliminating manual tuning. APML achieves near-quadratic runtime, comparable to Chamfer-based losses, and avoids non-differentiable operations. When integrated into state-of-the-art architectures (PoinTr, PCN, FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC) that generates 3D human point clouds from WiFi CSI measurements, APM loss yields faster convergence, superior spatial distribution, especially in low-density regions, and improved or on-par quantitative performance without additional hyperparameter search. The code is available at: https://github.com/apm-loss/apml.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2509.08205</link>
<guid>https://arxiv.org/abs/2509.08205</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, deep unfolding networks, lightweight framework, robust principal component analysis, channel attention mechanism 

Summary: 
The article introduces a new framework called L-RPCANet for infrared small target detection (ISTD) that addresses challenges in parameter lightweightness and noise robustness. L-RPCANet utilizes a hierarchical bottleneck structure to refine channel-wise features and reduce network parameter complexity. A noise reduction module is incorporated to enhance noise robustness, while the integration of squeeze-and-excitation networks (SENets) as a channel attention mechanism allows for focusing on important features across channels. Extensive experiments on ISTD datasets demonstrate the superior performance of L-RPCANet compared to existing methods such as RPCANet, DRPCANet, and RPCANet++. The code for L-RPCANet will be made available on GitHub at the specified link. <div>
arXiv:2509.08205v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) is one of the key techniques in image processing. Although deep unfolding networks (DUNs) have demonstrated promising performance in ISTD due to their model interpretability and data adaptability, existing methods still face significant challenges in parameter lightweightness and noise robustness. In this regard, we propose a highly lightweight framework based on robust principal component analysis (RPCA) called L-RPCANet. Technically, a hierarchical bottleneck structure is constructed to reduce and increase the channel dimension in the single-channel input infrared image to achieve channel-wise feature refinement, with bottleneck layers designed in each module to extract features. This reduces the number of channels in feature extraction and improves the lightweightness of network parameters. Furthermore, a noise reduction module is embedded to enhance the robustness against complex noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a channel attention mechanism to focus on the varying importance of different features across channels, thereby achieving excellent performance while maintaining both lightweightness and robustness. Extensive experiments on the ISTD datasets validate the superiority of our proposed method compared with state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code will be available at https://github.com/xianchaoxiu/L-RPCANet.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing</title>
<link>https://arxiv.org/abs/2509.08228</link>
<guid>https://arxiv.org/abs/2509.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: digital cameras, compressive measurement, Video Snapshot Compressive Imaging, Ultra-Sparse Sampling, Digital Micro-mirror Device

Summary:
Digital cameras consume a significant amount of power to capture and encode video, leading to concerns about sustainability with high-speed cameras. Compressive measurement techniques, such as Video Snapshot Compressive Imaging (SCI) and Ultra-Sparse Sampling (USS), have been proposed to reduce power consumption per pixel significantly. USS introduces a new sampling strategy where only one sub-frame is set to 1 at each spatial location, allowing for efficient image recovery. A Digital Micro-mirror Device (DMD) encoding system validates the effectiveness of USS. The proposed BSTFormer algorithm utilizes different attention mechanisms to exploit the sparsity of USS measurements successfully. USS offers a higher dynamic range than traditional random sampling methods and is suitable for implementing a complete video SCI system on a chip due to its fixed exposure time. This research presents a promising direction for sustainable high-speed imaging applications. 

<br /><br />Summary: <div>
arXiv:2509.08228v1 Announce Type: new 
Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps. Imagining gigapixel cameras operating at 100-1000 fps, the current processing model is unsustainable. To address this, physical layer compressive measurement has been proposed to reduce power consumption per pixel by 10-100X. Video Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the optical sensor layer to increase effective frame rate. A commonly used sampling strategy of video SCI is Random Sampling (RS) where each mask element value is randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated that images can be recovered from a fraction of the image pixels. Inspired by I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial location, only one sub-frame is set to 1 and all others are set to 0. We then build a Digital Micro-mirror Device (DMD) encoding system to verify the effectiveness of our USS strategy. Ideally, we can decompose the USS measurement into sub-measurements for which we can utilize I2P algorithms to recover high-speed frames. However, due to the mismatch between the DMD and CCD, the USS measurement cannot be perfectly decomposed. To this end, we propose BSTFormer, a sparse TransFormer that utilizes local Block attention, global Sparse attention, and global Temporal attention to exploit the sparsity of the USS measurement. Extensive results on both simulated and real-world data show that our method significantly outperforms all previous state-of-the-art algorithms. Additionally, an essential advantage of the USS strategy is its higher dynamic range than that of the RS strategy. Finally, from the application perspective, the USS strategy is a good choice to implement a complete video SCI system on chip due to its fixed exposure time.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation</title>
<link>https://arxiv.org/abs/2509.08232</link>
<guid>https://arxiv.org/abs/2509.08232</guid>
<content:encoded><![CDATA[
<div> Keywords: video anomaly detection, fatal incidents, GTA-Crime dataset, data generation framework, domain adaptation strategy

Summary:
Recent advancements in video anomaly detection have enabled the identification of criminal activities in surveillance videos, but the detection of fatal incidents such as shootings and stabbings remains challenging due to their rarity and ethical issues. To address this limitation, the authors introduce GTA-Crime, a dataset and generation framework using Grand Theft Auto 5 (GTA5) containing fatal situations captured from CCTV multiview perspectives under various conditions. They also release a framework for generating these types of videos. By incorporating GTA-Crime with a snippet-level domain adaptation strategy using Wasserstein adversarial training, they aim to bridge the gap between synthetic and real-world features, such as UCF-Crime, for enhanced fatal violence detection accuracy. Experimental results validate the effectiveness of the GTA-Crime dataset and demonstrate the improvement in real-world violence detection accuracy through the proposed domain adaptation strategy. The dataset and data generation framework are publicly available for research purposes. 

<br /><br />Summary: <div>
arXiv:2509.08232v1 Announce Type: new 
Abstract: Recent advancements in video anomaly detection (VAD) have enabled identification of various criminal activities in surveillance videos, but detecting fatal incidents such as shootings and stabbings remains difficult due to their rarity and ethical issues in data collection. Recognizing this limitation, we introduce GTA-Crime, a fatal video anomaly dataset and generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains fatal situations such as shootings and stabbings, captured from CCTV multiview perspectives under diverse conditions including action types, weather, time of day, and viewpoints. To address the rarity of such scenarios, we also release a framework for generating these types of videos. Additionally, we propose a snippet-level domain adaptation strategy using Wasserstein adversarial training to bridge the gap between synthetic GTA-Crime features and real-world features like UCF-Crime. Experimental results validate our GTA-Crime dataset and demonstrate that incorporating GTA-Crime with our domain adaptation strategy consistently enhances real world fatal violence detection accuracy. Our dataset and the data generation framework are publicly available at https://github.com/ta-ho/GTA-Crime.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification</title>
<link>https://arxiv.org/abs/2509.08234</link>
<guid>https://arxiv.org/abs/2509.08234</guid>
<content:encoded><![CDATA[
<div> ViT; CXR; medical imaging; channel replication; pneumonia<br />
<br />
Summary:<br />
A new channel replication strategy, RepViT-CXR, has been developed to adapt single-channel grayscale chest X-ray images for ViTs, showcasing high performance in TB and pneumonia detection. On the TB-CXR dataset, RepViT-CXR achieved a remarkable accuracy of 99.9% and AUC of 99.9%, surpassing previous state-of-the-art methods like Topo-CXR. The Pediatric Pneumonia dataset also saw impressive results, with RepViT-CXR obtaining 99.0% accuracy, superior to DCNN and VGG16 baselines. Additionally, on the Shenzhen TB dataset, the approach achieved 91.1% accuracy and an AUC of 91.2%, outperforming CNN-based methods. This research highlights the efficacy of simple yet effective techniques like channel replication in enabling ViTs to excel in grayscale medical imaging tasks, showcasing their potential for real-world clinical screening applications.<br /><br /> <div>
arXiv:2509.08234v1 Announce Type: new 
Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia. Recent advances in deep learning, particularly Vision Transformers (ViTs), have shown strong potential for automated medical image analysis. However, most ViT architectures are pretrained on natural images and require three-channel inputs, while CXR scans are inherently grayscale. To address this gap, we propose RepViT-CXR, a channel replication strategy that adapts single-channel CXR images into a ViT-compatible format without introducing additional information loss. We evaluate RepViT-CXR on three benchmark datasets. On the TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%, surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy, 99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0% accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%, outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a performance improvement over previously reported CNN-based methods. These results demonstrate that a simple yet effective channel replication strategy allows ViTs to fully leverage their representational power on grayscale medical imaging tasks. RepViT-CXR establishes a new state of the art for TB and pneumonia detection from chest X-rays, showing strong potential for deployment in real-world clinical screening systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI</title>
<link>https://arxiv.org/abs/2509.08243</link>
<guid>https://arxiv.org/abs/2509.08243</guid>
<content:encoded><![CDATA[
<div> Keywords: sMRI, deep learning, Alzheimer's disease, asymmetry, diagnostic accuracy 

Summary: 
This study introduces a novel end-to-end network for detecting Alzheimer's disease based on brain asymmetry induced by atrophy. The network combines a 3D CNN Encoder with a Symmetry Interactive Transformer (SIT) to analyze left and right hemisphere features. By focusing on asymmetrical regions caused by structural changes, the network achieves a higher diagnostic accuracy of 92.5% compared to other CNN-based methods. Visualization results demonstrate that the network effectively identifies regions of brain atrophy, especially the asymmetric pathological characteristics associated with Alzheimer's disease. This approach improves interpretability and diagnostic performance, highlighting the importance of considering brain asymmetry in the detection of AD. <br /><br />Summary: <div>
arXiv:2509.08243v1 Announce Type: new 
Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has achieved remarkable progress in the prediction and diagnosis of Alzheimer's disease (AD). Existing studies have used CNN and transformer to build a well-performing network, but most of them are based on pretraining or ignoring the asymmetrical character caused by brain disorders. We propose an end-to-end network for the detection of disease-based asymmetric induced by left and right brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive Transformer (SIT). Following the inter-equal grid block fetch operation, the corresponding left and right hemisphere features are aligned and subsequently fed into the SIT for diagnostic analysis. SIT can help the model focus more on the regions of asymmetry caused by structural changes, thus improving diagnostic performance. We evaluated our method based on the ADNI dataset, and the results show that the method achieves better diagnostic accuracy (92.5\%) compared to several CNN methods and CNNs combined with a general transformer. The visualization results show that our network pays more attention in regions of brain atrophy, especially for the asymmetric pathological characteristics induced by AD, demonstrating the interpretability and effectiveness of the method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.08260</link>
<guid>https://arxiv.org/abs/2509.08260</guid>
<content:encoded><![CDATA[
<div> Keywords: event-based cameras, video deblurring, frame interpolation, self-supervised learning, dataset construction

Summary: 
The paper introduces a unified self-supervised framework, EVDI++, for Event-based Video Deblurring and Interpolation using frame-based cameras with extended exposure times. The Learnable Double Integral (LDI) network is utilized to estimate the mapping relation between reference frames and sharp latent images. A learning-based division reconstruction module refines coarse results and optimizes overall training efficiency. An adaptive fusion strategy is employed for final results by utilizing confidence embedded in LDI outputs of concurrent events. A self-supervised learning framework enables network training with real-world blurry videos and events. The proposed method achieves state-of-the-art performance in video deblurring and interpolation tasks on both synthetic and real-world datasets, including a dataset constructed with a DAVIS346c camera. <div>
arXiv:2509.08260v1 Announce Type: new 
Abstract: Frame-based cameras with extended exposure times often produce perceptible visual blurring and information loss between frames, significantly degrading video quality. To address this challenge, we introduce EVDI++, a unified self-supervised framework for Event-based Video Deblurring and Interpolation that leverages the high temporal resolution of event cameras to mitigate motion blur and enable intermediate frame prediction. Specifically, the Learnable Double Integral (LDI) network is designed to estimate the mapping relation between reference frames and sharp latent images. Then, we refine the coarse results and optimize overall training efficiency by introducing a learning-based division reconstruction module, enabling images to be converted with varying exposure intervals. We devise an adaptive parameter-free fusion strategy to obtain the final results, utilizing the confidence embedded in the LDI outputs of concurrent events. A self-supervised learning framework is proposed to enable network training with real-world blurry videos and events by exploring the mutual constraints among blurry frames, latent images, and event streams. We further construct a dataset with real-world blurry images and events using a DAVIS346c camera, demonstrating the generalizability of the proposed EVDI++ in real-world scenarios. Extensive experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art performance in video deblurring and interpolation tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral Mamba for Hyperspectral Object Tracking</title>
<link>https://arxiv.org/abs/2509.08265</link>
<guid>https://arxiv.org/abs/2509.08265</guid>
<content:encoded><![CDATA[
<div> Spectral information, Cross-depth interactions, Temporal dependencies, Object tracking, Hyperspectral imaging <br />
Summary: <br />
The article introduces a new hyperspectral object tracking network called HyMamba, which addresses the limitations of existing hyperspectral trackers by unifying spectral, cross-depth, and temporal modeling through state space modules. The Spectral State Integration module enables the refinement and propagation of spectral features with cross-depth and temporal spectral information. The Hyperspectral Mamba module learns spatial and spectral information simultaneously through three directional scanning SSMs. HyMamba combines features from false-color and hyperspectral inputs, enhancing them with original spectral features from raw hyperspectral images. Experimental results show that HyMamba achieves state-of-the-art performance on seven benchmark datasets, with impressive AUC and DP@20 scores on the HOTC2020 dataset. The code for HyMamba will be made available on GitHub for further research and development. <br /> <div>
arXiv:2509.08265v1 Announce Type: new 
Abstract: Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features</title>
<link>https://arxiv.org/abs/2509.08266</link>
<guid>https://arxiv.org/abs/2509.08266</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, biases, attention values, image characteristics, prompt specificity

Summary: 
This study explores how Vision Language Models (VLMs) rely on biases during training to answer questions about visual properties, particularly when asked specific questions. The researchers developed a framework to analyze the impact of different input data characteristics on VLM performance. By examining attention values in VLMs with varying input parameters, they found that even minor changes in image and prompt details can significantly affect how the model generates answers. Specifically, modifications in image size, objects in the image, background color, and prompt specificity can lead to variations in VLM performance. This research highlights the importance of understanding how VLM behavior changes based on input data characteristics and suggests the need for methods to characterize and address these changes effectively. <br /><br />Summary: <div>
arXiv:2509.08266v1 Announce Type: new 
Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on inherent biases learned during training to respond to questions about visual properties of an image. These biases are exacerbated when VLMs are asked highly specific questions that require focusing on specific areas of the image. For example, a VLM tasked with counting stars on a modified American flag (e.g., with more than 50 stars) will often disregard the visual evidence and fail to answer accurately. We build upon this research and develop a multi-dimensional examination framework to systematically determine which characteristics of the input data, including both the image and the accompanying prompt, lead to such differences in performance. Using open-source VLMs, we further examine how attention values fluctuate with varying input parameters (e.g., image size, number of objects in the image, background color, prompt specificity). This research aims to learn how the behavior of vision language models changes and to explore methods for characterizing such changes. Our results suggest, among other things, that even minor modifications in image characteristics and prompt specificity can lead to large changes in how a VLM formulates its answer and, subsequently, its overall performance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title>
<link>https://arxiv.org/abs/2509.08280</link>
<guid>https://arxiv.org/abs/2509.08280</guid>
<content:encoded><![CDATA[
<div> uncertainty estimator, semantic segmentation, 3D point clouds, zero-shot learning, ScanNet v2<br />
<br />
Summary:<br /> 
The paper introduces E3DPC-GZSL, a novel approach for generalized zero-shot semantic segmentation of 3D point clouds. One of the main challenges in this task is the tendency of models to make biased predictions towards seen classes. E3DPC-GZSL addresses this issue by integrating an evidence-based uncertainty estimator into the classifier, allowing for adjustment of prediction probabilities based on prediction uncertainty. Additionally, the method introduces a training strategy that refines the semantic space by merging learnable parameters with text-derived features, improving model optimization for unseen data. Experimental results show that E3DPC-GZSL achieves state-of-the-art performance on datasets such as ScanNet v2 and S3DIS. <div>
arXiv:2509.08280v1 Announce Type: new 
Abstract: Generalized zero-shot semantic segmentation of 3D point clouds aims to classify each point into both seen and unseen classes. A significant challenge with these models is their tendency to make biased predictions, often favoring the classes encountered during training. This problem is more pronounced in 3D applications, where the scale of the training data is typically smaller than in image-based tasks. To address this problem, we propose a novel method called E3DPC-GZSL, which reduces overconfident predictions towards seen classes without relying on separate classifiers for seen and unseen data. E3DPC-GZSL tackles the overconfidence problem by integrating an evidence-based uncertainty estimator into a classifier. This estimator is then used to adjust prediction probabilities using a dynamic calibrated stacking factor that accounts for pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel training strategy that improves uncertainty estimation by refining the semantic space. This is achieved by merging learnable parameters with text-derived features, thereby improving model optimization for unseen data. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on generalized zero-shot semantic segmentation datasets, including ScanNet v2 and S3DIS.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection</title>
<link>https://arxiv.org/abs/2509.08289</link>
<guid>https://arxiv.org/abs/2509.08289</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly supervised object detection, Heatmap-guided proposal selector, Background class representation, Semantic gap, Negative certainty supervision loss

Summary:
Weakly supervised object detection (WSOD) aims to detect objects without requiring box-level annotations. Current WSOD methods face limitations in generating accurate pseudo GT boxes, lacking background class representation, and slow convergence during optimization. To address these challenges, a heatmap-guided proposal selector (HGPS) algorithm is proposed to pre-select proposals and refine pseudo GT boxes. Additionally, a weakly supervised basic detection network (WSBDN) is introduced to incorporate background class representation and bridge the semantic gap between branches. Furthermore, a negative certainty supervision loss is implemented to speed up convergence by utilizing ignored proposals. Experimental results on PASCAL VOC datasets show significant performance improvements compared to state-of-the-art WSOD methods, achieving mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012. The framework code is available for public use at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2509.08289v1 Announce Type: new 
Abstract: Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia</title>
<link>https://arxiv.org/abs/2509.08303</link>
<guid>https://arxiv.org/abs/2509.08303</guid>
<content:encoded><![CDATA[
<div> oil palm plantations, deforestation, Indonesia, geospatial dataset, sustainability

Summary: 
The article introduces a new open-access geospatial dataset focusing on oil palm plantations in Indonesia, a major contributor to deforestation in the region. The dataset, created through expert labeling of high-resolution satellite imagery, includes detailed annotations of oil palm plantations and related land cover types from 2020 to 2024. It distinguishes different stages of oil palm planting and similar perennial crops, ensuring data quality through consensus and field validation. By providing a comprehensive dataset under a CC-BY license, it aims to support monitoring of oil palm expansion, aid sustainability efforts, and align with global deforestation reduction goals. The dataset, suitable for training neural networks, fills a crucial gap in remote sensing data and promotes transparency in land cover mapping. Following FAIR data principles, the resource contributes to enhancing accuracy in tracking and addressing the challenges posed by oil palm cultivation in Indonesia. <br /><br /> <div>
arXiv:2509.08303v1 Announce Type: new 
Abstract: Oil palm cultivation remains one of the leading causes of deforestation in Indonesia. To better track and address this challenge, detailed and reliable mapping is needed to support sustainability efforts and emerging regulatory frameworks. We present an open-access geospatial dataset of oil palm plantations and related land cover types in Indonesia, produced through expert labeling of high-resolution satellite imagery from 2020 to 2024. The dataset provides polygon-based, wall-to-wall annotations across a range of agro-ecological zones and includes a hierarchical typology that distinguishes oil palm planting stages as well as similar perennial crops. Quality was ensured through multi-interpreter consensus and field validation. The dataset was created using wall-to-wall digitization over large grids, making it suitable for training and benchmarking both conventional convolutional neural networks and newer geospatial foundation models. Released under a CC-BY license, it fills a key gap in training data for remote sensing and aims to improve the accuracy of land cover types mapping. By supporting transparent monitoring of oil palm expansion, the resource contributes to global deforestation reduction goals and follows FAIR data principles.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</title>
<link>https://arxiv.org/abs/2509.08311</link>
<guid>https://arxiv.org/abs/2509.08311</guid>
<content:encoded><![CDATA[
<div> Keywords: medical vision-language, pre-training, CT scans, radiograph interpretation, SimCroP

Summary:
SimCroP is a novel framework designed for chest CT scans that leverages similarity-driven alignment and cross-granularity fusion to enhance radiograph interpretation. By optimizing the encoder using multi-modal masked modeling, SimCroP can better understand low-level semantics from radiographs. The similarity-driven alignment module enables the model to select and align the correct patches corresponding to each sentence in reports, addressing the challenge of spatial sparsity in lesions. The cross-granularity fusion module integrates information across different levels, improving the model's ability to capture key pathology structures in sparse radiographs. Pre-trained on a large-scale paired CT-reports dataset, SimCroP outperforms state-of-the-art medical self-supervised learning and vision-language pre-training methods on image classification and segmentation tasks. This framework is valuable for enhancing the performance of multi-scale downstream tasks in medical imaging analysis.<br /><br />Summary: <div>
arXiv:2509.08311v1 Announce Type: new 
Abstract: Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Experimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at https://github.com/ToniChopp/SimCroP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosted Training of Lightweight Early Exits for Optimizing CNN Image Classification Inference</title>
<link>https://arxiv.org/abs/2509.08318</link>
<guid>https://arxiv.org/abs/2509.08318</guid>
<content:encoded><![CDATA[
<div> Boosted Training Scheme for Early Exits, real-time image classification, resource-constrained platforms, convolutional neural networks, embedded deployment

Summary:
Boosted Training Scheme for Early Exits (BTS-EE) addresses the challenge of real-time image classification on resource-constrained platforms by introducing a sequential training approach for early-exit strategies. This approach aligns branch training with inference-time data distributions, improving efficiency in balancing accuracy with latency and power budgets. A lightweight branch architecture based on 1D convolutions and a Class Precision Margin (CPM) calibration method enable reliable exit decisions by allowing per-class threshold tuning. Experiments on the CINIC-10 dataset with a ResNet18 backbone show that BTS-EE consistently outperforms non-boosted training, achieving up to a 45 percent reduction in computation with only a 2 percent accuracy degradation. These findings expand the design space for deploying convolutional neural networks in real-time image processing systems, offering practical efficiency gains for applications such as industrial inspection, embedded vision, and UAV-based monitoring. 

<br /><br />Summary: <div>
arXiv:2509.08318v1 Announce Type: new 
Abstract: Real-time image classification on resource-constrained platforms demands inference methods that balance accuracy with strict latency and power budgets. Early-exit strategies address this need by attaching auxiliary classifiers to intermediate layers of convolutional neural networks (CNNs), allowing "easy" samples to terminate inference early. However, conventional training of early exits introduces a covariance shift: downstream branches are trained on full datasets, while at inference they process only the harder, non-exited samples. This mismatch limits efficiency--accuracy trade-offs in practice. We introduce the Boosted Training Scheme for Early Exits (BTS-EE), a sequential training approach that aligns branch training with inference-time data distributions. Each branch is trained and calibrated before the next, ensuring robustness under selective inference conditions. To further support embedded deployment, we propose a lightweight branch architecture based on 1D convolutions and a Class Precision Margin (CPM) calibration method that enables per-class threshold tuning for reliable exit decisions. Experiments on the CINIC-10 dataset with a ResNet18 backbone demonstrate that BTS-EE consistently outperforms non-boosted training across 64 configurations, achieving up to 45 percent reduction in computation with only 2 percent accuracy degradation. These results expand the design space for deploying CNNs in real-time image processing systems, offering practical efficiency gains for applications such as industrial inspection, embedded vision, and UAV-based monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2509.08338</link>
<guid>https://arxiv.org/abs/2509.08338</guid>
<content:encoded><![CDATA[
<div> Keywords: malignant melanoma, convolutional neural networks, vision-language models, retrieval-augmented framework, clinical decision support <br />
Summary: <br />
Accurate and early diagnosis of malignant melanoma is crucial for patient outcomes. While CNNs have shown promise in dermoscopic image analysis, they often overlook clinical metadata and need substantial preprocessing. Vision-language models offer a multimodal approach but struggle with clinical specificity on general-domain data. A retrieval-augmented VLM framework is proposed, incorporating semantically similar patient cases for diagnostic prompts. This method enables informed predictions without fine-tuning and significantly enhances accuracy and error correction compared to traditional methods. Results show that retrieval-augmented prompting is a robust strategy for clinical decision support. <div>
arXiv:2509.08338v1 Announce Type: new 
Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving patient outcomes. While convolutional neural networks (CNNs) have shown promise in dermoscopic image analysis, they often neglect clinical metadata and require extensive preprocessing. Vision-language models (VLMs) offer a multimodal alternative but struggle to capture clinical specificity when trained on general-domain data. To address this, we propose a retrieval-augmented VLM framework that incorporates semantically similar patient cases into the diagnostic prompt. Our method enables informed predictions without fine-tuning and significantly improves classification accuracy and error correction over conventional baselines. These results demonstrate that retrieval-augmented prompting provides a robust strategy for clinical decision support.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.08374</link>
<guid>https://arxiv.org/abs/2509.08374</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Detection, Multi-view Cameras, LiDAR, InsFusion, Autonomous Driving 

Summary:
InsFusion is a new approach proposed for three-dimensional object detection in autonomous driving and smart transportation systems. It addresses the issue of accumulated errors in the feature extraction process by extracting proposals from both raw and fused features and utilizing attention mechanisms applied to the raw features. By querying the raw features with the extracted proposals, InsFusion effectively mitigates the impact of the accumulated errors. Experimental results on the nuScenes dataset show that InsFusion outperforms various advanced baseline methods and achieves new state-of-the-art performance in 3D object detection. This innovative method combines the strengths of multi-view cameras and LiDAR data, highlighting the importance of feature fusion and attention mechanisms in enhancing object detection accuracy. 

<br /><br />Summary: <div>
arXiv:2509.08374v1 Announce Type: new 
Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a crucial component for autonomous driving and smart transportation. However, in the process of basic feature extraction, perspective transformation, and feature fusion, noise and error will gradually accumulate. To address this issue, we propose InsFusion, which can extract proposals from both raw and fused features and utilizes these proposals to query the raw features, thereby mitigating the impact of accumulated errors. Additionally, by incorporating attention mechanisms applied to the raw features, it thereby mitigates the impact of accumulated errors. Experiments on the nuScenes dataset demonstrate that InsFusion is compatible with various advanced baseline methods and delivers new state-of-the-art performance for 3D object detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</title>
<link>https://arxiv.org/abs/2509.08376</link>
<guid>https://arxiv.org/abs/2509.08376</guid>
<content:encoded><![CDATA[
<div> Keywords: disentangle, video data, self-supervised learning, transformer-based architecture, motion transfer<br />
Summary:<br />
The article proposes a new framework for disentangling video data into dynamic motion and static content components. It introduces a self-supervised pipeline that utilizes a transformer-based architecture to generate flexible features for motion and content. By incorporating low-bitrate vector quantization as an information bottleneck, the framework promotes disentanglement and creates a meaningful discrete motion space. The latent motion and content are then used as conditional inputs to a denoising diffusion model for self-supervised representation learning. The framework is tested on real-world talking head videos for tasks like motion transfer and auto-regressive motion generation, showing its effectiveness. Moreover, it demonstrates generalizability to other types of video data such as pixel sprites of 2D cartoon characters. This work contributes to the field of video analysis and generation by presenting a novel approach to self-supervised learning of disentangled video representations. <br /><br />Summary: <div>
arXiv:2509.08376v1 Announce Type: new 
Abstract: We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Causality-Aware Vision-Based 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2509.08388</link>
<guid>https://arxiv.org/abs/2509.08388</guid>
<content:encoded><![CDATA[
<div> Keyword: 3D semantic occupancy prediction, 2D-to-3D transformation pipeline, causal loss, Occ3D benchmark, camera perturbations <br />
Summary: 
The paper introduces a novel causal loss for 3D semantic occupancy prediction that enables end-to-end supervision of the 2D-to-3D transformation pipeline. This loss ensures gradient flow regulation from 3D voxel representations back to 2D features, making the entire pipeline differentiable and trainable. The proposed Semantic Causality-Aware 2D-to-3D Transformation includes Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for robustness against camera perturbations, and Normalized Convolution for feature propagation. Experimental results on the Occ3D benchmark show that the method achieves state-of-the-art performance, demonstrating improved robustness to camera perturbations and enhanced 2D-to-3D semantic consistency. <br /><br />Summary: <div>
arXiv:2509.08388v1 Announce Type: new 
Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
<link>https://arxiv.org/abs/2509.08392</link>
<guid>https://arxiv.org/abs/2509.08392</guid>
<content:encoded><![CDATA[
<div> Vehicle image enhancement, Traffic surveillance, License plate recognition, Vertical Residual Autoencoder, Image restoration <br />
Summary: <br />
The study addresses the challenge of enhancing vehicle images in traffic surveillance systems affected by various degradations like noise and blur. It introduces a Vertical Residual Autoencoder (VRAE) architecture that incorporates an enhancement strategy utilizing input-aware features to improve representation learning. Experiments on a vehicle image dataset show that the VRAE outperforms conventional methods such as Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. The VRAE significantly enhances peak signal-to-noise ratio (PSNR) by 20%, reduces normalized mean squared error (NMSE) by 50%, and improves structural similarity index (SSIM) by 1% compared to AE at the same depth, with only a marginal increase in parameters. The results demonstrate the effectiveness of the VRAE in improving the accuracy of license plate recognition systems in challenging real-world conditions. <div>
arXiv:2509.08392v1 Announce Type: new 
Abstract: In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%, and enhances SSIM by 1\%, while requiring only a marginal increase of roughly 1\% in parameters.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</title>
<link>https://arxiv.org/abs/2509.08421</link>
<guid>https://arxiv.org/abs/2509.08421</guid>
<content:encoded><![CDATA[
<div> Sparse Transformation, Density-aware Weighting, Multi-view Consistency Loss, SCFusion, Multi-View Multi-Object Tracking (MVMOT) <br />
Summary: <br />
SCFusion addresses challenges in multi-view object tracking by implementing sparse transformation to avoid feature distortion, density-aware weighting for adaptive feature fusion, and multi-view consistency loss for discriminative feature learning. The framework outperforms TrackTacular with an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX. SCFusion effectively integrates features from multiple cameras in a Bird's-Eye-View space, enhancing object tracking accuracy in surveillance, autonomous driving, and sports analytics applications. <div>
arXiv:2509.08421v1 Announce Type: new 
Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.08422</link>
<guid>https://arxiv.org/abs/2509.08422</guid>
<content:encoded><![CDATA[
<div> explanation, video-based AI systems, Latent Diffusion, counterfactuals, interpretability
Summary:
- The article introduces a novel framework called Latent Diffusion for Video Counterfactual Explanations (LD-ViCE) to explain the behavior of video-based AI models.
- LD-ViCE operates in latent space using a state-of-the-art diffusion model to reduce computational costs and improve the generation of realistic and interpretable counterfactual explanations.
- Experiments show that LD-ViCE outperforms a recent state-of-the-art method, achieving a significant increase in R2 score of up to 68% while reducing inference time by half.
- The framework is tested on diverse video datasets including EchoNet-Dynamic, FERV39k, and Something-Something V2, demonstrating its effectiveness in providing valuable insights into the target model behavior.
- LD-ViCE offers semantically meaningful and temporally coherent explanations, addressing limitations in existing explanation techniques and advancing the trustworthy deployment of AI in safety-critical domains.

Summary: <br /><br />Explanation: LD-ViCE framework introduced for explaining video-based AI model behavior. <br /> Effectiveness: Outperforms state-of-the-art method with significant R2 score increase. <br /> Efficiency: Reduces computational costs and inference time. <br /> Diverse Testing: Tested on EchoNet-Dynamic, FERV39k, and Something-Something V2 datasets. <br /> Interpretability: Offers valuable insights and generates meaningful explanations for target model behavior. <div>
arXiv:2509.08422v1 Announce Type: new 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time</title>
<link>https://arxiv.org/abs/2509.08436</link>
<guid>https://arxiv.org/abs/2509.08436</guid>
<content:encoded><![CDATA[
<div> Dataset, Robustness, Classification, Hyperspectral image, Degradations  
Summary:  
The article introduces HyperTTA, a framework aimed at improving hyperspectral image classification models' robustness to various real-world degradations. By creating a multi-degradation hyperspectral dataset, nine types of degradations are systematically simulated to evaluate classification performance comprehensively. The framework includes a spectral-spatial transformer classifier (SSTC) with a multi-level receptive field mechanism and label smoothing regularization to capture spatial context effectively. Additionally, a lightweight test-time adaptation strategy, confidence-aware entropy-minimized LayerNorm adapter (CELA), is introduced to dynamically adapt without access to source data or target annotations. Notably, the proposed HyperTTA framework outperforms existing baselines across different degradation scenarios according to extensive experiments on benchmark datasets, highlighting the effectiveness of both the classification backbone and the test-time adaptation strategy.Code will be made publicly available.  
<br /><br /> <div>
arXiv:2509.08436v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification models are highly sensitive to distribution shifts caused by various real-world degradations such as noise, blur, compression, and atmospheric effects. To address this challenge, we propose HyperTTA, a unified framework designed to enhance model robustness under diverse degradation conditions. Specifically, we first construct a multi-degradation hyperspectral dataset that systematically simulates nine representative types of degradations, providing a comprehensive benchmark for robust classification evaluation. Based on this, we design a spectral-spatial transformer classifier (SSTC) enhanced with a multi-level receptive field mechanism and label smoothing regularization to jointly capture multi-scale spatial context and improve generalization. Furthermore, HyperTTA incorporates a lightweight test-time adaptation (TTA) strategy, the confidence-aware entropy-minimized LayerNorm adapter (CELA), which updates only the affine parameters of LayerNorm layers by minimizing prediction entropy on high-confidence unlabeled target samples. This confidence-aware adaptation prevents unreliable updates from noisy predictions, enabling robust and dynamic adaptation without access to source data or target annotations. Extensive experiments on two benchmark datasets demonstrate that HyperTTA outperforms existing baselines across a wide range of degradation scenarios, validating the effectiveness of both its classification backbone and the proposed TTA scheme. Code will be made available publicly.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting</title>
<link>https://arxiv.org/abs/2509.08442</link>
<guid>https://arxiv.org/abs/2509.08442</guid>
<content:encoded><![CDATA[
<div> Keywords: cortical thickness, forecasting, Spherical Brownian Bridge Diffusion Model, denoising model, neurodegenerative processes

Summary: 
The article introduces a novel approach, the Spherical Brownian Bridge Diffusion Model (SBDM), for accurate forecasting of individualized cortical thickness trajectories. The model addresses the challenges posed by the complex geometry of the cerebral cortex and the integration of multi-modal data. SBDM utilizes a bidirectional conditional Brownian bridge diffusion process to forecast cortical thickness at the vertex level of cortical surfaces. A new denoising model, the conditional spherical U-Net (CoS-UNet), is introduced to seamlessly integrate cortical surfaces and tabular conditions. Experimental results based on longitudinal datasets from the ADNI and OASIS demonstrate significantly reduced prediction errors compared to previous approaches. SBDM also enables the generation of individual factual and counterfactual cortical thickness trajectories, providing a framework for exploring hypothetical scenarios of cortical development. <br /><br />Summary: <div>
arXiv:2509.08442v1 Announce Type: new 
Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First-order State Space Model for Lightweight Image Super-resolution</title>
<link>https://arxiv.org/abs/2509.08458</link>
<guid>https://arxiv.org/abs/2509.08458</guid>
<content:encoded><![CDATA[
<div> calculation process, State Space Model, lightweight super-resolution, First-order State Space Model, MambaIR 

Summary:
State space models (SSMs) like Mamba have shown promise in NLP and vision tasks. However, existing vision models based on Mamba focus more on network architecture and scan paths than on the SSM module. To explore SSMs' potential, researchers modified the calculation process of SSM to enhance performance on lightweight super-resolution tasks. They introduced the First-order State Space Model (FSSM) to improve the original Mamba module by incorporating token correlations. By applying a first-order hold condition and deriving a new discretized form, they analyzed cumulative error. Extensive experimental results demonstrated that FSSM improves the performance of MambaIR without increasing parameters, surpassing current lightweight super-resolution methods and achieving state-of-the-art results. <br /><br />Summary: <div>
arXiv:2509.08458v1 Announce Type: new 
Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximally Useful and Minimally Redundant: The Key to Self Supervised Learning for Imbalanced Data</title>
<link>https://arxiv.org/abs/2509.08469</link>
<guid>https://arxiv.org/abs/2509.08469</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive self-supervised learning, imbalanced datasets, multi-view assumptions, mutual information, state-of-the-art accuracy

Summary: 
In this paper, the authors address the issue of imbalanced datasets in contrastive self-supervised learning. They propose a new framework that extends the traditional multi-view approach to incorporate more than two views, leveraging the concept of mutual information to enhance learning. By distinguishing between intra and inter discriminatory characteristics, the method aims to extract representative features of underrepresented classes. A novel loss function is introduced to filter out extreme features, leading to improved representation learning. Experimental results demonstrate the effectiveness of the more than two view objective, achieving state-of-the-art accuracy in self-supervised imbalanced dataset classification tasks. Specifically, significant performance gains are observed in Cifar10-LT, Cifar100-LT, and Imagenet-LT datasets using popular neural network architectures. The proposed approach shows promise in addressing the challenges posed by imbalanced datasets in self-supervised learning methodologies.<br /><br />Summary: <div>
arXiv:2509.08469v1 Announce Type: new 
Abstract: The robustness of contrastive self-supervised learning (CSSL) for imbalanced datasets is largely unexplored. CSSL usually makes use of \emph{multi-view} assumptions to learn discriminatory features via similar and dissimilar data samples. CSSL works well on balanced datasets, but does not generalize well for imbalanced datasets. In a very recent paper, as part of future work, Yann LeCun pointed out that the self-supervised multiview framework can be extended to cases involving \emph{more than two views}. Taking a cue from this insight we propose a theoretical justification based on the concept of \emph{mutual information} to support the \emph{more than two views} objective and apply it to the problem of dataset imbalance in self-supervised learning. The proposed method helps extract representative characteristics of the tail classes by segregating between \emph{intra} and \emph{inter} discriminatory characteristics. We introduce a loss function that helps us to learn better representations by filtering out extreme features. Experimental evaluation on a variety of self-supervised frameworks (both contrastive and non-contrastive) also prove that the \emph{more than two view} objective works well for imbalanced datasets. We achieve a new state-of-the-art accuracy in self-supervised imbalanced dataset classification (2\% improvement in Cifar10-LT using Resnet-18, 5\% improvement in Cifar100-LT using Resnet-18, 3\% improvement in Imagenet-LT (1k) using Resnet-50).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation</title>
<link>https://arxiv.org/abs/2509.08489</link>
<guid>https://arxiv.org/abs/2509.08489</guid>
<content:encoded><![CDATA[
<div> detection, segmentation, inpainting, vision-language description, prompt-driven image analysis

Summary:
Prompt-driven image analysis involves converting natural-language instructions into multiple steps such as detection, segmentation, editing, and description. This study showcases a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a seamless workflow. The system allows for end-to-end processing from a single prompt, offers transparent debugging with intermediate artifacts, and supports interaction through both a user interface and a command-line interface. Integration choices are highlighted to enhance robustness, including threshold adjustments, mask inspection techniques, and resource-aware defaults. The study reveals high accuracy in detection and segmentation tasks, with inpainting consuming a significant portion of the total runtime. Recommendations are provided for tuning parameters and ensuring reliability through version pinning, artifact logging, and seed control. Overall, this work offers a reliable framework for integrating modern vision and multimodal models for tasks like object replacement and scene augmentation. 

<br /><br />Summary: <div>
arXiv:2509.08489v1 Announce Type: new 
Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</title>
<link>https://arxiv.org/abs/2509.08490</link>
<guid>https://arxiv.org/abs/2509.08490</guid>
<content:encoded><![CDATA[
<div> Keywords: Underwater object detection, challenges, image quality degradation, large vision-language models, synthetic data generation <br />
Summary:<br />
- Current underwater object detection (UOD) methods face challenges including image quality degradation, small object detection, and computational constraints. Traditional techniques have limitations in capturing the complexities of underwater environments.<br />
- Large vision-language models (LVLMs) show potential in UOD by combining multi-modal capabilities. However, real-time application of LVLMs in UOD needs further exploration.<br />
- Synthetic data generation using LVLMs like DALL-E 3 and fine-tuning Florence-2 shows promise in augmenting datasets, but requires refinement for realism and applicability.<br />
- UOD needs advanced methods to overcome challenges and improve performance in dynamic underwater environments.<br />
- Further research is necessary to optimize LVLM application in real-time UOD scenarios. <br /> <div>
arXiv:2509.08490v1 Announce Type: new 
Abstract: Underwater object detection (UOD) is vital to diverse marine applications, including oceanographic research, underwater robotics, and marine conservation. However, UOD faces numerous challenges that compromise its performance. Over the years, various methods have been proposed to address these issues, but they often fail to fully capture the complexities of underwater environments. This review systematically categorizes UOD challenges into five key areas: Image quality degradation, target-related issues, data-related challenges, computational and processing constraints, and limitations in detection methodologies. To address these challenges, we analyze the progression from traditional image processing and object detection techniques to modern approaches. Additionally, we explore the potential of large vision-language models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated in other domains. We also present case studies, including synthetic dataset generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review identifies three key insights: (i) Current UOD methods are insufficient to fully address challenges like image degradation and small object detection in dynamic underwater environments. (ii) Synthetic data generation using LVLMs shows potential for augmenting datasets but requires further refinement to ensure realism and applicability. (iii) LVLMs hold significant promise for UOD, but their real-time application remains under-explored, requiring further research on optimization techniques.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</title>
<link>https://arxiv.org/abs/2509.08502</link>
<guid>https://arxiv.org/abs/2509.08502</guid>
<content:encoded><![CDATA[
<div> Action recognition, time sensitivity, video representations, chiral pairs, self-supervised adaptation <br />
Summary:<br /> 
The study aims to create compact video representations sensitive to visual changes over time by introducing the task of chiral action recognition. This task involves differentiating between pairs of temporally opposite actions like opening and closing a door. The proposed model uses a self-supervised adaptation recipe to inject time-sensitivity into frozen image features, based on an auto-encoder with a latent space inspired by perceptual straightening. The model demonstrates linear separability between chiral pairs on three datasets – Something-Something, EPIC-Kitchens, and Charade, outperforming larger pre-trained video models. It also enhances classification performance on standard benchmarks when combined with existing models. <div>
arXiv:2509.08502v1 Announce Type: new 
Abstract: Our objective is to develop compact video representations that are sensitive to visual change over time. To measure such time-sensitivity, we introduce a new task: chiral action recognition, where one needs to distinguish between a pair of temporally opposite actions, such as "opening vs. closing a door", "approaching vs. moving away from something", "folding vs. unfolding paper", etc. Such actions (i) occur frequently in everyday life, (ii) require understanding of simple visual change over time (in object state, size, spatial position, count . . . ), and (iii) are known to be poorly represented by many video embeddings. Our goal is to build time aware video representations which offer linear separability between these chiral pairs. To that end, we propose a self-supervised adaptation recipe to inject time-sensitivity into a sequence of frozen image features. Our model is based on an auto-encoder with a latent space with inductive bias inspired by perceptual straightening. We show that this results in a compact but time-sensitive video representation for the proposed task across three datasets: Something-Something, EPIC-Kitchens, and Charade. Our method (i) outperforms much larger video models pre-trained on large-scale video datasets, and (ii) leads to an improvement in classification performance on standard benchmarks when combined with these existing models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</title>
<link>https://arxiv.org/abs/2509.08519</link>
<guid>https://arxiv.org/abs/2509.08519</guid>
<content:encoded><![CDATA[
<div> Keywords: Human-Centric Video Generation, multimodal inputs, collaborative control, task-specific strategies, progressive training

Summary: 
HuMo is a unified framework for Human-Centric Video Generation that tackles the challenges of coordinating multimodal inputs. It addresses the scarcity of training data by creating a dataset with diverse paired text, reference images, and audio. HuMo employs a two-stage progressive training approach with task-specific strategies for subject preservation and audio-visual sync. The model uses minimal-invasive image injection for subject preservation and a focus-by-predicting strategy for audio-visual sync. HuMo incorporates joint learning of controllabilities across multimodal inputs by progressively integrating the audio-visual sync task. During inference, a time-adaptive Classifier-Free Guidance strategy allows for flexible multimodal control. Experimental results show that HuMo outperforms specialized methods, establishing itself as a collaborative multimodal-conditioned HCVG framework. Visit the project page for more information: https://phantom-video.github.io/HuMo. 

Summary:<br /><br />Keywords: Human-Centric Video Generation, multimodal inputs, collaborative control, task-specific strategies, progressive training <br />HuMo is a unified framework for Human-Centric Video Generation that tackles the challenges of coordinating multimodal inputs. It addresses the scarcity of training data by creating a dataset with diverse paired text, reference images, and audio. HuMo employs a two-stage progressive training approach with task-specific strategies for subject preservation and audio-visual sync. The model uses minimal-invasive image injection for subject preservation and a focus-by-predicting strategy for audio-visual sync. HuMo incorporates joint learning of controllabilities across multimodal inputs by progressively integrating the audio-visual sync task. During inference, a time-adaptive Classifier-Free Guidance strategy allows for flexible multimodal control. Experimental results show that HuMo outperforms specialized methods, establishing itself as a collaborative multimodal-conditioned HCVG framework. Visit the project page for more information: https://phantom-video.github.io/HuMo. <div>
arXiv:2509.08519v1 Announce Type: new 
Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
<link>https://arxiv.org/abs/2509.08538</link>
<guid>https://arxiv.org/abs/2509.08538</guid>
<content:encoded><![CDATA[
<div> benchmark, video models, hallucinations, MESH, evaluation <br />
<br />
Summary: 
The article introduces a new benchmark, MESH, to assess the performance of Large Video Models (LVMs) in understanding dynamic video content. LVMs, which integrate temporal information with vision modules and language models, are prone to producing inaccurate or irrelevant descriptions known as hallucinations. MESH uses a Question-Answering framework with binary and multi-choice formats to systematically evaluate hallucinations in LVMs. It follows a bottom-up approach by assessing basic objects, coarse-to-fine subject features, and subject-action pairs, mirroring human video understanding processes. The evaluations demonstrate that while LVMs perform well in recognizing basic objects and features, they struggle with fine details and aligning multiple actions involving various subjects in longer videos, leading to an increase in hallucinations. MESH provides an effective and comprehensive method for identifying and addressing these hallucinations in video models. <br /> <div>
arXiv:2509.08538v1 Announce Type: new 
Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large Language Models (LLMs) and vision modules by integrating temporal information to better understand dynamic video content. Despite their progress, LVMs are prone to hallucinations-producing inaccurate or irrelevant descriptions. Current benchmarks for video hallucination depend heavily on manual categorization of video content, neglecting the perception-based processes through which humans naturally interpret videos. We introduce MESH, a benchmark designed to evaluate hallucinations in LVMs systematically. MESH uses a Question-Answering framework with binary and multi-choice formats incorporating target and trap instances. It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding. We demonstrate that MESH offers an effective and comprehensive approach for identifying hallucinations in videos. Our evaluations show that while LVMs excel at recognizing basic objects and features, their susceptibility to hallucinations increases markedly when handling fine details or aligning multiple actions involving various subjects in longer videos.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping</title>
<link>https://arxiv.org/abs/2509.08550</link>
<guid>https://arxiv.org/abs/2509.08550</guid>
<content:encoded><![CDATA[
<div> Plant phenotyping, deep learning, multi-view data, Growth Modelling Grand Challenge, Plant Age Prediction, Leaf Count Estimation <br />
Summary: 
The article introduces the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia 2025, which focuses on plant phenotyping using multi-view data. Traditional single-view models for plant analysis often lack the necessary information for accurate trait estimation. The challenge dataset includes multiple plants photographed from various heights and angles, requiring view-invariant embeddings for optimal results. The ViewSparsifier approach, incorporating 24 views in random selections, outperformed in both Plant Age Prediction and Leaf Count Estimation tasks. Experimental results with randomized view selection using selection matrices show promise for future research and enhancement of plant phenotyping accuracy. The use of multi-view data allows for a more comprehensive understanding of plant growth, health, and development, leading to improved plant health assessment and harvest readiness prediction. <br /> <div>
arXiv:2509.08550v1 Announce Type: new 
Abstract: Plant phenotyping involves analyzing observable characteristics of plants to better understand their growth, health, and development. In the context of deep learning, this analysis is often approached through single-view classification or regression models. However, these methods often fail to capture all information required for accurate estimation of target phenotypic traits, which can adversely affect plant health assessment and harvest readiness prediction. To address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia 2025 provides a multi-view dataset featuring multiple plants and two tasks: Plant Age Prediction and Leaf Count Estimation. Each plant is photographed from multiple heights and angles, leading to significant overlap and redundancy in the captured information. To learn view-invariant embeddings, we incorporate 24 views, referred to as the selection vector, in a random selection. Our ViewSparsifier approach won both tasks. For further improvement and as a direction for future research, we also experimented with randomized view selection across all five height levels (120 views total), referred to as selection matrices.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.08570</link>
<guid>https://arxiv.org/abs/2509.08570</guid>
<content:encoded><![CDATA[
<div> fusion, semantic gap, feature dispersion, Expectation-Maximization Aggregation, Text-Guided Pixel Decoder
<br />
Summary: 
Multimodal models have excelled in natural image segmentation but face challenges in medical applications due to semantic gap and feature dispersion. To address these issues, an Expectation-Maximization Aggregation mechanism is proposed to cluster features into semantic centers for better correspondence. A Text-Guided Pixel Decoder leverages textual knowledge to bridge the semantic gap and guide visual representations. The combination of these mechanisms significantly enhances the model's generalization ability. Experimental results on cardiac and fundus datasets show superiority over state-of-the-art approaches in domain generalization benchmarks. <div>
arXiv:2509.08570v1 Announce Type: new 
Abstract: Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data</title>
<link>https://arxiv.org/abs/2509.08571</link>
<guid>https://arxiv.org/abs/2509.08571</guid>
<content:encoded><![CDATA[
<div> GraphTopoNet, Greenland, subglacial bed, uncertainty, spatial graphs <br />
<br />
GraphTopoNet is a new framework for accurately mapping Greenland's subglacial bed, crucial for sea-level projections. It fuses various data sources like elevation, velocity, and mass balance into spatial graphs, incorporating uncertainty through Monte Carlo dropout. By capturing local variability and broad structure, it outperforms existing methods, reducing error by up to 60% while preserving fine glacial features. The hybrid loss function combines radar supervision with regularization techniques to handle data gaps effectively. The resulting bed maps enhance reliability for operational modeling, benefitting climate forecasting and policy-making agencies. GraphTopoNet demonstrates how graph machine learning can transform sparse and uncertain geophysical observations into actionable knowledge on a continental scale.<br /><br />Summary: <div>
arXiv:2509.08571v1 Announce Type: new 
Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level projections, but radar observations are sparse and uneven. We introduce GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built from surface observables (elevation, velocity, mass balance) are augmented with gradient features and polynomial trends to capture both local variability and broad structure. To handle data gaps, we employ a hybrid loss that combines confidence-weighted radar supervision with dynamically balanced regularization. Applied to three Greenland subregions, GraphTopoNet outperforms interpolation, convolutional, and graph-based baselines, reducing error by up to 60 percent while preserving fine-scale glacial features. The resulting bed maps improve reliability for operational modeling, supporting agencies engaged in climate forecasting and policy. More broadly, GraphTopoNet shows how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</title>
<link>https://arxiv.org/abs/2509.08580</link>
<guid>https://arxiv.org/abs/2509.08580</guid>
<content:encoded><![CDATA[
<div> Keywords: medical segmentation, implicit shape prior, sparse slice annotations, informative slice selection, assisted segmentation

Summary:
This paper proposes a method to reduce the manual workload for complex 3D segmentation tasks in medical imaging. By introducing an implicit shape prior and a framework for selecting informative slices, the method aims to assist in the segmentation of organs at risk in radiotherapy planning and in diagnosing age-related degenerative diseases such as sarcopenia. The approach involves segmenting volumes from sparse slice annotations and automatically selecting the most informative slices to guide further interactions, thereby speeding up the segmentation process. Experimental validation demonstrates the effectiveness of the method in assisting segmentation for brain cancer patients and in accelerating the creation of a new database for patients with sarcopenia. The proposed method shows promise in alleviating the burden of manual segmentation for medical professionals and improving the efficiency of complex segmentation tasks in medical imaging. 

<br /><br />Summary: <div>
arXiv:2509.08580v1 Announce Type: new 
Abstract: The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientIML: Efficient High-Resolution Image Manipulation Localization</title>
<link>https://arxiv.org/abs/2509.08583</link>
<guid>https://arxiv.org/abs/2509.08583</guid>
<content:encoded><![CDATA[
<div> Keywords: high-resolution SIF dataset, diffusion-based forgery, EfficientIML model, lightweight backbone, real-time forensic applications

Summary:
The article introduces a new dataset called the high-resolution SIF dataset, containing over 1200 diffusion-generated manipulations with semantically extracted masks. As imaging devices deliver increasingly higher resolutions, traditional forgery detection methods may struggle to detect these new manipulation types. To address this challenge, the authors propose the EfficientIML model with a lightweight EfficientRWKV backbone. This model utilizes a hybrid state-space and attention network to capture global context and local details simultaneously. Furthermore, a multi-scale supervision strategy ensures consistency across hierarchical predictions. Evaluations on the dataset and standard benchmarks show that the EfficientIML model outperforms ViT-based and other state-of-the-art lightweight baselines in localization performance, FLOPs, and inference speed. This makes the model suitable for real-time forensic applications. 

<br /><br />Summary: <div>
arXiv:2509.08583v1 Announce Type: new 
Abstract: With imaging devices delivering ever-higher resolutions and the emerging diffusion-based forgery methods, current detectors trained only on traditional datasets (with splicing, copy-moving and object removal forgeries) lack exposure to this new manipulation type. To address this, we propose a novel high-resolution SIF dataset of 1200+ diffusion-generated manipulations with semantically extracted masks. However, this also imposes a challenge on existing methods, as they face significant computational resource constraints due to their prohibitive computational complexities. Therefore, we propose a novel EfficientIML model with a lightweight, three-stage EfficientRWKV backbone. EfficientRWKV's hybrid state-space and attention network captures global context and local details in parallel, while a multi-scale supervision strategy enforces consistency across hierarchical predictions. Extensive evaluations on our dataset and standard benchmarks demonstrate that our approach outperforms ViT-based and other SOTA lightweight baselines in localization performance, FLOPs and inference speed, underscoring its suitability for real-time forensic applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging</title>
<link>https://arxiv.org/abs/2509.08618</link>
<guid>https://arxiv.org/abs/2509.08618</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, medical image segmentation, retinal imaging, CLAPS, unified framework

Summary: 
CLAPS is a novel method for unified segmentation in retinal imaging that addresses challenges such as modality ambiguity in textual descriptions, reliance on manual prompting, and lack of a unified framework. The approach involves pre-training a CLIP-based image encoder on a large retinal dataset, using GroundingDINO to generate spatial prompts, and enhancing text prompts with modality signatures. By automating both textual and spatial prompts, CLAPS guides SAM to achieve precise segmentation in a fully automated pipeline. Extensive experiments on diverse datasets demonstrate that CLAPS outperforms existing benchmarks and specialized expert models across various segmentation categories, showcasing its broad generalizability as a foundation model. 

<br /><br />Summary: <div>
arXiv:2509.08618v1 Announce Type: new 
Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have significantly impacted medical image segmentation, especially in retinal imaging, where precise segmentation is vital for diagnosis. Despite this progress, current methods face critical challenges: 1) modality ambiguity in textual disease descriptions, 2) a continued reliance on manual prompting for SAM-based workflows, and 3) a lack of a unified framework, with most methods being modality- and task-specific. To overcome these hurdles, we propose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method for unified segmentation across diverse tasks and modalities in retinal imaging. Our approach begins by pre-training a CLIP-based image encoder on a large, multi-modal retinal dataset to handle data scarcity and distribution imbalance. We then leverage GroundingDINO to automatically generate spatial bounding box prompts by detecting local lesions. To unify tasks and resolve ambiguity, we use text prompts enhanced with a unique "modality signature" for each imaging modality. Ultimately, these automated textual and spatial prompts guide SAM to execute precise segmentation, creating a fully automated and unified pipeline. Extensive experiments on 12 diverse datasets across 11 critical segmentation categories show that CLAPS achieves performance on par with specialized expert models while surpassing existing benchmarks across most metrics, demonstrating its broad generalizability as a foundation model.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdsQA: Towards Advertisement Video Understanding</title>
<link>https://arxiv.org/abs/2509.08621</link>
<guid>https://arxiv.org/abs/2509.08621</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, advertisement videos, ad Video QA benchmark, Deepseek-R1, ReAd-R

Summary:
(1) The study focuses on utilizing advertisement videos as a test-bed to evaluate the ability of Large language models (LLMs) beyond objective content perception.
(2) The authors contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips for 5 tasks.
(3) They introduce ReAd-R, an RL model that generates answers through reward-driven optimization based on questions.
(4) The study benchmarks 14 top-tier LLMs on AdsQA, with ReAd-R outperforming competitors with long-chain reasoning capabilities by a significant margin, achieving state-of-the-art results.
<br /><br />Summary: <div>
arXiv:2509.08621v1 Announce Type: new 
Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos' traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
<link>https://arxiv.org/abs/2509.08624</link>
<guid>https://arxiv.org/abs/2509.08624</guid>
<content:encoded><![CDATA[
<div> framework, multimodal, ophthalmic images, OCT, fundus photography

Summary: 
The study introduces a novel unpaired multimodal framework, UOPSL, for enhancing disease recognition in ophthalmic images. Traditional approaches relying solely on fundus or textual features lack fine-grained spatial information captured by distinct cues in different imaging modalities. The framework utilizes extensive OCT-derived spatial priors to dynamically identify lesion predilection sites, enhancing fundus image-based disease recognition. Through contrastive learning on unpaired OCT and fundus images, the framework learns the predilection sites matrix in the OCT latent space. This matrix captures lesion localization patterns, aiding in fundus image classification without paired OCT data. Extensive experiments across diverse datasets demonstrate the framework's superiority over existing benchmarks. <div>
arXiv:2509.08624v1 Announce Type: new 
Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have led to substantial improvements in ophthalmic disease identification in recent years. However, acquiring paired multimodal ophthalmic images remains prohibitively expensive. While fundus photography is simple and cost-effective, the limited availability of OCT data and inherent modality imbalance hinder further progress. Conventional approaches that rely solely on fundus or textual features often fail to capture fine-grained spatial information, as each imaging modality provides distinct cues about lesion predilection sites. In this study, we propose a novel unpaired multimodal framework \UOPSL that utilizes extensive OCT-derived spatial priors to dynamically identify predilection sites, enhancing fundus image-based disease recognition. Our approach bridges unpaired fundus and OCTs via extended disease text descriptions. Initially, we employ contrastive learning on a large corpus of unpaired OCT and fundus images while simultaneously learning the predilection sites matrix in the OCT latent space. Through extensive optimization, this matrix captures lesion localization patterns within the OCT feature space. During the fine-tuning or inference phase of the downstream classification task based solely on fundus images, where paired OCT data is unavailable, we eliminate OCT input and utilize the predilection sites matrix to assist in fundus image classification learning. Extensive experiments conducted on 9 diverse datasets across 28 critical categories demonstrate that our framework outperforms existing benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation</title>
<link>https://arxiv.org/abs/2509.08628</link>
<guid>https://arxiv.org/abs/2509.08628</guid>
<content:encoded><![CDATA[
<div> semi-supervised, diffusion models, domain translation, latent space, sample-to-sample translation
Summary: 
Latent Aligned Diffusion Bridges (LADB) is introduced as a semi-supervised framework for domain translation, utilizing pretrained diffusion models and a Latent Aligned Diffusion Model (LADM) to bridge domain gaps. By aligning source and target distributions in a shared latent space, LADB enables deterministic domain mapping without full supervision. It strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired data. Performance is demonstrated in depth-to-image translation under partial supervision, and LADB is extended to handle multi-source and multi-target translation tasks. The framework proves versatile in handling diverse use cases and scenarios where data annotation is costly or incomplete. LADB presents a scalable and effective solution for real-world domain translation challenges. 
<br /><br />Summary: <div>
arXiv:2509.08628v1 Announce Type: new 
Abstract: Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network</title>
<link>https://arxiv.org/abs/2509.08661</link>
<guid>https://arxiv.org/abs/2509.08661</guid>
<content:encoded><![CDATA[
<div> Keywords: ISolated Sign Language Recognition, Dual-SignLanguageNet, gesture morphology, trajectory modeling, optimal transport fusion <br />
Summary: 
Dual-SignLanguageNet (DSLNet) is introduced to address the challenge of recognizing isolated sign language gestures that are morphologically similar yet semantically distinct. The architecture decouples and models gesture morphology and trajectory in separate coordinate systems using wrist-centric and facial-centric frames. Specialized networks are employed for shape analysis and trajectory modeling, which are integrated via an optimal transport fusion mechanism. DSLNet achieves state-of-the-art performance on datasets like WLASL-100, WLASL-300, and LSA64 with high accuracy while utilizing fewer parameters than competing models. The use of dual-reference frames and dual-stream processing allows DSLNet to effectively capture the complexity of sign language gestures, leading to improved recognition performance. <div>
arXiv:2509.08661v1 Announce Type: new 
Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization</title>
<link>https://arxiv.org/abs/2509.08670</link>
<guid>https://arxiv.org/abs/2509.08670</guid>
<content:encoded><![CDATA[
<div> FractalPINN-Flow, deep learning, optical flow estimation, unsupervised, Fractal Deformation Network (FDN)<br />
<br />
Summary:<br />
FractalPINN-Flow introduces an unsupervised deep learning framework for dense optical flow estimation that operates without the need for ground truth data. The architecture of FractalPINN-Flow is built around the Fractal Deformation Network (FDN), a recursive encoder-decoder structure inspired by fractal geometry and self-similarity. Unlike traditional CNNs, FDN utilizes repeated encoder-decoder nesting with skip connections to capture fine details and long-range motion patterns. The training objective involves minimizing an energy functional incorporating data fidelity terms and total variation regularization to ensure brightness constancy, spatial smoothness, and coherent flow fields. Experimental results on synthetic and benchmark datasets demonstrate that FractalPINN-Flow generates accurate, smooth, and edge-preserving optical flow fields. The model excels particularly in high-resolution data settings and scenarios with limited annotations.<br /> <div>
arXiv:2509.08670v1 Announce Type: new 
Abstract: We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework</title>
<link>https://arxiv.org/abs/2509.08694</link>
<guid>https://arxiv.org/abs/2509.08694</guid>
<content:encoded><![CDATA[
<div> Keywords: coastal water segmentation, satellite imagery, Robust U-Net, HSV color space, multi-modal constraints

Summary:
Coastal water segmentation from satellite images is challenging due to complex spectral characteristics and irregular boundaries. Traditional RGB-based approaches face issues with training instability and poor generalization. The Robust U-Net framework introduced in this paper addresses these challenges by incorporating HSV color space supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. HSV supervision was found to have the highest impact, and the complete framework demonstrated superior training stability and segmentation quality. The method showed consistent improvements across evaluation metrics while maintaining computational efficiency. The training configurations and code are available for reproducibility at the provided Github link.  <div>
arXiv:2509.08694v1 Announce Type: new 
Abstract: Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Imaging for Enhanced Computer Vision</title>
<link>https://arxiv.org/abs/2509.08712</link>
<guid>https://arxiv.org/abs/2509.08712</guid>
<content:encoded><![CDATA[
<div> Keywords: computational imaging, computer vision, object detection, depth estimation, autonomous navigation

Summary: 
This paper provides a comprehensive survey of computational imaging (CI) techniques and their impact on computer vision (CV applications. Conventional imaging methods often struggle in challenging conditions, such as low light or motion blur, limiting the performance of CV systems. CI techniques like light field imaging and HDR imaging enhance image acquisition and reconstruction processes. The survey examines how these techniques synergize with core CV tasks such as object detection and face recognition. The paper also discusses the potential for adaptive imaging pipelines to improve efficiency in real-world scenarios like autonomous navigation and robotics. Overall, the paper highlights the opportunities, challenges, and future research directions in the field of computational imaging and its application to computer vision.<br /><br />Summary: <div>
arXiv:2509.08712v1 Announce Type: new 
Abstract: This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruc- tion processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</title>
<link>https://arxiv.org/abs/2509.08715</link>
<guid>https://arxiv.org/abs/2509.08715</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, lightweight framework, visual question answering, efficiency, BcQLM

Summary:
The article introduces a lightweight multimodal large language model (MLLM) framework called BcQLM for visual question answering. BcQLM is optimized for efficient multimodal understanding with only 1.2 billion parameters, significantly reducing computational costs while maintaining performance levels similar to standard MLLMs. The model, based on BreezeCLIP, is designed to balance accuracy and efficiency, making it suitable for real-world applications in resource-constrained environments. Experiments on various datasets validate the effectiveness of BcQLM in achieving this balance. The modular and extensible design of BcQLM allows for its adaptation to a wide range of multimodal tasks. The framework offers a potential solution for deploying MLLMs on practical hardware while addressing the challenges of energy efficiency, computational scalability, and environmental sustainability.<br /><br />Summary: <div>
arXiv:2509.08715v1 Announce Type: new 
Abstract: As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</title>
<link>https://arxiv.org/abs/2509.08738</link>
<guid>https://arxiv.org/abs/2509.08738</guid>
<content:encoded><![CDATA[
arXiv:2509.08738v1 Announce Type: new 
Abstract: This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</title>
<link>https://arxiv.org/abs/2509.08764</link>
<guid>https://arxiv.org/abs/2509.08764</guid>
<content:encoded><![CDATA[
arXiv:2509.08764v1 Announce Type: new 
Abstract: Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</title>
<link>https://arxiv.org/abs/2509.08777</link>
<guid>https://arxiv.org/abs/2509.08777</guid>
<content:encoded><![CDATA[
arXiv:2509.08777v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images</title>
<link>https://arxiv.org/abs/2509.08780</link>
<guid>https://arxiv.org/abs/2509.08780</guid>
<content:encoded><![CDATA[
arXiv:2509.08780v1 Announce Type: new 
Abstract: Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation</title>
<link>https://arxiv.org/abs/2509.08794</link>
<guid>https://arxiv.org/abs/2509.08794</guid>
<content:encoded><![CDATA[
arXiv:2509.08794v1 Announce Type: new 
Abstract: Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching</title>
<link>https://arxiv.org/abs/2509.08805</link>
<guid>https://arxiv.org/abs/2509.08805</guid>
<content:encoded><![CDATA[
arXiv:2509.08805v1 Announce Type: new 
Abstract: Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts</title>
<link>https://arxiv.org/abs/2509.08818</link>
<guid>https://arxiv.org/abs/2509.08818</guid>
<content:encoded><![CDATA[
arXiv:2509.08818v1 Announce Type: new 
Abstract: Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RewardDance: Reward Scaling in Visual Generation</title>
<link>https://arxiv.org/abs/2509.08826</link>
<guid>https://arxiv.org/abs/2509.08826</guid>
<content:encoded><![CDATA[
arXiv:2509.08826v1 Announce Type: new 
Abstract: Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video</title>
<link>https://arxiv.org/abs/2509.08828</link>
<guid>https://arxiv.org/abs/2509.08828</guid>
<content:encoded><![CDATA[
arXiv:2509.08828v1 Announce Type: new 
Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization</title>
<link>https://arxiv.org/abs/2509.07993</link>
<guid>https://arxiv.org/abs/2509.07993</guid>
<content:encoded><![CDATA[
arXiv:2509.07993v1 Announce Type: cross 
Abstract: The rapid evolution of deepfake generation technologies poses critical challenges for detection systems, as non-continual learning methods demand frequent and expensive retraining. We reframe deepfake detection (DFD) as a Continual Learning (CL) problem, proposing an efficient framework that incrementally adapts to emerging visual manipulation techniques while retaining knowledge of past generators. Our framework, unlike prior approaches that rely on unreal simulation sequences, simulates the real-world chronological evolution of deepfake technologies in extended periods across 7 years. Simultaneously, our framework builds upon lightweight visual backbones to allow for the real-time performance of DFD systems. Additionally, we contribute two novel metrics: Continual AUC (C-AUC) for historical performance and Forward Transfer AUC (FWT-AUC) for future generalization. Through extensive experimentation (over 600 simulations), we empirically demonstrate that while efficient adaptation (+155 times faster than full retraining) and robust retention of historical knowledge is possible, the generalization of current approaches to future generators without additional training remains near-random (FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing generator. Such observations are the foundation of our newly proposed Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery</title>
<link>https://arxiv.org/abs/2509.07994</link>
<guid>https://arxiv.org/abs/2509.07994</guid>
<content:encoded><![CDATA[
arXiv:2509.07994v1 Announce Type: cross 
Abstract: Despite advancements in rehabilitation protocols, clinical assessment of upper extremity (UE) function after stroke largely remains subjective, relying heavily on therapist observation and coarse scoring systems. This subjectivity limits the sensitivity of assessments to detect subtle motor improvements, which are critical for personalized rehabilitation planning. Recent progress in computer vision offers promising avenues for enabling objective, quantitative, and scalable assessment of UE motor function. Among standardized tests, the Box and Block Test (BBT) is widely utilized for measuring gross manual dexterity and tracking stroke recovery, providing a structured setting that lends itself well to computational analysis. However, existing datasets targeting stroke rehabilitation primarily focus on daily living activities and often fail to capture clinically structured assessments such as block transfer tasks. Furthermore, many available datasets include a mixture of healthy and stroke-affected individuals, limiting their specificity and clinical utility. To address these critical gaps, we introduce StrokeVision-Bench, the first-ever dedicated dataset of stroke patients performing clinically structured block transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized into four clinically meaningful action classes, with each sample represented in two modalities: raw video frames and 2D skeletal keypoints. We benchmark several state-of-the-art video action recognition and skeleton-based action classification methods to establish performance baselines for this domain and facilitate future research in automated stroke rehabilitation assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</title>
<link>https://arxiv.org/abs/2509.08007</link>
<guid>https://arxiv.org/abs/2509.08007</guid>
<content:encoded><![CDATA[
arXiv:2509.08007v1 Announce Type: cross 
Abstract: Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions-of-interests (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of a CT-brain analysis tool for measuring global cortical atrophy in older patient cohorts</title>
<link>https://arxiv.org/abs/2509.08012</link>
<guid>https://arxiv.org/abs/2509.08012</guid>
<content:encoded><![CDATA[
arXiv:2509.08012v1 Announce Type: cross 
Abstract: Quantification of brain atrophy currently requires visual rating scales which are time consuming and automated brain image analysis is warranted. We validated our automated deep learning (DL) tool measuring the Global Cerebral Atrophy (GCA) score against trained human raters, and associations with age and cognitive impairment, in representative older (>65 years) patients. CT-brain scans were obtained from patients in acute medicine (ORCHARD-EPR), acute stroke (OCS studies) and a legacy sample. Scans were divided in a 60/20/20 ratio for training, optimisation and testing. CT-images were assessed by two trained raters (rater-1=864 scans, rater-2=20 scans). Agreement between DL tool-predicted GCA scores (range 0-39) and the visual ratings was evaluated using mean absolute error (MAE) and Cohen's weighted kappa. Among 864 scans (ORCHARD-EPR=578, OCS=200, legacy scans=86), MAE between the DL tool and rater-1 GCA scores was 3.2 overall, 3.1 for ORCHARD-EPR, 3.3 for OCS and 2.6 for the legacy scans and half had DL-predicted GCA error between -2 and 2. Inter-rater agreement was Kappa=0.45 between the DL-tool and rater-1, and 0.41 between the tool and rater- 2 whereas it was lower at 0.28 for rater-1 and rater-2. There was no difference in GCA scores from the DL-tool and the two raters (one-way ANOVA, p=0.35) or in mean GCA scores between the DL-tool and rater-1 (paired t-test, t=-0.43, p=0.66), the tool and rater-2 (t=1.35, p=0.18) or between rater-1 and rater-2 (t=0.99, p=0.32). DL-tool GCA scores correlated with age and cognitive scores (both p<0.001). Our DL CT-brain analysis tool measured GCA score accurately and without user input in real-world scans acquired from older patients. Our tool will enable extraction of standardised quantitative measures of atrophy at scale for use in health data research and will act as proof-of-concept towards a point-of-care clinically approved tool.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
arXiv:2509.08015v1 Announce Type: cross 
Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators, enable the study of structure-function relationships for clinical research and medical device design. However, current models face a trade-off between controllability and anatomical realism. We propose a programmable and compositional framework for guiding unconditional diffusion models of human anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our method involves the selection of certain tissues within multi-tissue segmentation maps, upon which we apply geometric moment losses to guide the reverse diffusion process. This framework supports the independent control over size, shape, and position, as well as the composition of multi-component constraints during inference.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
<link>https://arxiv.org/abs/2509.08018</link>
<guid>https://arxiv.org/abs/2509.08018</guid>
<content:encoded><![CDATA[
arXiv:2509.08018v1 Announce Type: cross 
Abstract: The application of Digital Twin (DT) technology and Federated Learning (FL) has great potential to change the field of biomedical image analysis, particularly for Computed Tomography (CT) scans. This paper presents Federated Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm. FTL uses pre-trained models and knowledge transfer between peer nodes to solve problems such as data privacy, limited computing resources, and data heterogeneity. The proposed framework allows real-time collaboration between cloud servers and Digital Twin-enabled CT scanners while protecting patient identity. We apply the FTL method to a heterogeneous CT scan dataset and assess model performance using convergence time, model accuracy, precision, recall, F1 score, and confusion matrix. It has been shown to perform better than conventional FL and Clustered Federated Learning (CFL) methods with better precision, accuracy, recall, and F1-score. The technique is beneficial in settings where the data is not independently and identically distributed (non-IID), and it offers reliable, efficient, and secure solutions for medical diagnosis. These findings highlight the possibility of using FTL to improve decision-making in digital twin-based CT scan analysis, secure and efficient medical image analysis, promote privacy, and open new possibilities for applying precision medicine and smart healthcare systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quadrotor Navigation using Reinforcement Learning with Privileged Information</title>
<link>https://arxiv.org/abs/2509.08177</link>
<guid>https://arxiv.org/abs/2509.08177</guid>
<content:encoded><![CDATA[
arXiv:2509.08177v1 Announce Type: cross 
Abstract: This paper presents a reinforcement learning-based quadrotor navigation method that leverages efficient differentiable simulation, novel loss functions, and privileged information to navigate around large obstacles. Prior learning-based methods perform well in scenes that exhibit narrow obstacles, but struggle when the goal location is blocked by large walls or terrain. In contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged information and a yaw alignment loss to guide the robot around large obstacles. The policy is evaluated in photo-realistic simulation environments containing large obstacles, sharp corners, and dead-ends. Our approach achieves an 86% success rate and outperforms baseline strategies by 34%. We deploy the policy onboard a custom quadrotor in outdoor cluttered environments both during the day and night. The policy is validated across 20 flights, covering 589 meters without collisions at speeds up to 4 m/s.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</title>
<link>https://arxiv.org/abs/2509.08302</link>
<guid>https://arxiv.org/abs/2509.08302</guid>
<content:encoded><![CDATA[
arXiv:2509.08302v1 Announce Type: cross 
Abstract: Foundation models are revolutionizing autonomous driving perception, transitioning the field from narrow, task-specific deep learning models to versatile, general-purpose architectures trained on vast, diverse datasets. This survey examines how these models address critical challenges in autonomous perception, including limitations in generalization, scalability, and robustness to distributional shifts. The survey introduces a novel taxonomy structured around four essential capabilities for robust performance in dynamic driving environments: generalized knowledge, spatial understanding, multi-sensor robustness, and temporal reasoning. For each capability, the survey elucidates its significance and comprehensively reviews cutting-edge approaches. Diverging from traditional method-centric surveys, our unique framework prioritizes conceptual design principles, providing a capability-driven guide for model development and clearer insights into foundational aspects. We conclude by discussing key challenges, particularly those associated with the integration of these capabilities into real-time, scalable systems, and broader deployment challenges related to computational demands and ensuring model reliability against issues like hallucinations and out-of-distribution failures. The survey also outlines crucial future research directions to enable the safe and effective deployment of foundation models in autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Rectified Flow for Low-light RAW Image Enhancement</title>
<link>https://arxiv.org/abs/2509.08330</link>
<guid>https://arxiv.org/abs/2509.08330</guid>
<content:encoded><![CDATA[
arXiv:2509.08330v1 Announce Type: cross 
Abstract: Enhancing RAW images captured under low light conditions is a challenging task. Recent deep learning based RAW enhancement methods have shifted from using real paired data to relying on synthetic datasets. These synthetic datasets are typically generated by physically modeling sensor noise, but existing approaches often consider only additive noise, ignore multiplicative components, and rely on global calibration that overlooks pixel level manufacturing variations. As a result, such methods struggle to accurately reproduce real sensor noise. To address these limitations, this paper derives a noise model from the physical noise generation mechanisms that occur under low illumination and proposes a novel composite model that integrates both additive and multiplicative noise. To solve the model, we introduce a physics based per pixel noise simulation and calibration scheme that estimates and synthesizes noise for each individual pixel, thereby overcoming the restrictions of traditional global calibration and capturing spatial noise variations induced by microscopic CMOS manufacturing differences. Motivated by the strong performance of rectified flow methods in image generation and processing, we further combine the physics-based noise synthesis with a rectified flow generative framework and present PGRF a physics-guided rectified flow framework for low light image enhancement. PGRF leverages the ability of rectified flows to model complex data distributions and uses physical guidance to steer the generation toward the desired clean image. To validate the effectiveness of the proposed model, we established the LLID dataset, an indoor low light benchmark captured with the Sony A7S II camera. Experimental results demonstrate that the proposed framework achieves significant improvements in low light RAW image enhancement.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry</title>
<link>https://arxiv.org/abs/2509.08333</link>
<guid>https://arxiv.org/abs/2509.08333</guid>
<content:encoded><![CDATA[
arXiv:2509.08333v1 Announce Type: cross 
Abstract: Visual-based localization has made significant progress, yet its performance often drops in large-scale, outdoor, and long-term settings due to factors like lighting changes, dynamic scenes, and low-texture areas. These challenges degrade feature extraction and tracking, which are critical for accurate motion estimation. While learning-based methods such as SuperPoint and SuperGlue show improved feature coverage and robustness, they still face generalization issues with out-of-distribution data. We address this by enhancing deep feature extraction and tracking through self-supervised learning with task specific feedback. Our method promotes stable and informative features, improving generalization and reliability in challenging environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics</title>
<link>https://arxiv.org/abs/2509.08461</link>
<guid>https://arxiv.org/abs/2509.08461</guid>
<content:encoded><![CDATA[
arXiv:2509.08461v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their remarkable capacity to process and reason over structured and unstructured data modalities beyond natural language. In this work, we explore the applications of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa 3.2, to the task of identifying neutrino interactions in pixelated detector data from high-energy physics (HEP) experiments. We benchmark this model against a state-of-the-art convolutional neural network (CNN) architecture, similar to those used in the NOvA and DUNE experiments, which have achieved high efficiency and purity in classifying electron and muon neutrino events. Our evaluation considers both the classification performance and interpretability of the model predictions. We find that VLMs can outperform CNNs, while also providing greater flexibility in integrating auxiliary textual or semantic information and offering more interpretable, reasoning-based predictions. This work highlights the potential of VLMs as a general-purpose backbone for physics event classification, due to their high performance, interpretability, and generalizability, which opens new avenues for integrating multimodal reasoning in experimental neutrino physics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-ViT Hybrid for Pneumonia Detection: Theory and Empiric on Limited Data without Pretraining</title>
<link>https://arxiv.org/abs/2509.08586</link>
<guid>https://arxiv.org/abs/2509.08586</guid>
<content:encoded><![CDATA[
arXiv:2509.08586v1 Announce Type: cross 
Abstract: This research explored the hybridization of CNN and ViT within a training dataset of limited size, and introduced a distinct class imbalance. The training was made from scratch with a mere focus on theoretically and experimentally exploring the architectural strengths of the proposed hybrid model. Experiments were conducted across varied data fractions with balanced and imbalanced training datasets. Comparatively, the hybrid model, complementing the strengths of CNN and ViT, achieved the highest recall of 0.9443 (50% data fraction in balanced) and consistency in F1 score around 0.85, suggesting reliability in diagnosis. Additionally, the model was successful in outperforming CNN and ViT in imbalanced datasets. Despite its complex architecture, it required comparable training time to the transformers in all data fractions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts</title>
<link>https://arxiv.org/abs/2509.08640</link>
<guid>https://arxiv.org/abs/2509.08640</guid>
<content:encoded><![CDATA[
arXiv:2509.08640v1 Announce Type: cross 
Abstract: Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\% of cases, correctly incorporated the specified finding in 89-99\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\% AUC in internal validation and by 1-11\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Part: high fidelity and structure coherent shape decomposition</title>
<link>https://arxiv.org/abs/2509.08643</link>
<guid>https://arxiv.org/abs/2509.08643</guid>
<content:encoded><![CDATA[
arXiv:2509.08643v1 Announce Type: cross 
Abstract: Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
<link>https://arxiv.org/abs/2509.08699</link>
<guid>https://arxiv.org/abs/2509.08699</guid>
<content:encoded><![CDATA[
arXiv:2509.08699v1 Announce Type: cross 
Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</title>
<link>https://arxiv.org/abs/2509.08757</link>
<guid>https://arxiv.org/abs/2509.08757</guid>
<content:encoded><![CDATA[
arXiv:2509.08757v1 Announce Type: cross 
Abstract: Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PianoVAM: A Multimodal Piano Performance Dataset</title>
<link>https://arxiv.org/abs/2509.08800</link>
<guid>https://arxiv.org/abs/2509.08800</guid>
<content:encoded><![CDATA[
arXiv:2509.08800v1 Announce Type: cross 
Abstract: The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Information in Domain-Invariant Representation Improves Transfer Learning</title>
<link>https://arxiv.org/abs/2306.00262</link>
<guid>https://arxiv.org/abs/2306.00262</guid>
<content:encoded><![CDATA[
arXiv:2306.00262v5 Announce Type: replace 
Abstract: We propose MaxDIRep, a domain adaptation method that improves the decomposition of data representations into domain-independent and domain-dependent components. Existing methods, such as Domain-Separation Networks (DSN), use a weak orthogonality constraint between these components, which can lead to label-relevant features being partially encoded in the domain-dependent representation (DDRep) rather than the domain-independent representation (DIRep). As a result, information crucial for target-domain classification may be missing from the DIRep. MaxDIRep addresses this issue by applying a Kullback-Leibler (KL) divergence constraint to minimize the information content of the DDRep, thereby encouraging the DIRep to retain features that are both domain-invariant and predictive of target labels. Through geometric analysis and an ablation study on synthetic datasets, we show why DSN's weaker constraint can lead to suboptimal adaptation. Experiments on standard image benchmarks and a network intrusion detection task demonstrate that MaxDIRep achieves strong performance, works with pretrained models, and generalizes to non-image classification tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Channel Bias to Feature Redundancy: Uncovering the "Less is More" Principle in Few-Shot Learning</title>
<link>https://arxiv.org/abs/2310.03843</link>
<guid>https://arxiv.org/abs/2310.03843</guid>
<content:encoded><![CDATA[
arXiv:2310.03843v2 Announce Type: replace 
Abstract: Deep neural networks often fail to adapt representations to novel tasks under distribution shifts, especially when only a few examples are available. This paper identifies a core obstacle behind this failure: channel bias, where networks develop a rigid emphasis on feature dimensions that were discriminative for the source task, but this emphasis is misaligned and fails to adapt to the distinct needs of a novel task. This bias leads to a striking and detrimental consequence: feature redundancy. We demonstrate that for few-shot tasks, classification accuracy is significantly improved by using as few as 1-5% of the most discriminative feature dimensions, revealing that the vast majority are actively harmful. Our theoretical analysis confirms that this redundancy originates from confounding feature dimensions-those with high intra-class variance but low inter-class separability-which are especially problematic in low-data regimes. This "less is more" phenomenon is a defining characteristic of the few-shot setting, diminishing as more samples become available. To address this, we propose a simple yet effective soft-masking method, Augmented Feature Importance Adjustment (AFIA), which estimates feature importance from augmented data to mitigate the issue. By establishing the cohesive link from channel bias to its consequence of extreme feature redundancy, this work provides a foundational principle for few-shot representation transfer and a practical method for developing more robust few-shot learning algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Robust Representations via Bidirectional Transition for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2312.01915</link>
<guid>https://arxiv.org/abs/2312.01915</guid>
<content:encoded><![CDATA[
arXiv:2312.01915v2 Announce Type: replace 
Abstract: Visual reinforcement learning has proven effective in solving control tasks with high-dimensional observations. However, extracting reliable and generalizable representations from vision-based observations remains a central challenge. Inspired by the human thought process, when the representation extracted from the observation can predict the future and trace history, the representation is reliable and accurate in comprehending the environment. Based on this concept, we introduce a Bidirectional Transition (BiT) model, which leverages the ability to bidirectionally predict environmental transitions both forward and backward to extract reliable representations. Our model demonstrates competitive generalization performance and sample efficiency on two settings of the DeepMind Control suite. Additionally, we utilize robotic manipulation and CARLA simulators to demonstrate the wide applicability of our method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2404.04256</link>
<guid>https://arxiv.org/abs/2404.04256</guid>
<content:encoded><![CDATA[
arXiv:2404.04256v3 Announce Type: replace 
Abstract: Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable prediction. In this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic segmentation utilizing the advanced Mamba. Unlike conventional methods that rely on CNNs, with their limited local receptive fields, or Vision Transformers (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields with linear complexity. By employing a Siamese encoder and innovating a Mamba-based fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our proposed method is rigorously evaluated on both RGB-Thermal and RGB-Depth semantic segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks. Code is available at https://github.com/zifuwan/Sigma.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorCLIP: Visual Prior Guided Vision-Language Model for Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2405.10160</link>
<guid>https://arxiv.org/abs/2405.10160</guid>
<content:encoded><![CDATA[
arXiv:2405.10160v3 Announce Type: replace 
Abstract: Remote sensing image-text retrieval plays a crucial role in remote sensing interpretation, yet remains challenging under both closed-domain and open-domain scenarios due to semantic noise and domain shifts. To address these issues, we propose a visual prior-guided vision-language model, PriorCLIP, which leverages visual priors for unbiased representation learning and adaptive vision-language alignment. In the closed-domain setting, PriorCLIP introduces two Progressive Attention Encoder (PAE) structures: Spatial-PAE constructs a belief matrix with instruction embeddings to filter key features and mitigate semantic bias. At the same time, Temporal-PAE exploits cyclic activation across time steps to enhance text representation. For the open-domain setting, we design a two-stage prior representation learning strategy, consisting of large-scale pre-training on coarse-grained image-text pairs, followed by fine-tuning on fine-grained pairs using vision-instruction, which enables robust retrieval across long-tail concepts and vocabulary shifts. Furthermore, a cluster-based symmetric contrastive Attribution Loss is proposed to constrain inter-class relations and alleviate semantic confusion in the shared embedding space. Extensive experiments on RSICD and RSITMD benchmarks demonstrate that PriorCLIP achieves substantial improvements, outperforming existing methods by 4.9% and 4.0% in closed-domain retrieval, and by 7.3% and 9.4% in open-domain retrieval, respectively.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformer with Sparse Scan Prior</title>
<link>https://arxiv.org/abs/2405.13335</link>
<guid>https://arxiv.org/abs/2405.13335</guid>
<content:encoded><![CDATA[
arXiv:2405.13335v2 Announce Type: replace 
Abstract: In recent years, Transformers have achieved remarkable progress in computer vision tasks. However, their global modeling often comes with substantial computational overhead, in stark contrast to the human eye's efficient information processing. Inspired by the human eye's sparse scanning mechanism, we propose a \textbf{S}parse \textbf{S}can \textbf{S}elf-\textbf{A}ttention mechanism ($\rm{S}^3\rm{A}$). This mechanism predefines a series of Anchors of Interest for each token and employs local attention to efficiently model the spatial information around these anchors, avoiding redundant global modeling and excessive focus on local information. This approach mirrors the human eye's functionality and significantly reduces the computational load of vision models. Building on $\rm{S}^3\rm{A}$, we introduce the \textbf{S}parse \textbf{S}can \textbf{Vi}sion \textbf{T}ransformer (SSViT). Extensive experiments demonstrate the outstanding performance of SSViT across a variety of tasks. Specifically, on ImageNet classification, without additional supervision or training data, SSViT achieves top-1 accuracies of \textbf{84.4\%/85.7\%} with \textbf{4.4G/18.2G} FLOPs. SSViT also excels in downstream tasks such as object detection, instance segmentation, and semantic segmentation. Its robustness is further validated across diverse datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Have Large Vision-Language Models Mastered Art History?</title>
<link>https://arxiv.org/abs/2409.03521</link>
<guid>https://arxiv.org/abs/2409.03521</guid>
<content:encoded><![CDATA[
arXiv:2409.03521v2 Announce Type: replace 
Abstract: The emergence of large Vision-Language Models (VLMs) has established new baselines in image classification across multiple domains. We examine whether their multimodal reasoning can also address a challenge mastered by human experts. Specifically, we test whether VLMs can classify the style, author and creation date of paintings, a domain traditionally mastered by art historians. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. This requires a contextual and stylistic interpretation rather than straightforward object recognition. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively reason about the historical and stylistic attributes of paintings. We present the first study of its kind, conducting an in-depth analysis of three VLMs, namely CLIP, LLaVA, and GPT-4o, evaluating their zero-shot classification of art style, author and time period. Using two image benchmarks of artworks, we assess the models' ability to interpret style, evaluate their sensitivity to prompts, and examine failure cases. Additionally, we focus on how these models compare to human art historical expertise by analyzing misclassifications, providing insights into their reasoning and classification patterns.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Chinese Continuous Sign Language Dataset Based on Complex Environments</title>
<link>https://arxiv.org/abs/2409.11960</link>
<guid>https://arxiv.org/abs/2409.11960</guid>
<content:encoded><![CDATA[
arXiv:2409.11960v2 Announce Type: replace 
Abstract: The current bottleneck in continuous sign language recognition (CSLR) research lies in the fact that most publicly available datasets are limited to laboratory environments or television program recordings, resulting in a single background environment with uniform lighting, which significantly deviates from the diversity and complexity found in real-life scenarios. To address this challenge, we have constructed a new, large-scale dataset for Chinese continuous sign language (CSL) based on complex environments, termed the complex environment - chinese sign language dataset (CE-CSL). This dataset encompasses 5,988 continuous CSL video clips collected from daily life scenes, featuring more than 70 different complex backgrounds to ensure representativeness and generalization capability. To tackle the impact of complex backgrounds on CSLR performance, we propose a time-frequency network (TFNet) model for continuous sign language recognition. This model extracts frame-level features and then utilizes both temporal and spectral information to separately derive sequence features before fusion, aiming to achieve efficient and accurate CSLR. Experimental results demonstrate that our approach achieves significant performance improvements on the CE-CSL, validating its effectiveness under complex background conditions. Additionally, our proposed method has also yielded highly competitive results when applied to three publicly available CSL datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALOcc: Adaptive Lifting-Based 3D Semantic Occupancy and Cost Volume-Based Flow Predictions</title>
<link>https://arxiv.org/abs/2411.07725</link>
<guid>https://arxiv.org/abs/2411.07725</guid>
<content:encoded><![CDATA[
arXiv:2411.07725v2 Announce Type: replace 
Abstract: 3D semantic occupancy and flow prediction are fundamental to spatiotemporal scene understanding. This paper proposes a vision-based framework with three targeted improvements. First, we introduce an occlusion-aware adaptive lifting mechanism incorporating depth denoising. This enhances the robustness of 2D-to-3D feature transformation while mitigating reliance on depth priors. Second, we enforce 3D-2D semantic consistency via jointly optimized prototypes, using confidence- and category-aware sampling to address the long-tail classes problem. Third, to streamline joint prediction, we devise a BEV-centric cost volume to explicitly correlate semantic and flow features, supervised by a hybrid classification-regression scheme that handles diverse motion scales. Our purely convolutional architecture establishes new SOTA performance on multiple benchmarks for both semantic occupancy and joint occupancy semantic-flow prediction. We also present a family of models offering a spectrum of efficiency-performance trade-offs. Our real-time version exceeds all existing real-time methods in speed and accuracy, ensuring its practical viability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GloFinder: AI-empowered QuPath Plugin for WSI-level Glomerular Detection, Visualization, and Curation</title>
<link>https://arxiv.org/abs/2411.18795</link>
<guid>https://arxiv.org/abs/2411.18795</guid>
<content:encoded><![CDATA[
arXiv:2411.18795v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has demonstrated significant success in automating the detection of glomeruli, the key functional units of the kidney, from whole slide images (WSIs) in kidney pathology. However, existing open-source tools are often distributed as source code or Docker containers, requiring advanced programming skills that hinder accessibility for non-programmers, such as clinicians. Additionally, current models are typically trained on a single dataset and lack flexibility in adjusting confidence levels for predictions. To overcome these challenges, we introduce GloFinder, a QuPath plugin designed for single-click automated glomeruli detection across entire WSIs with online editing through the graphical user interface (GUI). GloFinder employs CircleNet, an anchor-free detection framework utilizing circle representations for precise object localization, with models trained on approximately 160,000 manually annotated glomeruli. To further enhance accuracy, the plugin incorporates Weighted Circle Fusion (WCF), an ensemble method that combines confidence scores from multiple CircleNet models to produce refined predictions, achieving superior performance in glomerular detection. GloFinder enables direct visualization and editing of results in QuPath, facilitating seamless interaction for clinicians and providing a powerful tool for nephropathology research and clinical practice. Code and the QuPath plugin are available at https://github.com/hrlblab/GloFinder
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition</title>
<link>https://arxiv.org/abs/2412.01137</link>
<guid>https://arxiv.org/abs/2412.01137</guid>
<content:encoded><![CDATA[
arXiv:2412.01137v2 Announce Type: replace 
Abstract: Scene text recognition (STR) suffers from challenges of either less realistic synthetic training data or the difficulty of collecting sufficient high-quality real-world data, limiting the effectiveness of trained models. Meanwhile, despite producing holistically appealing text images, diffusion-based visual text generation methods struggle to synthesize accurate and realistic instance-level text at scale. To tackle this, we introduce TextSSR: a novel pipeline for Synthesizing Scene Text Recognition training data. TextSSR targets three key synthesizing characteristics: accuracy, realism, and scalability. It achieves accuracy through a proposed region-centric text generation with position-glyph enhancement, ensuring proper character placement. It maintains realism by guiding style and appearance generation using contextual hints from surrounding text or background. This character-aware diffusion architecture enjoys precise character-level control and semantic coherence preservation, without relying on natural language prompts. Therefore, TextSSR supports large-scale generation through combinatorial text permutations. Based on these, we present TextSSR-F, a dataset of 3.55 million quality-screened text instances. Extensive experiments show that STR models trained on TextSSR-F outperform those trained on existing synthetic datasets by clear margins on common benchmarks, and further improvements are observed when mixed with real-world training data. Code is available at https://github.com/YesianRohn/TextSSR.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</title>
<link>https://arxiv.org/abs/2412.13155</link>
<guid>https://arxiv.org/abs/2412.13155</guid>
<content:encoded><![CDATA[
arXiv:2412.13155v2 Announce Type: replace 
Abstract: Artificial intelligence generative models exhibit remarkable capabilities in content creation, particularly in face image generation, customization, and restoration. However, current AI-generated faces (AIGFs) often fall short of human preferences due to unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation framework for AIGFs. To address this need, we introduce FaceQ, a large-scale, comprehensive database of AI-generated Face images with fine-grained Quality annotations reflecting human preferences. The FaceQ database comprises 12,255 images generated by 29 models across three tasks: (1) face generation, (2) face customization, and (3) face restoration. It includes 32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple dimensions: quality, authenticity, identity (ID) fidelity, and text-image correspondence. Using the FaceQ database, we establish F-Bench, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA), face quality assessment (FQA), AI-generated content image quality assessment (AIGCIQA), and preference evaluation metrics, manifesting that these standard metrics are relatively ineffective in evaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ database will be publicly available upon publication.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.06130</link>
<guid>https://arxiv.org/abs/2502.06130</guid>
<content:encoded><![CDATA[
arXiv:2502.06130v2 Announce Type: replace 
Abstract: While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, inspired by the observation that the text-to-image generation process is the inverse of image-conditioned response generation in LVLMs, we explore the potential of leveraging text-to-image generative models to assist in mitigating hallucinations in LVLMs. We discover that generative models can offer valuable self-feedback for mitigating hallucinations at both the response and token levels. Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free algorithm that incorporates feedback from text-to-image generative models into the decoding process to effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference and provides self-feedback to verify and correct the initial response through complementary or contrastive decoding. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations, consistently surpassing state-of-the-art methods across six benchmarks. Code is available at https://github.com/zhangce01/DeGF.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression</title>
<link>https://arxiv.org/abs/2503.02733</link>
<guid>https://arxiv.org/abs/2503.02733</guid>
<content:encoded><![CDATA[
arXiv:2503.02733v2 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference increases substantially, posing challenges in resource-constrained scenarios. Inspired by the success of traditional video compression frameworks, which process video frame by frame and can efficiently compress long videos, we adopt this modeling strategy for INRs to decrease memory consumption, while aiming to unify the frameworks from the perspective of timeline-based autoregressive modeling. In this work, we present a novel understanding of INR models from an autoregressive (AR) perspective and introduce a Unified AutoRegressive Framework for memory-efficient Neural Video Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural video compression under a unified autoregressive paradigm. It partitions videos into several clips and processes each clip using a different INR model instance, leveraging the advantages of both compression frameworks while allowing seamless adaptation to either in form. To further reduce temporal redundancy between clips, we design two modules to optimize the initialization, training, and compression of these model parameters. UAR-NVC supports adjustable latencies by varying the clip length. Extensive experimental results demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to resource-constrained environments and significantly improve performance compared to different baseline models. The project page: "https://wj-inf.github.io/UAR-NVC-page/".
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNF: Gaussian Neural Fields for Multidimensional Signal Representation and Reconstruction</title>
<link>https://arxiv.org/abs/2503.06762</link>
<guid>https://arxiv.org/abs/2503.06762</guid>
<content:encoded><![CDATA[
arXiv:2503.06762v2 Announce Type: replace 
Abstract: Neural fields have emerged as a powerful framework for representing continuous multidimensional signals such as images and videos, 3D and 4D objects and scenes, and radiance fields. While efficient, achieving high-quality representation requires the use of wide and deep neural networks. These, however, are slow to train and evaluate. Although several acceleration techniques have been proposed, they either trade memory for faster training and/or inference, rely on thousands of fitted primitives with considerable optimization time, or compromise the smooth, continuous nature of neural fields. In this paper, we introduce Gaussian Neural Fields (GNF), a novel compact neural decoder that maps learned feature grids into continuous non-linear signals, such as RGB images, Signed Distance Functions (SDFs), and radiance fields, using a single compact layer of Gaussian kernels defined in a high-dimensional feature space. Our key observation is that neurons in traditional MLPs perform simple computations, usually a dot product followed by an activation function, necessitating wide and deep MLPs or high-resolution feature grids to model complex functions. In this paper, we show that replacing MLP-based decoders with Gaussian kernels whose centers are learned features yields highly accurate representations of 2D (RGB), 3D (geometry), and 5D (radiance fields) signals with just a single layer of such kernels. This representation is highly parallelizable, operates on low-resolution grids, and trains in under $15$ seconds for 3D geometry and under $11$ minutes for view synthesis. GNF matches the accuracy of deep MLP-based decoders with far fewer parameters and significantly higher inference throughput.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
arXiv:2503.09151v3 Announce Type: replace 
Abstract: We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation</title>
<link>https://arxiv.org/abs/2503.13794</link>
<guid>https://arxiv.org/abs/2503.13794</guid>
<content:encoded><![CDATA[
arXiv:2503.13794v5 Announce Type: replace 
Abstract: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards properties of adversarial image perturbations</title>
<link>https://arxiv.org/abs/2503.14111</link>
<guid>https://arxiv.org/abs/2503.14111</guid>
<content:encoded><![CDATA[
arXiv:2503.14111v2 Announce Type: replace 
Abstract: Using stochastic gradient approach we study the properties of adversarial perturbations resulting in noticeable growth of VMAF image quality metric. The structure of the perturbations is investigated depending on the acceptable PSNR values and based on the Fourier power spectrum computations for the perturbations. It is demonstrated that moderate variation of image brightness ($\sim 10$ pixel units in a restricted region of an image can result in VMAF growth by $\sim 60\%$). Unlike some other methods demonstrating similar VMAF growth, the subjective quality of an image remains almost unchanged. It is also shown that the adversarial perturbations may demonstrate approximately linear dependence of perturbation amplitudes on the image brightness. The perturbations are studied based on the direct VMAF optimization in PyTorch. The significant discrepancies between the metric values and subjective judgements are also demonstrated when image restoration from noise is carried out using the same direct VMAF optimization.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CamC2V: Context-aware Controllable Video Generation</title>
<link>https://arxiv.org/abs/2504.06022</link>
<guid>https://arxiv.org/abs/2504.06022</guid>
<content:encoded><![CDATA[
arXiv:2504.06022v2 Announce Type: replace 
Abstract: Recently, image-to-video (I2V) diffusion models have demonstrated impressive scene understanding and generative quality, incorporating image conditions to guide generation. However, these models primarily animate static images without extending beyond their provided context. Introducing additional constraints, such as camera trajectories, can enhance diversity but often degrade visual quality, limiting their applicability for tasks requiring faithful scene representation. We propose CamC2V, a context-to-video (C2V) model that integrates multiple image conditions as context with 3D constraints alongside camera control to enrich both global semantics and fine-grained visual details. This enables more coherent and context-aware video generation. Moreover, we motivate the necessity of temporal awareness for an effective context representation. Our comprehensive study on the RealEstate10K dataset demonstrates improvements in visual quality and camera controllability. We will publish our code upon acceptance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TerraMind: Large-Scale Generative Multimodality for Earth Observation</title>
<link>https://arxiv.org/abs/2504.11171</link>
<guid>https://arxiv.org/abs/2504.11171</guid>
<content:encoded><![CDATA[
arXiv:2504.11171v4 Announce Type: replace 
Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces "Thinking-in-Modalities" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification</title>
<link>https://arxiv.org/abs/2504.11500</link>
<guid>https://arxiv.org/abs/2504.11500</guid>
<content:encoded><![CDATA[
arXiv:2504.11500v2 Announce Type: replace 
Abstract: Transit Origin-Destination (OD) data are fundamental for optimizing public transit services, yet current collection methods, such as manual surveys, Bluetooth and WiFi tracking, or Automated Passenger Counters, are either costly, device-dependent, or incapable of individual-level matching. Meanwhile, onboard surveillance cameras already deployed on most transit vehicles provide an underutilized opportunity for automated OD data collection. Leveraging this, we present TransitReID, a novel framework for individual-level and occlusion-resistant passenger re-identification tailored to transit environments. Our approach introduces three key innovations: (1) an occlusion-robust ReID algorithm that integrates a variational autoencoder-guided region-attention mechanism and selective quality feature averaging to dynamically emphasize visible and discriminative body regions under severe occlusions and viewpoint variations; (2) a Hierarchical Storage and Dynamic Matching HSDM mechanism that transforms static gallery matching into a dynamic process for robustness, accuracy, and speed in real-world bus operations; and (3) a multi-threaded edge implementation that enables near real-time OD estimation while ensuring privacy by processing all data locally. To support research in this domain, we also construct a new TransitReID dataset with over 17,000 images captured from bus front and rear cameras under diverse occlusion and viewpoint conditions. Experimental results demonstrate that TransitReID achieves state-of-the-art performance, with R-1 accuracy of 88.3 percent and mAP of 92.5 percent, and further sustains 90 percent OD estimation accuracy in bus route simulations on NVIDIA Jetson edge devices. This work advances both the algorithmic and system-level foundations of automated transit OD collection, paving the way for scalable, privacy-preserving deployment in intelligent transportation systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning</title>
<link>https://arxiv.org/abs/2505.13812</link>
<guid>https://arxiv.org/abs/2505.13812</guid>
<content:encoded><![CDATA[
arXiv:2505.13812v2 Announce Type: replace 
Abstract: Existing point cloud representation learning methods primarily rely on data-driven strategies to extract geometric information from large amounts of scattered data. However, most methods focus solely on the spatial distribution features of point clouds while overlooking the relationship between local information and the whole structure, which limits the accuracy of point cloud representation. Local information reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object's shape. In real-world, objects undergo deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object's geometric features. Therefore, the appropriate introduction of physics-driven mechanism can effectively compensate for the limitations of data-driven methods in structural modeling and significantly enhance the generalization and interpretability of point cloud representations in downstream tasks such as understanding and recognition. Inspired by this, we incorporate a physics-driven mechanism into the data-driven method to learn fine-grained features in point clouds and model the structural relationship between local regions and the whole shape. Specifically, we design a dual-task encoder-decoder framework that combines the geometric modeling capability of data-driven implicit fields with physics-driven elastic deformation. Through the integration of physics-based loss functions, the framework is guided to predict localized deformation and explicitly capture the correspondence between local structural changes and whole shape variations. Experimental results show that our method outperforms existing approaches in object classification and segmentation, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenFlow: Interactive Modular System for Image Generation</title>
<link>https://arxiv.org/abs/2506.21369</link>
<guid>https://arxiv.org/abs/2506.21369</guid>
<content:encoded><![CDATA[
arXiv:2506.21369v2 Announce Type: replace 
Abstract: Generative art unlocks boundless creative possibilities, yet its full potential remains untapped due to the technical expertise required for advanced architectural concepts and computational workflows. To bridge this gap, we present GenFlow, a novel modular framework that empowers users of all skill levels to generate images with precision and ease. Featuring a node-based editor for seamless customization and an intelligent assistant powered by natural language processing, GenFlow transforms the complexity of workflow creation into an intuitive and accessible experience. By automating deployment processes and minimizing technical barriers, our framework makes cutting-edge generative art tools available to everyone. A user study demonstrated GenFlow's ability to optimize workflows, reduce task completion times, and enhance user understanding through its intuitive interface and adaptive features. These results position GenFlow as a groundbreaking solution that redefines accessibility and efficiency in the realm of generative art.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</title>
<link>https://arxiv.org/abs/2508.16654</link>
<guid>https://arxiv.org/abs/2508.16654</guid>
<content:encoded><![CDATA[
arXiv:2508.16654v3 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a "black-box" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability</title>
<link>https://arxiv.org/abs/2508.21197</link>
<guid>https://arxiv.org/abs/2508.21197</guid>
<content:encoded><![CDATA[
arXiv:2508.21197v2 Announce Type: replace 
Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for interpreting deep neural networks by quantifying their sensitivity to human-defined concepts. However, when computed independently at different layers, CAVs often exhibit inconsistencies, making cross-layer comparisons unreliable. To address this issue, we propose the Global Concept Activation Vector (GCAV), a novel framework that unifies CAVs into a single, semantically consistent representation. Our method leverages contrastive learning to align concept representations across layers and employs an attention-based fusion mechanism to construct a globally integrated CAV. By doing so, our method significantly reduces the variance in TCAV scores while preserving concept relevance, ensuring more stable and reliable concept attributions. To evaluate the effectiveness of GCAV, we introduce Testing with Global Concept Activation Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We conduct extensive experiments on multiple deep neural networks, demonstrating that our method effectively mitigates concept inconsistency across layers, enhances concept localization, and improves robustness against adversarial perturbations. By integrating cross-layer information into a coherent framework, our method offers a more comprehensive and interpretable understanding of how deep learning models encode human-defined concepts. Code and models are available at https://github.com/Zhenghao-He/GCAV.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternating Minimization Schemes for Computing Rate-Distortion-Perception Functions with $f$-Divergence Perception Constraints</title>
<link>https://arxiv.org/abs/2408.15015</link>
<guid>https://arxiv.org/abs/2408.15015</guid>
<content:encoded><![CDATA[
arXiv:2408.15015v2 Announce Type: replace-cross 
Abstract: We study the computation of the rate-distortion-perception function (RDPF) for discrete memoryless sources subject to a single-letter average distortion constraint and a perception constraint belonging to the family of $f$-divergences. In this setting, the RDPF forms a convex programming problem for which we characterize optimal parametric solutions. We employ the developed solutions in an alternating minimization scheme, namely Optimal Alternating Minimization (OAM), for which we provide convergence guarantees. Nevertheless, the OAM scheme does not lead to a direct implementation of a generalized Blahut-Arimoto (BA) type of algorithm due to implicit equations in the iteration's structure. To overcome this difficulty, we propose two alternative minimization approaches whose applicability depends on the smoothness of the used perception metric: a Newton-based Alternating Minimization (NAM) scheme, relying on Newton's root-finding method for the approximation of the optimal solution of the iteration, and a Relaxed Alternating Minimization (RAM) scheme, based on relaxing the OAM iterates. We show, by deriving necessary and sufficient conditions, that both schemes guarantee convergence to a globally optimal solution. We also provide sufficient conditions on the distortion and perception constraints, which guarantee that the proposed algorithms converge exponentially fast in the number of iteration steps. We corroborate our theoretical results with numerical simulations and establish connections with existing results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of World Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.11260</link>
<guid>https://arxiv.org/abs/2501.11260</guid>
<content:encoded><![CDATA[
arXiv:2501.11260v4 Announce Type: replace-cross 
Abstract: Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</title>
<link>https://arxiv.org/abs/2503.22943</link>
<guid>https://arxiv.org/abs/2503.22943</guid>
<content:encoded><![CDATA[
arXiv:2503.22943v3 Announce Type: replace-cross 
Abstract: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-based Loss Functions in Computer Vision: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2504.04242</link>
<guid>https://arxiv.org/abs/2504.04242</guid>
<content:encoded><![CDATA[
arXiv:2504.04242v2 Announce Type: replace-cross 
Abstract: Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse</title>
<link>https://arxiv.org/abs/2506.14107</link>
<guid>https://arxiv.org/abs/2506.14107</guid>
<content:encoded><![CDATA[
arXiv:2506.14107v2 Announce Type: replace-cross 
Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces D\'ej\`a Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Free Analytical Quantization Scheme for Deep Learning Models</title>
<link>https://arxiv.org/abs/2412.07391</link>
<guid>https://arxiv.org/abs/2412.07391</guid>
<content:encoded><![CDATA[
<div> quantization, CNN models, Image classification, model optimization, computational efficiency
Summary:
- The paper addresses the challenges faced by CNN models due to their computational and storage demands.
- A novel post-training quantization method for model weights is introduced, aiming to reduce model size and computational requirements.
- The method optimizes clipping thresholds and scaling factors while minimizing quantization noise.
- Empirical results on real-world datasets showcase the effectiveness of the quantization scheme in preserving model accuracy.
- The research contributes towards making CNN models more feasible for deployment on resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2412.07391v3 Announce Type: replace 
Abstract: Despite the success of CNN models on a variety of Image classification and segmentation tasks, their extensive computational and storage demands pose considerable challenges for real-world deployment on resource-constrained devices. Quantization is one technique that aims to alleviate these large storage requirements and speed up the inference process by reducing the precision of model parameters to lower-bit representations. In this paper, we introduce a novel post-training quantization method for model weights. Our method finds optimal clipping thresholds and scaling factors along with mathematical guarantees that our method minimizes quantization noise. Empirical results on real-world datasets demonstrate that our quantization scheme significantly reduces model size and computational requirements while preserving model accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
<div> eigenvalue decomposition, Transformer architectures, mesh structure, Hodge Laplacian operator, deep learning layer

Summary:
This paper introduces a new approach for encoding mesh structures in Transformer architectures used for shape analysis tasks. Traditional methods rely on spectral features and eigenvalue decomposition operations, making them computationally expensive. The proposed method is inspired by the construction of the Hodge Laplacian operator in Discrete Exterior Calculus and utilizes discrete Hodge operators and exterior derivatives. By incorporating multi-head attention mechanisms to approximate Hodge matrices and learn discrete operators, the new architecture achieves comparable performance in mesh segmentation and classification tasks without the need for costly eigenvalue decomposition operations. This approach results in a more computationally efficient system and eliminates the complexity of preprocessing operations. <div>
arXiv:2509.01839v3 Announce Type: replace-cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis</title>
<link>https://arxiv.org/abs/2509.06986</link>
<guid>https://arxiv.org/abs/2509.06986</guid>
<content:encoded><![CDATA[
<div> Transformer-based architecture, cellular morphology, batch effects, out-of-distribution generalization, biological signal preservation <br /> 
Summary: 
CellPainTR is a new Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods, CellPainTR's design allows for effective out-of-distribution generalization to entirely unseen datasets without the need for fine-tuning. In validation on the JUMP dataset, CellPainTR outperformed established methods like ComBat and Harmony in batch integration and biological signal preservation. It demonstrated robustness through an out-of-distribution task on the Bray et al. dataset, maintaining high performance despite significant domain and feature shifts. This work represents a significant advancement in creating foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis. <br /> <div>
arXiv:2509.06986v1 Announce Type: new 
Abstract: Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection</title>
<link>https://arxiv.org/abs/2509.06987</link>
<guid>https://arxiv.org/abs/2509.06987</guid>
<content:encoded><![CDATA[
<div> multimodal fusion, defect detection, rail structure, YOLOv8n, Vision Transformer

Summary:<br />
- This paper introduces a new multimodal fusion architecture for defect detection in rail structures.
- The fusion combines YOLO and Vision Transformer backbones to enhance object detection accuracy.
- Two defect classes, rail Rupture and Surface defect, are targeted in the experimental evaluation.
- Results show a 0.2-point improvement in precision and overall accuracy compared to vision-only approaches.
- Statistical analysis confirms the significance of the accuracy differences between multimodal and single modality methods.

<br /><br />Summary: <div>
arXiv:2509.06987v1 Announce Type: new 
Abstract: Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.06988</link>
<guid>https://arxiv.org/abs/2509.06988</guid>
<content:encoded><![CDATA[
<div> Keywords: out-of-distribution detection, post-hoc method, data privacy protection, subspace projection, feature reconstruction<br />
<br />
Summary: 
The paper introduces a new post-hoc method called Classifier-based Feature Reconstruction (ClaFR) for out-of-distribution (OOD) detection, crucial for security applications. Unlike existing methods, ClaFR does not require access to training data, making it suitable for scenarios where data privacy protection is a concern. The method works by performing an orthogonal decomposition of classifier weights to extract the class-known subspace, mapping original data features into this subspace, and calculating the feature reconstruction error to determine the OOD score. Despite its simplicity, ClaFR outperforms other algorithms on various OOD benchmarks. The code for ClaFR is available on GitHub, allowing for easy implementation and testing. <div>
arXiv:2509.06988v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at https://github.com/Aie0923/ClaFR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining</title>
<link>https://arxiv.org/abs/2509.06990</link>
<guid>https://arxiv.org/abs/2509.06990</guid>
<content:encoded><![CDATA[
<div> Continued pretraining, specialized domains, SSL methods, DIET-CP, performance boost <br />
Summary: <br />
The article introduces DIET-CP, a continued pretraining strategy that aims to adapt foundation models to new target domains, especially in specialized domains with small datasets. DIET-CP requires no labels and introduces minimal hyperparameters, making it suitable for a wide range of data modalities and backbone choices. It can significantly improve the performance of state-of-the-art models like DINOv3 with just 1000 images. This strategy bridges the gap between available pretrained models and the need for continued pretraining, providing a simple and effective solution for adapting foundation models to new data distributions. <div>
arXiv:2509.06990v1 Announce Type: new 
Abstract: Continued pretraining offers a promising solution for adapting foundation models to a new target domain. However, in specialized domains, available datasets are often very small, limiting the applicability of SSL methods developed for large-scale pretraining and making hyperparameter search infeasible. In addition, pretrained models are usually released as backbone-weights only, lacking important information to continue pretraining. We propose to bridge this gap with DIET-CP, a simple continued pretraining strategy, where any strong foundation model can be steered towards the new data distribution of interest. DIET-CP relies on a very simple objective, requires no labels, and introduces no more hyperparameters than supervised finetuning. It is stable across data modalities and backbone choices, while providing a significant performance boost for state-of-the-art models such as DINOv3 using only 1000 images.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06992</link>
<guid>https://arxiv.org/abs/2509.06992</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Prompt Tuning, Vision-Language Models, Adversarial Robustness, Global Label Embedding, Cross-Layer Generator Sharing

Summary:
Federated Adversarial Prompt Tuning (FedAPT) is introduced to enhance adversarial robustness in large Vision-Language Models tuned using Federated Prompt Tuning (FPT). When faced with non-independent and identically distributed settings, FedAPT addresses the "class information gap" by implementing a class-aware prompt generator guided by a Global Label Embedding. This generator creates more globally-aligned visual prompts, improving robustness against adversarial attacks. Additionally, a cross-layer generator sharing strategy enhances prompt coupling across different layers, further strengthening defense mechanisms. Through extensive experiments on various image classification datasets, FedAPT outperforms existing methods, demonstrating superior adversarial robustness and generalization capabilities in cross-domain and cross-dataset scenarios. FedAPT proves effective for real-world applications requiring enhanced model security. 

<br /><br />Summary: <div>
arXiv:2509.06992v1 Announce Type: new 
Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client collaborative fine-tuning of large Vision-Language Models (VLMs). However, models tuned using FPT are vulnerable to adversarial attacks, leading to misclassification in downstream tasks. In this work, we introduce Federated Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance the adversarial robustness of FPT. We identify a key issue in FedAPT under non-independent and identically distributed (non-IID) settings: a \textit{class information gap} between clients and the global model. Clients rely solely on limited local label information to generate adversarial samples for training, while the global model must defend against adversarial attacks from global labels. To address this issue, we propose a \textbf{class-aware prompt generator} that generates visual prompts from text prompts. This generator is guided by a \emph{Global Label Embedding} (serving as a ``beacon") which encodes cross-client label information to create more globally-aligned visual prompts. Additionally, we propose a \textbf{cross-layer generator sharing} strategy to enhance prompt coupling across different layers of the model, further boosting adversarial robustness. Extensive experiments on multiple image classification datasets demonstrate the superiority of FedAPT in improving adversarial robustness, outperforming existing methods by a large margin. FedAPT also exhibits exceptional generalization in cross-domain and cross-dataset scenarios, indicating its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)</title>
<link>https://arxiv.org/abs/2509.06993</link>
<guid>https://arxiv.org/abs/2509.06993</guid>
<content:encoded><![CDATA[
<div> Embed2Scale Challenge, EarthVision, geospatial models, hyperspectral data cubes, classification<br />
<br />
Summary:
The EarthVision Embed2Scale Challenge at CVPR 2025 focuses on developing geospatial models to embed SSL4EO-S12 hyperspectral data cubes into vectors for various tasks like classification and regression. The technical report presents the Top-1 winning solution for this challenge. The proposed method aims to efficiently transform the hyperspectral geospatial data cubes into embedding vectors to enhance downstream tasks. By integrating innovative techniques, the solution demonstrates effectiveness in handling complex geospatial data and achieving high performance in the challenge. The method showcases advancements in geospatial modeling and sets a benchmark for future research in utilizing hyperspectral data for diverse applications. <div>
arXiv:2509.06993v1 Announce Type: new 
Abstract: EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into embedding vectors that faciliatetes various downstream tasks, e.g., classification, regression, etc. In this technical report, we introduce our proposed method for the Top-1 winning solution on the Embed2Scale Challenge.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality</title>
<link>https://arxiv.org/abs/2509.06994</link>
<guid>https://arxiv.org/abs/2509.06994</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, enterprise applications, ViLD framework, benchmark dataset, operational requirements

Summary: 
The paper introduces the ViLD framework, designed to evaluate Vision-Language Models (VLMs) based on operational enterprise requirements. Ten business-critical tasks are defined, such as logo detection, OCR, object detection, and demographic analysis, among others. The BlockWeaver Algorithm is introduced to compare OCR outputs from VLMs effectively. A benchmark dataset of 7,500 diverse samples is constructed from real-world images and videos. ViLD combines semantic matching, traditional metrics, and novel methods to assess VLM capabilities. Leading open-source VLMs are benchmarked against a proprietary baseline using the ViLD framework, providing insights for their deployment in enterprise environments.

<br /><br />Summary: <div>
arXiv:2509.06994v1 Announce Type: new 
Abstract: Open-source Vision-Language Models show immense promise for enterprise applications, yet a critical disconnect exists between academic evaluation and enterprise deployment requirements. Current benchmarks rely heavily on multiple-choice questions and synthetic data, failing to capture the complexity of real-world business applications like social media content analysis. This paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge this gap by evaluating VLMs on operational enterprise requirements. We define ten business-critical tasks: logo detection, OCR, object detection, human presence and demographic analysis, human activity and appearance analysis, scene detection, camera perspective and media quality assessment, dominant colors, comprehensive description, and NSFW detection. To this framework, we bring an innovative BlockWeaver Algorithm that solves the challenging problem of comparing unordered, variably-grouped OCR outputs from VLMs without relying on embeddings or LLMs, achieving remarkable speed and reliability. To demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500 diverse samples, carefully stratified from a corpus of one million real-world images and videos. ViLD provides actionable insights by combining semantic matching (both embedding-based and LLM-as-a-judge approaches), traditional metrics, and novel methods to measure the completeness and faithfulness of descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and InternVL) against a powerful proprietary baseline as per ViLD framework, we provide one of the first industry-grounded, task-driven assessment of VLMs capabilities, offering actionable insights for their deployment in enterprise environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
<link>https://arxiv.org/abs/2509.06995</link>
<guid>https://arxiv.org/abs/2509.06995</guid>
<content:encoded><![CDATA[
<div> DICOM headers, self-supervised learning, Protocol Genome, clinical imaging, calibration improvements <br />
<br />Summary: 
Protocol Genome is a self-supervised learning system that utilizes DICOM headers to improve the performance of clinical imaging tasks such as chest CT triage for PE, brain MRI glioma grading, and chest radiograph cardiomegaly detection. By leveraging structured DICOM headers as labels, Protocol Genome achieves higher AUROC scores and improved calibration and robustness across different imaging modalities and vendors. The system learns protocol-aware image representations through protocol-image contrastive learning, masked protocol prediction, and protocol-protocol translation. It significantly outperforms strong SSL baselines and ImageNet transfer methods, with notable calibration improvements. The technique reduces false positives at protocol borders and is deployable in a clinical setting through PACS integration. A model card and deployment guide are provided, including de-identification and bias audits. <div>
arXiv:2509.06995v1 Announce Type: new 
Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</title>
<link>https://arxiv.org/abs/2509.06996</link>
<guid>https://arxiv.org/abs/2509.06996</guid>
<content:encoded><![CDATA[
<div> universal cultural technology, vision, symbolic communication, resilience, language models  
Writing is a universal cultural technology that allows for symbolic communication using vision. Humans have a remarkable ability to recognize words even when they are fragmented or occluded. This study investigates whether advanced vision language models (VLMs) possess the same resilience. Two benchmarks were created using Chinese logographs and English alphabetic words to test VLMs' ability to recognize "visible but unreadable" stimuli. Despite performing well on clean text, current VLMs struggle with these perturbations, often producing irrelevant outputs. This indicates a limitation in the models' reliance on visual invariances rather than compositional priors essential for strong literacy skills. The study suggests the need for architectures and training strategies that incorporate symbol segmentation and composition in cross-script contexts. These findings highlight challenges in deploying multimodal systems in education, accessibility, cultural heritage, and security.<br /><br />Summary: Writing is a universal cultural technology that allows for symbolic communication through vision, and humans have a remarkable ability to recognize words despite visual challenges. The study explores the resilience of advanced vision language models (VLMs) by testing their performance on "visible but unreadable" stimuli in different writing systems. While VLMs excel with clean text, they struggle with perturbations, indicating a need to incorporate symbol segmentation and composition in training architectures for improved literacy. This has implications for the development of multimodal systems in various fields, emphasizing the importance of addressing challenges in education, accessibility, cultural heritage, and security. <div>
arXiv:2509.06996v1 Announce Type: new 
Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Syn: K-space Data Synthesis in Ultra Low-data Regimes</title>
<link>https://arxiv.org/abs/2509.06997</link>
<guid>https://arxiv.org/abs/2509.06997</guid>
<content:encoded><![CDATA[
<div> Feature-level learning, cardiac magnetic resonance imaging, k-space data, frequency domain, dynamic MRI reconstruction <br />
Summary: <br />
This study addresses the challenge of robust reconstruction of dynamic cardiac MRI due to limited and diverse k-space data by performing feature-level learning in the frequency domain. By leveraging the global feature space of the Fourier transform, stable and rich generation of k-space data is achieved even in low-data scenarios. The integration of k-space data across time frames with multiple fusion strategies further optimizes the generative trajectory. Experimental results demonstrate the method's strong generative ability in low-data regimes, showing promise in alleviating data scarcity in dynamic MRI reconstruction. <div>
arXiv:2509.06997v1 Announce Type: new 
Abstract: Owing to the inherently dynamic and complex characteristics of cardiac magnetic resonance (CMR) imaging, high-quality and diverse k-space data are rarely available in practice, which in turn hampers robust reconstruction of dynamic cardiac MRI. To address this challenge, we perform feature-level learning directly in the frequency domain and employ a temporal-fusion strategy as the generative guidance to synthesize k-space data. Specifically, leveraging the global representation capacity of the Fourier transform, the frequency domain can be considered a natural global feature space. Therefore, unlike traditional methods that use pixel-level convolution for feature learning and modeling in the image domain, this letter focuses on feature-level modeling in the frequency domain, enabling stable and rich generation even with ultra low-data regimes. Moreover, leveraging the advantages of feature-level modeling in the frequency domain, we integrate k-space data across time frames with multiple fusion strategies to steer and further optimize the generative trajectory. Experimental results demonstrate that the proposed method possesses strong generative ability in low-data regimes, indicating practical potential to alleviate data scarcity in dynamic MRI reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
<div> attribute prediction, generalization, model evaluation, semantic grouping, clustering

Summary:
This work explores the ability of models to generalize attribute knowledge across conceptually distant categories. By introducing novel train-test split strategies, such as semantic grouping and clustering, the study evaluates the robustness of attribute prediction under conditions of reduced correlation between training and test sets. Results indicate a significant decrease in performance as the correlation decreases, highlighting the sensitivity of models to split design. Among the methods evaluated, clustering shows the most effective trade-off, reducing hidden correlations while maintaining learnability. These findings shed light on the limitations of current representations and offer valuable insights for the development of future benchmarks in attribute reasoning. 

<br /><br />Summary: <div>
arXiv:2509.06998v1 Announce Type: new 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models</title>
<link>https://arxiv.org/abs/2509.07010</link>
<guid>https://arxiv.org/abs/2509.07010</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, 3D shape generation, CAD design, quantitative evaluation, generative models<br />
<br />
Summary: 
This paper introduces a human-in-the-loop framework for evaluating 3D models generated by Large Language Models (LLMs). It proposes a range of metrics to assess the geometric fidelity of LLM-generated models compared to ground truth CAD references. The study compares LLM performance using different input modalities, showing that models generated with code-based correction prompts achieve the highest fidelity. The research highlights the importance of semantic richness in improving generation accuracy. The quantitative evaluation approach introduced in the study allows for faster convergence towards ground truth compared to traditional qualitative methods. This work contributes to advancing AI-assisted shape synthesis and provides a scalable methodology for validating and refining generative models for various CAD applications.<br /> 
Summary: <div>
arXiv:2509.07010v1 Announce Type: new 
Abstract: Large Language Models are increasingly capable of interpreting multimodal inputs to generate complex 3D shapes, yet robust methods to evaluate geometric and structural fidelity remain underdeveloped. This paper introduces a human in the loop framework for the quantitative evaluation of LLM generated 3D models, supporting applications such as democratization of CAD design, reverse engineering of legacy designs, and rapid prototyping. We propose a comprehensive suite of similarity and complexity metrics, including volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy, to benchmark generated models against ground truth CAD references. Using an L bracket component as a case study, we systematically compare LLM performance across four input modalities: 2D orthographic views, isometric sketches, geometric structure trees, and code based correction prompts. Our findings demonstrate improved generation fidelity with increased semantic richness, with code level prompts achieving perfect reconstruction across all metrics. A key contribution of this work is demonstrating that our proposed quantitative evaluation approach enables significantly faster convergence toward the ground truth, especially compared to traditional qualitative methods based solely on visual inspection and human intuition. This work not only advances the understanding of AI assisted shape synthesis but also provides a scalable methodology to validate and refine generative models for diverse CAD applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</title>
<link>https://arxiv.org/abs/2509.07021</link>
<guid>https://arxiv.org/abs/2509.07021</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, new-view synthesis, edge devices, memory compression, rendering memory<br />
Summary:<br />
MEGS$^{2}$ is a novel framework designed to optimize memory usage in 3D Gaussian Splatting (3DGS) for new-view synthesis. By focusing on reducing the total primitive number and parameters per primitive, MEGS$^{2}$ successfully achieves significant memory compression. It replaces memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes for color representations. Additionally, MEGS$^{2}$ introduces a unified soft pruning framework that addresses both primitive and lobe pruning in a single optimization problem. Experimental results demonstrate a 50% reduction in static VRAM and a 40% reduction in rendering VRAM compared to existing methods, while maintaining high rendering quality. <br /><br /> <div>
arXiv:2509.07021v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.07027</link>
<guid>https://arxiv.org/abs/2509.07027</guid>
<content:encoded><![CDATA[
<div> Gaussianity regularization, text-to-image models, moment-based regularization, power spectrum-based regularization, permutation invariance 

Summary: 
Gaussianity regularization is proposed to enforce alignment with a standard Gaussian distribution in the latent space of text-to-image models. The regularization combines moment-based and power spectrum-based approaches to ensure conformity to Gaussian properties. By applying the regularization to randomly permuted inputs, permutation invariance is maintained. Existing Gaussianity regularizations are encompassed within this unified framework, with some corresponding to specific moment losses. The proposed regularization outperforms previous methods by enhancing aesthetics, preventing reward hacking, and accelerating convergence in generative modeling for text-to-image models. The analytical knowledge of expected values in moments and power spectrum distributions allows for effective regularization and facilitates improved performance in downstream tasks.<br /><br />Summary: <div>
arXiv:2509.07027v1 Announce Type: new 
Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
<link>https://arxiv.org/abs/2509.07047</link>
<guid>https://arxiv.org/abs/2509.07047</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, microscopy, reward function optimization, SAM framework, real-time streaming data

Summary:
Image segmentation is crucial in microscopy for analyzing complex visual data. Different approaches like custom models, transfer learning, and foundational models can be used for this task. However, foundational models often have many tuning parameters that require manual optimization, hindering real-time data analysis. In this study, a reward function-based optimization approach was introduced to fine-tune the SAM framework by Meta for image segmentation tasks. By constructing reward functions that represent the physics of the imaged system, including particle size distributions and geometries, the SAM model was enhanced to align better with diverse segmentation requirements, leading to an optimized variant, SAM$^{*}$, suitable for real-time streaming data segmentation. This approach was demonstrated to be effective in microscopy imaging, where precise segmentation is essential for analyzing cellular structures, material interfaces, and nanoscale features. 

<br /><br />Summary: Image segmentation in microscopy is critical and can be done using different models. However, foundational models often require manual optimization. A reward function-based optimization approach was introduced in this study for fine-tuning the SAM framework. By integrating reward functions representing the physics of the imaged system, an optimized variant, SAM$^{*}$, was developed for diverse segmentation tasks, including real-time streaming data segmentation. This approach was effective in accurately analyzing cellular structures, material interfaces, and nanoscale features in microscopy imaging. <div>
arXiv:2509.07047v1 Announce Type: new 
Abstract: Image segmentation is a critical task in microscopy, essential for accurately analyzing and interpreting complex visual data. This task can be performed using custom models trained on domain-specific datasets, transfer learning from pre-trained models, or foundational models that offer broad applicability. However, foundational models often present a considerable number of non-transparent tuning parameters that require extensive manual optimization, limiting their usability for real-time streaming data analysis. Here, we introduce a reward function-based optimization to fine-tune foundational models and illustrate this approach for SAM (Segment Anything Model) framework by Meta. The reward functions can be constructed to represent the physics of the imaged system, including particle size distributions, geometries, and other criteria. By integrating a reward-driven optimization framework, we enhance SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$, that better aligns with the requirements of diverse segmentation tasks and particularly allows for real-time streaming data segmentation. We demonstrate the effectiveness of this approach in microscopy imaging, where precise segmentation is crucial for analyzing cellular structures, material interfaces, and nanoscale features.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Classification of Streaming Data with Image Distillation</title>
<link>https://arxiv.org/abs/2509.07049</link>
<guid>https://arxiv.org/abs/2509.07049</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming data classification, data distillation, limited resources, image data, computational efficiency

Summary:
The study explores a novel approach to efficiently classify streaming image data in resource-constrained environments. It introduces a method called Distillation Based Classification (DBC) that distills essential features from data streams to enhance classification accuracy while reducing computational demands. Comparing DBC with traditional algorithms like Hoeffding Trees and Adaptive Random Forest, DBC outperformed them and Reservoir Sampling Based Classification (RBC) with a 73.1% accuracy rate. This demonstrates the effectiveness of the DBC method in processing complex data streams and improving classification accuracy and efficiency. Overall, this research represents a significant advancement in streaming data classification, offering a new standard for handling streaming data with limited memory and computational resources. 

<br /><br />Summary: <div>
arXiv:2509.07049v1 Announce Type: new 
Abstract: This study tackles the challenge of efficiently classifying streaming data in envi-ronments with limited memory and computational resources. It delves into the application of data distillation as an innovative approach to improve the precision of streaming image data classification. By focusing on distilling essential features from data streams, our method aims to minimize computational demands while preserving crucial information for accurate classification. Our investigation com-pares this approach against traditional algorithms like Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for image data. The Distillation Based Classification (DBC) demonstrated superior performance, achieving a 73.1% accuracy rate, surpassing both traditional methods and Reservoir Sam-pling Based Classification (RBC) technique. This marks a significant advance-ment in streaming data classification, showcasing the effectiveness of our method in processing complex data streams and setting a new standard for accuracy and efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Evaluation of Gender Bias Across 13 Large Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07050</link>
<guid>https://arxiv.org/abs/2509.07050</guid>
<content:encoded><![CDATA[
<div> gender bias, multimodal models, AI-generated images, occupational stereotypes, fairness evaluation

Summary:
- Large multimodal models (LMMs) used for text-to-image generation have been found to perpetuate gender bias in AI-generated images.
- The Aymara Image Fairness Evaluation benchmark was introduced to assess social bias in AI-generated images.
- 13 commercially available LMMs were tested using gender-neutral prompts to generate images of people in different professions.
- The results showed that LMMs systematically amplified occupational gender stereotypes, with a default bias towards male representation.
- Variation in bias across models suggests that high bias is not inevitable and can be influenced by design choices.
- The study highlights the importance of standardized, automated evaluation tools for ensuring accountability and fairness in AI development.<br /><br />Summary: <div>
arXiv:2509.07050v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster VGGT with Block-Sparse Global Attention</title>
<link>https://arxiv.org/abs/2509.07120</link>
<guid>https://arxiv.org/abs/2509.07120</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, such as VGGT and $\pi^3$, for multi-view reconstruction in computer vision face a runtime bottleneck due to the quadratic complexity of global attention layers. The global attention matrix of these models shows concentration on cross-view geometric matches. This paper proposes a replacement for dense global attention using block-sparse kernels, resulting in up to 4x faster inference with comparable performance. The approach does not require retraining of the backbone, works for both VGGT and $\pi^3$, and supports large image collections. Evaluations on various multi-view benchmarks demonstrate the effectiveness of the proposed method.

Keywords: multi-view reconstruction, computer vision, transformer-based models, global attention, block-sparse kernels<br /><br />Summary: 
- Transformer-based models like VGGT and $\pi^3$ face a runtime bottleneck due to the quadratic complexity of global attention layers.
- The global attention matrix of these models indicates a concentration on cross-view geometric matches.
- A replacement for dense global attention using block-sparse kernels is proposed, resulting in up to 4x faster inference with similar performance.
- The approach does not necessitate retraining of the backbone and is applicable to both VGGT and $\pi^3.
- Evaluations on various multi-view benchmarks confirm the efficacy of the proposed method. <div>
arXiv:2509.07120v1 Announce Type: new 
Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been an important task in computer vision. Recent transformer-based models like VGGT and $\pi^3$ have achieved impressive results with simple architectures, yet they face an inherent runtime bottleneck, due to the quadratic complexity of the global attention layers, that limits the scalability to large image sets. In this paper, we empirically analyze the global attention matrix of these models and observe that probability mass concentrates on a small subset of patch-patch interactions that correspond to cross-view geometric matches. Motivated by the structured attention and inspired by recent advancement in large language models, we propose a replacement for the dense global attention operation based on highly optimized block-sparse kernels, yielding up to $4\times$ faster inference with comparable task performance. Our retrofit requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and supports large image collections. Evaluations on a comprehensive suite of multi-view benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</title>
<link>https://arxiv.org/abs/2509.07130</link>
<guid>https://arxiv.org/abs/2509.07130</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual-Inertial Odometry, Virtual Reality, pose spoofing, edge servers, unsupervised detection

Summary:
Visual-Inertial Odometry (VIO) fusion of camera and IMU data for real-time pose estimation in VR is vulnerable to pose spoofing when offloaded to edge servers. This paper introduces an unsupervised detection and recovery mechanism trained on attack-free sessions to detect runtime deviations and restore pose consistency. The model learns temporal regularities of motion to identify pose spoofing and initiates recovery measures. Evaluation in an offloaded-VIO environment using the ILLIXR testbed demonstrates significant reductions in trajectory and pose error compared to a no-defense baseline. This approach addresses the server-side threat surface posed by subtle pose spoofing, providing enhanced security and reliability for immersive VR experiences. 

<br /><br />Summary: <div>
arXiv:2509.07130v1 Announce Type: new 
Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by fusing camera and Inertial Measurement Unit (IMU) data for real-time pose. However, current trend of offloading VIO to edge servers can lead server-side threat surface where subtle pose spoofing can accumulate into substantial drift, while evading heuristic checks. In this paper, we study this threat and present an unsupervised, label-free detection and recovery mechanism. The proposed model is trained on attack-free sessions to learn temporal regularities of motion to detect runtime deviations and initiate recovery to restore pose consistency. We evaluate the approach in a realistic offloaded-VIO environment using ILLIXR testbed across multiple spoofing intensities. Experimental results in terms of well-known performance metrics show substantial reductions in trajectory and pose error compared to a no-defense baseline.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement</title>
<link>https://arxiv.org/abs/2509.07178</link>
<guid>https://arxiv.org/abs/2509.07178</guid>
<content:encoded><![CDATA[
<div> face enhancement techniques, deepfake detectors, anti-forensic, GAN-based enhancements, detection accuracy <br />
Summary: 

This study explores the impact of face enhancement techniques on the accuracy of deepfake detectors. The research suggests that while these enhancement methods improve facial appearance, they can unintentionally distort biometric features and hinder deepfake detection. By analyzing traditional image processing and GAN-based enhancements, the study evaluates the effectiveness of these techniques on various detection methods. Results show that even basic enhancement filters can significantly decrease detection accuracy, with ASR reaching up to 64.63%. Moreover, GAN-based techniques exploit these vulnerabilities further, achieving ASR up to 75.12%. The study also investigates the potential use of face enhancement methods as anti-forensic tools, highlighting the importance of developing more resilient forensic methods in response to these advancements. <div>
arXiv:2509.07178v1 Announce Type: new 
Abstract: Face enhancement techniques are widely used to enhance facial appearance. However, they can inadvertently distort biometric features, leading to significant decrease in the accuracy of deepfake detectors. This study hypothesizes that these techniques, while improving perceptual quality, can degrade the performance of deepfake detectors. To investigate this, we systematically evaluate whether commonly used face enhancement methods can serve an anti-forensic role by reducing detection accuracy. We use both traditional image processing methods and advanced GAN-based enhancements to evaluate the robustness of deepfake detectors. We provide a comprehensive analysis of the effectiveness of these enhancement techniques, focusing on their impact on Na\"ive, Spatial, and Frequency-based detection methods. Furthermore, we conduct adversarial training experiments to assess whether exposure to face enhancement transformations improves model robustness. Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2 datasets indicate that even basic enhancement filters can significantly reduce detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%. Our results demonstrate that face enhancement methods can effectively function as anti-forensic tools, emphasizing the need for more resilient and adaptive forensic methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensionally Reduced Open-World Clustering: DROWCULA</title>
<link>https://arxiv.org/abs/2509.07184</link>
<guid>https://arxiv.org/abs/2509.07184</guid>
<content:encoded><![CDATA[
<div> Keywords: annotated data, supervised learning, open-world, Vision Transformers, clustering

Summary:
In the field of supervised learning, providing labels to instances is a labor-intensive task, particularly in open-world scenarios where new classes may appear unpredictably. This study presents a fully unsupervised approach to identifying novel categories in image datasets. By leveraging Vision Transformers and attention mechanisms to generate embeddings, and incorporating manifold learning techniques to enhance clustering performance through data geometry, the proposed method achieves State-of-the-Art results on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet datasets. The approach is effective regardless of whether the number of clusters is known in advance or not. The code for implementing the method is publicly available on GitHub. This work not only showcases the potential of unsupervised techniques but also contributes to advancing the field of image classification in challenging open-world settings.<br /><br />Summary: <div>
arXiv:2509.07184v1 Announce Type: new 
Abstract: Working with annotated data is the cornerstone of supervised learning. Nevertheless, providing labels to instances is a task that requires significant human effort. Several critical real-world applications make things more complicated because no matter how many labels may have been identified in a task of interest, it could be the case that examples corresponding to novel classes may appear in the future. Not unsurprisingly, prior work in this, so-called, `open-world' context has focused a lot on semi-supervised approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully unsupervised approach to the problem of determining the novel categories in a particular dataset. Our approach relies on estimating the number of clusters using Vision Transformers, which utilize attention mechanisms to generate vector embeddings. Furthermore, we incorporate manifold learning techniques to refine these embeddings by exploiting the intrinsic geometry of the data, thereby enhancing the overall image clustering performance. Overall, we establish new State-of-the-Art results on single-modal clustering and Novel Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do so, both when the number of clusters is known or unknown ahead of time. The code is available at: https://github.com/DROWCULA/DROWCULA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning</title>
<link>https://arxiv.org/abs/2509.07213</link>
<guid>https://arxiv.org/abs/2509.07213</guid>
<content:encoded><![CDATA[
<div> Keywords: breast ultrasound segmentation, multimodal model, global context, local precision, lesion size

Summary: 
Breast ultrasound segmentation is essential for reliable measurement and analysis, but can be challenging for small or low-contrast lesions. The XBusNet model, a dual-prompt, dual-branch multimodal approach, combines image features with clinically relevant text prompts to improve segmentation accuracy. The model includes a global pathway for whole-image semantics and a local pathway for precise boundary modeling, enhanced by prompts describing lesion shape, margin, and BI-RADS terms. By automatically assembling prompts from structured metadata, manual intervention is minimized. Evaluation on the BLU dataset through cross-validation shows that XBusNet outperforms six strong baselines, achieving state-of-the-art performance with a mean Dice of 0.8765 and IoU of 0.8149. Small lesions benefit the most from the model, with improved segmentation accuracy and fewer errors. Ablation studies confirm the complementary roles of global context, local precision, and prompt-based modulation in enhancing segmentation results. <br /><br />Summary: <div>
arXiv:2509.07213v1 Announce Type: new 
Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion</title>
<link>https://arxiv.org/abs/2509.07277</link>
<guid>https://arxiv.org/abs/2509.07277</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, medical imaging, breast cancer classification, data augmentation, generative models

Summary:
This article introduces a framework for breast cancer classification in thermograms that addresses data scarcity in medical imaging. The framework utilizes a Diffusion Probabilistic Model (DPM) for data augmentation, showing superiority over traditional methods and a ProGAN baseline. By combining deep features from a pre-trained ResNet-50 with handcrafted nonlinear features like Fractal Dimension derived from U-Net segmented tumors, an XGBoost classifier achieves high accuracy (98.0%) and sensitivity (98.1%). Ablation studies and statistical tests confirm the importance of both DPM augmentation and nonlinear feature fusion in achieving this success. The study validates the effectiveness of advanced generative models and interpretable features in creating highly accurate medical diagnostic tools.<br /><br />Summary: <div>
arXiv:2509.07277v1 Announce Type: new 
Abstract: Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction Alignment Improves Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified multimodal models, Reconstruction Alignment, visual understanding, generation, post-training

Summary:
Reconstruction Alignment (RecA) is introduced as a post-training method for Unified Multimodal Models (UMMs) that leverages visual understanding encoder embeddings as dense "text prompts" to provide rich supervision without captions. By conditioning a UMM on its own visual understanding embeddings and optimizing it to reconstruct the input image with a self-supervised reconstruction loss, RecA realigns understanding and generation, improving generation and editing fidelity across different UMM architectures. With just 27 GPU-hours, RecA significantly enhances image generation performance on various benchmarks while surpassing larger open-source models. This resource-efficient method boosts editing benchmarks and establishes itself as an efficient and general post-training alignment strategy for UMMs. <div>
arXiv:2509.07295v1 Announce Type: new 
Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion</title>
<link>https://arxiv.org/abs/2509.07327</link>
<guid>https://arxiv.org/abs/2509.07327</guid>
<content:encoded><![CDATA[
<div> Keywords: multispectral remote sensing, unmanned aerial vehicle, object detection, dual-domain enhancement, mamba fusion

Summary: 
DEPF is proposed to address challenges in multispectral remote sensing object detection using UAVs. It introduces Dual-Domain Enhancement Module (DDE) for enhancing low-light images with Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). The Priority-Guided Mamba Fusion Module (PGMF) enhances local target modeling by prioritizing features based on modality difference. The method outperforms state-of-the-art techniques on DroneVehicle and VEDAI datasets. The code is available in the supplementary material. <div>
arXiv:2509.07327v1 Announce Type: new 
Abstract: Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition</title>
<link>https://arxiv.org/abs/2509.07335</link>
<guid>https://arxiv.org/abs/2509.07335</guid>
<content:encoded><![CDATA[
<div> GCNs, skeleton-based action recognition, Gaussian Topology Refinement, Gated Graph Convolution, ambiguous actions <br />
Summary: <br />
The article introduces a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. By incorporating a Gaussian filter to refine the skeleton topology graph, G$^{3}$CN improves the representation of ambiguous actions. Additionally, the integration of Gated Recurrent Units (GRUs) in the GCN framework enhances information propagation between skeleton points. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples. The method shows strong generalization across various GCN backbones, highlighting its potential for enhancing the performance of skeleton-based action recognition systems. <br /> <div>
arXiv:2509.07335v1 Announce Type: new 
Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for skeleton-based action recognition, primarily due to their ability to leverage graph topology for feature aggregation, a key factor in extracting meaningful representations. However, despite their success, GCNs often struggle to effectively distinguish between ambiguous actions, revealing limitations in the representation of learned topological and spatial features. To address this challenge, we propose a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates a Gaussian filter to refine the skeleton topology graph, improving the representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs) are integrated into the GCN framework to enhance information propagation between skeleton points. Our method shows strong generalization across various GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parse Graph-Based Visual-Language Interaction for Human Pose Estimation</title>
<link>https://arxiv.org/abs/2509.07385</link>
<guid>https://arxiv.org/abs/2509.07385</guid>
<content:encoded><![CDATA[
<div> Parse Graphs, Human Pose Estimation, Multimodal Fusion, Visual-Language Interaction, Guided Module 

Summary: 
This article introduces Parse Graph-based Visual-Language interaction (PGVL) for improving human pose estimation (HPE) by integrating visual and language modalities. Existing methods often struggle with occluded scenes, leading to alignment and location failures. PGVL addresses this issue by incorporating a novel Guided Module (GM) that allows for effective fusion of diverse information. The approach involves top-down decomposition and bottom-up composition, constructing modality-specific parse graphs and utilizing recursive bidirectional cross-attention guided by the GM. By focusing on local features for occluded areas and integrating global features for inferencing invisible parts, PGVL enhances the performance of HPE models. The proposed network design based on PGVL shows promising results on major pose estimation datasets, emphasizing the importance of multimodal fusion in enhancing context and hierarchies for accurate human pose estimation. <div>
arXiv:2509.07385v1 Announce Type: new 
Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and hierarchies, yet prior work mostly focuses on single modality modeling, ignoring the potential of multimodal fusion. Notably, language offers rich HPE priors like spatial relations for occluded scenes, but existing visual-language fusion via global feature integration weakens occluded region responses and causes alignment and location failures. To address this issue, we propose Parse Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module (GM). In PGVL, low-level nodes focus on local features, maximizing the maintenance of responses in occluded areas and high-level nodes integrate global features to infer occluded or invisible parts. GM enables high semantic nodes to guide the feature update of low semantic nodes that have undergone cross attention. It ensuring effective fusion of diverse information. PGVL includes top-down decomposition and bottom-up composition. In the first stage, modality specific parse graphs are constructed. Next stage. recursive bidirectional cross-attention is used, purified by GM. We also design network based on PGVL. The PGVL and our network is validated on major pose estimation datasets. We will release the code soon.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</title>
<link>https://arxiv.org/abs/2509.07435</link>
<guid>https://arxiv.org/abs/2509.07435</guid>
<content:encoded><![CDATA[
<div> modular design, PBR materials, 3D asset generation, Gaussian Asset Adapter, multi-view diffusion priors  
Summary:  
The article introduces the Lightweight Gaussian Asset Adapter (LGAA), a framework designed to simplify the creation of 3D assets with physically based rendering (PBR) materials. The LGAA unifies geometry modeling and PBR material synthesis by utilizing multi-view (MV) diffusion priors. Its modular design includes the LGAA Wrapper, LGAA Switcher, and LGAA Decoder components, enabling the incorporation of multiple diffusion priors for efficient convergence. Through the use of text-and image-conditioned MV diffusion models, the LGAA demonstrates superior performance in generating high-quality, relightable mesh assets. The framework is trained on a limited dataset of 69k multi-view instances, showcasing its efficiency in 3D asset creation. The code, pre-trained weights, and dataset will be made publicly available, allowing for further research and implementation in autonomous 3D asset generation.  
<br /><br />Summary: <div>
arXiv:2509.07435v1 Announce Type: new 
Abstract: The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting</title>
<link>https://arxiv.org/abs/2509.07447</link>
<guid>https://arxiv.org/abs/2509.07447</guid>
<content:encoded><![CDATA[
<div> multimodal large language models, egocentric videos, gaze information, personalized AI user experiences, EgoGazeVQA
Summary:
multimodal large language models have improved AI assistants' capabilities, egocentric videos capture user focus and context, gaze information is crucial for user intent indication, EgoGazeVQA benchmark leverages gaze for better video understanding, existing models struggle with user intentions, gaze-guided intent prompting enhances performance, spatial, temporal, and intent-related cues improve interpretation, gaze-related fine-tuning impacts prompting effectiveness, value of gaze in personalized AI assistants in egocentric settings<br /><br />Summary: <div>
arXiv:2509.07447v1 Announce Type: new 
Abstract: The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</title>
<link>https://arxiv.org/abs/2509.07450</link>
<guid>https://arxiv.org/abs/2509.07450</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-View Geo-Localization, Multi-modal, Multi-view alignment, Explainable reasoning, GLEAM-C<br />
Summary:<br />
The article introduces a new Cross-View Geo-Localization (CVGL) model, GLEAM-C, that integrates multiple views and modalities for accurate location identification. The model aligns different perspectives like UAV imagery, street maps, and ground photographs with satellite imagery. It uses a two-phase training strategy to enhance efficiency and accuracy. To improve interpretability, the authors introduce GLEAM-X, a task that combines cross-view correspondence prediction with explainable reasoning using multimodal large language models. They create a bilingual benchmark for training and testing data using GPT-4o and Doubao-1.5-Thinking-Vision-Pro. Human revision of the test set ensures detailed evaluation of explainable cross-view reasoning. This comprehensive CVGL pipeline aims to advance transparency and scalability in Geo-Localization by combining accurate matching with interpretable reasoning. The code and datasets used in the study will be publicly available on GitHub at https://github.com/Lucky-Lance/GLEAM. <br /> <div>
arXiv:2509.07450v1 Announce Type: new 
Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they merely predict whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/GLEAM.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning</title>
<link>https://arxiv.org/abs/2509.07455</link>
<guid>https://arxiv.org/abs/2509.07455</guid>
<content:encoded><![CDATA[
<div> framework, deep learning, OCTA, retinal vasculature, vascular reconstruction <br />
Summary:
The article introduces XOCT, a novel deep learning framework designed to improve the reconstruction of retinal vasculature using OCTA. XOCT utilizes Cross-Dimensional Supervision (CDS) along with a Multi-Scale Feature Fusion (MSFF) network to enhance the accuracy and resolution of vascular details in OCTA images. The CDS module provides targeted guidance by leveraging layer-wise en-face projections, allowing the network to learn distinct representations for each retinal layer. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction and channel reweighting strategies. Experimental results on the OCTA-500 dataset show significant improvements in vascular reconstruction, particularly in en-face projections critical for clinical evaluation of retinal diseases. XOCT has the potential to enhance OCTA accessibility, reliability, and diagnostic value in ophthalmic disease detection and monitoring. The code for XOCT is available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2509.07455v1 Announce Type: new 
Abstract: Optical Coherence Tomography Angiography (OCTA) and its derived en-face projections provide high-resolution visualization of the retinal and choroidal vasculature, which is critical for the rapid and accurate diagnosis of retinal diseases. However, acquiring high-quality OCTA images is challenging due to motion sensitivity and the high costs associated with software modifications for conventional OCT devices. Moreover, current deep learning methods for OCT-to-OCTA translation often overlook the vascular differences across retinal layers and struggle to reconstruct the intricate, dense vascular details necessary for reliable diagnosis. To overcome these limitations, we propose XOCT, a novel deep learning framework that integrates Cross-Dimensional Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise en-face projections, generated via segmentation-weighted z-axis averaging, as supervisory signals to compel the network to learn distinct representations for each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction combined with a channel reweighting strategy, effectively capturing vascular details at multiple spatial scales. Our experiments on the OCTA-500 dataset demonstrate XOCT's improvements, especially for the en-face projections which are significant for clinical evaluation of retinal pathologies, underscoring its potential to enhance OCTA accessibility, reliability, and diagnostic value for ophthalmic disease detection and monitoring. The code is available at https://github.com/uci-cbcl/XOCT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting</title>
<link>https://arxiv.org/abs/2509.07456</link>
<guid>https://arxiv.org/abs/2509.07456</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, bias mitigation, machine unlearning, vision models, fairness

Summary:
Deep neural networks often exhibit bias due to spurious correlations in training data, leading to unfair predictions in critical fields like medicine and autonomous driving. Traditional bias mitigation methods require retraining or data pipeline redesign, but Bias-Aware Machine Unlearning offers a post-hoc solution. By selectively removing biased samples or features, this approach can significantly reduce subgroup disparities in vision models without the need for full retraining. Through experiments on various datasets, improvements in demographic parity were observed, with gains of up to 94.86% on CUB-200, 30.28% on CIFAR-10, and 97.37% on CelebA. Achieving these results with minimal accuracy loss, machine unlearning methods scored favorably across utility, fairness, quality, and privacy evaluations. This research demonstrates the practicality of machine unlearning in enhancing fairness in deployed vision systems. 

<br /><br />Summary: <div>
arXiv:2509.07456v1 Announce Type: new 
Abstract: Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANYPORTAL: Zero-Shot Consistent Video Background Replacement</title>
<link>https://arxiv.org/abs/2509.07472</link>
<guid>https://arxiv.org/abs/2509.07472</guid>
<content:encoded><![CDATA[
<div> diffusion models, video generation technology, foreground consistency, video content creation, image diffusion models
<br />
Summary: 
ANYPORTAL is a new zero-shot framework for video background replacement that combines temporal and relighting features of diffusion models. It addresses the challenge of foreground consistency by introducing a Refinement Projection Algorithm for detailed manipulation. The framework does not require training and ensures precise foreground preservation and temporally coherent relighting. Experimental results show high-quality video results can be achieved efficiently on consumer-grade GPUs, making it a practical solution for video editing and content creation. <div>
arXiv:2509.07472v1 Announce Type: new 
Abstract: Despite the rapid advancements in video generation technology, creating high-quality videos that precisely align with user intentions remains a significant challenge. Existing methods often fail to achieve fine-grained control over video details, limiting their practical applicability. We introduce ANYPORTAL, a novel zero-shot framework for video background replacement that leverages pre-trained diffusion models. Our framework collaboratively integrates the temporal prior of video diffusion models with the relighting capabilities of image diffusion models in a zero-shot setting. To address the critical challenge of foreground consistency, we propose a Refinement Projection Algorithm, which enables pixel-level detail manipulation to ensure precise foreground preservation. ANYPORTAL is training-free and overcomes the challenges of achieving foreground consistency and temporally coherent relighting. Experimental results demonstrate that ANYPORTAL achieves high-quality results on consumer-grade GPUs, offering a practical and efficient solution for video content creation and editing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification</title>
<link>https://arxiv.org/abs/2509.07477</link>
<guid>https://arxiv.org/abs/2509.07477</guid>
<content:encoded><![CDATA[
<div> Interpretable deep learning, MedicalPatchNet, chest X-ray classification, image segmentation, CheXpert dataset<br />
<br />
Summary: 
MedicalPatchNet is a novel approach for chest X-ray classification that enhances interpretability in deep neural networks. By splitting images into patches and classifying them independently before aggregating predictions, the model provides transparent attribution of decisions to distinct regions in the image. Trained on the CheXpert dataset, MedicalPatchNet achieves comparable classification performance to EfficientNet-B0 while significantly improving interpretability. It outperforms existing methods in pathology localization accuracy, making it more reliable and accessible for non-AI experts. By mitigating risks associated with shortcut learning, the model enhances clinical trust in AI-assisted diagnostics. The publicly available code for MedicalPatchNet contributes to safer and explainable AI in medical imaging applications. <br /><br /> <div>
arXiv:2509.07477v1 Announce Type: new 
Abstract: Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors</title>
<link>https://arxiv.org/abs/2509.07484</link>
<guid>https://arxiv.org/abs/2509.07484</guid>
<content:encoded><![CDATA[
<div> Keywords: Vector graphics, implicit neural representations, text-to-video diffusion models, animation, automation

Summary: 
This paper introduces a new method for automating the animation of vector graphics by combining implicit neural representations with text-to-video diffusion models. The use of layered implicit neural representations helps reconstruct vector graphics while preserving their unique properties like infinite resolution and precise color and shape constraints. By leveraging motion priors from pretrained text-to-video diffusion models, the neural representations are optimized using video score distillation sampling to generate smooth animations. The proposed method demonstrates improved flexibility and animation quality compared to existing techniques, providing vivid and natural vector graphic animations. <div>
arXiv:2509.07484v1 Announce Type: new 
Abstract: Vector graphics, known for their scalability and user-friendliness, provide a unique approach to visual content compared to traditional pixel-based images. Animation of these graphics, driven by the motion of their elements, offers enhanced comprehensibility and controllability but often requires substantial manual effort. To automate this process, we propose a novel method that integrates implicit neural representations with text-to-video diffusion models for vector graphic animation. Our approach employs layered implicit neural representations to reconstruct vector graphics, preserving their inherent properties such as infinite resolution and precise color and shape constraints, which effectively bridges the large domain gap between vector graphics and diffusion models. The neural representations are then optimized using video score distillation sampling, which leverages motion priors from pretrained text-to-video diffusion models. Finally, the vector graphics are warped to match the representations resulting in smooth animation. Experimental results validate the effectiveness of our method in generating vivid and natural vector graphic animations, demonstrating significant improvement over existing techniques that suffer from limitations in flexibility and animation quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Visual Navigation Assistance</title>
<link>https://arxiv.org/abs/2509.07488</link>
<guid>https://arxiv.org/abs/2509.07488</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language, indoor navigation, visually impaired, BLIP-2 model, Low Rank Adaptation (LoRA)

Summary: 
The study focuses on vision-language-driven indoor navigation for the visually impaired, utilizing a combination of images and natural language guidance. Traditional navigation systems struggle indoors due to a lack of precise location data, prompting the integration of vision and language models to offer step-by-step navigational instructions. By fine-tuning the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset, the researchers were able to significantly enhance the generation of directional instructions. Additionally, a novel evaluation metric was proposed to assess navigational performance, placing emphasis on directional and sequential elements for a more comprehensive analysis. Overall, the study highlights the effectiveness of the integrated vision-language approach in improving accessibility and independence for individuals with visual impairments when navigating indoor spaces. 

<br /><br />Summary: <div>
arXiv:2509.07488v1 Announce Type: new 
Abstract: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning</title>
<link>https://arxiv.org/abs/2509.07493</link>
<guid>https://arxiv.org/abs/2509.07493</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, Signed Distance Field, 3D surface reconstruction, DiGS framework, multi-scale hierarchy <br />
Summary:<br />
The article introduces a new framework called DiGS that integrates Signed Distance Field (SDF) learning into the 3D Gaussian Splatting (3DGS) pipeline for improved surface reconstruction. By assigning each Gaussian primitive a learnable SDF value, DiGS enhances the alignment of primitives with underlying geometry, resulting in increased accuracy and consistency across views. A novel geometry-guided grid growth strategy is implemented to ensure dense and coherent distribution of Gaussians along geometry-consistent regions in a multi-scale hierarchy. Experimental results on various benchmarks, including DTU, Mip-NeRF 360, and Tanks&amp;Temples, demonstrate that the DiGS framework significantly enhances reconstruction quality and completeness while maintaining high rendering fidelity. <div>
arXiv:2509.07493v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition</title>
<link>https://arxiv.org/abs/2509.07495</link>
<guid>https://arxiv.org/abs/2509.07495</guid>
<content:encoded><![CDATA[
<div> vulnerable, adversarial attacks, deep neural networks, remote sensing applications, robustness <br />
Summary:<br />
- The article discusses the vulnerability of Deep Neural Networks (DNNs) to adversarial attacks in remote sensing applications.
- Current mixing-based strategies to increase transferability of adversarial examples may destroy global semantic features.
- The proposed framework focuses on non-targeted attacks and introduces a local mixing strategy to generate diverse yet semantically consistent inputs.
- Logit loss is adapted from targeted attacks to non-targeted scenarios to address gradient vanishing issues of cross-entropy loss.
- A perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability.
- Extensive experiments on FGSCR-42 and MTARSI datasets show superior performance compared to 12 state-of-the-art methods across 6 surrogate models, with a significant improvement in black-box attack success rate with ResNet as the surrogate on MTARSI.<br /> 
Summary: <div>
arXiv:2509.07495v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.07507</link>
<guid>https://arxiv.org/abs/2509.07507</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised annotation, 3D object detection, temporal multi-view, Teacher-Student distillation, multi-view projection loss

Summary: <br /><br /> Annotation of 3D data for object detection is a costly process, leading to the development of weakly supervised methods using 2D box annotations. However, relying solely on 2D boxes can introduce ambiguities due to multiple valid 3D poses. The MVAT framework addresses this by leveraging temporal multi-view data to create dense 3D object representations. A Teacher-Student distillation approach is used, with the Teacher network learning from multiple viewpoints and generating high-quality pseudo-labels for the Student network to predict 3D boxes accurately. A multi-view 2D projection loss ensures consistency between predicted 3D boxes and 2D annotations. Experimental results on nuScenes and Waymo Open datasets show that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, narrowing the gap with fully supervised methods without requiring 3D box annotations. The code for MVAT is available in the public repository. <div>
arXiv:2509.07507v1 Announce Type: new 
Abstract: Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \footnote{Code available upon acceptance} Our code is available in our public repository (\href{https://github.com/CEA-LIST/MVAT}{code}).
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHWGesture -- A dataset for multimodal understanding of clinical gestures</title>
<link>https://arxiv.org/abs/2509.07525</link>
<guid>https://arxiv.org/abs/2509.07525</guid>
<content:encoded><![CDATA[
<div> Gesture understanding, deep learning, multimodal dataset, clinical assessment, hand dexterity

Summary: 
The paper introduces EHWGesture, a multimodal video dataset for gesture understanding that addresses challenges in dynamic gesture recognition for applications such as automatic clinical assessment of hand dexterity. The dataset features five clinically relevant gestures captured from 25 healthy subjects using high-resolution RGB-Depth cameras and an event camera. Precise ground-truth hand landmark tracking is provided by a motion capture system, and all devices are spatially calibrated and synchronized for cross-modal alignment. The dataset includes over 1,100 recordings organized based on execution speed to embed an action quality task within gesture understanding. Baseline experiments demonstrate the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment, positioning EHWGesture as a comprehensive benchmark for advancing multimodal clinical gesture understanding. <br /><br />Summary: <div>
arXiv:2509.07525v1 Announce Type: new 
Abstract: Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Few-Shot Spatial Control for Diffusion Models</title>
<link>https://arxiv.org/abs/2509.07530</link>
<guid>https://arxiv.org/abs/2509.07530</guid>
<content:encoded><![CDATA[
<div> Few-Shot Control, Pretrained Models, Image Generation, Spatial Conditioning, Fine-Grained Control<br />
<br />
Summary: Universal Few-Shot Control (UFC) improves fine-grained control in pretrained text-to-image diffusion models by adapting to novel spatial conditions. UFC constructs task-specific control features using a matching mechanism and parameter updates from a small set. With just 30 annotated examples, UFC achieves precise control in six novel spatial tasks. Even with only 0.1% of training data, UFC performs competitively with fully supervised methods. It is applicable to various diffusion backbones like UNet and DiT, demonstrating effectiveness across architectures. The code for UFC is available on GitHub for further exploration and implementation. <div>
arXiv:2509.07530v1 Announce Type: new 
Abstract: Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
<link>https://arxiv.org/abs/2509.07534</link>
<guid>https://arxiv.org/abs/2509.07534</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked Image Modeling, 3D medical imaging, Hounsfield Unit, Foreground Masking, Segmentation.

Summary:
Masked Image Modeling (MIM) has made significant advances in computer vision, but its application in 3D medical image processing has been limited due to random masking techniques. To address this, a new approach using Hounsfield Unit (HU) measurements for Foreground Masking was developed, focusing on anatomical structures and excluding non-tissue regions like air and fluid. Experiments on multiple medical imaging datasets showed improved segmentation quality and Dice scores, reflecting the effectiveness of domain-centric MIM. This innovative strategy presents a promising direction for representation learning in medical image segmentation.

<br /><br />Summary: <div>
arXiv:2509.07534v1 Announce Type: new 
Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%, BraTS:~78.55\%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at github.com/AISeedHub/SubFore/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextlessRAG: End-to-End Visual Document RAG by Speech Without Text</title>
<link>https://arxiv.org/abs/2509.07538</link>
<guid>https://arxiv.org/abs/2509.07538</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-based question answering, document images, TextlessRAG, layout-aware reranking, bilingual dataset

Summary: 
TextlessRAG is introduced as an innovative framework for speech-based question answering over document images, eliminating the need for ASR, TTS, and OCR by directly interpreting speech. The framework retrieves relevant visual knowledge and generates answers in a fully textless pipeline. A layout-aware reranking mechanism is integrated to enhance retrieval performance. Experiments show significant improvements in efficiency and accuracy. The release of a bilingual speech-document RAG dataset, including Chinese and English voice queries paired with document content, aims to advance research in the field. The dataset and the TextlessRAG pipeline are accessible on the GitHub repository for further exploration and development.<br /><br />Summary: <div>
arXiv:2509.07538v1 Announce Type: new 
Abstract: Document images encapsulate a wealth of knowledge, while the portability of spoken queries enables broader and flexible application scenarios. Yet, no prior work has explored knowledge base question answering over visual document images with queries provided directly in speech. We propose TextlessRAG, the first end-to-end framework for speech-based question answering over large-scale document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR, directly interpreting speech, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. To further boost performance, we integrate a layout-aware reranking mechanism to refine retrieval. Experiments demonstrate substantial improvements in both efficiency and accuracy. To advance research in this direction, we also release the first bilingual speech--document RAG dataset, featuring Chinese and English voice queries paired with multimodal document content. Both the dataset and our pipeline will be made available at repository:https://github.com/xiepeijinhit-hue/textlessrag
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</title>
<link>https://arxiv.org/abs/2509.07552</link>
<guid>https://arxiv.org/abs/2509.07552</guid>
<content:encoded><![CDATA[
<div> Framework, Gaussian full-head synthesis, single unposed image, 3D GANs, large-scale synthetic dataset

Summary:
Our proposed feed-forward framework allows for efficient Gaussian full-head synthesis from a single unposed image in a single forward pass. By utilizing a large-scale synthetic dataset created from trained 3D GANs, we are able to train our framework effectively. A coarse-to-fine Gaussian head generation pipeline enables high-fidelity reconstruction, with sparse points from the FLAME model interacting with image features for feature extraction and shape reconstruction. A dual-branch framework integrates structured spherical triplane and unstructured point-based features for improved reconstruction. The results of our experiments demonstrate the effectiveness of our framework compared to existing methods.<br /><br />Summary: <div>
arXiv:2509.07552v1 Announce Type: new 
Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks</title>
<link>https://arxiv.org/abs/2509.07581</link>
<guid>https://arxiv.org/abs/2509.07581</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, 3D shape recognition, Graph Attention Network, Attention mechanism, Transparency<br />
Summary:<br />
- This paper introduces the Class Node Graph Attention Network (CGAT) architecture for transparent 3D shape recognition tasks, addressing the black-box nature of deep learning models.
- CGAT utilizes graph attention convolutions and an attention mechanism to provide explanations of its decision-making process, enhancing trust and accountability.
- By analyzing local mean curvature and distance to centroid node features, as well as model depth, optimal settings for the CGAT model were proposed.
- Models incorporating directed edges to a global CLS node produced more intuitive attention maps and desirable classification performance.
- The combination of local mean curvature and distance to centroid as node features yielded increased performance with a weighted F1 score of 0.76, along with more comprehensive attention visualizations. <div>
arXiv:2509.07581v1 Announce Type: new 
Abstract: Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Image Forensics: A Review and Critical Evaluation</title>
<link>https://arxiv.org/abs/2509.07591</link>
<guid>https://arxiv.org/abs/2509.07591</guid>
<content:encoded><![CDATA[
<div> Age traces, temporal image forensics, in-field sensor defects, sensor dust, eXplainable Artificial Intelligence<br />
Summary:<br />Temporal image forensics focuses on estimating the age of digital images using time-dependent traces from the image acquisition pipeline. This review provides insights into known age traces such as in-field sensor defects and sensor dust, along with various techniques used in temporal image forensics. The review also highlights the issue of content bias and emphasizes the importance of eXplainable Artificial Intelligence in validating forensic techniques. Additionally, a new forensic setting is proposed, the properties of in-field sensor defects are confirmed, and it is demonstrated that some methods may rely on content bias rather than age traces. The features learned by a neural network for dating palmprint images are analyzed, revealing potential distractions in learning age traces. The study includes a thorough review, re-implementation of previous work, and experimental verification of findings. <div>
arXiv:2509.07591v1 Announce Type: new 
Abstract: Temporal image forensics is the science of estimating the age of a digital image. Usually, time-dependent traces (age traces) introduced by the image acquisition pipeline are exploited for this purpose. In this review, a comprehensive overview of the field of temporal image forensics based on time-dependent traces from the image acquisition pipeline is given. This includes a detailed insight into the properties of known age traces (i.e., in-field sensor defects and sensor dust) and temporal image forensics techniques. Another key aspect of this work is to highlight the problem of content bias and to illustrate how important eXplainable Artificial Intelligence methods are to verify the reliability of temporal image forensics techniques. Apart from reviewing material presented in previous works, in this review: (i) a new (probably more realistic) forensic setting is proposed; (ii) the main properties (growth rate and spatial distribution) of in-field sensor defects are verified; (iii) it is shown that a method proposed to utilize in-field sensor defects for image age approximation actually exploits other traces (most likely content bias); (iv) the features learned by a neural network dating palmprint images are further investigated; (v) it is shown how easily a neural network can be distracted from learning age traces. For this purpose, previous work is analyzed, re-implemented if required and experiments are conducted.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</title>
<link>https://arxiv.org/abs/2509.07596</link>
<guid>https://arxiv.org/abs/2509.07596</guid>
<content:encoded><![CDATA[
<div> gender bias, vision-language foundation models, benchmarks, evaluation, spurious features

Summary: Gender bias in vision-language foundation models (VLMs) is a concern, often evaluated using benchmarks with gender annotations. However, spurious correlations between gender and non-gender features in these benchmarks can skew bias evaluation. To address this issue, the impact of perturbing non-gender features on bias evaluation was systematically studied across four benchmarks and various VLMs. Minimal perturbations, such as masking objects or blurring backgrounds, were found to significantly alter bias scores, indicating that current bias evaluations may be influenced by spurious features rather than true gender bias. Creating spurious feature-free benchmarks is difficult, so it is recommended to report bias metrics alongside feature-sensitivity measurements for a more reliable bias assessment. <div>
arXiv:2509.07596v1 Announce Type: new 
Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about their safe deployment and is typically evaluated using benchmarks with gender annotations on real-world images. However, as these benchmarks often contain spurious correlations between gender and non-gender features, such as objects and backgrounds, we identify a critical oversight in gender bias evaluation: Do spurious features distort gender bias evaluation? To address this question, we systematically perturb non-gender features across four widely used benchmarks (COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact on bias evaluation. Our findings reveal that even minimal perturbations, such as masking just 10% of objects or weakly blurring backgrounds, can dramatically alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in CLIP variants. This suggests that current bias evaluations often reflect model responses to spurious features rather than gender bias, undermining their reliability. Since creating spurious feature-free benchmarks is fundamentally challenging, we recommend reporting bias metrics alongside feature-sensitivity measurements to enable a more reliable bias assessment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2509.07613</link>
<guid>https://arxiv.org/abs/2509.07613</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical vision-language models, 3D imaging, Alzheimer's disease, fine-tuning pipeline, cognitive function<br />
<br />
Summary: 
This article introduces a data-efficient fine-tuning pipeline for 3D medical imaging models to improve Alzheimer's disease (AD) diagnosis. The proposed system addresses limitations in current models by leveraging patient metadata and integrating clinical diagnostic knowledge. By converting structured metadata into synthetic reports and incorporating an auxiliary token to predict cognitive function scores, the model enhances image-text alignment and provides additional supervision for fine-tuning. Through lightweight prompt tuning on both image and text modalities, the approach achieves state-of-the-art performance on AD datasets with only 1,500 training images, surpassing existing methods that require 10,000 images. This innovative methodology demonstrates the potential for more effective and efficient diagnosis of AD using 3D imaging modalities. Code for the system will be made available upon publication. <br /><br />Summary: <div>
arXiv:2509.07613v1 Announce Type: new 
Abstract: Medical vision-language models (Med-VLMs) have shown impressive results in tasks such as report generation and visual question answering, but they still face several limitations. Most notably, they underutilize patient metadata and lack integration of clinical diagnostic knowledge. Moreover, most existing models are typically trained from scratch or fine-tuned on large-scale 2D image-text pairs, requiring extensive computational resources, and their effectiveness on 3D medical imaging is often limited due to the absence of structural information. To address these gaps, we propose a data-efficient fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate its application in Alzheimer's disease (AD) diagnosis. Our system introduces two key innovations. First, we convert structured metadata into synthetic reports, enriching textual input for improved image-text alignment. Second, we add an auxiliary token trained to predict the mini-mental state examination (MMSE) score, a widely used clinical measure of cognitive function that correlates with AD severity. This provides additional supervision for fine-tuning. Applying lightweight prompt tuning to both image and text modalities, our approach achieves state-of-the-art performance on two AD datasets using 1,500 training images, outperforming existing methods fine-tuned on 10,000 images. Code will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</title>
<link>https://arxiv.org/abs/2509.07623</link>
<guid>https://arxiv.org/abs/2509.07623</guid>
<content:encoded><![CDATA[
<div> framework, self-supervised, deep learning, neurodegenerative diseases, MRI data  
Summary:  
- The article introduces a self-supervised cross-encoder framework for diagnosing neurodegenerative diseases from MRI data.  
- It leverages the temporal continuity in longitudinal MRI scans for supervision, disentangling learned representations into static and dynamic components.
- The static representation captures stable anatomical features through contrastive learning. 
- The dynamic representation reflects temporal changes and can be fine-tuned for downstream classification tasks with input-gradient regularization.
- Experimental results on the ADNI dataset show superior classification accuracy and interpretability, with strong zero-shot generalization on the OASIS dataset and cross-task generalization on the PPMI dataset.  
<br /><br />Summary: <div>
arXiv:2509.07623v1 Announce Type: new 
Abstract: Deep learning has shown significant potential in diagnosing neurodegenerative diseases from MRI data. However, most existing methods rely heavily on large volumes of labeled data and often yield representations that lack interpretability. To address both challenges, we propose a novel self-supervised cross-encoder framework that leverages the temporal continuity in longitudinal MRI scans for supervision. This framework disentangles learned representations into two components: a static representation, constrained by contrastive learning, which captures stable anatomical features; and a dynamic representation, guided by input-gradient regularization, which reflects temporal changes and can be effectively fine-tuned for downstream classification tasks. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves superior classification accuracy and improved interpretability. Furthermore, the learned representations exhibit strong zero-shot generalization on the Open Access Series of Imaging Studies (OASIS) dataset and cross-task generalization on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the proposed method will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</title>
<link>https://arxiv.org/abs/2509.07647</link>
<guid>https://arxiv.org/abs/2509.07647</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic watermarking, Latent diffusion models, Hermitian Symmetric Fourier Watermarking, Center-aware embedding, Robustness.

Summary: 
Hermitian Symmetric Fourier Watermarking (SFW) is proposed as a novel embedding method for semantic watermarking techniques in latent diffusion models (LDMs). This method enforces Hermitian symmetry to maintain frequency integrity and introduces a center-aware embedding strategy to enhance robustness against cropping attacks. By applying these techniques to existing watermarking schemes, the frequency-domain structures are improved, leading to better robustness and retrieval accuracy. Experimental results demonstrate that SFW achieves state-of-the-art verification and identification performance across various attack scenarios. Ablation studies confirm the positive impact of SFW on detection capabilities, the effectiveness of center-aware embedding against cropping attacks, and the influence of message capacity on identification accuracy. SFW achieves the highest detection accuracy while maintaining superior image fidelity, as validated by FID and CLIP scores. Overall, SFW provides an effective framework for balancing robustness and image fidelity in semantic watermarking. 

<br /><br />Summary: <div>
arXiv:2509.07647v1 Announce Type: new 
Abstract: Semantic watermarking techniques for latent diffusion models (LDMs) are robust against regeneration attacks, but often suffer from detection performance degradation due to the loss of frequency integrity. To tackle this problem, we propose a novel embedding method called Hermitian Symmetric Fourier Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian symmetry. Additionally, we introduce a center-aware embedding strategy that reduces the vulnerability of semantic watermarking due to cropping attacks by ensuring robust information retention. To validate our approach, we apply these techniques to existing semantic watermarking schemes, enhancing their frequency-domain structures for better robustness and retrieval accuracy. Extensive experiments demonstrate that our methods achieve state-of-the-art verification and identification performance, surpassing previous approaches across various attack scenarios. Ablation studies confirm the impact of SFW on detection capabilities, the effectiveness of the center-aware embedding against cropping, and how message capacity influences identification accuracy. Notably, our method achieves the highest detection accuracy while maintaining superior image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed SFW is shown to be an effective framework for balancing robustness and image fidelity, addressing the inherent trade-offs in semantic watermarking. Code available at https://github.com/thomas11809/SFWMark
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection</title>
<link>https://arxiv.org/abs/2509.07654</link>
<guid>https://arxiv.org/abs/2509.07654</guid>
<content:encoded><![CDATA[
arXiv:2509.07654v1 Announce Type: new 
Abstract: Small moving target detection is crucial for many defense applications but remains highly challenging due to low signal-to-noise ratios, ambiguous visual cues, and cluttered backgrounds. In this work, we propose a novel deep learning framework that differs fundamentally from existing approaches, which often rely on target-specific features or motion cues and tend to lack robustness in complex environments. Our key insight is that small target detection and background discrimination are inherently coupled, even cluttered video backgrounds often exhibit strong low-rank structures that can serve as stable priors for detection. We reformulate the task as a tensor-based low-rank and sparse decomposition problem and conduct a theoretical analysis of the background, target, and noise components to guide model design. Building on these insights, we introduce TenRPCANet, a deep neural network that requires minimal assumptions about target characteristics. Specifically, we propose a tokenization strategy that implicitly enforces multi-order tensor low-rank priors through a self-attention mechanism. This mechanism captures both local and non-local self-similarity to model the low-rank background without relying on explicit iterative optimization. In addition, inspired by the sparse component update in tensor RPCA, we design a feature refinement module to enhance target saliency. The proposed method achieves state-of-the-art performance on two highly distinct and challenging tasks: multi-frame infrared small target detection and space object detection. These results demonstrate both the effectiveness and the generalizability of our approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration</title>
<link>https://arxiv.org/abs/2509.07662</link>
<guid>https://arxiv.org/abs/2509.07662</guid>
<content:encoded><![CDATA[
arXiv:2509.07662v1 Announce Type: new 
Abstract: Previous deep image registration methods that employ single homography, multi-grid homography, or thin-plate spline often struggle with real scenes containing depth disparities due to their inherent limitations. To address this, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet), which employs free-form deformation with an exponential-decay basis function. This design achieves higher efficiency and performs well in scenes with depth disparities, benefiting from its inherent locality. We also introduce an Adaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion aggregator used in previous methods. By transforming dense interactions into sparse ones, ASMA reduces parameters and improves accuracy. Additionally, we propose a progressive correlation refinement strategy that leverages global-local correlation patterns for coarse-to-fine motion estimation, further enhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet reduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%, respectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art method. With an additional local refinement stage,EDFFDNet-2 further improves PSNR by 1.06 dB while maintaining lower computational costs. Our method also demonstrates strong generalization ability across datasets, outperforming previous deep learning methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor Projection Removal Adversarial Training</title>
<link>https://arxiv.org/abs/2509.07673</link>
<guid>https://arxiv.org/abs/2509.07673</guid>
<content:encoded><![CDATA[
arXiv:2509.07673v1 Announce Type: new 
Abstract: Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
<link>https://arxiv.org/abs/2509.07680</link>
<guid>https://arxiv.org/abs/2509.07680</guid>
<content:encoded><![CDATA[
arXiv:2509.07680v1 Announce Type: new 
Abstract: Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</title>
<link>https://arxiv.org/abs/2509.07704</link>
<guid>https://arxiv.org/abs/2509.07704</guid>
<content:encoded><![CDATA[
arXiv:2509.07704v1 Announce Type: new 
Abstract: Recently, learned image compression has attracted considerable attention due to its superior performance over traditional methods. However, most existing approaches employ a single entropy model to estimate the probability distribution of pixel values across the entire image, which limits their ability to capture the diverse statistical characteristics of different semantic regions. To overcome this limitation, we propose Segmentation-Assisted Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework utilizes semantic segmentation to guide the selection and adaptation of multiple entropy models, enabling more accurate probability distribution estimation for distinct semantic regions. Specifically, SEEC first extracts image features and then applies semantic segmentation to identify different regions, each assigned a specialized entropy model to better capture its unique statistical properties. Finally, a multi-channel discrete logistic mixture likelihood is employed to model the pixel value distributions effectively. Experimental results on benchmark datasets demonstrate that SEEC achieves state-of-the-art compression ratios while introducing only minimal encoding and decoding latency. With superior performance, the proposed model also supports Regions of Interest (ROIs) coding condition on the provided segmentation mask. Our code is available at https://github.com/chunbaobao/SEEC.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XSRD-Net: EXplainable Stroke Relapse Detection</title>
<link>https://arxiv.org/abs/2509.07772</link>
<guid>https://arxiv.org/abs/2509.07772</guid>
<content:encoded><![CDATA[
arXiv:2509.07772v1 Announce Type: new 
Abstract: Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.07774</link>
<guid>https://arxiv.org/abs/2509.07774</guid>
<content:encoded><![CDATA[
arXiv:2509.07774v1 Announce Type: new 
Abstract: Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.
  While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</title>
<link>https://arxiv.org/abs/2509.07782</link>
<guid>https://arxiv.org/abs/2509.07782</guid>
<content:encoded><![CDATA[
arXiv:2509.07782v1 Announce Type: new 
Abstract: RayGauss has achieved state-of-the-art rendering quality for novel-view synthesis on synthetic and indoor scenes by representing radiance and density fields with irregularly distributed elliptical basis functions, rendered via volume ray casting using a Bounding Volume Hierarchy (BVH). However, its computational cost prevents real-time rendering on real-world scenes. Our approach, RayGaussX, builds on RayGauss by introducing key contributions that accelerate both training and inference. Specifically, we incorporate volumetric rendering acceleration strategies such as empty-space skipping and adaptive sampling, enhance ray coherence, and introduce scale regularization to reduce false-positive intersections. Additionally, we propose a new densification criterion that improves density distribution in distant regions, leading to enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x to 12x faster training and 50x to 80x higher rendering speeds (FPS) on real-world datasets while improving visual quality by up to +0.56 dB in PSNR. Project page with videos and code: https://raygaussx.github.io/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss</title>
<link>https://arxiv.org/abs/2509.07798</link>
<guid>https://arxiv.org/abs/2509.07798</guid>
<content:encoded><![CDATA[
arXiv:2509.07798v1 Announce Type: new 
Abstract: Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose a novel approach for fusing two orthogonal anisotropic LR MR images to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinate-based loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patient-agnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at https://github.com/MajaSchle/tripleSR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.07809</link>
<guid>https://arxiv.org/abs/2509.07809</guid>
<content:encoded><![CDATA[
arXiv:2509.07809v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</title>
<link>https://arxiv.org/abs/2509.07825</link>
<guid>https://arxiv.org/abs/2509.07825</guid>
<content:encoded><![CDATA[
arXiv:2509.07825v1 Announce Type: new 
Abstract: 3D object segmentation with Large Language Models (LLMs) has become a prevailing paradigm due to its broad semantics, task flexibility, and strong generalization. However, this paradigm is hindered by representation misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds convey only dense geometric structures. In prior methods, misalignment limits both input and output. At the input stage, dense point patches require heavy pre-alignment, weakening object-level semantics and confusing similar distractors. At the output stage, predictions depend only on dense features without explicit geometric cues, leading to a loss of fine-grained accuracy. To address these limitations, we present the Point Linguist Model (PLM), a general framework that bridges the representation gap between LLMs and dense 3D point clouds without requiring large-scale pre-alignment between 3D-text or 3D-images. Specifically, we introduce Object-centric Discriminative Representation (OcDR), which learns object-centric tokens that capture target semantics and scene relations under a hard negative-aware training objective. This mitigates the misalignment between LLM tokens and 3D points, enhances resilience to distractors, and facilitates semantic-level reasoning within LLMs. For accurate segmentation, we introduce the Geometric Reactivation Decoder (GRD), which predicts masks by combining OcDR tokens carrying LLM-inferred geometry with corresponding dense features, preserving comprehensive dense features throughout the pipeline. Extensive experiments show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and +6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness of comprehensive object-centric reasoning for robust 3D understanding.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets</title>
<link>https://arxiv.org/abs/2509.07852</link>
<guid>https://arxiv.org/abs/2509.07852</guid>
<content:encoded><![CDATA[
arXiv:2509.07852v1 Announce Type: new 
Abstract: Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics</title>
<link>https://arxiv.org/abs/2509.07864</link>
<guid>https://arxiv.org/abs/2509.07864</guid>
<content:encoded><![CDATA[
arXiv:2509.07864v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Results show our D-LEAF delivers a 53% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4%, substantially suppressing hallucinations while preserving efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.07879</link>
<guid>https://arxiv.org/abs/2509.07879</guid>
<content:encoded><![CDATA[
arXiv:2509.07879v1 Announce Type: new 
Abstract: Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object-level Correlation for Few-Shot Segmentation</title>
<link>https://arxiv.org/abs/2509.07917</link>
<guid>https://arxiv.org/abs/2509.07917</guid>
<content:encoded><![CDATA[
arXiv:2509.07917v1 Announce Type: new 
Abstract: Few-shot semantic segmentation (FSS) aims to segment objects of novel categories in the query images given only a few annotated support samples. Existing methods primarily build the image-level correlation between the support target object and the entire query image. However, this correlation contains the hard pixel noise, \textit{i.e.}, irrelevant background objects, that is intractable to trace and suppress, leading to the overfitting of the background. To address the limitation of this correlation, we imitate the biological vision process to identify novel objects in the object-level information. Target identification in the general objects is more valid than in the entire image, especially in the low-data regime. Inspired by this, we design an Object-level Correlation Network (OCNet) by establishing the object-level correlation between the support target object and query general objects, which is mainly composed of the General Object Mining Module (GOMM) and Correlation Construction Module (CCM). Specifically, GOMM constructs the query general object feature by learning saliency and high-level similarity cues, where the general objects include the irrelevant background objects and the target foreground object. Then, CCM establishes the object-level correlation by allocating the target prototypes to match the general object feature. The generated object-level correlation can mine the query target feature and suppress the hard pixel noise for the final prediction. Extensive experiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model achieves the state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion</title>
<link>https://arxiv.org/abs/2509.07920</link>
<guid>https://arxiv.org/abs/2509.07920</guid>
<content:encoded><![CDATA[
arXiv:2509.07920v1 Announce Type: new 
Abstract: Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title>
<link>https://arxiv.org/abs/2509.07923</link>
<guid>https://arxiv.org/abs/2509.07923</guid>
<content:encoded><![CDATA[
arXiv:2509.07923v1 Announce Type: new 
Abstract: Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\% for CBCT segmentation and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title>
<link>https://arxiv.org/abs/2509.07928</link>
<guid>https://arxiv.org/abs/2509.07928</guid>
<content:encoded><![CDATA[
arXiv:2509.07928v1 Announce Type: new 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object</title>
<link>https://arxiv.org/abs/2509.07932</link>
<guid>https://arxiv.org/abs/2509.07932</guid>
<content:encoded><![CDATA[
arXiv:2509.07932v1 Announce Type: new 
Abstract: Characterization of uncooperative Resident Space Objects (RSO) play a crucial role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to assess the geometry and motion properties. To address the challenges of reconstructing tumbling uncooperative targets, this study evaluates the performance of existing state-of-the-art 3D reconstruction algorithms for dynamic scenes, focusing on their ability to generate geometrically accurate models with high-fidelity. To support our evaluation, we developed a simulation environment using Isaac Sim to generate physics-accurate 2D image sequences of tumbling satellite under realistic orbital lighting conditions. Our preliminary results on static scenes using Neuralangelo demonstrate promising reconstruction quality. The generated 3D meshes closely match the original CAD models with minimal errors and artifacts when compared using Cloud Compare (CC). The reconstructed models were able to capture critical fine details for mission planning. This provides a baseline for our ongoing evaluation of dynamic scene reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Space Analysis by Guided Diffusion Model</title>
<link>https://arxiv.org/abs/2509.07936</link>
<guid>https://arxiv.org/abs/2509.07936</guid>
<content:encoded><![CDATA[
arXiv:2509.07936v1 Announce Type: new 
Abstract: One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</title>
<link>https://arxiv.org/abs/2509.07966</link>
<guid>https://arxiv.org/abs/2509.07966</guid>
<content:encoded><![CDATA[
arXiv:2509.07966v1 Announce Type: new 
Abstract: Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</title>
<link>https://arxiv.org/abs/2509.07969</link>
<guid>https://arxiv.org/abs/2509.07969</guid>
<content:encoded><![CDATA[
arXiv:2509.07969v1 Announce Type: new 
Abstract: Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</title>
<link>https://arxiv.org/abs/2509.07978</link>
<guid>https://arxiv.org/abs/2509.07978</guid>
<content:encoded><![CDATA[
arXiv:2509.07978v1 Announce Type: new 
Abstract: Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: https://gzwsama.github.io/OnePoseviaGen.github.io/
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Representation Alignment for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.07979</link>
<guid>https://arxiv.org/abs/2509.07979</guid>
<content:encoded><![CDATA[
arXiv:2509.07979v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation</title>
<link>https://arxiv.org/abs/2509.07039</link>
<guid>https://arxiv.org/abs/2509.07039</guid>
<content:encoded><![CDATA[
arXiv:2509.07039v1 Announce Type: cross 
Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring faces interpretability barriers that limit adoption in energy infrastructure applications. While deep learning achieves high accuracy in thermal fault detection, validation that model decisions align with thermal physics principles remains lacking, creating deployment hesitancy where understanding model reasoning is critical. This study provides a systematic comparison of convolutional neural networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI saliency analysis to assess alignment with thermal physics principles. This represents the first systematic comparison of CNNs and vision transformers for thermal PV fault detection with physics-validated interpretability. Evaluation on 20,000 infrared images spanning normal operation and 11 fault categories shows that Swin Transformer achieves the highest performance (94% binary accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis reveals that models learn physically meaningful features, such as localized hotspots for cell defects, linear thermal paths for diode failures, and thermal boundaries for vegetation shading, consistent with expected thermal signatures. However, performance varies significantly across fault types: electrical faults achieve strong detection (F1-scores >0.90) while environmental factors like soiling remain challenging (F1-scores 0.20-0.33), indicating limitations imposed by thermal imaging resolution. The thermal physics-guided interpretability approach provides methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVGauge: Towards Human-Aligned Evaluation for SVG Generation</title>
<link>https://arxiv.org/abs/2509.07127</link>
<guid>https://arxiv.org/abs/2509.07127</guid>
<content:encoded><![CDATA[
arXiv:2509.07127v1 Announce Type: cross 
Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria tuned to their symbolic and vectorial nature: criteria that existing metrics such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce SVGauge, the first human-aligned, reference based metric for text-to-SVG generation. SVGauge jointly measures (i) visual fidelity, obtained by extracting SigLIP image embeddings and refining them with PCA and whitening for domain alignment, and (ii) semantic consistency, captured by comparing BLIP-2-generated captions of the SVGs against the original prompts in the combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark shows that SVGauge attains the highest correlation with human judgments and reproduces system-level rankings of eight zero-shot LLM-based generators more faithfully than existing metrics. Our results highlight the necessity of vector-specific evaluation and provide a practical tool for benchmarking future text-to-SVG generation models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study</title>
<link>https://arxiv.org/abs/2509.07132</link>
<guid>https://arxiv.org/abs/2509.07132</guid>
<content:encoded><![CDATA[
arXiv:2509.07132v1 Announce Type: cross 
Abstract: The widespread use of generative AI has shown remarkable success in producing highly realistic deepfakes, posing a serious threat to various voice biometric applications, including speaker verification, voice biometrics, audio conferencing, and criminal investigations. To counteract this, several state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been proposed to identify generative AI signatures to distinguish between real and deepfake audio. However, the effectiveness of these methods is severely undermined by anti-forensic (AF) attacks that conceal generative signatures. These AF attacks span a wide range of techniques, including statistical modifications (e.g., pitch shifting, filtering, noise addition, and quantization) and optimization-based attacks (e.g., FGSM, PGD, C \& W, and DeepFool). In this paper, we investigate the SoTA ADD methods and provide a comparative analysis to highlight their effectiveness in exposing deepfake signatures, as well as their vulnerabilities under adversarial conditions. We conducted an extensive evaluation of ADD methods on five deepfake benchmark datasets using two categories: raw and spectrogram-based approaches. This comparative analysis enables a deeper understanding of the strengths and limitations of SoTA ADD methods against diverse AF attacks. It does not only highlight vulnerabilities of ADD methods, but also informs the design of more robust and generalized detectors for real-world voice biometrics. It will further guide future research in developing adaptive defense strategies that can effectively counter evolving AF techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Machine Learning Reconstruction Techniques for Accelerated Brain MRI Scans</title>
<link>https://arxiv.org/abs/2509.07193</link>
<guid>https://arxiv.org/abs/2509.07193</guid>
<content:encoded><![CDATA[
arXiv:2509.07193v1 Announce Type: cross 
Abstract: This retrospective-prospective study evaluated whether a deep learning-based MRI reconstruction algorithm can preserve diagnostic quality in brain MRI scans accelerated up to fourfold, using both public and prospective clinical data. The study included 18 healthy volunteers (scans acquired at 3T, January 2024-March 2025), as well as selected fastMRI public datasets with diverse pathologies. Phase-encoding-undersampled 2D/3D T1, T2, and FLAIR sequences were reconstructed with DeepFoqus-Accelerate and compared with standard-of-care (SOC). Three board-certified neuroradiologists and two MRI technologists independently reviewed 36 paired SOC/AI reconstructions from both datasets using a 5-point Likert scale, while quantitative similarity was assessed for 408 scans and 1224 datasets using Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Haar wavelet-based Perceptual Similarity Index (HaarPSI). No AI-reconstructed scan scored below 3 (minimally acceptable), and 95% scored $\geq 4$. Mean SSIM was 0.95 $\pm$ 0.03 (90% cases >0.90), PSNR >41.0 dB, and HaarPSI >0.94. Inter-rater agreement was slight to moderate. Rare artifacts did not affect diagnostic interpretation. These findings demonstrate that DeepFoqus-Accelerate enables robust fourfold brain MRI acceleration with 75% reduced scan time, while preserving diagnostic image quality and supporting improved workflow efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.07252</link>
<guid>https://arxiv.org/abs/2509.07252</guid>
<content:encoded><![CDATA[
arXiv:2509.07252v1 Announce Type: cross 
Abstract: In multi-task learning (MTL), gradient conflict poses a significant challenge. Effective methods for addressing this problem, including PCGrad, CAGrad, and GradNorm, in their original implementations are computationally demanding, which significantly limits their application in modern large models and transformers. We propose Gradient Conductor (GCond), a method that builds upon PCGrad principles by combining them with gradient accumulation and an adaptive arbitration mechanism. We evaluated GCond on self-supervised learning tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K dataset and a combined head and neck CT scan dataset, comparing the proposed method against baseline linear combinations and state-of-the-art gradient conflict resolution methods. The stochastic mode of GCond achieved a two-fold computational speedup while maintaining optimization quality, and demonstrated superior performance across all evaluated metrics, achieving lower L1 and SSIM losses compared to other methods on both datasets. GCond exhibited high scalability, being successfully applied to both compact models (MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base). It also showed compatibility with modern optimizers such as AdamW and Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the problem of gradient conflicts in multi-task learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2509.07289</link>
<guid>https://arxiv.org/abs/2509.07289</guid>
<content:encoded><![CDATA[
arXiv:2509.07289v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning by optimizing geometric objectives--such as invariance to augmentations, variance preservation, and feature decorrelation--without requiring labels. However, most existing methods operate in Euclidean space, limiting their ability to capture nonlinear dependencies and geometric structures. In this work, we propose Kernel VICReg, a novel self-supervised learning framework that lifts the VICReg objective into a Reproducing Kernel Hilbert Space (RKHS). By kernelizing each term of the loss-variance, invariance, and covariance--we obtain a general formulation that operates on double-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinear feature learning without explicit mappings.
  We demonstrate that Kernel VICReg not only avoids representational collapse but also improves performance on tasks with complex or small-scale data. Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, and ImageNet100 show consistent gains over Euclidean VICReg, with particularly strong improvements on datasets where nonlinear structures are prominent. UMAP visualizations further confirm that kernel-based embeddings exhibit better isometry and class separation. Our results suggest that kernelizing SSL objectives is a promising direction for bridging classical kernel methods with modern representation learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis</title>
<link>https://arxiv.org/abs/2509.07388</link>
<guid>https://arxiv.org/abs/2509.07388</guid>
<content:encoded><![CDATA[
arXiv:2509.07388v1 Announce Type: cross 
Abstract: Cardiac arrest is one of the biggest global health problems, and early identification and management are key to enhancing the patient's prognosis. In this paper, we propose a novel framework that combines an EfficientNet-based deep learning model with a digital twin system to improve the early detection and analysis of cardiac arrest. We use compound scaling and EfficientNet to learn the features of cardiovascular images. In parallel, the digital twin creates a realistic and individualized cardiovascular system model of the patient based on data received from the Internet of Things (IoT) devices attached to the patient, which can help in the constant assessment of the patient and the impact of possible treatment plans. As shown by our experiments, the proposed system is highly accurate in its prediction abilities and, at the same time, efficient. Combining highly advanced techniques such as deep learning and digital twin (DT) technology presents the possibility of using an active and individual approach to predicting cardiac disease.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A smart fridge with AI-enabled food computing</title>
<link>https://arxiv.org/abs/2509.07400</link>
<guid>https://arxiv.org/abs/2509.07400</guid>
<content:encoded><![CDATA[
arXiv:2509.07400v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) plays a crucial role in enabling seamless connectivity and intelligent home automation, particularly in food management. By integrating IoT with computer vision, the smart fridge employs an ESP32-CAM to establish a monitoring subsystem that enhances food management efficiency through real-time food detection, inventory tracking, and temperature monitoring. This benefits waste reduction, grocery planning improvement, and household consumption optimization. In high-density inventory conditions, capturing partial or layered images complicates object detection, as overlapping items and occluded views hinder accurate identification and counting. Besides, varied angles and obscured details in multi-layered setups reduce algorithm reliability, often resulting in miscounts or misclassifications. Our proposed system is structured into three core modules: data pre-processing, object detection and management, and a web-based visualization. To address the challenge of poor model calibration caused by overconfident predictions, we implement a variant of focal loss that mitigates over-confidence and under-confidence in multi-category classification. This approach incorporates adaptive, class-wise error calibration via temperature scaling and evaluates the distribution of predicted probabilities across methods. Our results demonstrate that robust functional calibration significantly improves detection reliability under varying lighting conditions and scalability challenges. Further analysis demonstrates a practical, user-focused approach to modern food management, advancing sustainable living goals through reduced waste and more informed consumption.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis</title>
<link>https://arxiv.org/abs/2509.07463</link>
<guid>https://arxiv.org/abs/2509.07463</guid>
<content:encoded><![CDATA[
arXiv:2509.07463v1 Announce Type: cross 
Abstract: Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials</title>
<link>https://arxiv.org/abs/2509.07522</link>
<guid>https://arxiv.org/abs/2509.07522</guid>
<content:encoded><![CDATA[
arXiv:2509.07522v1 Announce Type: cross 
Abstract: Modeling of high-frequency outgoing radiance distributions has long been a key challenge in rendering, particularly for glossy material. Such distributions concentrate radiative energy within a narrow lobe and are highly sensitive to changes in view direction. However, existing neural radiosity methods, which primarily rely on positional feature encoding, exhibit notable limitations in capturing these high-frequency, strongly view-dependent radiance distributions. To address this, we propose a highly-efficient approach by reflectance-aware ray cone encoding based on the neural radiosity framework, named neural cone radiosity. The core idea is to employ a pre-filtered multi-resolution hash grid to accurately approximate the glossy BSDF lobe, embedding view-dependent reflectance characteristics directly into the encoding process through continuous spatial aggregation. Our design not only significantly improves the network's ability to model high-frequency reflection distributions but also effectively handles surfaces with a wide range of glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our method reduces the network's burden in fitting complex radiance distributions, allowing the overall architecture to remain compact and efficient. Comprehensive experimental results demonstrate that our method consistently produces high-quality, noise-free renderings in real time under various glossiness conditions, and delivers superior fidelity and realism compared to baseline approaches.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?</title>
<link>https://arxiv.org/abs/2509.07593</link>
<guid>https://arxiv.org/abs/2509.07593</guid>
<content:encoded><![CDATA[
arXiv:2509.07593v1 Announce Type: cross 
Abstract: End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Ice Crystal Habit Diversity with Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.07688</link>
<guid>https://arxiv.org/abs/2509.07688</guid>
<content:encoded><![CDATA[
arXiv:2509.07688v1 Announce Type: cross 
Abstract: Ice-containing clouds strongly impact climate, but they are hard to model due to ice crystal habit (i.e., shape) diversity. We use self-supervised learning (SSL) to learn latent representations of crystals from ice crystal imagery. By pre-training a vision transformer with many cloud particle images, we learn robust representations of crystal morphology, which can be used for various science-driven tasks. Our key contributions include (1) validating that our SSL approach can be used to learn meaningful representations, and (2) presenting a relevant application where we quantify ice crystal diversity with these latent representations. Our results demonstrate the power of SSL-driven representations to improve the characterization of ice crystals and subsequently constrain their role in Earth's climate system.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review</title>
<link>https://arxiv.org/abs/2509.07742</link>
<guid>https://arxiv.org/abs/2509.07742</guid>
<content:encoded><![CDATA[
arXiv:2509.07742v1 Announce Type: cross 
Abstract: In modern online learning, understanding and predicting student behavior is crucial for enhancing engagement and optimizing educational outcomes. This systematic review explores the integration of biosensors and Multimodal Learning Analytics (MmLA) to analyze and predict student behavior during computer-based learning sessions. We examine key challenges, including emotion and attention detection, behavioral analysis, experimental design, and demographic considerations in data collection. Our study highlights the growing role of physiological signals, such as heart rate, brain activity, and eye-tracking, combined with traditional interaction data and self-reports to gain deeper insights into cognitive states and engagement levels. We synthesize findings from 54 key studies, analyzing commonly used methodologies such as advanced machine learning algorithms and multimodal data pre-processing techniques. The review identifies current research trends, limitations, and emerging directions in the field, emphasizing the transformative potential of biosensor-driven adaptive learning systems. Our findings suggest that integrating multimodal data can facilitate personalized learning experiences, real-time feedback, and intelligent educational interventions, ultimately advancing toward a more customized and adaptive online learning experience.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2509.07756</link>
<guid>https://arxiv.org/abs/2509.07756</guid>
<content:encoded><![CDATA[
arXiv:2509.07756v1 Announce Type: cross 
Abstract: Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images</title>
<link>https://arxiv.org/abs/2509.07795</link>
<guid>https://arxiv.org/abs/2509.07795</guid>
<content:encoded><![CDATA[
arXiv:2509.07795v1 Announce Type: cross 
Abstract: Optical Coherence Tomography (OCT) is essential for diagnosing conditions such as glaucoma, diabetic retinopathy, and age-related macular degeneration. Accurate retinal layer segmentation enables quantitative biomarkers critical for clinical decision-making, but manual segmentation is time-consuming and variable, while conventional deep learning models often lack interpretability. This work proposes an improved SegNet-based deep learning framework for automated and interpretable retinal layer segmentation. Architectural innovations, including modified pooling strategies, enhance feature extraction from noisy OCT images, while a hybrid loss function combining categorical cross-entropy and Dice loss improves performance for thin and imbalanced retinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to provide visual explanations, allowing clinical validation of model decisions. Trained and validated on the Duke OCT dataset, the framework achieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a Jaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance across most layers, with challenges remaining for thinner boundaries. Grad-CAM visualizations highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers and improving transparency. By combining architectural improvements, a customized hybrid loss, and explainable AI, this study delivers a high-performing SegNet-based framework that bridges the gap between accuracy and interpretability. The approach offers strong potential for standardizing OCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in AI-driven ophthalmic tools.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Challenging Benchmark of Anime Style Recognition</title>
<link>https://arxiv.org/abs/2204.14034</link>
<guid>https://arxiv.org/abs/2204.14034</guid>
<content:encoded><![CDATA[
arXiv:2204.14034v2 Announce Type: replace 
Abstract: Given two images of different anime roles, anime style recognition (ASR) aims to learn abstract painting style to determine whether the two images are from the same work, which is an interesting but challenging problem. Unlike biometric recognition, such as face recognition, iris recognition, and person re-identification, ASR suffers from a much larger semantic gap but receives less attention. In this paper, we propose a challenging ASR benchmark. Firstly, we collect a large-scale ASR dataset (LSASRD), which contains 20,937 images of 190 anime works and each work at least has ten different roles. In addition to the large-scale, LSASRD contains a list of challenging factors, such as complex illuminations, various poses, theatrical colors and exaggerated compositions. Secondly, we design a cross-role protocol to evaluate ASR performance, in which query and gallery images must come from different roles to validate an ASR model is to learn abstract painting style rather than learn discriminative features of roles. Finally, we apply two powerful person re-identification methods, namely, AGW and TransReID, to construct the baseline performance on LSASRD. Surprisingly, the recent transformer model (i.e., TransReID) only acquires a 42.24% mAP on LSASRD. Therefore, we believe that the ASR task of a huge semantic gap deserves deep and long-term research. We will open our dataset and code at https://github.com/nkjcqvcpi/ASR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE Distillation and Diffusion Probabilistic Feedback</title>
<link>https://arxiv.org/abs/2402.02346</link>
<guid>https://arxiv.org/abs/2402.02346</guid>
<content:encoded><![CDATA[
arXiv:2402.02346v2 Announce Type: replace 
Abstract: Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance</title>
<link>https://arxiv.org/abs/2404.00992</link>
<guid>https://arxiv.org/abs/2404.00992</guid>
<content:encoded><![CDATA[
arXiv:2404.00992v3 Announce Type: replace 
Abstract: Neural Radiance Field (NeRF) technology has made significant strides in creating novel viewpoints. However, its effectiveness is hampered when working with sparsely available views, often leading to performance dips due to overfitting. FreeNeRF attempts to overcome this limitation by integrating implicit geometry regularization, which incrementally improves both geometry and textures. Nonetheless, an initial low positional encoding bandwidth results in the exclusion of high-frequency elements. The quest for a holistic approach that simultaneously addresses overfitting and the preservation of high-frequency details remains ongoing. This study presents a novel feature-matching-based sparse geometry regularization module, enhanced by a spatially consistent geometry filtering mechanism and a frequency-guided geometric regularization strategy. This module excels at accurately identifying high-frequency keypoints, effectively preserving fine structural details. Through progressive refinement of geometry and textures across NeRF iterations, we unveil an effective few-shot neural rendering architecture, designated as SGCNeRF, for enhanced novel view synthesis. Our experiments demonstrate that SGCNeRF not only achieves superior geometry-consistent outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB in PSNR on LLFF and DTU.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TractGraphFormer: Anatomically Informed Hybrid Graph CNN-Transformer Network for Interpretable Sex and Age Prediction from Diffusion MRI Tractography</title>
<link>https://arxiv.org/abs/2407.08883</link>
<guid>https://arxiv.org/abs/2407.08883</guid>
<content:encoded><![CDATA[
arXiv:2407.08883v2 Announce Type: replace 
Abstract: The relationship between brain connections and non-imaging phenotypes is increasingly studied using deep neural networks. However, the local and global properties of brain white matter networks are often overlooked in convolutional network design. We introduce TractGraphFormer, a hybrid Graph CNN-Transformer deep learning framework tailored for diffusion MRI tractography. This model leverages local anatomical characteristics and global feature dependencies of white matter structures. The Graph CNN module captures white matter geometry and grey matter connectivity to aggregate local features from anatomically similar white matter connections, while the Transformer module uses self-attention to enhance global information learning. Additionally, TractGraphFormer includes an attention module for interpreting predictive white matter connections. We apply TractGraphFormer to tasks of sex and age prediction. TractGraphFormer shows strong performance in large datasets of children (n=9345) and young adults (n=1065). Overall, our approach suggests that widespread connections in the WM are predictive of the sex and age of an individual. For each prediction task, consistent predictive anatomical tracts are identified across the two datasets. The proposed approach highlights the potential of integrating local anatomical information and global feature dependencies to improve prediction performance in machine learning with diffusion MRI tractography.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning</title>
<link>https://arxiv.org/abs/2408.11505</link>
<guid>https://arxiv.org/abs/2408.11505</guid>
<content:encoded><![CDATA[
arXiv:2408.11505v3 Announce Type: replace 
Abstract: Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLM's text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multiple scales, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to derive the WSI-level features. Extensive experiments, visualizations, and interpretability analyses were conducted on five datasets and three downstream tasks using three VLMs, demonstrating the strong performance of our MSCPT. All codes have been made publicly accessible at https://github.com/Hanminghao/MSCPT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InteractPro: A Unified Framework for Motion-Aware Image Composition</title>
<link>https://arxiv.org/abs/2409.10090</link>
<guid>https://arxiv.org/abs/2409.10090</guid>
<content:encoded><![CDATA[
arXiv:2409.10090v3 Announce Type: replace 
Abstract: We introduce InteractPro, a comprehensive framework for dynamic motion-aware image composition. At its core is InteractPlan, an intelligent planner that leverages a Large Vision Language Model (LVLM) for scenario analysis and object placement, determining the optimal composition strategy to achieve realistic motion effects. Based on each scenario, InteractPlan selects between our two specialized modules: InteractPhys and InteractMotion. InteractPhys employs an enhanced Material Point Method (MPM)-based simulation to produce physically faithful and controllable object-scene interactions, capturing diverse and abstract events that require true physical modeling. InteractMotion, in contrast, is a training-free method based on pretrained video diffusion. Traditional composition approaches suffer from two major limitations: requiring manual planning for object placement and generating static, motionless outputs. By unifying simulation-based and diffusion-based methods under planner guidance, InteractPro overcomes these challenges, ensuring richly motion-aware compositions. Extensive quantitative and qualitative evaluations demonstrate InteractPro's effectiveness in producing controllable, and coherent compositions across varied scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PnP-Flow: Plug-and-Play Image Restoration with Flow Matching</title>
<link>https://arxiv.org/abs/2410.02423</link>
<guid>https://arxiv.org/abs/2410.02423</guid>
<content:encoded><![CDATA[
arXiv:2410.02423v3 Announce Type: replace 
Abstract: In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the data-fidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self Supervised Networks for Learning Latent Space Representations of Human Body Scans and Motions</title>
<link>https://arxiv.org/abs/2411.03475</link>
<guid>https://arxiv.org/abs/2411.03475</guid>
<content:encoded><![CDATA[
arXiv:2411.03475v2 Announce Type: replace 
Abstract: This paper introduces self-supervised neural network models to tackle several fundamental problems in the field of 3D human body analysis and processing. First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel architecture for the retrieval of latent space representations of body shapes and poses. This network offers a fast and robust method to estimate the embedding of arbitrary unregistered meshes into the latent space. Second, we complement the estimation of latent codes with MoGeN (Motion Geometry Network) a framework that learns the geometry on the latent space itself. This is achieved by lifting the body pose parameter space into a higher dimensional Euclidean space in which body motion mini-sequences from a training set of 4D data can be approximated by simple linear interpolation. Using the SMPL latent space representation we illustrate how the combination of these network models, once trained, can be used to perform a variety of tasks with very limited computational cost. This includes operations such as motion interpolation, extrapolation and transfer as well as random shape and pose generation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OOD-SEG: Exploiting out-of-distribution detection techniques for learning image segmentation from sparse multi-class positive-only annotations</title>
<link>https://arxiv.org/abs/2411.09553</link>
<guid>https://arxiv.org/abs/2411.09553</guid>
<content:encoded><![CDATA[
arXiv:2411.09553v3 Announce Type: replace 
Abstract: Despite significant advancements, segmentation based on deep neural networks in medical and surgical imaging faces several challenges, two of which we aim to address in this work. First, acquiring complete pixel-level segmentation labels for medical images is time-consuming and requires domain expertise. Second, typical segmentation pipelines cannot detect out-of-distribution (OOD) pixels, leaving them prone to spurious outputs during deployment. In this work, we propose a novel segmentation approach which broadly falls within the positive-unlabelled (PU) learning paradigm and exploits tools from OOD detection techniques. Our framework learns only from sparsely annotated pixels from multiple positive-only classes and does not use any annotation for the background class. These multi-class positive annotations naturally fall within the in-distribution (ID) set. Unlabelled pixels may contain positive classes but also negative ones, including what is typically referred to as \emph{background} in standard segmentation formulations. Here, we forgo the need for background annotation and consider these together with any other unseen classes as part of the OOD set. Our framework can integrate, at a pixel-level, any OOD detection approaches designed for classification tasks. To address the lack of existing OOD datasets and established evaluation metric for medical image segmentation, we propose a cross-validation strategy that treats held-out labelled classes as OOD. Extensive experiments on both multi-class hyperspectral and RGB surgical imaging datasets demonstrate the robustness and generalisation capability of our proposed framework.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Sparse Priors Guided Diffusion Compression Model for Point Clouds</title>
<link>https://arxiv.org/abs/2411.13860</link>
<guid>https://arxiv.org/abs/2411.13860</guid>
<content:encoded><![CDATA[
arXiv:2411.13860v2 Announce Type: replace 
Abstract: Lossy compression methods rely on an autoencoder to transform a point cloud into latent points for storage, leaving the inherent redundancy of latent representations unexplored. To reduce redundancy in latent points, we propose a sparse priors guided method that achieves high reconstruction quality, especially at high compression ratios. This is accomplished by a dual-density scheme separately processing the latent points (intended for reconstruction) and the decoupled sparse priors (intended for storage). Our approach features an efficient dual-density data flow that relaxes size constraints on latent points, and hybridizes a progressive conditional diffusion model to encapsulate essential details for reconstruction within the conditions, which are decoupled hierarchically to intra-point and inter-point priors. Specifically, our method encodes the original point cloud into latent points and decoupled sparse priors through separate encoders. Latent points serve as intermediates, while sparse priors act as adaptive conditions. We then employ a progressive attention-based conditional denoiser to generate latent points conditioned on the decoupled priors, allowing the denoiser to dynamically attend to geometric and semantic cues from the priors at each encoding and decoding layer. Additionally, we integrate the local distribution into the arithmetic encoder and decoder to enhance local context modeling of the sparse points. The original point cloud is reconstructed through a point decoder. Compared to state-of-the-art, our method obtains superior rate-distortion trade-off, evidenced by extensive evaluations on the ShapeNet dataset and standard test datasets from MPEG group including 8iVFB, and Owlii.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Museum Exhibits using Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2412.01370</link>
<guid>https://arxiv.org/abs/2412.01370</guid>
<content:encoded><![CDATA[
arXiv:2412.01370v2 Announce Type: replace 
Abstract: Museums serve as repositories of cultural heritage and historical artifacts from diverse epochs, civilizations, and regions, preserving well-documented collections that encapsulate vast knowledge, which, when systematically structured into large-scale datasets, can train specialized models. Visitors engage with exhibits through curiosity and questions, making expert domain-specific models essential for interactive query resolution and gaining historical insights. Understanding exhibits from images requires analyzing visual features and linking them to historical knowledge to derive meaningful correlations. We facilitate such reasoning by (a) collecting and curating a large-scale dataset of 65M images and 200M question-answer pairs for exhibits from all around the world; (b) training large vision-language models (VLMs) on the collected dataset; (c) benchmarking their ability on five visual question answering tasks, specifically designed to reflect real-world inquiries and challenges observed in museum settings. The complete dataset is labeled by museum experts, ensuring the quality and the practical significance of the labels. We train two VLMs from different categories: BLIP with vision-language aligned embeddings, but lacking the expressive power of large language models, and the LLaVA model, a powerful instruction-tuned LLM enriched with vision-language reasoning capabilities. Through extensive experiments, we find that while both model types effectively answer visually grounded questions, large vision-language models excel in queries requiring deeper historical context and reasoning. We further demonstrate the necessity of fine-tuning models on large-scale domain-specific datasets by showing that our fine-tuned models significantly outperform current SOTA VLMs in answering questions related to specific attributes, highlighting their limitations in handling complex, nuanced queries.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture- and Shape-based Adversarial Attacks for Overhead Image Vehicle Detection</title>
<link>https://arxiv.org/abs/2412.16358</link>
<guid>https://arxiv.org/abs/2412.16358</guid>
<content:encoded><![CDATA[
arXiv:2412.16358v2 Announce Type: replace 
Abstract: Detecting vehicles in aerial images is difficult due to complex backgrounds, small object sizes, shadows, and occlusions. Although recent deep learning advancements have improved object detection, these models remain susceptible to adversarial attacks (AAs), challenging their reliability. Traditional AA strategies often ignore practical implementation constraints. Our work proposes realistic and practical constraints on texture (lowering resolution, limiting modified areas, and color ranges) and analyzes the impact of shape modifications on attack performance. We conducted extensive experiments with three object detector architectures, demonstrating the performance-practicality trade-off: more practical modifications tend to be less effective, and vice versa. We release both code and data to support reproducibility at https://github.com/humansensinglab/texture-shape-adversarial-attacks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection</title>
<link>https://arxiv.org/abs/2412.16918</link>
<guid>https://arxiv.org/abs/2412.16918</guid>
<content:encoded><![CDATA[
arXiv:2412.16918v2 Announce Type: replace 
Abstract: When given two similar images, humans identify their differences by comparing the appearance (e.g., color, texture) with the help of semantics (e.g., objects, relations). However, mainstream binary change detection models adopt a supervised training paradigm, where the annotated binary change map is the main constraint. Thus, such methods primarily emphasize difference-aware features between bi-temporal images, and the semantic understanding of changed landscapes is undermined, resulting in limited accuracy in the face of noise and illumination variations. To this end, this paper explores incorporating semantic priors from visual foundation models to improve the ability to detect changes. Firstly, we propose a Semantic-Aware Change Detection network (SA-CDNet), which transfers the knowledge of visual foundation models (i.e., FastSAM) to change detection. Inspired by the human visual paradigm, a novel dual-stream feature decoder is derived to distinguish changes by combining semantic-aware features and difference-aware features. Secondly, we explore a single-temporal pre-training strategy for better adaptation of visual foundation models. With pseudo-change data constructed from single-temporal segmentation datasets, we employ an extra branch of proxy semantic segmentation task for pre-training. We explore various settings like dataset combinations and landscape types, thus providing valuable insights. Experimental results on five challenging benchmarks demonstrate the superiority of our method over the existing state-of-the-art methods. The code is available at $\href{https://github.com/DREAMXFAR/SA-CDNet}{github}$.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models</title>
<link>https://arxiv.org/abs/2501.08226</link>
<guid>https://arxiv.org/abs/2501.08226</guid>
<content:encoded><![CDATA[
arXiv:2501.08226v2 Announce Type: replace 
Abstract: Glioblastoma, a highly aggressive brain tumor, poses major challenges due to its poor prognosis and high morbidity rates. Partial differential equation-based models offer promising potential to enhance therapeutic outcomes by simulating patient-specific tumor behavior for improved radiotherapy planning. However, model calibration remains a bottleneck due to the high computational demands of optimization methods like Monte Carlo sampling and evolutionary algorithms. To address this, we recently introduced an approach leveraging a neural forward solver with gradient-based optimization to significantly reduce calibration time. This approach requires a highly accurate and fully differentiable forward model. We investigate multiple architectures, including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a 3D Vision Transformer (ViT). The nnU-Net achieved the best overall results, excelling in both tumor outline matching and voxel-level prediction of tumor cell concentration. It yielded the lowest MSE in tumor cell concentration compared to ground truth numerical simulation and the highest Dice score across all tumor cell concentration thresholds. Our study demonstrates significant enhancement in forward solver performance and outlines important future research directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency Domain Enhanced U-Net for Low-Frequency Information-Rich Image Segmentation in Surgical and Deep-Sea Exploration Robots</title>
<link>https://arxiv.org/abs/2502.03829</link>
<guid>https://arxiv.org/abs/2502.03829</guid>
<content:encoded><![CDATA[
arXiv:2502.03829v3 Announce Type: replace 
Abstract: In deep-sea exploration and surgical robotics scenarios, environmental lighting and device resolution limitations often cause high-frequency feature attenuation. Addressing the differences in frequency band sensitivity between CNNs and the human visual system (mid-frequency sensitivity with low-frequency sensitivity surpassing high-frequency), we experimentally quantified the CNN contrast sensitivity function and proposed a wavelet adaptive spectrum fusion (WASF) method inspired by biological vision mechanisms to balance cross-frequency image features. Furthermore, we designed a perception frequency block (PFB) that integrates WASF to enhance frequency-domain feature extraction. Based on this, we developed the FE-UNet model, which employs a SAM2 backbone network and incorporates fine-tuned Hiera-Large modules to ensure segmentation accuracy while improving generalization capability. Experiments demonstrate that FE-UNet achieves state-of-the-art performance in cross-domain tasks such as marine organism segmentation and polyp segmentation, showcasing robust adaptability and significant application potential. The code will be released soon.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA</title>
<link>https://arxiv.org/abs/2502.18536</link>
<guid>https://arxiv.org/abs/2502.18536</guid>
<content:encoded><![CDATA[
arXiv:2502.18536v2 Announce Type: replace 
Abstract: Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</title>
<link>https://arxiv.org/abs/2503.00374</link>
<guid>https://arxiv.org/abs/2503.00374</guid>
<content:encoded><![CDATA[
arXiv:2503.00374v3 Announce Type: replace 
Abstract: Histopathology and transcriptomics are fundamental modalities in oncology, encapsulating the morphological and molecular aspects of the disease. Multi-modal self-supervised learning has demonstrated remarkable potential in learning pathological representations by integrating diverse data sources. Conventional multi-modal integration methods primarily emphasize modality alignment, while paying insufficient attention to retaining the modality-specific structures. However, unlike conventional scenarios where multi-modal inputs share highly overlapping features, histopathology and transcriptomics exhibit pronounced heterogeneity, offering orthogonal yet complementary insights. Histopathology provides morphological and spatial context, elucidating tissue architecture and cellular topology, whereas transcriptomics delineates molecular signatures through gene expression patterns. This inherent disparity introduces a major challenge in aligning them while maintaining modality-specific fidelity. To address these challenges, we present MIRROR, a novel multi-modal representation learning method designed to foster both modality alignment and retention. MIRROR employs dedicated encoders to extract comprehensive features for each modality, which is further complemented by a modality alignment module to achieve seamless integration between phenotype patterns and molecular profiles. Furthermore, a modality retention module safeguards unique attributes from each modality, while a style clustering module mitigates redundancy and enhances disease-relevant information by modeling and aligning consistent pathological signatures within a clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping and survival analysis highlight MIRROR's superior performance, demonstrating its effectiveness in constructing comprehensive oncological feature representations and benefiting the cancer diagnosis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Alignment-Regularity Characteristics in Deformable Image Registration</title>
<link>https://arxiv.org/abs/2503.07185</link>
<guid>https://arxiv.org/abs/2503.07185</guid>
<content:encoded><![CDATA[
arXiv:2503.07185v2 Announce Type: replace 
Abstract: Evaluating deformable image registration (DIR) is challenging due to the inherent trade-off between achieving high alignment accuracy and maintaining deformation regularity. In this work, we introduce a novel evaluation scheme based on the alignment-regularity characteristic (ARC) to systematically capture and analyze this trade-off. We first introduce the ARC curves, which describe the performance of a given registration algorithm as a spectrum measured by alignment and regularity metrics. We further adopt a HyperNetwork-based approach that learns to continuously interpolate across the full regularization range, accelerating the construction and improving the sample density of ARC curves. We empirically demonstrate our evaluation scheme using representative learning-based deformable image registration methods with various network architectures and transformation models on two public datasets. We present a range of findings not evident from existing evaluation practices and provide general recommendations for model evaluation and selection using our evaluation scheme. All code relevant is made publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-scale Pre-training for Grounded Video Caption Generation</title>
<link>https://arxiv.org/abs/2503.10781</link>
<guid>https://arxiv.org/abs/2503.10781</guid>
<content:encoded><![CDATA[
arXiv:2503.10781v3 Announce Type: replace 
Abstract: We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates frame-level captions grounded with bounding boxes into temporally dense and consistent annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce iGround--a dataset of 3513 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset, as well as on the VidSTG, ActivityNet-Entities, GroundingYouTube, and YouCook-Interactions datasets. Our ablations demonstrate the importance of pre-training on our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model. The dataset and code are available at https://ekazakos.github.io/grounded_video_caption_generation/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-centric Video Understanding Benchmark without Text Shortcut</title>
<link>https://arxiv.org/abs/2503.19951</link>
<guid>https://arxiv.org/abs/2503.19951</guid>
<content:encoded><![CDATA[
arXiv:2503.19951v2 Announce Type: replace 
Abstract: Audio often serves as an auxiliary modality in video understanding tasks of audio-visual large language models (LLMs), merely assisting in the comprehension of visual information. However, a thorough understanding of videos significantly depends on auditory information, as audio offers critical context, emotional cues, and semantic meaning that visual data alone often lacks. This paper proposes an audio-centric video understanding benchmark (AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with a particular focus on auditory information. AVUT introduces a suite of carefully designed audio-centric tasks, holistically testing the understanding of both audio content and audio-visual interactions in videos. Moreover, this work points out the text shortcut problem that largely exists in other benchmarks where the correct answer can be found from question text alone without needing videos. AVUT addresses this problem by proposing a answer permutation-based filtering mechanism. A thorough evaluation across a diverse range of open-source and proprietary multimodal LLMs is performed, followed by the analyses of deficiencies in audio-visual LLMs. Demos and data are available at https://github.com/lark-png/AVUT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Traffic Incident Response through Sub-Second Temporal Localization with HybridMamba</title>
<link>https://arxiv.org/abs/2504.03235</link>
<guid>https://arxiv.org/abs/2504.03235</guid>
<content:encoded><![CDATA[
arXiv:2504.03235v2 Announce Type: replace 
Abstract: Traffic crash detection in long-form surveillance videos is essential for improving emergency response and infrastructure planning, yet remains difficult due to the brief and infrequent nature of crash events. We present \textbf{HybridMamba}, a novel architecture integrating visual transformers with state-space temporal modeling to achieve high-precision crash time localization. Our approach introduces multi-level token compression and hierarchical temporal processing to maintain computational efficiency without sacrificing temporal resolution. Evaluated on a large-scale dataset from the Iowa Department of Transportation, HybridMamba achieves a mean absolute error of \textbf{1.50 seconds} for 2-minute videos ($p<0.01$ compared to baselines), with \textbf{65.2%} of predictions falling within one second of the ground truth. It outperforms recent video-language models (e.g., TimeChat, VideoLLaMA-2) by up to 3.95 seconds while using significantly fewer parameters (3B vs. 13--72B). Our results demonstrate effective temporal localization across various video durations (2--40 minutes) and diverse environmental conditions, highlighting HybridMamba's potential for fine-grained temporal localization in traffic surveillance while identifying challenges that remain for extended deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decade of Wheat Mapping for Lebanon</title>
<link>https://arxiv.org/abs/2504.11366</link>
<guid>https://arxiv.org/abs/2504.11366</guid>
<content:encoded><![CDATA[
arXiv:2504.11366v3 Announce Type: replace 
Abstract: Wheat accounts for approximately 20% of the world's caloric intake, making it a vital component of global food security. Given this importance, mapping wheat fields plays a crucial role in enabling various stakeholders, including policy makers, researchers, and agricultural organizations, to make informed decisions regarding food security, supply chain management, and resource allocation. In this paper, we tackle the problem of accurately mapping wheat fields out of satellite images by introducing an improved pipeline for winter wheat segmentation, as well as presenting a case study on a decade-long analysis of wheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer (TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing pipeline based on the Fields of The World (FTW) framework. Our proposed pipeline addresses key challenges encountered in existing approaches, such as the clustering of small agricultural parcels in a single large field. By merging wheat segmentation with precise field boundary extraction, our method produces geometrically coherent and semantically rich maps that enable us to perform in-depth analysis such as tracking crop rotation pattern over years. Extensive evaluations demonstrate improved boundary delineation and field-level precision, establishing the potential of the proposed framework in operational agricultural monitoring and historical trend analysis. By allowing for accurate mapping of wheat fields, this work lays the foundation for a range of critical studies and future advances, including crop monitoring and yield estimation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification</title>
<link>https://arxiv.org/abs/2504.18046</link>
<guid>https://arxiv.org/abs/2504.18046</guid>
<content:encoded><![CDATA[
arXiv:2504.18046v3 Announce Type: replace 
Abstract: Ophthalmic diseases pose a significant global health burden. However, traditional diagnostic methods and existing monocular image-based deep learning approaches often overlook the pathological correlations between the two eyes. In practical medical robotic diagnostic scenarios, paired retinal images (binocular fundus images) are frequently required as diagnostic evidence. To address this, we propose DMS-Net-a dual-modal multi-scale siamese network for binocular retinal image classification. The framework employs a weight-sharing siamese ResNet-152 architecture to concurrently extract deep semantic features from bilateral fundus images. To tackle challenges like indistinct lesion boundaries and diffuse pathological distributions, we introduce the OmniPool Spatial Integrator Module (OSIM), which achieves multi-resolution feature aggregation through multi-scale adaptive pooling and spatial attention mechanisms. Furthermore, the Calibrated Analogous Semantic Fusion Module (CASFM) leverages spatial-semantic recalibration and bidirectional attention mechanisms to enhance cross-modal interaction, aggregating modality-agnostic representations of fundus structures. To fully exploit the differential semantic information of lesions present in bilateral fundus features, we introduce the Cross-Modal Contrastive Alignment Module (CCAM). Additionally, to enhance the aggregation of lesion-correlated semantic information, we introduce the Cross-Modal Integrative Alignment Module (CIAM). Evaluation on the ODIR-5K dataset demonstrates that DMS-Net achieves state-of-the-art performance with an accuracy of 82.9%, recall of 84.5%, and a Cohen's kappa coefficient of 83.2%, showcasing robust capacity in detecting symmetrical pathologies and improving clinical decision-making for ocular diseases. Code and the processed dataset will be released subsequently.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices</title>
<link>https://arxiv.org/abs/2505.03303</link>
<guid>https://arxiv.org/abs/2505.03303</guid>
<content:encoded><![CDATA[
arXiv:2505.03303v2 Announce Type: replace 
Abstract: This paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. Five state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (FLOPs), and model size. Additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on MobileNetV3 Small. Our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest accuracy, while MobileNetV3 offers the best balance between accuracy and efficiency, and SqueezeNet excels in inference speed and compactness. This study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. By addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRep: Generalized SDR-to-HDR Conversion via Attribute-Disentangled Representation Learning</title>
<link>https://arxiv.org/abs/2505.07322</link>
<guid>https://arxiv.org/abs/2505.07322</guid>
<content:encoded><![CDATA[
arXiv:2505.07322v2 Announce Type: replace 
Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly widespread, driving a growing need for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which struggle to handle the diverse appearances and degradations commonly present in real-world SDR content. To address this limitation, we propose a generalized SDR-to-HDR framework that enhances robustness by learning attribute-disentangled representations. Central to our approach is Realistic Attribute-Disentangled Representation Learning (RealRep), which explicitly disentangles luminance and chrominance components to capture intrinsic content variations across different SDR distributions. Furthermore, we design a Luma-/Chroma-aware negative exemplar generation strategy that constructs degradation-sensitive contrastive pairs, effectively modeling tone discrepancies across SDR styles. Building on these attribute-level priors, we introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a lightweight, two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned features, enabling robust adaptation across diverse degradation domains. Extensive experiments demonstrate that RealRep consistently outperforms state-of-the-art methods in both generalization and perceptually faithful HDR color gamut reconstruction.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
arXiv:2505.08665v4 Announce Type: replace 
Abstract: Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v4 Announce Type: replace 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v4 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMba-UNet: SAM2-Mamba UNet for Cardiac MRI in Medical Robotic Perception</title>
<link>https://arxiv.org/abs/2505.16304</link>
<guid>https://arxiv.org/abs/2505.16304</guid>
<content:encoded><![CDATA[
arXiv:2505.16304v2 Announce Type: replace 
Abstract: To address complex pathological feature extraction in automated cardiac MRI segmentation, we propose SAMba-UNet, a novel dual-encoder architecture that synergistically combines the vision foundation model SAM2, the linear-complexity state-space model Mamba, and the classical UNet to achieve cross-modal collaborative feature learning; to overcome domain shifts between natural images and medical scans, we introduce a Dynamic Feature Fusion Refiner that employs multi-scale pooling and channel-spatial dual-path calibration to strengthen small-lesion and fine-structure representation, and we design a Heterogeneous Omni-Attention Convergence Module (HOACM) that fuses SAM2's local positional semantics with Mamba's long-range dependency modeling via global contextual attention and branch-selective emphasis, yielding substantial gains in both global consistency and boundary precision-on the ACDC cardiac MRI benchmark, SAMba-UNet attains a Dice of 0.9103 and HD95 of 1.0859 mm, notably improving boundary localization for challenging structures like the right ventricle, and its robust, high-fidelity segmentation maps are directly applicable as a perception module within intelligent medical and surgical robotic systems to support preoperative planning, intraoperative navigation, and postoperative complication screening; the code will be open-sourced to facilitate clinical translation and further validation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HueManity: Probing Fine-Grained Visual Perception in MLLMs</title>
<link>https://arxiv.org/abs/2506.03194</link>
<guid>https://arxiv.org/abs/2506.03194</guid>
<content:encoded><![CDATA[
arXiv:2506.03194v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill Assessment</title>
<link>https://arxiv.org/abs/2506.04996</link>
<guid>https://arxiv.org/abs/2506.04996</guid>
<content:encoded><![CDATA[
arXiv:2506.04996v3 Announce Type: replace 
Abstract: Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations</title>
<link>https://arxiv.org/abs/2506.10559</link>
<guid>https://arxiv.org/abs/2506.10559</guid>
<content:encoded><![CDATA[
arXiv:2506.10559v2 Announce Type: replace 
Abstract: Explaining why the species lives at a particular location is important for understanding ecological systems and conserving biodiversity. However, existing ecological workflows are fragmented and often inaccessible to non-specialists. We propose an end-to-end visual-to-causal framework that transforms a species image into interpretable causal insights about its habitat preference. The system integrates species recognition, global occurrence retrieval, pseudo-absence sampling, and climate data extraction. We then discover causal structures among environmental features and estimate their influence on species occurrence using modern causal inference methods. Finally, we generate statistically grounded, human-readable causal explanations from structured templates and large language models. We demonstrate the framework on a bee and a flower species and report early results as part of an ongoing project, showing the potential of the multimodal AI assistant backed up by a recommended ecological modeling practice for describing species habitat in human-understandable language. Our code is available at: https://github.com/Yutong-Zhou-cv/BioX.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE-iT: Spatial-Aware Curriculum Exploration and Feedback-Driven Adaptive Augmentation for Vision Transformer Distillation</title>
<link>https://arxiv.org/abs/2506.10582</link>
<guid>https://arxiv.org/abs/2506.10582</guid>
<content:encoded><![CDATA[
arXiv:2506.10582v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) has proven to be a powerful technique for improving the performance of Vision Transformers (ViTs). However, traditional KD methods often treat all image patches uniformly, overlooking spatial variations in learning difficulty. To address this limitation, we propose SPACE-iT, a novel framework for Spatial-Aware Curriculum Exploration via Feedback-Driven Adaptive Augmentation. At its core, SPACE-iT computes spatial confidence scores at the attention, patch, and logit levels. This confidence map supports a two-fold strategy: (1) dynamically modulating the distillation loss, and (2) guiding an adaptive augmentation module that intensifies reverse curriculum learning. By establishing a feedback-driven reverse curriculum that initially exposes students to challenging regions-progressing from hard to easy-SPACE-iT enables more effective learning of complex spatial patterns and achieves superior performance over vanilla distillation, without introducing additional memory overhead.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Text-Guided Image Clustering via Iterative Search</title>
<link>https://arxiv.org/abs/2506.12514</link>
<guid>https://arxiv.org/abs/2506.12514</guid>
<content:encoded><![CDATA[
arXiv:2506.12514v2 Announce Type: replace 
Abstract: Traditional clustering methods aim to group unlabeled data points based on their similarity to each other. However, clustering, in the absence of additional information, is an ill-posed problem as there may be many different, yet equally valid, ways to partition a dataset. Distinct users may want to use different criteria to form clusters in the same data, e.g. shape v.s. color. Recently introduced text-guided image clustering methods aim to address this ambiguity by allowing users to specify the criteria of interest using natural language instructions. This instruction provides the necessary context and control needed to obtain clusters that are more aligned with the users' intent. We propose a new text-guided clustering approach named ITGC that uses an iterative discovery process, guided by an unsupervised clustering objective, to generate interpretable visual concepts that better capture the criteria expressed in a user's instructions. We report superior performance compared to existing methods across a wide variety of image clustering and fine-grained classification benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomizer: Generalizing to new modalities by breaking satellite images down to a set of scalars</title>
<link>https://arxiv.org/abs/2506.13542</link>
<guid>https://arxiv.org/abs/2506.13542</guid>
<content:encoded><![CDATA[
arXiv:2506.13542v2 Announce Type: replace 
Abstract: The growing number of Earth observation satellites has led to increasingly diverse remote sensing data, with varying spatial, spectral, and temporal configurations. Most existing models rely on fixed input formats and modality-specific encoders, which require retraining when new configurations are introduced, limiting their ability to generalize across modalities. We introduce Atomizer, a flexible architecture that represents remote sensing images as sets of scalars, each corresponding to a spectral band value of a pixel. Each scalar is enriched with contextual metadata (acquisition time, spatial resolution, wavelength, and bandwidth), producing an atomic representation that allows a single encoder to process arbitrary modalities without interpolation or resampling. Atomizer uses structured tokenization with Fourier features and non-uniform radial basis functions to encode content and context, and maps tokens into a latent space via cross-attention. Under modality-disjoint evaluations, Atomizer outperforms standard models and demonstrates robust performance across varying resolutions and spatial sizes.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIP: Unsupervised Dense In-Context Post-training of Visual Representations</title>
<link>https://arxiv.org/abs/2506.18463</link>
<guid>https://arxiv.org/abs/2506.18463</guid>
<content:encoded><![CDATA[
arXiv:2506.18463v3 Announce Type: replace 
Abstract: We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges</title>
<link>https://arxiv.org/abs/2507.02074</link>
<guid>https://arxiv.org/abs/2507.02074</guid>
<content:encoded><![CDATA[
arXiv:2507.02074v2 Announce Type: replace 
Abstract: Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer</title>
<link>https://arxiv.org/abs/2507.08741</link>
<guid>https://arxiv.org/abs/2507.08741</guid>
<content:encoded><![CDATA[
arXiv:2507.08741v2 Announce Type: replace 
Abstract: Hierarchical land cover and land use (LCLU) classification aims to assign pixel-wise labels with multiple levels of semantic granularity to remote sensing (RS) imagery. However, existing deep learning-based methods face two major challenges: 1) They predominantly adopt a flat classification paradigm, which limits their ability to generate end-to-end multi-granularity hierarchical predictions aligned with tree-structured hierarchies used in practice. 2) Most cross-domain studies focus on performance degradation caused by sensor or scene variations, with limited attention to transferring LCLU models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop classification). These limitations hinder the flexibility and generalization of LCLU models in practical applications. To address these challenges, we propose HieraRS, a novel hierarchical interpretation paradigm that enables multi-granularity predictions and supports the efficient transfer of LCLU models to cross-domain tasks with heterogeneous tree-structured hierarchies. We introduce the Bidirectional Hierarchical Consistency Constraint Mechanism (BHCCM), which can be seamlessly integrated into mainstream flat classification models to generate hierarchical predictions, while improving both semantic consistency and classification accuracy. Furthermore, we present TransLU, a dual-branch cross-domain transfer framework comprising two key components: Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment (CDSA). TransLU supports dynamic category expansion and facilitates the effective adaptation of LCLU models to heterogeneous hierarchies. In addition, we construct MM-5B, a large-scale multi-modal hierarchical land use dataset featuring pixel-wise annotations. The code and MM-5B dataset will be released at: https://github.com/AI-Tianlong/HieraRS.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\pi^3$: Permutation-Equivariant Visual Geometry Learning</title>
<link>https://arxiv.org/abs/2507.13347</link>
<guid>https://arxiv.org/abs/2507.13347</guid>
<content:encoded><![CDATA[
arXiv:2507.13347v2 Announce Type: replace 
Abstract: We introduce $\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design not only makes our model inherently robust to input ordering, but also leads to higher accuracy and performance. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</title>
<link>https://arxiv.org/abs/2507.22291</link>
<guid>https://arxiv.org/abs/2507.22291</guid>
<content:encoded><![CDATA[
arXiv:2507.22291v2 Announce Type: replace 
Abstract: Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform a suite of other well-known/widely accepted featurization approaches tested on a diverse set of mapping evaluations without re-training. We have released a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</title>
<link>https://arxiv.org/abs/2508.03692</link>
<guid>https://arxiv.org/abs/2508.03692</guid>
<content:encoded><![CDATA[
arXiv:2508.03692v2 Announce Type: replace 
Abstract: Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection</title>
<link>https://arxiv.org/abs/2508.19742</link>
<guid>https://arxiv.org/abs/2508.19742</guid>
<content:encoded><![CDATA[
arXiv:2508.19742v2 Announce Type: replace 
Abstract: Line segment detection in images has been studied for several decades. Existing line segment detectors can be roughly divided into two categories: generic line segment detectors and wireframe line segment detectors. Generic line segment detectors aim to detect all meaningful line segments in images and traditional approaches usually fall into this category. Recent deep learning based approaches are mostly wireframe line segment detectors. They detect only line segments that are geometrically meaningful and have large spatial support. Due to the difference in the aim of design, the performance of generic line segment detectors for the task of wireframe line segment detection won't be satisfactory, and vice versa. In this work, we propose a robust framework that can be used for both generic line segment detection and wireframe line segment detection. The proposed method is an improved version of the Pixel Orientation Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments from edge strength maps, and can be combined with any edge detector. We show in our experiments that by combining the proposed POEv2 with an efficient edge detector, it achieves state-of-the-art performance on three publicly available datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions</title>
<link>https://arxiv.org/abs/2508.19762</link>
<guid>https://arxiv.org/abs/2508.19762</guid>
<content:encoded><![CDATA[
arXiv:2508.19762v4 Announce Type: replace 
Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food production and ecosystem stability, yet their populations are declining due to anthropogenic and environmental stressors. Scalable, automated monitoring in agricultural environments remains an open challenge due to the difficulty of detecting small, fast-moving, and often camouflaged insects. To address this, we present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator images collected under real field conditions. BuzzSet contains 7,856 manually verified images with more than 8,000 annotated instances across three classes: honeybees, bumblebees, and unidentified insects. Initial annotations were produced using a YOLOv12 model trained on external data and refined through human verification with open-source tools. All images were preprocessed into 256 x 256 tiles to improve the detection of small insects. We provide baselines using the RF-DETR transformer-based object detector. The model achieves strong classification accuracy with F1 scores of 0.94 and 0.92 for honeybees and bumblebees, with minimal confusion between these categories. The unidentified class remains more difficult due to label ambiguity and fewer samples, yet still contributes insights for robustness evaluation. Overall detection performance (mAP at 0.50 of 0.559) illustrates the challenging nature of the dataset and its potential to drive advances in small object detection under realistic ecological conditions. Future work focuses on expanding the dataset to version 2.0 with additional annotations and evaluating further detection strategies. BuzzSet establishes a benchmark for ecological computer vision, with the primary challenge being reliable detection of insects frequently camouflaged within natural vegetation, highlighting an open problem for future research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection</title>
<link>https://arxiv.org/abs/2508.20392</link>
<guid>https://arxiv.org/abs/2508.20392</guid>
<content:encoded><![CDATA[
arXiv:2508.20392v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection</title>
<link>https://arxiv.org/abs/2508.20670</link>
<guid>https://arxiv.org/abs/2508.20670</guid>
<content:encoded><![CDATA[
arXiv:2508.20670v2 Announce Type: replace 
Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media</title>
<link>https://arxiv.org/abs/2405.15425</link>
<guid>https://arxiv.org/abs/2405.15425</guid>
<content:encoded><![CDATA[
arXiv:2405.15425v3 Announce Type: replace-cross 
Abstract: Efficient scene representations are essential for many computer graphics applications. A general unified representation that can handle both surfaces and volumes simultaneously, remains a research challenge. Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for different kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer. We demonstrate our method as a compact and efficient alternative to other forms of volume modeling for forward and inverse rendering of scattering media. Furthermore, we adapt and showcase our method in radiance field optimization and rendering, providing additional flexibility compared to current state of the art given its ray-tracing formulation. We also introduce the Epanechnikov kernel and demonstrate its potential as an efficient alternative to the traditionally-used Gaussian kernel in scene reconstruction tasks. The versatility and physically-based nature of our approach allows us to go beyond radiance fields and bring to kernel-based modeling and rendering any path-tracing enabled functionality such as scattering, relighting and complex camera models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Humanoid Manipulation with 3D Diffusion Policies</title>
<link>https://arxiv.org/abs/2410.10803</link>
<guid>https://arxiv.org/abs/2410.10803</guid>
<content:encoded><![CDATA[
arXiv:2410.10803v3 Announce Type: replace-cross 
Abstract: Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data. We run more than 2000 episodes of policy rollouts on the real robot for rigorous policy evaluation. Empowered by this system, we show that using only data collected in one single scene and with only onboard computing, a full-sized humanoid robot can autonomously perform skills in diverse real-world scenarios. Videos are available at https://humanoid-manipulation.github.io .
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VMGNet: A Low Computational Complexity Robotic Grasping Network Based on VMamba with Multi-Scale Feature Fusion</title>
<link>https://arxiv.org/abs/2411.12520</link>
<guid>https://arxiv.org/abs/2411.12520</guid>
<content:encoded><![CDATA[
arXiv:2411.12520v2 Announce Type: replace-cross 
Abstract: While deep learning-based robotic grasping technology has demonstrated strong adaptability, its computational complexity has also significantly increased, making it unsuitable for scenarios with high real-time requirements. Therefore, we propose a low computational complexity and high accuracy model named VMGNet for robotic grasping. For the first time, we introduce the Visual State Space into the robotic grasping field to achieve linear computational complexity, thereby greatly reducing the model's computational cost. Meanwhile, to improve the accuracy of the model, we propose an efficient and lightweight multi-scale feature fusion module, named Fusion Bridge Module, to extract and fuse information at different scales. We also present a new loss function calculation method to enhance the importance differences between subtasks, improving the model's fitting ability. Experiments show that VMGNet has only 8.7G Floating Point Operations and an inference time of 8.1 ms on our devices. VMGNet also achieved state-of-the-art performance on the Cornell and Jacquard public datasets. To validate VMGNet's effectiveness in practical applications, we conducted real grasping experiments in multi-object scenarios, and VMGNet achieved an excellent performance with a 94.4% success rate in real-world grasping tasks. The video for the real-world robotic grasping experiments is available at https://youtu.be/S-QHBtbmLc4.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid-Regularized Magnitude Pruning for Robust Federated Learning under Covariate Shift</title>
<link>https://arxiv.org/abs/2412.15010</link>
<guid>https://arxiv.org/abs/2412.15010</guid>
<content:encoded><![CDATA[
arXiv:2412.15010v2 Announce Type: replace-cross 
Abstract: Federated Learning offers a solution for decentralised model training, addressing the difficulties associated with distributed data and privacy in machine learning. However, the fact of data heterogeneity in federated learning frequently hinders the global model's generalisation, leading to low performance and adaptability to unseen data. This problem is particularly critical for specialised applications such as medical imaging, where both the data and the number of clients are limited. In this paper, we empirically demonstrate that inconsistencies in client-side training distributions substantially degrade the performance of federated learning models across multiple benchmark datasets. We propose a novel FL framework using a combination of pruning and regularisation of clients' training to improve the sparsity, redundancy, and robustness of neural connections, and thereby the resilience to model aggregation. To address a relatively unexplored dimension of data heterogeneity, we further introduce a novel benchmark dataset, CelebA-Gender, specifically designed to control for within-class distributional shifts across clients based on attribute variations, thereby complementing the predominant focus on inter-class imbalance in prior federated learning research. Comprehensive experiments on many datasets like CIFAR-10, MNIST, and the newly introduced CelebA-Gender dataset demonstrate that our method consistently outperforms standard FL baselines, yielding more robust and generalizable models in heterogeneous settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map</title>
<link>https://arxiv.org/abs/2502.05752</link>
<guid>https://arxiv.org/abs/2502.05752</guid>
<content:encoded><![CDATA[
arXiv:2502.05752v2 Announce Type: replace-cross 
Abstract: Robots benefit from high-fidelity reconstructions of their environment, which should be geometrically accurate and photorealistic to support downstream tasks. While this can be achieved by building distance fields from range sensors and radiance fields from cameras, realising scalable incremental mapping of both fields consistently and at the same time with high quality is challenging. In this paper, we propose a novel map representation that unifies a continuous signed distance field and a Gaussian splatting radiance field within an elastic and compact point-based implicit neural map. By enforcing geometric consistency between these fields, we achieve mutual improvements by exploiting both modalities. We present a novel LiDAR-visual SLAM system called PINGS using the proposed map representation and evaluate it on several challenging large-scale datasets. Experimental results demonstrate that PINGS can incrementally build globally consistent distance and radiance fields encoded with a compact set of neural points. Compared to state-of-the-art methods, PINGS achieves superior photometric and geometric rendering at novel views by constraining the radiance field with the distance field. Furthermore, by utilizing dense photometric cues and multi-view consistency from the radiance field, PINGS produces more accurate distance fields, leading to improved odometry estimation and mesh reconstruction. We also provide an open-source implementation of PING at: https://github.com/PRBonn/PINGS.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video</title>
<link>https://arxiv.org/abs/2502.08297</link>
<guid>https://arxiv.org/abs/2502.08297</guid>
<content:encoded><![CDATA[
arXiv:2502.08297v2 Announce Type: replace-cross 
Abstract: Volumetric video enables immersive experiences by capturing dynamic 3D scenes, enabling diverse applications for virtual reality, education, and telepresence. However, traditional methods struggle with fixed lighting conditions, while neural approaches face trade-offs in efficiency, quality, or adaptability for relightable scenarios. To address these limitations, we present BEAM, a novel pipeline that bridges 4D Gaussian representations with physically-based rendering (PBR) to produce high-quality, relightable volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry and PBR properties via a series of available Gaussian-based techniques. It first combines Gaussian-based human performance tracking with geometry-aware rasterization in a coarse-to-fine optimization framework to recover spatially and temporally consistent geometries. We further enhance Gaussian attributes by incorporating PBR properties step by step. We generate roughness via a multi-view-conditioned diffusion model, and then derive AO and base color using a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for efficient visibility computation. Once recovered, these dynamic, relightable assets integrate seamlessly into traditional CG pipelines, supporting real-time rendering with deferred shading and offline rendering with ray tracing. By offering realistic, lifelike visualizations under diverse lighting conditions, BEAM opens new possibilities for interactive entertainment, storytelling, and creative visualization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2502.13061</link>
<guid>https://arxiv.org/abs/2502.13061</guid>
<content:encoded><![CDATA[
arXiv:2502.13061v3 Announce Type: replace-cross 
Abstract: Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution</title>
<link>https://arxiv.org/abs/2503.14779</link>
<guid>https://arxiv.org/abs/2503.14779</guid>
<content:encoded><![CDATA[
arXiv:2503.14779v2 Announce Type: replace-cross 
Abstract: Single-image super-resolution (SISR) is a fundamental problem in computer vision that aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Although convolutional neural networks (CNNs) have achieved substantial advancements, deeper architectures often introduce excessive parameters, higher memory usage, and computational cost, limiting their applicability on resource-constrained devices. Recent research has thus focused on lightweight architectures that preserve accuracy while reducing complexity. This paper presents the Involution and BSConv Multi-Depth Distillation Network (IBMDN), a lightweight and effective architecture for SISR. The proposed IBMDN comprises Involution and BSConv Multi-Depth Distillation Blocks (IBMDB) and a Contrast and High-Frequency Attention Block (CHFAB). IBMDB employs varying combinations of Involution and BSConv at multiple depths to perform efficient feature extraction while minimizing computational complexity. CHFAB, a lightweight self-attention mechanism, focuses on extracting high-frequency and contrast information to enhance perceptual quality in the reconstructed images. The flexible design of IBMDB enables it to be seamlessly integrated into diverse SISR frameworks, including information distillation, transformer-based, and GAN-based models. Extensive experiments demonstrate that incorporating IBMDB significantly reduces memory usage, parameters, and floating-point operations (FLOPs), while achieving improvements in both pixel-wise accuracy and visual quality. The source code is available at: https://github.com/akramkhatami/IBMDN.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions</title>
<link>https://arxiv.org/abs/2503.16013</link>
<guid>https://arxiv.org/abs/2503.16013</guid>
<content:encoded><![CDATA[
arXiv:2503.16013v2 Announce Type: replace-cross 
Abstract: Flexible instruction-guided 6-DoF grasping is a significant yet challenging task for real-world robotic systems. Existing methods utilize the contextual understanding capabilities of the large language models (LLMs) to establish mappings between expressions and targets, allowing robots to comprehend users' intentions in the instructions. However, the LLM's knowledge about objects' physical properties remains underexplored despite its tight relevance to grasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework that integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to physical properties, guided by auxiliary question-answering (QA) tasks. Particularly, we design a set of QA templates to enable hierarchical reasoning that includes three stages: target parsing, physical property analysis, and grasp action selection. Moreover, GraspCoT presents a unified multimodal LLM architecture, which encodes multi-view observations of 3D scenes into 3D-aware visual tokens, and then jointly embeds these visual tokens with CoT-derived textual tokens within LLMs to generate grasp pose predictions. Furthermore, we present IntentGrasp, a large-scale benchmark that fills the gap in public datasets for multi-object grasp detection under diverse and indirect verbal commands. Extensive experiments on IntentGrasp demonstrate the superiority of our method, with additional validation in real-world robotic applications confirming its practicality. The code is available at https://github.com/cxmomo/GraspCoT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[
arXiv:2506.00785v3 Announce Type: replace-cross 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntuiTF: MLLM-Guided Transfer Function Optimization for Direct Volume Rendering</title>
<link>https://arxiv.org/abs/2506.18407</link>
<guid>https://arxiv.org/abs/2506.18407</guid>
<content:encoded><![CDATA[
arXiv:2506.18407v2 Announce Type: replace-cross 
Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, where transfer functions (TFs) play a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Although numerous TF optimization methods have been proposed to mitigate this issue, existing approaches still face two major challenges: the vast exploration space and limited generalizability. To address these issues, we propose IntuiTF, a novel framework that leverages Multimodal Large Language Models (MLLMs) to guide TF optimization in alignment with user intent. Specifically, our method consists of two key components: (1) an evolution-driven explorer for effective exploration of the TF space, and (2) an MLLM-guided human-aligned evaluator that provides generalizable visual feedback on rendering quality. The explorer and the evaluator together establish an efficient Trial-Insight-Replanning paradigm for TF space exploration. We further extend our framework with an interactive TF design system. We demonstrate the broad applicability of our framework through three case studies and validate the effectiveness of each component through extensive experiments. We strongly recommend readers check our cases, demo video, and source code at: https://github.com/wyysteelhead/IntuiTF
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountQA: How Well Do MLLMs Count in the Wild?</title>
<link>https://arxiv.org/abs/2508.06585</link>
<guid>https://arxiv.org/abs/2508.06585</guid>
<content:encoded><![CDATA[
arXiv:2508.06585v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in understanding visual scenes, yet they exhibit a critical lack in a fundamental cognitive skill: object counting. This blind spot severely limits their reliability in real-world applications. To date, this capability has been largely unevaluated in complex scenarios, as existing benchmarks either feature sparse object densities or are confined to specific visual domains, failing to test models under realistic conditions. Addressing this gap, we introduce CountQA, a challenging new benchmark designed to probe this deficiency. Comprising over 1,500 question-answer pairs, CountQA features real-world images with high object density, clutter, and occlusion. We investigate this weakness by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the top-performing model achieves a mere 42.9% accuracy, with performance declining as object counts rise. By providing a dedicated benchmark to diagnose and rectify this core weakness, CountQA paves the way for a new generation of MLLMs that are not only descriptively fluent but also numerically grounded and spatially aware. We will open-source the dataset and code upon paper acceptance to foster further research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</title>
<link>https://arxiv.org/abs/2508.17180</link>
<guid>https://arxiv.org/abs/2508.17180</guid>
<content:encoded><![CDATA[
arXiv:2508.17180v2 Announce Type: replace-cross 
Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-task neural network for atypical mitosis recognition under domain shift</title>
<link>https://arxiv.org/abs/2508.21035</link>
<guid>https://arxiv.org/abs/2508.21035</guid>
<content:encoded><![CDATA[
arXiv:2508.21035v3 Announce Type: replace-cross 
Abstract: Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</title>
<link>https://arxiv.org/abs/2508.21770</link>
<guid>https://arxiv.org/abs/2508.21770</guid>
<content:encoded><![CDATA[
<div> Keywords: open-world learning, atypical videos, representation learning, out-of-distribution detection, zero-shot action recognition

Summary:
In this study, the researchers explore the impact of atypical unusual videos on open-world learning. They introduce a new dataset containing various types of atypical data, such as sci-fi and animation, and investigate how incorporating these videos into model training can improve performance in tasks like out-of-distribution detection, novel category discovery, and zero-shot action recognition. The results show that using atypical data can consistently enhance performance in different settings. The study also reveals that increasing the categorical diversity of atypical samples improves out-of-distribution detection, while using a smaller yet more semantically diverse set of atypical samples leads to better novel category discovery results. Furthermore, the semantic diversity of atypical videos aids in generalizing to unseen action classes in zero-shot action recognition. These findings highlight the benefits of incorporating atypical videos in visual representation learning for open-world scenarios, indicating the potential for further exploration in this area. 

Summary: <br /><br />In this study on open-world learning, the researchers introduce a dataset of atypical videos and demonstrate their positive impact on representation learning. By incorporating atypical data, they observe improvements in out-of-distribution detection, novel category discovery, and zero-shot action recognition tasks. The study shows that increasing the categorical diversity of atypical samples enhances out-of-distribution detection, while using a smaller but more semantically diverse set improves novel category discovery. Additionally, the semantic diversity of atypical videos aids in generalizing to unseen action classes in zero-shot action recognition. Overall, the findings indicate the potential benefits of atypical videos for visual representation learning in open-world scenarios and encourage further investigations in this direction. <div>
arXiv:2508.21770v2 Announce Type: replace 
Abstract: Humans usually show exceptional generalisation and discovery ability in the open world, when being shown uncommon new concepts. Whereas most existing studies in the literature focus on common typical data from closed sets, open-world novel discovery is under-explored in videos. In this paper, we are interested in asking: What if atypical unusual videos are exposed in the learning process? To this end, we collect a new video dataset consisting of various types of unusual atypical data (e.g., sci-fi, animation, etc.). To study how such atypical data may benefit open-world learning, we feed them into the model training process for representation learning. Focusing on three key tasks in open-world learning: out-of-distribution (OOD) detection, novel category discovery (NCD), and zero-shot action recognition (ZSAR), we found that even straightforward learning approaches with atypical data consistently improve performance across various settings. Furthermore, we found that increasing the categorical diversity of the atypical samples further boosts OOD detection performance. Additionally, in the NCD task, using a smaller yet more semantically diverse set of atypical samples leads to better performance compared to using a larger but more typical dataset. In the ZSAR setting, the semantic diversity of atypical videos helps the model generalise better to unseen action classes. These observations in our extensive experimental evaluations reveal the benefits of atypical videos for visual representation learning in the open world, together with the newly proposed dataset, encouraging further studies in this direction. The project page is at: https://julysun98.github.io/atypical_dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19153</link>
<guid>https://arxiv.org/abs/2508.19153</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-guided motion control, reinforcement learning, proprioception-vision fusion, spline-parameterized policy, quadriped locomotion

Summary: 
The study explores vision-guided quadruped motion control using reinforcement learning (RL) and emphasizes the importance of integrating proprioception with vision for robust control. The proposed QuadKAN framework utilizes a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs), incorporating a spline encoder for proprioception and a fusion head for proprioception-vision inputs. This structured function class aligns state-to-action mapping with the piecewise-smooth nature of gait, enhancing sample efficiency and producing interpretable posture-action sensitivities. Multi-Modal Delay Randomization (MMDR) is adopted, and end-to-end training is conducted using Proximal Policy Optimization (PPO). Evaluation across diverse terrains demonstrates that QuadKAN outperforms state-of-the-art baselines in terms of returns, distances covered, and collision avoidance. The results highlight the effectiveness and interpretability of spline-parameterized policies for robust vision-guided locomotion. A repository will be accessible upon acceptance. 

<br /><br />Summary: <div>
arXiv:2508.19153v2 Announce Type: replace-cross 
Abstract: We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Smoothing++: Enhanced Label Regularization for Training Neural Networks</title>
<link>https://arxiv.org/abs/2509.05307</link>
<guid>https://arxiv.org/abs/2509.05307</guid>
<content:encoded><![CDATA[
<div> Label Smoothing++, neural networks, overconfidence, overfitting, inter-class relationships <br />
Summary: <br />
Training neural networks with one-hot target labels often leads to overconfidence and overfitting. Label smoothing is a technique that addresses this issue by introducing regularization through perturbing the one-hot target labels. However, traditional label smoothing assigns equal importance to all non-target classes, which can disrupt inter-class relationships. In this study, a novel label regularization strategy called Label Smoothing++ is introduced. This approach maintains a fixed label for the target class while allowing the network to learn the labels associated with non-target classes. Through extensive experiments on various datasets, Label Smoothing++ is shown to reduce overconfident predictions, enhance inter-class relationships, and improve generalization capabilities of neural networks. <div>
arXiv:2509.05307v1 Announce Type: new 
Abstract: Training neural networks with one-hot target labels often results in overconfidence and overfitting. Label smoothing addresses this issue by perturbing the one-hot target labels by adding a uniform probability vector to create a regularized label. Although label smoothing improves the network's generalization ability, it assigns equal importance to all the non-target classes, which destroys the inter-class relationships. In this paper, we propose a novel label regularization training strategy called Label Smoothing++, which assigns non-zero probabilities to non-target classes and accounts for their inter-class relationships. Our approach uses a fixed label for the target class while enabling the network to learn the labels associated with non-target classes. Through extensive experiments on multiple datasets, we demonstrate how Label Smoothing++ mitigates overconfident predictions while promoting inter-class relationships and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VILOD: A Visual Interactive Labeling Tool for Object Detection</title>
<link>https://arxiv.org/abs/2509.05317</link>
<guid>https://arxiv.org/abs/2509.05317</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Detection, Deep Learning, Active Learning, Human-in-the-Loop, Visual Analytics

Summary: 
This thesis introduces "VILOD: A Visual Interactive Labeling tool for Object Detection," a novel approach that combines Human-in-the-Loop (HITL) methods with Visual Analytics (VA) to improve Object Detection (OD) model performance. By integrating human expertise and intuition throughout the machine learning process, VILOD aims to make the annotation workflow more transparent, manageable, and effective. The tool incorporates components like t-SNE projections, uncertainty heatmaps, and model state views to enable users to explore data, interpret model states, implement diverse sample selection strategies, and receive Active Learning (AL) suggestions. Empirical research conducted using comparative use cases demonstrated that visually-guided labeling strategies within VILOD can lead to competitive OD performance trajectories compared to traditional AL methods. This work provides valuable insights into enhancing the HITL-AL workflow for OD annotation, offering a potentially more efficient and intuitive approach to dataset labeling. 

<br /><br />Summary: <div>
arXiv:2509.05317v1 Announce Type: new 
Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often hindered by the significant challenge of acquiring large, accurately labeled datasets, a process that is time-consuming and expensive. While techniques like Active Learning (AL) can reduce annotation effort by intelligently querying informative samples, they often lack transparency, limit the strategic insight of human experts, and may overlook informative samples not aligned with an employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL) approaches integrating human intelligence and intuition throughout the machine learning life-cycle have gained traction. Leveraging Visual Analytics (VA), effective interfaces can be created to facilitate this human-AI collaboration. This thesis explores the intersection of these fields by developing and investigating "VILOD: A Visual Interactive Labeling tool for Object Detection". VILOD utilizes components such as a t-SNE projection of image features, together with uncertainty heatmaps and model state views. Enabling users to explore data, interpret model states, AL suggestions, and implement diverse sample selection strategies within an iterative HITL workflow for OD. An empirical investigation using comparative use cases demonstrated how VILOD, through its interactive visualizations, facilitates the implementation of distinct labeling strategies by making the model's state and dataset characteristics more interpretable (RQ1). The study showed that different visually-guided labeling strategies employed within VILOD result in competitive OD performance trajectories compared to an automated uncertainty sampling AL baseline (RQ2). This work contributes a novel tool and empirical insight into making the HITL-AL workflow for OD annotation more transparent, manageable, and potentially more effective.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification</title>
<link>https://arxiv.org/abs/2509.05319</link>
<guid>https://arxiv.org/abs/2509.05319</guid>
<content:encoded><![CDATA[
<div> KD, Knowledge Distillation, Adaptive Knowledge Distillation, AKD, ResNet-50, ResNet-18 <br />
<br />
Summary: 
Knowledge distillation (KD) is enhanced through the Adaptive Knowledge Distillation (AKD) framework introduced in this study. The traditional method uses a fixed alpha parameter, but the proposed approach uses a learnable alpha to dynamically adjust the balance between hard-label cross-entropy loss and soft-label distillation loss during training. The AKD framework incorporates a formula that computes alpha based on student-teacher discrepancies, along with a Context-Aware Module (CAM) utilizing MLP + Attention to adaptively reweight class-wise teacher outputs. Experimental results on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as student demonstrate that AKD outperforms fixed-weight KD baselines in terms of accuracy and convergence stability. This framework offers a more effective and efficient approach to knowledge distillation, particularly in scenarios where the optimal trade-off between hard and soft supervision may vary during the training process. <br /> <div>
arXiv:2509.05319v1 Announce Type: new 
Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge from a large teacher network to a smaller student model. Traditional KD uses a fixed balancing factor alpha as a hyperparameter to combine the hard-label cross-entropy loss with the soft-label distillation loss. However, a static alpha is suboptimal because the optimal trade-off between hard and soft supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework. First we try to make alpha as learnable parameter that can be automatically learned and optimized during training. Then we introduce a formula to reflect the gap between the student and the teacher to compute alpha dynamically, guided by student-teacher discrepancies, and further introduce a Context-Aware Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as student demonstrate that our approach achieves superior accuracy compared to fixed-weight KD baselines, and yields more stable convergence.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD</title>
<link>https://arxiv.org/abs/2509.05321</link>
<guid>https://arxiv.org/abs/2509.05321</guid>
<content:encoded><![CDATA[
<div> framework, Video2EEG-SPGN-Diffusion, SEED-VD dataset, multimodal dataset, EEG signals, video stimuli 

Summary:<br /><br />This paper introduces the open-source framework Video2EEG-SPGN-Diffusion, utilizing the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. An engineering pipeline is presented for aligning video and EEG data pairs, allowing for training large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. A new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels is released, facilitating video-EEG alignment and advancing multimodal research. This framework provides valuable tools for emotion analysis, data augmentation, and brain-computer interface applications, making a significant contribution to both research and engineering fields. <br /><br /> <div>
arXiv:2509.05321v1 Announce Type: new 
Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19</title>
<link>https://arxiv.org/abs/2509.05322</link>
<guid>https://arxiv.org/abs/2509.05322</guid>
<content:encoded><![CDATA[
<div> Randomly Wired Neural Networks (RWNNs), edge-centric network measures, pruning, COVID-19 chest x-ray image classification, compression ratio

Summary: 
Randomly Wired Neural Networks (RWNNs) are studied for their network topology impact on learning efficiency and model performance. The study focuses on three edge-centric network measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC) for compressing RWNNs while maintaining performance. RWNNs are trained for COVID-19 chest x-ray image classification, and the measures are integrated across Erdős-Rényi (ER), Watts-Strogatz (WS), and Barabási-Albert (BA) models. The study compares the pruning performance of the measures in terms of compression ratio and speedup, evaluating FRC's efficiency and effectiveness. Structural properties of the pruned networks are analyzed in terms of modularity and global efficiency, highlighting the trade-off between modular segregation and network efficiency. Results show that FRC-based pruning can simplify RWNNs with computational advantages comparable to ORC. <br /><br />Summary: <div>
arXiv:2509.05322v1 Announce Type: new 
Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for investigating the impact of network topology in deep learning by capturing how different connectivity patterns impact both learning efficiency and model performance. At the same time, they provide a natural framework for exploring edge-centric network measures as tools for pruning and optimization. In this study, we investigate three edge-centric network measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC), to compress RWNNs by selectively retaining important synapses (or edges) while pruning the rest. As a baseline, RWNNs are trained for COVID-19 chest x-ray image classification, aiming to reduce network complexity while preserving performance in terms of accuracy, specificity, and sensitivity. We extend prior work on pruning RWNN using ORC by incorporating two additional edge-centric measures, FRC and EBC, across three network generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the pruning performance of the three measures in terms of compression ratio and theoretical speedup. A central focus of our study is to evaluate whether FRC, which is computationally more efficient than ORC, can achieve comparable pruning effectiveness. Along with performance evaluation, we further investigate the structural properties of the pruned networks through modularity and global efficiency, offering insights into the trade-off between modular segregation and network efficiency in compressed RWNNs. Our results provide initial evidence that FRC-based pruning can effectively simplify RWNNs, offering significant computational advantages while maintaining performance comparable to ORC.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optical Music Recognition of Jazz Lead Sheets</title>
<link>https://arxiv.org/abs/2509.05329</link>
<guid>https://arxiv.org/abs/2509.05329</guid>
<content:encoded><![CDATA[
<div> Dataset, Optical Music Recognition, Jazz Lead Sheets, Handwritten Images, Chords
Summary:<br /><br />In this paper, the authors introduce a new dataset consisting of 293 handwritten jazz lead sheets, including both real and synthetic score images. The dataset addresses the challenge of Optical Music Recognition (OMR) for jazz lead sheets, which contain melody and chords. A novel OMR model is developed specifically for jazz lead sheets, considering the unique characteristics of this type of musical score. The model makes use of pretrained models and synthetic scores to improve performance. Specific tokenisation choices are discussed in relation to the dataset. All code, data, and models are publicly released, providing a valuable resource for future research in OMR for handwritten jazz lead sheets. <div>
arXiv:2509.05329v1 Announce Type: new 
Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness</title>
<link>https://arxiv.org/abs/2509.05333</link>
<guid>https://arxiv.org/abs/2509.05333</guid>
<content:encoded><![CDATA[
<div> Keywords: object recognition, domain shift, synthetic dataset, multimodal evidence, self critique loop

Summary: 
The article discusses the challenges faced by object recognition models in real-world deployments due to domain shifts. These shifts can include changes in image statistics, object pose, occlusions, and visual confusion. To address this issue, the Re-Thinking Vision Language Model (RT-VLM) framework is proposed. It utilizes a synthetic dataset generation pipeline to create images annotated with "4-Clues": bounding boxes, class names, object-level captions, and context-level captions. The Llama 3.2 11B Vision Instruct model is then supervised tuned using this dataset. During inference, a two-stage Re-Thinking scheme is employed, where the model first generates its own clues and then corrects them iteratively. RT-VLM outperforms strong baselines in robustness benchmarks for individual domain shifts, showcasing the effectiveness of integrating structured multimodal evidence with a self-critique loop in improving visual understanding.<br /><br />Summary: <div>
arXiv:2509.05333v1 Announce Type: new 
Abstract: Real world deployments often expose modern object recognition models to domain shifts that precipitate a severe drop in accuracy. Such shifts encompass (i) variations in low level image statistics, (ii) changes in object pose and viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent classes. To mitigate this degradation, we introduce the Re-Thinking Vision Language Model (RT-VLM) framework. The foundation of this framework is a unique synthetic dataset generation pipeline that produces images annotated with "4-Clues": precise bounding boxes, class names, detailed object-level captions, and a comprehensive context-level caption for the entire scene. We then perform parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this resource. At inference time, a two stage Re-Thinking scheme is executed: the model first emits its own four clues, then re examines these responses as evidence and iteratively corrects them. Across robustness benchmarks that isolate individual domain shifts, RT-VLM consistently surpasses strong baselines. These findings indicate that the integration of structured multimodal evidence with an explicit self critique loop constitutes a promising route toward reliable and transferable visual understanding.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices</title>
<link>https://arxiv.org/abs/2509.05334</link>
<guid>https://arxiv.org/abs/2509.05334</guid>
<content:encoded><![CDATA[
<div> Keywords: sports, badminton, performance metrics, smartphone technology, video-based kinematic

Summary: 
In the realm of sports, performance metrics play a crucial role in enhancing athlete development. However, the technology required to capture such metrics has traditionally been costly and complicated, limiting access for amateur and recreational players. This paper introduces a innovative system aimed at measuring smash speed in badminton using readily available smartphone technology. By employing a custom-trained YOLOv5 model for shuttlecock detection and a Kalman filter for trajectory tracking, the system efficiently calculates the shuttlecock's velocity from standard video recordings. Through the implementation of a video-based kinematic speed estimation method with spatiotemporal scaling, this user-friendly mobile application democratizes access to advanced performance analytics in badminton, enabling players of all levels to analyze and enhance their gameplay. <br /><br />Summary: <div>
arXiv:2509.05334v1 Announce Type: new 
Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial feedback for athlete development. However, the technology to capture these metrics has historically been expensive, complex, and largely inaccessible to amateur and recreational players. This paper addresses this gap in the context of badminton, one of the world's most popular sports, by introducing a novel, cost-effective, and user-friendly system for measuring smash speed using ubiquitous smartphone technology. Our approach leverages a custom-trained YOLOv5 model for shuttlecock detection, combined with a Kalman filter for robust trajectory tracking. By implementing a video-based kinematic speed estimation method with spatiotemporal scaling, the system automatically calculates the shuttlecock's velocity from a standard video recording. The entire process is packaged into an intuitive mobile application, democratizing access to high-level performance analytics and empowering players at all levels to analyze and improve their game.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research</title>
<link>https://arxiv.org/abs/2509.05335</link>
<guid>https://arxiv.org/abs/2509.05335</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese handwriting, orthographic predictors, phonological factors, OpenHandWrite_Toolbox, large-scale handwriting database

Summary:
Orthographic and phonological factors play a significant role in Chinese handwriting at the character, radical, and stroke levels. The study involved 42 Chinese speakers handwriting 1200 characters using an upgraded OpenHandWrite_Toolbox for data capture and batch-processing. Results from multiple regression analyses revealed that orthographic predictors impacted handwriting preparation and execution, with phonological factors also influencing execution across all levels. These linguistic effects displayed hierarchical attenuation, with the strongest impact at the character level, followed by the radical, and weakest at the stroke level. The findings emphasize the interplay between linguistic components and handwriting at different levels. The large-scale database and toolbox created in this study provide valuable resources for future research in psycholinguistics and neurolinguistics, enabling further investigation into handwriting of characters and sub-characters across various languages.

<br /><br />Summary: <div>
arXiv:2509.05335v1 Announce Type: new 
Abstract: Understanding what linguistic components (e.g., phonological, semantic, and orthographic systems) modulate Chinese handwriting at the character, radical, and stroke levels remains an important yet understudied topic. Additionally, there is a lack of comprehensive tools for capturing and batch-processing fine-grained handwriting data. To address these issues, we constructed a large-scale handwriting database in which 42 Chinese speakers for each handwriting 1200 characters in a handwriting-to-dictation task. Additionally, we enhanced the existing handwriting package and provided comprehensive documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify the experimental design, capture the stroke-level handwriting trajectory, and batch-process handwriting measurements (e.g., latency, duration, and pen-pressure). In analysing our large-scale database, multiple regression results show that orthographic predictors impact handwriting preparation and execution across character, radical, and stroke levels. Phonological factors also influence execution at all three levels. Importantly, these lexical effects demonstrate hierarchical attenuation - they were most pronounced at the character level, followed by the radical, and were weakest at the stroke levels. These findings demonstrate that handwriting preparation and execution at the radical and stroke levels are closely intertwined with linguistic components. This database and toolbox offer valuable resources for future psycholinguistic and neurolinguistic research on the handwriting of characters and sub-characters across different languages.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</title>
<link>https://arxiv.org/abs/2509.05337</link>
<guid>https://arxiv.org/abs/2509.05337</guid>
<content:encoded><![CDATA[
<div> Keywords: anticipatory fall detection, dynamic graph neural networks, long short-term memory, gait classification, transient state

Summary:
The paper introduces a novel approach for anticipatory fall detection in humans using a hybrid model of Dynamic Graph Neural Networks (DGNN) and Long Short-Term Memory (LSTM) networks. By separating the tasks of motion prediction and gait classification, the model achieves high accuracy in anticipating falls. Real-time skeletal features from video sequences are utilized for input, with the DGNN classifying gait states and the LSTM predicting human movements. This decoupled approach outperforms models solely relying on DGNN and those in existing literature, demonstrating superior prediction error and recognition accuracy. The method not only facilitates early fall detection but also allows for monitoring the transient state between stability and an impending fall, providing valuable insights for improving assistive robotic systems. Overall, the proposed model offers a promising solution for enhancing fall detection capabilities in assistive technology.<br /><br />Summary: <div>
arXiv:2509.05337v1 Announce Type: new 
Abstract: Detecting and preventing falls in humans is a critical component of assistive robotic systems. While significant progress has been made in detecting falls, the prediction of falls before they happen, and analysis of the transient state between stability and an impending fall remain unexplored. In this paper, we propose a anticipatory fall detection method that utilizes a hybrid model combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory (LSTM) networks that decoupled the motion prediction and gait classification tasks to anticipate falls with high accuracy. Our approach employs real-time skeletal features extracted from video sequences as input for the proposed model. The DGNN acts as a classifier, distinguishing between three gait states: stable, transient, and fall. The LSTM-based network then predicts human movement in subsequent time steps, enabling early detection of falls. The proposed model was trained and validated using the OUMVLP-Pose and URFD datasets, demonstrating superior performance in terms of prediction error and recognition accuracy compared to models relying solely on DGNN and models from literature. The results indicate that decoupling prediction and classification improves performance compared to addressing the unified problem using only the DGNN. Furthermore, our method allows for the monitoring of the transient state, offering valuable insights that could enhance the functionality of advanced assistance systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging</title>
<link>https://arxiv.org/abs/2509.05340</link>
<guid>https://arxiv.org/abs/2509.05340</guid>
<content:encoded><![CDATA[
<div> Keywords: brain tumor segmentation, MRI, K-Means algorithm, Fuzzy C-Means, evaluation metrics

Summary:<br />
Segmentation of brain tumors from MRI is challenging due to heterogeneous morphology and intensity distributions. Accurate delineation is crucial for treatment planning and monitoring. A comparative analysis of K-Means and FCM clustering paradigms was conducted using the BraTS2020 dataset with preprocessing. K-Means assigns each pixel to a single cluster, while FCM allows partial memberships. Results showed K-Means is faster (0.3s/image) but less accurate (DSC 0.43), while FCM is more precise (DSC 0.67) but slower (1.3s/image). The study highlights a trade-off between computational efficiency and segmentation accuracy. <br /><br /> <div>
arXiv:2509.05340v1 Announce Type: new 
Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a pivotal challenge in medical image analysis due to the heterogeneous nature of tumor morphology and intensity distributions. Accurate delineation of tumor boundaries is critical for clinical decision-making, radiotherapy planning, and longitudinal disease monitoring. In this study, we perform a comprehensive comparative analysis of two major clustering paradigms applied in MRI tumor segmentation: hard clustering, exemplified by the K-Means algorithm, and soft clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each pixel strictly to a single cluster, FCM introduces partial memberships, meaning each pixel can belong to multiple clusters with varying degrees of association. Experimental validation was performed using the BraTS2020 dataset, incorporating pre-processing through Gaussian filtering and Contrast Limited Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice Similarity Coefficient (DSC) and processing time, which collectively demonstrated that K-Means achieved superior speed with an average runtime of 0.3s per image, whereas FCM attained higher segmentation accuracy with an average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher computational cost (1.3s per image). These results highlight the inherent trade-off between computational efficiency and boundary precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling imbalance and few-sample size in ML based Onion disease classification</title>
<link>https://arxiv.org/abs/2509.05341</link>
<guid>https://arxiv.org/abs/2509.05341</guid>
<content:encoded><![CDATA[
<div> Keywords: precision agriculture, deep learning, multi-class classification, attention-based modules, data augmentation

Summary:<br /><br />
Accurate classification of pests and diseases is crucial in precision agriculture for targeted interventions and prevention of spread. Existing methods often focus on binary classification, limiting practical applications where specific identification is necessary. A robust deep learning model for multi-class classification of onion crop diseases and pests is proposed. Enhancing a pre-trained Convolutional Neural Network (CNN) with attention-based modules and utilizing comprehensive data augmentation techniques helps mitigate class imbalance. The model achieves an impressive 96.90% overall accuracy and 0.96 F1 score on real-world field image datasets, outperforming other approaches. This integrated approach demonstrates the effectiveness of deep learning in accurately identifying and categorizing onion crop diseases and pests in precision agriculture applications. 

Summary: <div>
arXiv:2509.05341v1 Announce Type: new 
Abstract: Accurate classification of pests and diseases plays a vital role in precision agriculture, enabling efficient identification, targeted interventions, and preventing their further spread. However, current methods primarily focus on binary classification, which limits their practical applications, especially in scenarios where accurately identifying the specific type of disease or pest is essential. We propose a robust deep learning based model for multi-class classification of onion crop diseases and pests. We enhance a pre-trained Convolutional Neural Network (CNN) model by integrating attention based modules and employing comprehensive data augmentation pipeline to mitigate class imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1 score on real-world field image dataset. This model gives better results than other approaches using the same datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Velocity Rectified Flow for Text-to-Image Editing</title>
<link>https://arxiv.org/abs/2509.05342</link>
<guid>https://arxiv.org/abs/2509.05342</guid>
<content:encoded><![CDATA[
<div> Keywords: Delta Velocity Rectified Flow, text-to-image editing, distillation-based method, time-dependent shift, editing quality

Summary:
Delta Velocity Rectified Flow (DVRF) is a new framework for text-to-image editing that addresses over-smoothing artifacts by modeling the discrepancy between source and target velocity fields. It includes a time-dependent shift term to improve alignment with the target distribution. DVRF bridges diffusion optimization and rectified-flow optimization, with a linear schedule for the shift term enabling a principled interpretation. Experimental results show that DVRF outperforms prior methods in editing quality, fidelity, and controllability without requiring architectural changes. The approach is efficient and widely applicable to various text-to-image editing tasks. The code for DVRF is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2509.05342v1 Announce Type: new 
Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis</title>
<link>https://arxiv.org/abs/2509.05343</link>
<guid>https://arxiv.org/abs/2509.05343</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, medical imaging, attention mechanisms, CNN architectures, classification accuracy

Summary: 
This study focuses on integrating attention mechanisms into five popular CNN architectures to enhance their ability to capture fine-grained features in medical images. The models were tested on brain tumor MRI and histopathological datasets, showing that attention-augmented CNNs outperformed baseline models across all metrics. EfficientNetB5 with hybrid attention stood out as the most effective, demonstrating significant improvements in classification accuracy. The attention mechanisms not only improved accuracy but also enhanced feature localization, promoting better generalization across different imaging modalities. This systematic comparison of attention modules in various CNN architectures provides valuable insights for developing robust and clinically applicable deep learning decision support systems.<br /><br />Summary: <div>
arXiv:2509.05343v1 Announce Type: new 
Abstract: Deep learning has become a powerful tool for medical image analysis; however, conventional Convolutional Neural Networks (CNNs) often fail to capture the fine-grained and complex features critical for accurate diagnosis. To address this limitation, we systematically integrate attention mechanisms into five widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3, DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient regions and improve discriminative performance. Specifically, each baseline model is augmented with either a Squeeze and Excitation block or a hybrid Convolutional Block Attention Module, allowing adaptive recalibration of channel and spatial feature representations. The proposed models are evaluated on two distinct medical imaging datasets, a brain tumor MRI dataset comprising multiple tumor subtypes, and a Products of Conception histopathological dataset containing four tissue categories. Experimental results demonstrate that attention augmented CNNs consistently outperform baseline architectures across all metrics. In particular, EfficientNetB5 with hybrid attention achieves the highest overall performance, delivering substantial gains on both datasets. Beyond improved classification accuracy, attention mechanisms enhance feature localization, leading to better generalization across heterogeneous imaging modalities. This work contributes a systematic comparative framework for embedding attention modules in diverse CNN architectures and rigorously assesses their impact across multiple medical imaging tasks. The findings provide practical insights for the development of robust, interpretable, and clinically applicable deep learning based decision support systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset</title>
<link>https://arxiv.org/abs/2509.05348</link>
<guid>https://arxiv.org/abs/2509.05348</guid>
<content:encoded><![CDATA[
<div> object detection, solar panels, defects, contaminants, dataset

Summary:
This study evaluates five object detection models for identifying defects and contaminants on solar panels. A custom dataset in COCO format was created for this purpose. The models assessed include YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer. The evaluation criteria include mean Average Precision (mAP), precision, recall, and inference speed. The results show the trade-offs between detection accuracy and computational efficiency, providing insights into the strengths and limitations of each model. The findings offer valuable guidance for choosing suitable detection methods for solar panel monitoring and maintenance. The dataset developed for this study will be publicly available at the provided GitHub link. 

<br /><br />Summary: <div>
arXiv:2509.05348v1 Announce Type: new 
Abstract: Timely and accurate detection of defects and contaminants in solar panels is critical for maintaining the efficiency and reliability of photovoltaic systems. This study presents a comprehensive evaluation of five state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer, for identifying physical and electrical defects as well as surface contaminants such as dust, dirt, and bird droppings on solar panels. A custom dataset, annotated in the COCO format and specifically designed for solar panel defect and contamination detection, was developed alongside a user interface to train and evaluate the models. The performance of each model is assessed and compared based on mean Average Precision (mAP), precision, recall, and inference speed. The results demonstrate the trade-offs between detection accuracy and computational efficiency, highlighting the relative strengths and limitations of each model. These findings provide valuable guidance for selecting appropriate detection approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Instance Segmentation with Superpixels</title>
<link>https://arxiv.org/abs/2509.05352</link>
<guid>https://arxiv.org/abs/2509.05352</guid>
<content:encoded><![CDATA[
<div> Instance segmentation, self-supervised features, MultiCut algorithm, mask filter, superpixel-guided mask loss.
<br />
The article presents a new framework for instance segmentation that does not require human annotations. It utilizes a MultiCut algorithm and mask filter to obtain high-quality coarse masks. A novel superpixel-guided mask loss, involving hard and soft loss components, is used for training the segmentation network. Additionally, a self-training process with an adaptive loss is introduced to enhance predicted mask quality. Experimental results on public datasets demonstrate that the proposed framework surpasses existing state-of-the-art methods in instance segmentation and object detection tasks.
<br /><br />Summary: <div>
arXiv:2509.05352v1 Announce Type: new 
Abstract: Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Structure Preserving Neural Networks for cell biomechanics</title>
<link>https://arxiv.org/abs/2509.05388</link>
<guid>https://arxiv.org/abs/2509.05388</guid>
<content:encoded><![CDATA[
<div> Keywords: Cell biomechanics, Structure Preserving Neural Networks, Artificial Neural Networks, Computer Vision techniques, Mitosis event prediction

Summary: 
Cell biomechanics play a crucial role in various biological processes, but understanding the complex interactions and decision-making mechanisms within cell clusters remains a challenge. A new approach combining Structure Preserving Neural Networks with Artificial Neural Networks and Computer Vision techniques offers a comprehensive model to study cell movements and predict complete cell trajectories accurately. The model, tested on simulated and real cell migration cases, shows promising results in predicting cell behavior. Additionally, a mitosis event prediction model based on Neural Networks architecture is developed using observed features. This integrated approach provides a deeper understanding of cell behavior and paves the way for further research in cell biomechanics and related fields.<br /><br />Summary: <div>
arXiv:2509.05388v1 Announce Type: new 
Abstract: Cell biomechanics involve a great number of complex phenomena that are fundamental to the evolution of life itself and other associated processes, ranging from the very early stages of embryo-genesis to the maintenance of damaged structures or the growth of tumors. Given the importance of such phenomena, increasing research has been dedicated to their understanding, but the many interactions between them and their influence on the decisions of cells as a collective network or cluster remain unclear. We present a new approach that combines Structure Preserving Neural Networks, which study cell movements as a purely mechanical system, with other Machine Learning tools (Artificial Neural Networks), which allow taking into consideration environmental factors that can be directly deduced from an experiment with Computer Vision techniques. This new model, tested on simulated and real cell migration cases, predicts complete cell trajectories following a roll-out policy with a high level of accuracy. This work also includes a mitosis event prediction model based on Neural Networks architectures which makes use of the same observed features.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding</title>
<link>https://arxiv.org/abs/2509.05431</link>
<guid>https://arxiv.org/abs/2509.05431</guid>
<content:encoded><![CDATA[
<div> Efficient Multi-Scale Convolutional Attention Decoder, Brain Tumor Segmentation, MRI Scans, Computational Efficiency, BraTs2020 Dataset  
Summary:  
- Brain tumor segmentation is essential for medical image analysis, particularly MRI scans, requiring precise delineation of tumor regions from healthy brain tissue.  
- A new efficient multi-scale convolutional attention decoder, EMCAD, was developed to optimize brain tumor segmentation performance and computational efficiency on the BraTs2020 dataset.  
- The model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 +/- 0.015 during training.  
- The model showed moderate performance and consistency across the validation set without signs of overfitting.  
- EMCAD provides a promising solution for efficient and effective brain tumor segmentation in scenarios with limited computational resources.  
<br /><br />Summary: <div>
arXiv:2509.05431v1 Announce Type: new 
Abstract: Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAVAE-Effective Frequency Aware Latent Tokenizer</title>
<link>https://arxiv.org/abs/2509.05441</link>
<guid>https://arxiv.org/abs/2509.05441</guid>
<content:encoded><![CDATA[
<div> latent generative models, image synthesis, wavelet-based, frequency-aware, variational autoencoder

Summary:
The article explores the limitations of existing latent tokenizers used in image synthesis models, revealing a bias towards low-frequency information that results in over-smoothed and visually unappealing outputs. To address this issue, the authors propose a novel approach called frequency-aware variational autoencoder (FA-VAE) that separates the optimization of low and high-frequency components. By decoupling these frequencies, fine textures can be reconstructed more effectively while preserving the overall structure of the image. This approach bridges the fidelity gap in current latent tokenizers, emphasizing the importance of frequency-aware optimization for realistic image representation. The FA-VAE framework has implications for a range of applications including content creation, neural rendering, and medical imaging. <div>
arXiv:2509.05441v1 Announce Type: new 
Abstract: Latent generative models have shown remarkable progress in high-fidelity image synthesis, typically using a two-stage training process that involves compressing images into latent embeddings via learned tokenizers in the first stage. The quality of generation strongly depends on how expressive and well-optimized these latent embeddings are. While various methods have been proposed to learn effective latent representations, the reconstructed images often lack realism, particularly in textured regions with sharp transitions, due to loss of fine details governed by high frequencies. We conduct a detailed frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers and show that conventional objectives inherently prioritize low-frequency reconstruction, often at the expense of high-frequency fidelity. Our analysis reveals these latent tokenizers exhibit a bias toward low-frequency information, when jointly optimized, leading to over-smoothed outputs and visual artifacts that diminish perceptual quality. To address this, we propose a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework that explicitly decouples the optimization of low- and high-frequency components. This decoupling enables improved reconstruction of fine textures while preserving global structure. Our approach bridges the fidelity gap in current latent tokenizers and emphasizes the importance of frequency-aware optimization for realistic image representation, with broader implications for applications in content creation, neural rendering, and medical imaging.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's</title>
<link>https://arxiv.org/abs/2509.05446</link>
<guid>https://arxiv.org/abs/2509.05446</guid>
<content:encoded><![CDATA[
<div> pruning, Deep Convolutional Neural Networks, filter importance, model complexity, compression

Summary:<br />
This paper introduces Differential Sensitivity Fusion Pruning, a single shot filter pruning framework for Deep Convolutional Neural Networks. It focuses on evaluating stability and redundancy of filter importance scores using differential sensitivity scores computed from multiple metrics. The method efficiently prunes filters with inconsistent importance, reducing model complexity by over 80 percent while maintaining high accuracy. Experimental results demonstrate superiority over traditional heuristics in compression and generalization, retaining up to 98.23 percent baseline accuracy at 70 percent pruning. The approach is deterministic and requires only a single forward-backward pass, making it suitable for practical deployment. Differential Sensitivity Fusion Pruning offers an effective solution for scalable and adaptive model compression, enabling efficient deployment on edge and mobile platforms.<br /> <div>
arXiv:2509.05446v1 Announce Type: new 
Abstract: Deep Convolutional Neural Networks have achieved state of the art performance across various computer vision tasks, however their practical deployment is limited by computational and memory overhead. This paper introduces Differential Sensitivity Fusion Pruning, a novel single shot filter pruning framework that focuses on evaluating the stability and redundancy of filter importance scores across multiple criteria. Differential Sensitivity Fusion Pruning computes a differential sensitivity score for each filter by fusing the discrepancies among gradient based sensitivity, first order Taylor expansion, and KL divergence of activation distributions. An exponential scaling mechanism is applied to emphasize filters with inconsistent importance across metrics, identifying candidates that are structurally unstable or less critical to the model performance. Unlike iterative or reinforcement learning based pruning strategies, Differential Sensitivity Fusion Pruning is efficient and deterministic, requiring only a single forward-backward pass for scoring and pruning. Extensive experiments across varying pruning rates between 50 to 70 percent demonstrate that Differential Sensitivity Fusion Pruning significantly reduces model complexity, achieving over 80 percent Floating point Operations Per Seconds reduction while maintaining high accuracy. For instance, at 70 percent pruning, our approach retains up to 98.23 percent of baseline accuracy, surpassing traditional heuristics in both compression and generalization. The proposed method presents an effective solution for scalable and adaptive Deep Convolutional Neural Networks compression, paving the way for efficient deployment on edge and mobile platforms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging</title>
<link>https://arxiv.org/abs/2509.05483</link>
<guid>https://arxiv.org/abs/2509.05483</guid>
<content:encoded><![CDATA[
<div> dataset, deep learning, registration, X-ray images, computer vision <br />
Summary:<br />Veriserum is a new open-source dataset designed for deep learning registration in dual-plane fluoroscopic analysis. It consists of 110,000 X-ray images of 10 knee implant pair combinations captured during 1,600 trials, annotated with ground-truth poses for algorithm training. The dataset includes images of poses related to everyday activities and provides calibration tools for accurate analysis. Key features include dual-plane images and tools for applications like 2D/3D image registration, image segmentation, X-ray distortion correction, and 3D reconstruction. The dataset is freely accessible and aims to advance computer vision and medical imaging research by providing a reproducible benchmark for algorithm development and evaluation. The Veriserum dataset can be accessed online through the ETH Zürich Research Collections, allowing researchers to utilize the data for improving medical imaging techniques. <br /><br />Summary: <div>
arXiv:2509.05483v1 Announce Type: new 
Abstract: Veriserum is an open-source dataset designed to support the training of deep learning registration for dual-plane fluoroscopic analysis. It comprises approximately 110,000 X-ray images of 10 knee implant pair combinations (2 femur and 5 tibia implants) captured during 1,600 trials, incorporating poses associated with daily activities such as level gait and ramp descent. Each image is annotated with an automatically registered ground-truth pose, while 200 images include manually registered poses for benchmarking.
  Key features of Veriserum include dual-plane images and calibration tools. The dataset aims to support the development of applications such as 2D/3D image registration, image segmentation, X-ray distortion correction, and 3D reconstruction. Freely accessible, Veriserum aims to advance computer vision and medical imaging research by providing a reproducible benchmark for algorithm development and evaluation. The Veriserum dataset used in this study is publicly available via https://movement.ethz.ch/data-repository/veriserum.html, with the data stored at ETH Z\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
<link>https://arxiv.org/abs/2509.05490</link>
<guid>https://arxiv.org/abs/2509.05490</guid>
<content:encoded><![CDATA[
<div> YOLOv8, YOLOv10, transfer learning, layer freezing, object detection 
Summary: 
This research investigates the impact of different layer-freezing configurations on YOLOv8 and YOLOv10 architectures for real-time object detection in resource-constrained environments like UAVs. Various freezing strategies are analyzed across datasets representing critical infrastructure monitoring, utilizing gradient analysis and visual explanations to understand training dynamics. Results show that the optimal freezing strategy depends on dataset characteristics, with freezing the backbone preserving general features and shallower freezes handling class imbalances effectively. These configurations reduce GPU memory consumption by up to 28% and achieve high mAP@50 scores compared to full fine-tuning. Gradient analysis demonstrates distinct convergence patterns for moderately frozen models, offering practical guidelines for balanced transfer learning in limited resource scenarios. Ultimately, this work provides evidence-based recommendations for selecting freezing strategies in object detection tasks. 
<br /><br />Summary: <div>
arXiv:2509.05490v1 Announce Type: new 
Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object detection. However, deploying it in resource-constrained environments such as unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although layer freezing is a common technique, the specific impact of various freezing configurations on contemporary YOLOv8 and YOLOv10 architectures remains unexplored, particularly with regard to the interplay between freezing depth, dataset characteristics, and training dynamics. This research addresses this gap by presenting a detailed analysis of layer-freezing strategies. We systematically investigate multiple freezing configurations across YOLOv8 and YOLOv10 variants using four challenging datasets that represent critical infrastructure monitoring. Our methodology integrates a gradient behavior analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper insights into training dynamics under different freezing strategies. Our results reveal that there is no universal optimal freezing strategy but, rather, one that depends on the properties of the data. For example, freezing the backbone is effective for preserving general-purpose features, while a shallower freeze is better suited to handling extreme class imbalance. These configurations reduce graphics processing unit (GPU) memory consumption by up to 28% compared to full fine-tuning and, in some cases, achieve mean average precision (mAP@50) scores that surpass those of full fine-tuning. Gradient analysis corroborates these findings, showing distinct convergence patterns for moderately frozen models. Ultimately, this work provides empirical findings and practical guidelines for selecting freezing strategies. It offers a practical, evidence-based approach to balanced transfer learning for object detection in scenarios with limited resources.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection</title>
<link>https://arxiv.org/abs/2509.05512</link>
<guid>https://arxiv.org/abs/2509.05512</guid>
<content:encoded><![CDATA[
<div> Quaternion Approximate Networks, image classification, object detection, deep learning, rotation equivariant <br />
Summary: <br />
Quaternion Approximate Networks (QUAN) is a new deep learning framework that uses quaternion algebra for rotation equivariant image classification and object detection. QUAN approximates quaternion convolution through real-valued operations, maintaining geometric properties while allowing for efficient implementation with custom CUDA kernels. The framework introduces Independent Quaternion Batch Normalization (IQBN) for training stability and extends quaternion operations to spatial attention mechanisms. In classification tasks, QUAN achieves higher accuracy with fewer parameters and faster convergence compared to existing models. For object detection, QUAN demonstrates improved parameter efficiency and rotation handling over Convolutional Neural Networks (CNNs), setting the state-of-the-art for quaternion CNNs in this area. These results showcase QUAN's potential for deployment in resource-constrained robotic systems that require rotation-aware perception and applications in various domains. <br /> <div>
arXiv:2509.05512v1 Announce Type: new 
Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep learning framework that leverages quaternion algebra for rotation equivariant image classification and object detection. Unlike conventional quaternion neural networks attempting to operate entirely in the quaternion domain, QUAN approximates quaternion convolution through Hamilton product decomposition using real-valued operations. This approach preserves geometric properties while enabling efficient implementation with custom CUDA kernels. We introduce Independent Quaternion Batch Normalization (IQBN) for training stability and extend quaternion operations to spatial attention mechanisms. QUAN is evaluated on image classification (CIFAR-10/100, ImageNet), object detection (COCO, DOTA), and robotic perception tasks. In classification tasks, QUAN achieves higher accuracy with fewer parameters and faster convergence compared to existing convolution and quaternion-based models. For objection detection, QUAN demonstrates improved parameter efficiency and rotation handling over standard Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion CNNs in this downstream task. These results highlight its potential for deployment in resource-constrained robotic systems requiring rotation-aware perception and application in other domains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2509.05513</link>
<guid>https://arxiv.org/abs/2509.05513</guid>
<content:encoded><![CDATA[
<div> dataset, egocentric, manipulation, hand-pose annotations, action primitives  
Summary:  
OpenEgo is a new dataset designed to facilitate imitation learning from egocentric human videos. It addresses the lack of fine-grained action descriptions and dexterous hand annotations in existing corpora by providing standardized hand-pose annotations and intention-aligned action primitives across 1107 hours of video covering 290 manipulation tasks in 600+ environments. The dataset unifies hand-pose layouts and includes descriptive, timestamped action primitives. To showcase its utility, language-conditioned imitation-learning policies are trained to predict dexterous hand trajectories using OpenEgo. This dataset aims to lower the barriers to learning dexterous manipulation from egocentric video and support reproducible research in vision-language-action learning. Resources and instructions for OpenEgo will be made available at www.openegocentric.com.  
<br /><br />Summary: <div>
arXiv:2509.05513v1 Announce Type: new 
Abstract: Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.05515</link>
<guid>https://arxiv.org/abs/2509.05515</guid>
<content:encoded><![CDATA[
<div> Keywords: open-vocabulary language features, 3D Gaussians, visibility-aware gate, multi-view inconsistencies, geometric median<br />
Summary:
VALA is a new method that addresses issues in distilling language features from 2D images into 3D Gaussians. It tackles the problem of background Gaussians having the same feature as foreground ones by computing marginal contributions for each ray and applying a visibility-aware gate. Additionally, VALA deals with multi-view inconsistencies by using a streaming weighted geometric median in cosine space to merge noisy features. The method improves open-vocabulary localization and segmentation across datasets, surpassing existing works. VALA is lightweight, fast, and memory-efficient, providing a robust and view-consistent language feature embedding. <div>
arXiv:2509.05515v1 Announce Type: new 
Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D Gaussians has attracted significant attention. Although existing methods achieve impressive language-based interactions of 3D scenes, we observe two fundamental issues: background Gaussians contributing negligibly to a rendered pixel get the same feature as the dominant foreground ones, and multi-view inconsistencies due to view-specific noise in language embeddings. We introduce Visibility-Aware Language Aggregation (VALA), a lightweight yet effective method that computes marginal contributions for each ray and applies a visibility-aware gate to retain only visible Gaussians. Moreover, we propose a streaming weighted geometric median in cosine space to merge noisy multi-view features. Our method yields a robust, view-consistent language feature embedding in a fast and memory-efficient manner. VALA improves open-vocabulary localization and segmentation across reference datasets, consistently surpassing existing works.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</title>
<link>https://arxiv.org/abs/2509.05543</link>
<guid>https://arxiv.org/abs/2509.05543</guid>
<content:encoded><![CDATA[
<div> Representation learning, human action segmentation, contrastive learning, data augmentation, dual-surrogate contrastive learning<br />
Summary: <br />
This paper presents a novel contrastive representation learning framework for enhancing human action segmentation. The framework focuses on leveraging multi-scale representations and cross-sequence variations by pre-training on trimmed skeleton sequences. A data augmentation strategy called 'Shuffle and Warp' is introduced to exploit diverse multi-action permutations. Two surrogate tasks, Cross Permutation Contrasting (CPC) and Relative Order Reasoning (ROR), are proposed in the contrastive learning process to learn intra-class similarities and inter-class contexts. The Dual-Surrogate Contrastive Learning (DuoCLR) network, optimized for action segmentation, shows significant performance improvements over existing methods in multi-class and multi-label action segmentation tasks. Ablation studies are conducted to assess the individual contributions of each component of the proposed approach. <div>
arXiv:2509.05543v1 Announce Type: new 
Abstract: In this paper, a contrastive representation learning framework is proposed to enhance human action segmentation via pre-training using trimmed (single action) skeleton sequences. Unlike previous representation learning works that are tailored for action recognition and that build upon isolated sequence-wise representations, the proposed framework focuses on exploiting multi-scale representations in conjunction with cross-sequence variations. More specifically, it proposes a novel data augmentation strategy, 'Shuffle and Warp', which exploits diverse multi-action permutations. The latter effectively assists two surrogate tasks that are introduced in contrastive learning: Cross Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In optimization, CPC learns intra-class similarities by contrasting representations of the same action class across different permutations, while ROR reasons about inter-class contexts by predicting relative mapping between two permutations. Together, these tasks enable a Dual-Surrogate Contrastive Learning (DuoCLR) network to learn multi-scale feature representations optimized for action segmentation. In experiments, DuoCLR is pre-trained on a trimmed skeleton dataset and evaluated on an untrimmed dataset where it demonstrates a significant boost over state-the-art comparatives in both multi-class and multi-label action segmentation tasks. Lastly, ablation studies are conducted to evaluate the effectiveness of each component of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation</title>
<link>https://arxiv.org/abs/2509.05554</link>
<guid>https://arxiv.org/abs/2509.05554</guid>
<content:encoded><![CDATA[
<div> Sparse event cameras Motion deblurring RED network Robustness Disentangled representation<br />
Summary:<br />
Event cameras capture sparse motion information at high temporal resolution, making them promising for motion deblurring. However, the inherent incompleteness of event streams due to sensor thresholding compromises motion priors. To address this, a Robust Event-guided Deblurring (RED) network with modality-specific disentangled representation is proposed. A Robustness-Oriented Perturbation Strategy (RPS) introduces random masking to events to enhance robustness under various conditions. An OmniAttention module explicitly models correlations within and between motion and modalities. Interactive modules enhance motion-sensitive areas in blurry images and inject context into incomplete event representations. RED achieves top performance in accuracy and robustness on synthetic and real data, demonstrating its effectiveness in overcoming challenges in event-guided deblurring. <br /> <div>
arXiv:2509.05554v1 Announce Type: new 
Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion information, demonstrating great potential for motion deblurring. Existing methods focus on cross-modal interaction, overlooking the inherent incompleteness of event streams, which arises from the trade-off between sensitivity and noise introduced by the thresholding mechanism of Dynamic Vision Sensors (DVS). Such degradation compromises the integrity of motion priors and limits the effectiveness of event-guided deblurring. To tackle these challenges, we propose a Robust Event-guided Deblurring (RED) network with modality-specific disentangled representation. First, we introduce a Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to events, which exposes RED to incomplete patterns and then foster robustness against various unknown scenario conditions.Next, a disentangled OmniAttention is presented to explicitly model intra-motion, inter-motion, and cross-modality correlations from two inherently distinct but complementary sources: blurry images and partially disrupted events. Building on these reliable features, two interactive modules are designed to enhance motion-sensitive areas in blurry images and inject semantic context into incomplete event representations. Extensive experiments on synthetic and real-world datasets demonstrate RED consistently achieves state-of-the-art performance in both accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Aware Post-Training Quantization for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.05576</link>
<guid>https://arxiv.org/abs/2509.05576</guid>
<content:encoded><![CDATA[
<div> quantization, neural network, compression, parameter sensitivity analysis, edge computing

Summary:
- The paper introduces an efficient post-training quantization (PTQ) method guided by parameter sensitivity analysis.
- The approach prioritizes high-sensitivity parameters for quantization and uses unquantized low-sensitivity parameters to compensate for errors, mitigating accuracy loss.
- By utilizing column-wise clustering of parameter sensitivity, a row-parallel quantization framework is introduced, reducing computational complexity significantly.
- Experimental results on ResNet-50 and YOLOv5s show a 20-200-fold speedup in quantization over baseline methods, with a mean accuracy loss below 0.3%.
- This method balances efficiency and accuracy, making it suitable for resource-constrained edge computing and real-time inference scenarios.

<br /><br />Summary: <div>
arXiv:2509.05576v1 Announce Type: new 
Abstract: Model quantization reduces neural network parameter precision to achieve compression, but often compromises accuracy. Existing post-training quantization (PTQ) methods employ iterative parameter updates to preserve accuracy under high compression ratios, incurring significant computational complexity and resource overhead, which limits applicability in resource-constrained edge computing and real-time inference scenarios. This paper proposes an efficient PTQ method guided by parameter sensitivity analysis. The approach prioritizes quantization of high-sensitivity parameters, leveraging unquantized low-sensitivity parameters to compensate for quantization errors, thereby mitigating accuracy degradation. Furthermore, by exploiting column-wise clustering of parameter sensitivity, the method introduces a row-parallel quantization framework with a globally shared inverse Hessian matrix update mechanism, reducing computational complexity by an order of magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a 20-200-fold quantization speedup over the Optimal Brain Quantization baseline, with mean accuracy loss below 0.3%, confirming the method's efficacy in balancing efficiency and accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction and Reenactment Separated Method for Realistic Gaussian Head</title>
<link>https://arxiv.org/abs/2509.05582</link>
<guid>https://arxiv.org/abs/2509.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian head, reconstruction, reenactment, avatar, high frame-rate rendering 

Summary: 
- The paper introduces a reconstruction and reenactment framework for 3D Gaussian heads that can generate avatars from a single portrait image input.
- A large-scale one-shot Gaussian head generator is developed using WebSSL, with a two-stage training approach improving generalization and texture reconstruction.
- An ultra-lightweight Gaussian avatar driven by control signals achieves high frame-rate rendering at 90 FPS with a resolution of 512x512.
- The framework follows the scaling law, showing improved performance with increased parameter scale without affecting driving efficiency.
- Extensive experiments demonstrate that the proposed approach surpasses current state-of-the-art methods in both quantitative and qualitative evaluations. 

<br /><br />Summary: <div>
arXiv:2509.05582v1 Announce Type: new 
Abstract: In this paper, we explore a reconstruction and reenactment separated framework for 3D Gaussians head, which requires only a single portrait image as input to generate controllable avatar. Specifically, we developed a large-scale one-shot gaussian head generator built upon WebSSL and employed a two-stage training approach that significantly enhances the capabilities of generalization and high-frequency texture reconstruction. During inference, an ultra-lightweight gaussian avatar driven by control signals enables high frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further demonstrate that the proposed framework follows the scaling law, whereby increasing the parameter scale of the reconstruction module leads to improved performance. Moreover, thanks to the separation design, driving efficiency remains unaffected. Finally, extensive quantitative and qualitative experiments validate that our approach outperforms current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios</title>
<link>https://arxiv.org/abs/2509.05592</link>
<guid>https://arxiv.org/abs/2509.05592</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence Generated Content, Face Forgery, Deepfake detection, Multi-dimensional Face Forgery Image dataset, Real-world scenarios<br />
<br />
Summary: 
The article introduces the Multi-dimensional Face Forgery Image (MFFI) dataset, designed to address the limitations of current Deepfake detection methods. The dataset focuses on enhancing realism in four key areas: wider forgery methods, varied facial scenes, diversified authentic data, and multi-level degradation operations. With 50 different forgery methods and over 1 million image samples, MFFI outperforms existing datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. The benchmark evaluations validate the technical advancement and practical utility of MFFI in simulating real-world conditions. The dataset, along with additional details, is publicly available on GitHub for research and development purposes. <br /><br />Summary: <div>
arXiv:2509.05592v1 Announce Type: new 
Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have enabled increasingly sophisticated face forgeries, posing a significant threat to social security. However, current Deepfake detection methods are limited by constraints in existing datasets, which lack the diversity necessary in real-world scenarios. Specifically, these data sets fall short in four key areas: unknown of advanced forgery techniques, variability of facial scenes, richness of real data, and degradation of real-world propagation. To address these challenges, we propose the Multi-dimensional Face Forgery Image (\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation Operations. MFFI integrates $50$ different forgery methods and contains $1024K$ image samples. Benchmark evaluations show that MFFI outperforms existing public datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. These results validate the technical advance and practical utility of MFFI in simulating real-world conditions. The dataset and additional details are publicly available at {https://github.com/inclusionConf/MFFI}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
<link>https://arxiv.org/abs/2509.05604</link>
<guid>https://arxiv.org/abs/2509.05604</guid>
<content:encoded><![CDATA[
<div> Video summarization, language-guided, spatiotemporal graph modeling, objects, frames<br />
Summary:<br />
Video summarization is enhanced by considering fine-grained visual entities like objects and utilizing language queries to understand complex videos. The VideoGraph approach models videos as spatiotemporal graphs, with objects and frames represented as nodes connected by edges defining semantic relationships. Language queries are incorporated into node representations to enhance semantic knowledge and distinguish edges based on visual similarity. A recursive strategy refines graphs for accurate keyframe classification. VideoGraph outperforms existing methods in generic and query-focused video summarization for both supervised and unsupervised scenarios. The code is accessible at the provided GitHub repository. <div>
arXiv:2509.05604v1 Announce Type: new 
Abstract: Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at https://github.com/park-jungin/videograph.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning</title>
<link>https://arxiv.org/abs/2509.05606</link>
<guid>https://arxiv.org/abs/2509.05606</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense representations, self-supervised learning, semantic knowledge transfer, Patch-level Kernel Alignment, augmentation strategies

Summary: 
This article introduces a framework for dense representation learning that enhances spatial precision and fine-grained detail in vision tasks. While existing self-supervised methods focus on global representations, this approach aims to capture localized semantics necessary for dense prediction tasks. The proposed method aligns distributions of dense features between teacher and student models through Patch-level Kernel Alignment (PaKA), capturing statistical dependencies and matching structural relationships of dense patches. Additionally, augmentation strategies designed for dense representation learning are investigated. The framework achieves state-of-the-art results across various dense vision benchmarks, showcasing its effectiveness in transferring existing semantic knowledge into the dense feature space. <div>
arXiv:2509.05606v1 Announce Type: new 
Abstract: Dense representations are essential for vision tasks that require spatial precision and fine-grained detail. While most self-supervised representation learning methods focus on global representations that summarize the image as a whole, such approaches often fall short in capturing the localized semantics necessary for dense prediction tasks. To overcome these limitations, we propose a framework that builds on pretrained representations through additional self-supervised learning, aiming to transfer existing semantic knowledge into the dense feature space. Our method aligns the distributions of dense features between a teacher and a student model. Specifically, we introduce Patch-level Kernel Alignment (PaKA), a simple yet effective alignment objective that captures statistical dependencies, thereby matching the structural relationships of dense patches across the two models. In addition, we investigate augmentation strategies specifically designed for dense representation learning. Our framework achieves state-of-the-art results across a variety of dense vision benchmarks, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</title>
<link>https://arxiv.org/abs/2509.05614</link>
<guid>https://arxiv.org/abs/2509.05614</guid>
<content:encoded><![CDATA[
<div> Keywords: Pruning, Vision-Language-Action (VLA) models, SpecPrune-VLA, dynamic pruning, token selection<br />
Summary:  
- Pruning is used to accelerate compute-bound models by reducing computation.
- Existing token pruning methods for Vision-Language-Action (VLA) models only consider local information from the current action, leading to a significant success rate drop and limited speedup.
- SpecPrune-VLA proposes a training-free method that leverages both local and global information for smarter token selection.
- It utilizes static pruning at the action level based on global history and local context, as well as dynamic pruning at the layer level.
- The method also includes a lightweight action-aware controller that adjusts pruning aggressiveness based on the speed of actions, resulting in 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 compared to OpenVLA-OFT, with minimal success rate loss.<br /><br />Summary: <div>
arXiv:2509.05614v1 Announce Type: new 
Abstract: Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2509.05625</link>
<guid>https://arxiv.org/abs/2509.05625</guid>
<content:encoded><![CDATA[
<div> Subspace Mapping, erasure, concept, image quality, robustness<br />
Summary:<br />
The paper introduces Subspace Mapping (SuMa), a novel method for effectively and robustly erasing narrow concepts in images such as copyrighted characters or celebrities. SuMa derives a target subspace representing the concept to be erased and maps it to a reference subspace to ensure robust erasure while maintaining image quality. The method is tested across various tasks including subclass erasure, celebrity erasure, artistic style erasure, and instance erasure, demonstrating comparable image quality to approaches focused on effectiveness. SuMa's results are also on par with methods targeting completeness. This novel approach addresses concerns regarding the misuse of text-to-image models and provides a solution for erasing narrow concepts in images to comply with copyright and legal requirements. <div>
arXiv:2509.05625v1 Announce Type: new 
Abstract: The rapid growth of text-to-image diffusion models has raised concerns about their potential misuse in generating harmful or unauthorized contents. To address these issues, several Concept Erasure methods have been proposed. However, most of them fail to achieve both robustness, i.e., the ability to robustly remove the target concept., and effectiveness, i.e., maintaining image quality. While few recent techniques successfully achieve these goals for NSFW concepts, none could handle narrow concepts such as copyrighted characters or celebrities. Erasing these narrow concepts is critical in addressing copyright and legal concerns. However, erasing them is challenging due to their close distances to non-target neighboring concepts, requiring finer-grained manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel method specifically designed to achieve both robustness and effectiveness in easing these narrow concepts. SuMa first derives a target subspace representing the concept to be erased and then neutralizes it by mapping it to a reference subspace that minimizes the distance between the two. This mapping ensures the target concept is robustly erased while preserving image quality. We conduct extensive experiments with SuMa across four tasks: subclass erasure, celebrity erasure, artistic style erasure, and instance erasure and compare the results with current state-of-the-art methods. Our method achieves image quality comparable to approaches focused on effectiveness, while also yielding results that are on par with methods targeting completeness.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning for Hyperspectral Images of Trees</title>
<link>https://arxiv.org/abs/2509.05630</link>
<guid>https://arxiv.org/abs/2509.05630</guid>
<content:encoded><![CDATA[
<div> Keywords: Aerial remote sensing, multispectral imaging, RGB imaging, self-supervised learning, hyperspectral images

Summary: 
Aerial remote sensing with multispectral and RGB imagers has significantly boosted precision agriculture. However, analyzing hyperspectral images without sufficient labels poses challenges. This study focuses on utilizing self-supervised learning techniques to develop neural network embeddings that capture vegetation properties of trees from aerial hyperspectral images of crop fields. The results of the experiments indicate that a tree representation generated in a vegetation property-related embedding space outperforms direct use of hyperspectral vegetation properties as tree representations in subsequent machine learning tasks. This novel approach showcases the potential of self-supervised learning in extracting meaningful information from hyperspectral images for precision agriculture applications.<br /><br />Summary: <div>
arXiv:2509.05630v1 Announce Type: new 
Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh</title>
<link>https://arxiv.org/abs/2509.05652</link>
<guid>https://arxiv.org/abs/2509.05652</guid>
<content:encoded><![CDATA[
<div> dataset, vehicle detection systems, YOLO model variants, performance evaluation, Bangladesh

Summary: 
This study evaluates the performance of six YOLO model variants on a custom dataset of 29 distinct vehicle classes, focusing on region-specific vehicles in Bangladesh. The top performer was YOLOv11x with 63.7% mAP@0.5, though it had a longer inference time of 45.8 milliseconds per image. Medium variants like YOLOv8m and YOLOv11m achieved good detection performance with mAP@0.5 values of 62.5% and 61.8% respectively, at faster inference times. Challenges were identified in detecting rare vehicle classes like Construction Vehicles and Desi Nosimons due to imbalances in the dataset. Confusion matrices showed frequent misclassifications between visually similar vehicles such as Mini Trucks and Mini Covered Vans. This research highlights the need for object detection systems tailored to Bangladesh's unique traffic environment, addressing gaps in autonomous vehicle technology development for developing regions where generic models struggle to perform effectively.<br /><br />Summary: <div>
arXiv:2509.05652v1 Announce Type: new 
Abstract: Vehicle detection systems trained on Non-Bangladeshi datasets struggle to accurately identify local vehicle types in Bangladesh's unique road environments, creating critical gaps in autonomous driving technology for developing regions. This study evaluates six YOLO model variants on a custom dataset featuring 29 distinct vehicle classes, including region-specific vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and ``CNG''. The dataset comprises high-resolution images (1920x1080) captured across various Bangladeshi roads using mobile phone cameras and manually annotated using LabelImg with YOLO format bounding boxes. Performance evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5, 43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8 milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m) struck an optimal balance, delivering robust detection performance with mAP@0.5 values of 62.5\% and 61.8\% respectively, while maintaining moderate inference times around 14-15 milliseconds. The study identified significant detection challenges for rare vehicle classes, with Construction Vehicles and Desi Nosimons showing near-zero accuracy due to dataset imbalances and insufficient training samples. Confusion matrices revealed frequent misclassifications between visually similar vehicles, particularly Mini Trucks versus Mini Covered Vans. This research provides a foundation for developing robust object detection systems specifically adapted to Bangladesh traffic conditions, addressing critical needs in autonomous vehicle technology advancement for developing regions where conventional generic-trained models fail to perform adequately.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.05659</link>
<guid>https://arxiv.org/abs/2509.05659</guid>
<content:encoded><![CDATA[
<div> Keywords: EditIDv2, high-complexity narrative scenes, long text inputs, identity consistency, semantic editing

Summary:
EditIDv2 is a tuning-free solution designed for complex narrative scenes and lengthy text inputs. Existing character editing methods struggle with maintaining semantic understanding, identity consistency, and editing capabilities in such scenarios. EditIDv2 addresses these issues by analyzing the impact of the ID integration module and exploring the influence of the ID feature integration module. By incorporating sophisticated techniques like PerceiverAttention decomposition, ID loss introduction, joint dynamic training with the diffusion model, and an offline fusion strategy for the integration module, EditIDv2 enables deep, multi-level semantic editing while preserving identity consistency with minimal data. This approach proves effective for generating high-quality images and performing well in the IBench evaluation. <div>
arXiv:2509.05659v1 Announce Type: new 
Abstract: We propose EditIDv2, a tuning-free solution specifically designed for high-complexity narrative scenes and long text inputs. Existing character editing methods perform well under simple prompts, but often suffer from degraded editing capabilities, semantic understanding biases, and identity consistency breakdowns when faced with long text narratives containing multiple semantic layers, temporal logic, and complex contextual relationships. In EditID, we analyzed the impact of the ID integration module on editability. In EditIDv2, we further explore and address the influence of the ID feature integration module. The core of EditIDv2 is to discuss the issue of editability injection under minimal data lubrication. Through a sophisticated decomposition of PerceiverAttention, the introduction of ID loss and joint dynamic training with the diffusion model, as well as an offline fusion strategy for the integration module, we achieve deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments using only a small amount of data lubrication. This meets the demands of long prompts and high-quality image generation, and achieves excellent results in the IBench evaluation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation</title>
<link>https://arxiv.org/abs/2509.05661</link>
<guid>https://arxiv.org/abs/2509.05661</guid>
<content:encoded><![CDATA[
<div> Keywords: Scene Graph Anticipation, Commonsense knowledge, Linguistic Scene Graph Anticipation, Object-Oriented Two-Staged Method, Large Language Model

Summary: <br /><br />Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, but existing approaches struggle to integrate commonsense knowledge, affecting long-term prediction robustness. To address this, a new approach called Linguistic Scene Graph Anticipation (LSGA) is proposed, focusing on decoupling the SGA task into two steps. The Object-Oriented Two-Staged Method (OOTSM) is introduced for LSGA, where a Large Language Model (LLM) predicts object appearances and disappearances before generating human-object relations. Extensive experiments show that OOTSM, in combination with STTran++, achieves state-of-the-art performance, improving short-term mean-Recall (@10) by 3.4% and long-term mean-Recall (@50) by 21.9%. The code for this approach is available on GitHub, providing a valuable resource for researchers and developers in the field of scene graph anticipation. <div>
arXiv:2509.05661v1 Announce Type: new 
Abstract: A scene graph is a structured represention of objects and their relationships in a scene. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications as intelligent surveillance and human-machine collaboration. Existing SGA approaches primarily leverage visual cues, often struggling to integrate valuable commonsense knowledge, thereby limiting long-term prediction robustness. To explicitly leverage such commonsense knowledge, we propose a new approach to better understand the objects, concepts, and relationships in a scene graph. Our approach decouples the SGA task in two steps: first a scene graph capturing model is used to convert a video clip into a sequence of scene graphs, then a pure text-based model is used to predict scene graphs in future frames. Our focus in this work is on the second step, and we call it Linguistic Scene Graph Anticipation (LSGA) and believes it should have independent interest beyond the use in SGA discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method (OOTSM) where an Large Language Model (LLM) first forecasts object appearances and disappearances before generating detailed human-object relations. We conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o, GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome annotations. For SGA, we combine our OOTSM with STTran++ from, and our experiments demonstrate effective state-of-the-art performance: short-term mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising</title>
<link>https://arxiv.org/abs/2509.05662</link>
<guid>https://arxiv.org/abs/2509.05662</guid>
<content:encoded><![CDATA[
<div> physics-inspired priors, image denoising, neural architectures, convolutional neural network, high noise level
Summary:<br /><br />In this paper, the authors explore the application of physics-inspired priors in improving image denoising algorithms. They introduce a hierarchy of pileup-inspired denoising models, including a residual CNN with conservation constraints, Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet). These models are tested on CIFAR-10 and BSD500 datasets with varying levels of Gaussian noise. The results show that physics-inspired priors enhance the robustness of denoising models, particularly at high noise levels. The key contributions of the study include translating pileup-mitigation principles into modular inductive biases, integrating them into UNet architecture, and demonstrating improved performance without relying on state-of-the-art techniques. <div>
arXiv:2509.05662v1 Announce Type: new 
Abstract: In high-energy particle physics, collider measurements are contaminated by "pileup", overlapping soft interactions that obscure the hard-scatter signal of interest. Dedicated subtraction strategies exploit physical priors such as conservation, locality, and isolation. Inspired by this analogy, we investigate how such principles can inform image denoising by embedding physics-guided inductive biases into neural architectures. This paper is a proof of concept: rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether physics-inspired priors improve robustness under strong corruption.
  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with conservation constraints, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at $\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard baselines, while WIPUNet shows a \emph{widening margin} at higher noise. Complementary BSD500 experiments show the same trend, suggesting physics-inspired priors provide stability where purely data-driven models degrade. Our contributions are: (i) translating pileup-mitigation principles into modular inductive biases; (ii) integrating them into UNet; and (iii) demonstrating robustness gains at high noise without relying on heavy SOTA machinery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance</title>
<link>https://arxiv.org/abs/2509.05669</link>
<guid>https://arxiv.org/abs/2509.05669</guid>
<content:encoded><![CDATA[
<div> dynamic read-write memory network, visual-textual inference, visual reasoning, multi-turn interactions, deep contextual understanding <br />
Summary: 
The article introduces the Context-Aware Multi-Turn Visual Reasoning (CAMVR) framework to enhance the capabilities of Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) in multi-turn interactions. CAMVR includes a Visual-Textual Context Memory Unit (VCMU) to store and manage visual features and textual representations from each interaction turn, and an Adaptive Visual Focus Guidance (AVFG) mechanism to adjust visual attention based on contextual relevance. The framework ensures coherent response generation by integrating multi-level reasoning with current and historical context. Extensive experiments on challenging datasets show that CAMVR achieves state-of-the-art performance in tasks requiring deep contextual understanding and complex visual reasoning. <br /><br />Summary: <div>
arXiv:2509.05669v1 Announce Type: new 
Abstract: Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) excel in single-turn tasks but face significant challenges in multi-turn interactions requiring deep contextual understanding and complex visual reasoning, often leading to fragmented reasoning, context loss, and hallucinations. To address these limitations, we propose Context-Aware Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower LVLMs with robust and coherent multi-turn visual-textual inference capabilities. CAMVR introduces two key innovations: a Visual-Textual Context Memory Unit (VCMU), a dynamic read-write memory network that stores and manages critical visual features, textual semantic representations, and their cross-modal correspondences from each interaction turn; and an Adaptive Visual Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to dynamically adjust the visual encoder's attention to contextually relevant image regions. Our multi-level reasoning integration strategy ensures that response generation is deeply coherent with both current inputs and accumulated historical context. Extensive experiments on challenging datasets, including VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following (MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics</title>
<link>https://arxiv.org/abs/2509.05670</link>
<guid>https://arxiv.org/abs/2509.05670</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, performance evaluation, distance-based metrics, MeshMetrics, open-source Python package 
Summary: 
MeshMetrics introduces a mesh-based framework to improve the accuracy and precision of distance-based metric computation in image segmentation. The research addresses the reproducibility crisis in this field by focusing on the reliability of metric implementation. By analyzing common pitfalls in distance-based metric implementation, MeshMetrics achieves higher accuracy and precision compared to conventional grid-based approaches. It is also less affected by discretization artifacts, providing more consistent results. The framework has been validated through theoretical analysis and empirical testing, demonstrating its effectiveness in overcoming discrepancies between open-source tools. MeshMetrics is released as an open-source Python package, making it accessible for researchers in the field of image segmentation. This tool contributes to the advancement of performance evaluation in image segmentation research. <br /><br />Summary: <div>
arXiv:2509.05670v1 Announce Type: new 
Abstract: The surge of research in image segmentation has yielded remarkable performance gains but also exposed a reproducibility crisis. A major contributor is performance evaluation, where both selection and implementation of metrics play critical roles. While recent efforts have improved the former, the reliability of metric implementation has received far less attention. Pitfalls in distance-based metric implementation can lead to considerable discrepancies between common open-source tools, for instance, exceeding 100 mm for the Hausdorff distance and 30%pt for the normalized surface distance for the same pair of segmentations. To address these pitfalls, we introduce MeshMetrics, a mesh-based framework that provides a more precise computation of distance-based metrics than conventional grid-based approaches. Through theoretical analysis and empirical validation, we demonstrate that MeshMetrics achieves higher accuracy and precision than established tools, and is substantially less affected by discretization artifacts, such as distance quantization. We release MeshMetrics as an open-source Python package, available at https://github.com/gasperpodobnik/MeshMetrics.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization</title>
<link>https://arxiv.org/abs/2509.05695</link>
<guid>https://arxiv.org/abs/2509.05695</guid>
<content:encoded><![CDATA[
<div> Keywords: Human action recognition, Vision-Language Large Models, Video-to-Semantic-Tokens Module, LoRA-fine-tuned LVLM, Interpretability<br />
Summary:<br />
- The paper introduces LVLM-VAR, a framework that utilizes pre-trained Vision-Language Large Models for video action recognition, enhancing accuracy and interpretability.<br />
- LVLM-VAR includes a Video-to-Semantic-Tokens Module to convert raw video sequences into semantic action tokens, facilitating better understanding for LVLMs.<br />
- The framework combines these tokens with natural language instructions and processes them using a LoRA-fine-tuned LVLM for robust action classification and reasoning.<br />
- LVLM-VAR achieves state-of-the-art or competitive performance on challenging datasets like NTU RGB+D and NTU RGB+D 120, showing significant improvements in accuracy.<br />
- The framework also enhances model interpretability by generating natural language explanations for its predictions, making it more understandable and transparent.<br /> 
Summary: <div>
arXiv:2509.05695v1 Announce Type: new 
Abstract: Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization</title>
<link>https://arxiv.org/abs/2509.05696</link>
<guid>https://arxiv.org/abs/2509.05696</guid>
<content:encoded><![CDATA[
<div> viewpoint differences, appearance variations, structural information, Joint perception network, UAV localization

Summary:
The article introduces a novel approach, JRN-Geo, for cross-view geo-localization in UAV navigation. It addresses challenges of viewpoint differences and appearance variations by incorporating geometric structural information from normal images. A dual-branch feature extraction framework with DAFM and JCIA enables deep fusion and joint-constrained semantic and structural information representation. A 3D geographic augmentation technique generates potential viewpoint variation samples to enhance the network's ability to learn viewpoint-invariant features. Experimental results on University-1652 and SUES-200 datasets validate the method's robustness against complex viewpoint variations, achieving state-of-the-art performance.<br /><br />Summary: <div>
arXiv:2509.05696v1 Announce Type: new 
Abstract: Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle (UAV) localization and navigation. However, significant challenges arise from the drastic viewpoint differences and appearance variations between images. Existing methods predominantly rely on semantic features from RGB images, often neglecting the importance of spatial structural information in capturing viewpoint-invariant features. To address this issue, we incorporate geometric structural information from normal images and introduce a Joint perception network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a dual-branch feature extraction framework, leveraging a Difference-Aware Fusion Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to enable deep fusion and joint-constrained semantic and structural information representation. Furthermore, we propose a 3D geographic augmentation technique to generate potential viewpoint variation samples, enhancing the network's ability to learn viewpoint-invariant features. Extensive experiments on the University-1652 and SUES-200 datasets validate the robustness of our method against complex viewpoint ariations, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis</title>
<link>https://arxiv.org/abs/2509.05703</link>
<guid>https://arxiv.org/abs/2509.05703</guid>
<content:encoded><![CDATA[
<div> Keywords: Marine mammal vocalization, bioacoustic spectrograms, Vision Language Models, LLM-based validation, domain knowledge

Summary: 
Vision Language Models (VLMs) are typically not trained on marine mammal vocalization analysis data represented in bioacoustic spectrograms. This study explores the ability of VLMs to extract meaningful patterns visually from spectrograms. The framework developed integrates VLM interpretation with Language Model (LLM)-based validation to establish domain knowledge. By leveraging this approach, adaptation to acoustic data becomes feasible without the need for manual annotation or model retraining. This innovation marks a significant advancement in the field of marine mammal vocalization analysis, enabling researchers to gain valuable insights and streamline the process of interpreting bioacoustic spectrograms. <div>
arXiv:2509.05703v1 Announce Type: new 
Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic spectrograms. Vision Language Models (VLMs) are not trained on these domain-specific visualizations. We investigate whether VLMs can extract meaningful patterns from spectrograms visually. Our framework integrates VLM interpretation with LLM-based validation to build domain knowledge. This enables adaptation to acoustic data without manual annotation or model retraining.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</title>
<link>https://arxiv.org/abs/2509.05728</link>
<guid>https://arxiv.org/abs/2509.05728</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR-BIND, sensor fusion, temporal consistency, SLAM, modality fusion<br />
Summary:<br />
This paper introduces enhancements to the LiDAR-BIND framework for sensor fusion, specifically focusing on enforcing temporal consistency. The proposed enhancements include a temporal embedding similarity mechanism to align consecutive latent spaces, a motion-aligned transformation loss to match displacement with ground truth LiDAR data, and a specialized temporal module for windowed temporal fusion. The updated model architecture also better preserves spatial structure. Evaluations show improved temporal and spatial coherence, with lower absolute trajectory error and better occupancy map accuracy in SLAM applications. New metrics based on the Fréchet Video Motion Distance and correlation-peak distance provide indicators of temporal quality for SLAM performance assessment. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains the flexibility of modality fusion while significantly improving temporal stability, leading to enhanced robustness and performance for SLAM applications. <br /> <div>
arXiv:2509.05728v1 Announce Type: new 
Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</title>
<link>https://arxiv.org/abs/2509.05740</link>
<guid>https://arxiv.org/abs/2509.05740</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-camera, LiDAR, visual-inertial odometry, panoramic visual feature model, factor graph 

Summary: 
The article introduces Multi-LVI-SAM, a framework that combines data from multiple fisheye cameras, LiDAR, and inertial sensors for accurate state estimation. A panoramic visual feature model is proposed to integrate information from multiple cameras into a unified representation, enabling global geometric optimization and simplifying system design. An extrinsic compensation method is introduced to address triangulation inconsistency, improving feature consistency and reducing errors. The framework is integrated into a tightly coupled LiDAR-visual-inertial system based on a factor graph. Experimental results show that the panoramic visual feature model enhances multi-camera constraints, leading to higher accuracy and robustness compared to existing systems. <br /><br />Summary: <div>
arXiv:2509.05740v1 Announce Type: new 
Abstract: We propose a multi-camera LiDAR-visual-inertial odometry framework, Multi-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and inertial sensors for highly accurate and robust state estimation. To enable efficient and consistent integration of visual information from multiple fisheye cameras, we introduce a panoramic visual feature model that unifies multi-camera observations into a single representation. The panoramic model serves as a global geometric optimization framework that consolidates multi-view constraints, enabling seamless loop closure and global pose optimization, while simplifying system design by avoiding redundant handling of individual cameras. To address the triangulation inconsistency caused by the misalignment between each camera's frame and the panoramic model's frame, we propose an extrinsic compensation method. This method improves feature consistency across views and significantly reduces triangulation and optimization errors, leading to more accurate pose estimation. We integrate the panoramic visual feature model into a tightly coupled LiDAR-visual-inertial system based on a factor graph. Extensive experiments on public datasets demonstrate that the panoramic visual feature model enhances the quality and consistency of multi-camera constraints, resulting in higher accuracy and robustness than existing multi-camera LiDAR-visual-inertial systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation</title>
<link>https://arxiv.org/abs/2509.05746</link>
<guid>https://arxiv.org/abs/2509.05746</guid>
<content:encoded><![CDATA[
<div> Distance-Adaptive Super-Resolution, Spatially-Varying Inverse Problem, Neural Architecture, Spectral Constraints, Depth-Conditional Convolution Kernels
<br />
Summary: 
This research introduces a novel distance-adaptive super-resolution framework that addresses spatially-varying degradation models in real-world imaging systems. It formulates super-resolution as a spatially-varying inverse problem, utilizing pseudodifferential operators with distance-dependent spectral characteristics. The neural architecture incorporates depth-conditional convolution kernels and adaptive regularization terms based on scene geometry. By implementing spectral constraints from atmospheric scattering theory and adaptive kernel generation networks, the framework achieves state-of-the-art performance on depth-variant scenarios and traditional benchmarks. Evaluation on various datasets demonstrates significant improvements in reconstruction quality, with superior PSNR and SSIM scores compared to existing methods. This work represents a theoretically-grounded approach to super-resolution and showcases the importance of incorporating geometric scene understanding for optimal image reconstruction. 
<br /> <div>
arXiv:2509.05746v1 Announce Type: new 
Abstract: Single image super-resolution traditionally assumes spatially-invariant degradation models, yet real-world imaging systems exhibit complex distance-dependent effects including atmospheric scattering, depth-of-field variations, and perspective distortions. This fundamental limitation necessitates spatially-adaptive reconstruction strategies that explicitly incorporate geometric scene understanding for optimal performance. We propose a rigorous variational framework that characterizes super-resolution as a spatially-varying inverse problem, formulating the degradation operator as a pseudodifferential operator with distance-dependent spectral characteristics that enable theoretical analysis of reconstruction limits across depth ranges. Our neural architecture implements discrete gradient flow dynamics through cascaded residual blocks with depth-conditional convolution kernels, ensuring convergence to stationary points of the theoretical energy functional while incorporating learned distance-adaptive regularization terms that dynamically adjust smoothness constraints based on local geometric structure. Spectral constraints derived from atmospheric scattering theory prevent bandwidth violations and noise amplification in far-field regions, while adaptive kernel generation networks learn continuous mappings from depth to reconstruction filters. Comprehensive evaluation across five benchmark datasets demonstrates state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by 0.44dB and 0.36dB respectively. This work establishes the first theoretically-grounded distance-adaptive super-resolution framework and demonstrates significant improvements on depth-variant scenarios while maintaining competitive performance across traditional benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</title>
<link>https://arxiv.org/abs/2509.05747</link>
<guid>https://arxiv.org/abs/2509.05747</guid>
<content:encoded><![CDATA[
<div> Dataset, interactive behaviors, motion sequences, multi-modal, facial expressions <br />
Summary: 
The article introduces a new dataset called InterAct, which focuses on capturing accurate interactive behaviors between two individuals in daily scenarios. Unlike previous works that often consider only one person or conversational gestures, InterAct simultaneously models the activities of two individuals engaged in objective-driven, dynamic interactions over longer durations. The dataset comprises 241 motion sequences where both individuals perform a realistic scenario with different roles and emotions, collaborating to complete a task or engage in interactive activities. The dataset captures diverse and complex movements, providing valuable insights into relatively long-term interaction patterns. The article also presents a diffusion-based method that estimates interactive facial expressions and body motions from speech inputs, regressing body motions hierarchically and fine-tuning facial expression accuracy. The dataset and code are made publicly available to facilitate further research. <br /> <div>
arXiv:2509.05747v1 Announce Type: new 
Abstract: We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation</title>
<link>https://arxiv.org/abs/2509.05751</link>
<guid>https://arxiv.org/abs/2509.05751</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Video Object Segmentation, Language Models, Spatio-temporal grounding, Hierarchical reasoning, State-of-the-art performance<br />
Summary:<br />
Referring Video Object Segmentation (RVOS) is a challenging task that requires aligning text descriptions with dynamic visual content. Current methods struggle with complex descriptions and similar-looking objects. In response, the proposed PARSE-VOS framework utilizes Large Language Models (LLMs) for hierarchical reasoning across text and video domains. It parses natural language queries into structured commands and generates candidate trajectories guided by semantics. A hierarchical identification module performs coarse-to-fine reasoning, first using an LLM for motion reasoning and then triggering a pose verification stage if needed. PARSE-VOS achieved state-of-the-art performance on benchmarks like Ref-YouTube-VOS, Ref-DAVIS17, and MeViS. <div>
arXiv:2509.05751v1 Announce Type: new 
Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of interest throughout a video based on a language description. The prominent challenge lies in aligning static text with dynamic visual content, particularly when objects exhibiting similar appearances with inconsistent motion and poses. However, current methods often rely on a holistic visual-language fusion that struggles with complex, compositional descriptions. In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine reasoning across text and video domains. Our approach begins by parsing the natural language query into structured semantic commands. Next, we introduce a spatio-temporal grounding module that generates all candidate trajectories for all potential target objects, guided by the parsed semantics. Finally, a hierarchical identification module select the correct target through a two-stage reasoning process: it first performs coarse-grained motion reasoning with an LLM to narrow down candidates; if ambiguity remains, a fine-grained pose verification stage is conditionally triggered to disambiguate. The final output is an accurate segmentation mask for the target object. \textbf{PARSE-VOS} achieved state-of-the-art performance on three major benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters</title>
<link>https://arxiv.org/abs/2509.05773</link>
<guid>https://arxiv.org/abs/2509.05773</guid>
<content:encoded><![CDATA[
<div> decipherment, oracle bone characters, visual perception, large multimodal models, dataset

Summary: 
Deciphering oracle bone characters (OBCs) is crucial for understanding early Chinese written language. A new dataset called PictOBI-20k has been created to evaluate the use of large multimodal models (LMMs) in visually deciphering OBCs. The dataset consists of 20k OBC and real object images, along with multi-choice questions. Experiments show that LMMs have some visual decipherment skills but are often limited by language priors. The dataset aims to improve visual attention in future OBC-oriented LMMs. The code and dataset are available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2509.05773v1 Announce Type: new 
Abstract: Deciphering oracle bone characters (OBCs), the oldest attested form of written Chinese, has remained the ultimate, unwavering goal of scholars, offering an irreplaceable key to understanding humanity's early modes of production. Current decipherment methodologies of OBC are primarily constrained by the sporadic nature of archaeological excavations and the limited corpus of inscriptions. With the powerful visual perception capability of large multimodal models (LMMs), the potential of using LMMs for visually deciphering OBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It includes 20k meticulously collected OBC and real object images, forming over 15k multi-choice questions. We also conduct subjective annotations to investigate the consistency of the reference point between humans and LMMs in visual reasoning. Experiments indicate that general LMMs possess preliminary visual decipherment skills, and LMMs are not effectively using visual information, while most of the time they are limited by language priors. We hope that our dataset can facilitate the evaluation and optimization of visual attention in future OBC-oriented LMMs. The code and dataset will be available at https://github.com/OBI-Future/PictOBI-20k.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models</title>
<link>https://arxiv.org/abs/2509.05776</link>
<guid>https://arxiv.org/abs/2509.05776</guid>
<content:encoded><![CDATA[
arXiv:2509.05776v1 Announce Type: new 
Abstract: In medical imaging, point distribution models are often used to reconstruct and complete partial shapes using a statistical model of the full shape. A commonly overlooked, but crucial factor in this reconstruction process, is the pose of the training data relative to the partial target shape. A difference in pose alignment of the training and target shape leads to biased solutions, particularly when observing small parts of a shape. In this paper, we demonstrate the importance of pose alignment for partial shape reconstructions and propose an efficient method to adjust an existing model to a specific target. Our method preserves the computational efficiency of linear models while significantly improving reconstruction accuracy and predicted variance. It exactly recovers the intended aligned model for translations, and provides a good approximation for small rotations, all without access to the original training data. Hence, existing shape models in reconstruction pipelines can be adapted by a simple preprocessing step, making our approach widely applicable in plug-and-play scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DPillars: Pillar-based two-stage 3D object detection</title>
<link>https://arxiv.org/abs/2509.05780</link>
<guid>https://arxiv.org/abs/2509.05780</guid>
<content:encoded><![CDATA[
arXiv:2509.05780v1 Announce Type: new 
Abstract: PointPillars is the fastest 3D object detector that exploits pseudo image representations to encode features for 3D objects in a scene. Albeit efficient, PointPillars is typically outperformed by state-of-the-art 3D detection methods due to the following limitations: 1) The pseudo image representations fail to preserve precise 3D structures, and 2) they make it difficult to adopt a two-stage detection pipeline using 3D object proposals that typically shows better performance than a single-stage approach. We introduce in this paper the first two-stage 3D detection framework exploiting pseudo image representations, narrowing the performance gaps between PointPillars and state-of-the-art methods, while retaining its efficiency. Our framework consists of two novel components that overcome the aforementioned limitations of PointPillars: First, we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D voxel-based features from the pseudo image representation efficiently using 2D convolutions. The basic idea behind 3DPillars is that 3D features from voxels can be viewed as a stack of pseudo images. To implement this idea, we propose a separable voxel feature module that extracts voxel-based features without using 3D convolutions. Second, we introduce an RoI head with a sparse scene context feature module that aggregates multi-scale features from 3DPillars to obtain a sparse scene feature. This enables adopting a two-stage pipeline effectively, and fully leveraging contextual information of a scene to refine 3D object proposals. Experimental results on the KITTI and Waymo Open datasets demonstrate the effectiveness and efficiency of our approach, achieving a good compromise in terms of speed and accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation</title>
<link>https://arxiv.org/abs/2509.05785</link>
<guid>https://arxiv.org/abs/2509.05785</guid>
<content:encoded><![CDATA[
arXiv:2509.05785v1 Announce Type: new 
Abstract: Recently, camera-radar fusion-based 3D object detection methods in bird's eye view (BEV) have gained attention due to the complementary characteristics and cost-effectiveness of these sensors. Previous approaches using forward projection struggle with sparse BEV feature generation, while those employing backward projection overlook depth ambiguity, leading to false positives. In this paper, to address the aforementioned limitations, we propose a novel camera-radar fusion-based 3D object detection and segmentation model named CRAB (Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based view transformation), using a backward projection that leverages radar to mitigate depth ambiguity. During the view transformation, CRAB aggregates perspective view image context features into BEV queries. It improves depth distinction among queries along the same ray by combining the dense but unreliable depth distribution from images with the sparse yet precise depth information from radar occupancy. We further introduce spatial cross-attention with a feature map containing radar context information to enhance the comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our proposed approach achieves a state-of-the-art performance among backward projection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in 3D object detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance</title>
<link>https://arxiv.org/abs/2509.05796</link>
<guid>https://arxiv.org/abs/2509.05796</guid>
<content:encoded><![CDATA[
arXiv:2509.05796v1 Announce Type: new 
Abstract: Automating visual inspection in medical device manufacturing remains challenging due to small and imbalanced datasets, high-resolution imagery, and stringent regulatory requirements. This work proposes two attention-guided autoencoder architectures for deep anomaly detection designed to address these constraints. The first employs a structural similarity-based anomaly score (4-MS-SSIM), offering lightweight and accurate real-time defect detection, yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised thresholding) on the - Surface Seal Image - Test split with only 10% of defective samples. The second applies a feature-distance approach using Mahalanobis scoring on reduced latent features, providing high sensitivity to distributional shifts for supervisory monitoring, achieving ACC 0.722 with supervised thresholding. Together, these methods deliver complementary capabilities: the first supports reliable inline inspection, while the second enables scalable post-production surveillance and regulatory compliance monitoring. Experimental results demonstrate that both approaches surpass re-implemented baselines and provide a practical pathway for deploying deep anomaly detection in regulated manufacturing environments, aligning accuracy, efficiency, and the regulatory obligations defined for high-risk AI systems under the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.05809</link>
<guid>https://arxiv.org/abs/2509.05809</guid>
<content:encoded><![CDATA[
arXiv:2509.05809v1 Announce Type: new 
Abstract: Recent advances in promptable segmentation, such as the Segment Anything Model (SAM), have enabled flexible, high-quality mask generation across a wide range of visual domains. However, SAM and similar models remain fundamentally deterministic, producing a single segmentation per object per prompt, and fail to capture the inherent ambiguity present in many real-world tasks. This limitation is particularly troublesome in medical imaging, where multiple plausible segmentations may exist due to annotation uncertainty or inter-expert variability. In this paper, we introduce Probabilistic SAM, a probabilistic extension of SAM that models a distribution over segmentations conditioned on both the input image and prompt. By incorporating a latent variable space and training with a variational objective, our model learns to generate diverse and plausible segmentation masks reflecting the variability in human annotations. The architecture integrates a prior and posterior network into the SAM framework, allowing latent codes to modulate the prompt embeddings during inference. The latent space allows for efficient sampling during inference, enabling uncertainty-aware outputs with minimal overhead. We evaluate Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate its ability to produce diverse outputs that align with expert disagreement, outperforming existing probabilistic baselines on uncertainty-aware metrics. Our code is available at: https://github.com/tbwa233/Probabilistic-SAM/.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data</title>
<link>https://arxiv.org/abs/2509.05887</link>
<guid>https://arxiv.org/abs/2509.05887</guid>
<content:encoded><![CDATA[
arXiv:2509.05887v1 Announce Type: new 
Abstract: Dust storms harm health and reduce visibility; quick detection from satellites is needed. We present a near real-time system that flags dust at the pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D convolutional network learns patterns across all 36 bands, plus split thermal bands, to separate dust from clouds and surface features. Simple normalization and local filling handle missing data. An improved version raises training speed by 21x and supports fast processing of full scenes. On 17 independent MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error of 0.014. Maps show strong agreement in plume cores, with most misses along edges. These results show that joint band-and-space learning can provide timely dust alerts at global scale; using wider input windows or attention-based models may further sharpen edges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
<link>https://arxiv.org/abs/2509.05892</link>
<guid>https://arxiv.org/abs/2509.05892</guid>
<content:encoded><![CDATA[
arXiv:2509.05892v1 Announce Type: new 
Abstract: Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2509.05895</link>
<guid>https://arxiv.org/abs/2509.05895</guid>
<content:encoded><![CDATA[
arXiv:2509.05895v1 Announce Type: new 
Abstract: Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers visual-semantic alignment in change understanding, thereby constraining the overall effectiveness of current approaches. To address this gap, we propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change understanding capability. BTCChat supports bi-temporal change captioning and retains single-image interpretation capability. To better capture temporal features and spatial semantic changes in image pairs, we design a Change Extraction module. Moreover, to enhance the model's attention to spatial details, we introduce a Prompt Augmentation mechanism, which incorporates contextual clues into the prompt to enhance model performance. Experimental results demonstrate that BTCChat achieves state-of-the-art performance on change captioning and visual question answering tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features</title>
<link>https://arxiv.org/abs/2509.05913</link>
<guid>https://arxiv.org/abs/2509.05913</guid>
<content:encoded><![CDATA[
arXiv:2509.05913v1 Announce Type: new 
Abstract: Musculoskeletal disorders pose significant risks to athletes, and assessing risk early is important for prevention. However, most existing methods are designed for controlled settings and fail to reliably assess risk in complex environments due to their reliance on a single type of data. This research proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel multimodal deep learning framework designed to classify musculoskeletal risk using visual and skeletal coordinate-based features. In addition, a custom multimodal dataset is constructed by combining visual data and skeletal coordinates for risk assessment. Each sample is labeled into eight risk categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines a Residual Block with a Lightweight Transformer Block to learn spatial and temporal dependencies jointly. It incorporates two novel modules: the Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature refinement through cross-attention between visual and skeletal inputs, and the Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal coherence by aligning image features with coordinate-based representations. ViSK-GAT achieved strong performance with validation and test accuracies of 93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of 93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. The regression results also indicated a low Root Mean Square Error of the predicted probability distribution of 0.1205 and a corresponding Mean Absolute Error of 0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT consistently outperformed previous methods. The ViSK-GAT model advances artificial intelligence implementation and application, transforming musculoskeletal risk classification and enabling impactful early interventions in sports.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2509.05925</link>
<guid>https://arxiv.org/abs/2509.05925</guid>
<content:encoded><![CDATA[
arXiv:2509.05925v1 Announce Type: new 
Abstract: Recent deep learning-based methods for lossy image compression achieve competitive rate-distortion performance through extensive end-to-end training and advanced architectures. However, emerging applications increasingly prioritize semantic preservation over pixel-level reconstruction and demand robust performance across diverse data distributions and downstream tasks. These challenges call for advanced semantic compression paradigms. Motivated by the zero-shot and representational capabilities of multimodal foundation models, we propose a novel semantic compression method based on the contrastive language-image pretraining (CLIP) model. Rather than compressing images for reconstruction, we propose compressing the CLIP feature embeddings into minimal bits while preserving semantic information across different tasks. Experiments show that our method maintains semantic integrity across benchmark datasets, achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This is less than 5% of the bitrate required by mainstream image compression approaches for comparable performance. Remarkably, even under extreme compression, the proposed approach exhibits zero-shot robustness across diverse data distributions and downstream tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttriPrompt: Dynamic Prompt Composition Learning for CLIP</title>
<link>https://arxiv.org/abs/2509.05949</link>
<guid>https://arxiv.org/abs/2509.05949</guid>
<content:encoded><![CDATA[
arXiv:2509.05949v1 Announce Type: new 
Abstract: The evolution of prompt learning methodologies has driven exploration of deeper prompt designs to enhance model performance. However, current deep text prompting approaches suffer from two critical limitations: Over-reliance on constrastive learning objectives that prioritize high-level semantic alignment, neglecting fine-grained feature optimization; Static prompts across all input categories, preventing content-aware adaptation. To address these limitations, we propose AttriPrompt-a novel framework that enhances and refines textual semantic representations by leveraging the intermediate-layer features of CLIP's vision encoder. We designed an Attribute Retrieval module that first clusters visual features from each layer. The aggregated visual features retrieve semantically similar prompts from a prompt pool, which are then concatenated to the input of every layer in the text encoder. Leveraging hierarchical visual information embedded in prompted text features, we introduce Dual-stream Contrastive Learning to realize fine-grained alignment. Furthermore, we introduce a Self-Regularization mechanism by applying explicit regularization constraints between the prompted and non-prompted text features to prevent overfitting on limited training data. Extensive experiments across three benchmarks demonstrate AttriPrompt's superiority over state-of-the-art methods, achieving up to 7.37\% improvement in the base-to-novel setting. The observed strength of our method in cross-domain knowledge transfer positions vision-language pre-trained models as more viable solutions for real-world implementation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching</title>
<link>https://arxiv.org/abs/2509.05952</link>
<guid>https://arxiv.org/abs/2509.05952</guid>
<content:encoded><![CDATA[
arXiv:2509.05952v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.05953</link>
<guid>https://arxiv.org/abs/2509.05953</guid>
<content:encoded><![CDATA[
arXiv:2509.05953v1 Announce Type: new 
Abstract: Medical image segmentation is a crucial method for assisting professionals in diagnosing various diseases through medical imaging. However, various factors such as noise, blurriness, and low contrast often hinder the accurate diagnosis of diseases. While numerous image enhancement techniques can mitigate these issues, they may also alter crucial information needed for accurate diagnosis in the original image. Conventional image fusion strategies, such as feature concatenation can address this challenge. However, they struggle to fully leverage the advantages of both original and enhanced images while suppressing the side effects of the enhancements. To overcome the problem, we propose a dual interactive fusion module (DIFM) that effectively exploits mutual complementary information from the original and enhanced images. DIFM employs cross-attention bidirectionally to simultaneously attend to corresponding spatial information across different images, subsequently refining the complementary features via global spatial attention. This interaction leverages low- to high-level features implicitly associated with diverse structural attributes like edges, blobs, and object shapes, resulting in enhanced features that embody important spatial characteristics. In addition, we introduce a multi-scale boundary loss based on gradient extraction to improve segmentation accuracy at object boundaries. Experimental results on the ACDC and Synapse datasets demonstrate the superiority of the proposed method quantitatively and qualitatively. Code available at: https://github.com/JJeong-Gari/DIN
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud</title>
<link>https://arxiv.org/abs/2509.05954</link>
<guid>https://arxiv.org/abs/2509.05954</guid>
<content:encoded><![CDATA[
arXiv:2509.05954v1 Announce Type: new 
Abstract: The deployment of high-accuracy 3D object detection models from point cloud remains a significant challenge due to their substantial computational and memory requirements. To address this, we introduce StripDet, a novel lightweight framework designed for on-device efficiency. First, we propose the novel Strip Attention Block (SAB), a highly efficient module designed to capture long-range spatial dependencies. By decomposing standard 2D convolutions into asymmetric strip convolutions, SAB efficiently extracts directional features while reducing computational complexity from quadratic to linear. Second, we design a hardware-friendly hierarchical backbone that integrates SAB with depthwise separable convolutions and a simple multiscale fusion strategy, achieving end-to-end efficiency. Extensive experiments on the KITTI dataset validate StripDet's superiority. With only 0.65M parameters, our model achieves a 79.97% mAP for car detection, surpassing the baseline PointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms recent lightweight and knowledge distillation-based methods, achieving a superior accuracy-efficiency trade-off while establishing itself as a practical solution for real-world 3D detection on edge devices.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Bloom: A Deep Learning Approach to Real-Time Lighting</title>
<link>https://arxiv.org/abs/2509.05963</link>
<guid>https://arxiv.org/abs/2509.05963</guid>
<content:encoded><![CDATA[
arXiv:2509.05963v1 Announce Type: new 
Abstract: We propose a novel method to generate bloom lighting effect in real time using neural networks. Our solution generate brightness mask from given 3D scene view up to 30% faster than state-of-the-art methods. The existing traditional techniques rely on multiple blur appliances and texture sampling, also very often have existing conditional branching in its implementation. These operations occupy big portion of the execution time. We solve this problem by proposing two neural network-based bloom lighting methods, Neural Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on their quality and performance. Both methods were tested on a variety of 3D scenes, with evaluations conducted on brightness mask accuracy and inference speed. The main contribution of this work is that both methods produce high-quality bloom effects while outperforming the standard state-of-the-art bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%. These findings highlight that we can achieve realistic bloom lighting phenomena faster, moving us towards more realism in real-time environments in the future. This improvement saves computational resources, which is a major bottleneck in real-time rendering. Furthermore, it is crucial for sustaining immersion and ensuring smooth experiences in high FPS environments, while maintaining high-quality realism.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks</title>
<link>https://arxiv.org/abs/2509.05967</link>
<guid>https://arxiv.org/abs/2509.05967</guid>
<content:encoded><![CDATA[
arXiv:2509.05967v1 Announce Type: new 
Abstract: The application of self-supervised techniques has become increasingly prevalent within medical visualization tasks, primarily due to its capacity to mitigate the data scarcity prevalent in the healthcare sector. The majority of current works are influenced by designs originating in the generic 2D visual domain, which lack the intuitive demonstration of the model's learning process regarding 3D spatial knowledge. Consequently, these methods often fall short in terms of medical interpretability. We propose a method consisting of three sub-tasks to capture the spatially relevant semantics in medical 3D imaging. Their design adheres to observable principles to ensure interpretability, and minimize the performance loss caused thereby as much as possible. By leveraging the enhanced semantic depth offered by the extra dimension in 3D imaging, this approach incorporates multi-granularity spatial relationship modeling to maintain training stability. Experimental findings suggest that our approach is capable of delivering performance that is on par with current methodologies, while facilitating an intuitive understanding of the self-supervised learning process.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization</title>
<link>https://arxiv.org/abs/2509.05970</link>
<guid>https://arxiv.org/abs/2509.05970</guid>
<content:encoded><![CDATA[
arXiv:2509.05970v1 Announce Type: new 
Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by reframing it as a data problem. Our key insight is destylization, reversing style transfer by removing stylistic elements from artworks to recover natural, style-free counterparts. This yields DST-100K, a large-scale dataset that provides authentic supervision signals by aligning real artistic styles with their underlying content. To build DST-100K, we develop (1) DST, a text-guided destylization model that reconstructs stylefree content, and (2) DST-Filter, a multi-stage evaluation model that employs Chain-of-Thought reasoning to automatically discard low-quality pairs while ensuring content fidelity and style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently surpasses state-of-the-art methods across both qualitative and quantitative benchmarks. Our results demonstrate that scalable data generation via destylization provides a reliable supervision paradigm, overcoming the fundamental challenge posed by the lack of ground-truth data in artistic style transfer.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConstStyle: Robust Domain Generalization with Unified Style Transformation</title>
<link>https://arxiv.org/abs/2509.05975</link>
<guid>https://arxiv.org/abs/2509.05975</guid>
<content:encoded><![CDATA[
arXiv:2509.05975v1 Announce Type: new 
Abstract: Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction</title>
<link>https://arxiv.org/abs/2509.05992</link>
<guid>https://arxiv.org/abs/2509.05992</guid>
<content:encoded><![CDATA[
arXiv:2509.05992v1 Announce Type: new 
Abstract: Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
<link>https://arxiv.org/abs/2509.05999</link>
<guid>https://arxiv.org/abs/2509.05999</guid>
<content:encoded><![CDATA[
arXiv:2509.05999v1 Announce Type: new 
Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task due to the nature of the input used, which is a single 2D image, lacking in any depth cues and placing the depth estimation problem as an ill-posed one. Existing solutions leverage the information extracted from the input by using Convolutional Neural Networks or Transformer architectures as feature extraction backbones, followed by specific detection heads for 3D parameters prediction. In this paper, we introduce a decoupled strategy based on injecting precomputed segmentation information priors and fusing them directly into the feature space for guiding the detection, without expanding the detection model or jointly learning the priors. The focus is on evaluating the impact of additional segmentation information on existing detection pipelines without adding additional prediction branches. The proposed method is evaluated on the KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture that relies only on RGB image features for small objects in the scene: pedestrians and cyclists, and proving that understanding the input data can balance the need for additional sensors or training data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation</title>
<link>https://arxiv.org/abs/2509.06000</link>
<guid>https://arxiv.org/abs/2509.06000</guid>
<content:encoded><![CDATA[
arXiv:2509.06000v1 Announce Type: new 
Abstract: Monocular 6-DoF pose estimation plays an important role in multiple spacecraft missions. Most existing pose estimation approaches rely on single images with static keypoint localisation, failing to exploit valuable temporal information inherent to space operations. In this work, we adapt a deep learning framework from human pose estimation to the spacecraft pose estimation domain that integrates motion-aware heatmaps and optical flow to capture motion dynamics. Our approach combines image features from a Vision Transformer (ViT) encoder with motion cues from a pre-trained optical flow model to localise 2D keypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers 6-DoF poses from known 2D-3D correspondences. We train and evaluate our method on the SPADES-RGB dataset and further assess its generalisation on real and synthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates improved performance over single-image baselines in both 2D keypoint localisation and 6-DoF pose estimation. Furthermore, it shows promising generalisation capabilities when testing on different data distributions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khana: A Comprehensive Indian Cuisine Dataset</title>
<link>https://arxiv.org/abs/2509.06006</link>
<guid>https://arxiv.org/abs/2509.06006</guid>
<content:encoded><![CDATA[
arXiv:2509.06006v1 Announce Type: new 
Abstract: As global interest in diverse culinary experiences grows, food image models are essential for improving food-related applications by enabling accurate food recognition, recipe suggestions, dietary tracking, and automated meal planning. Despite the abundance of food datasets, a noticeable gap remains in capturing the nuances of Indian cuisine due to its vast regional diversity, complex preparations, and the lack of comprehensive labeled datasets that cover its full breadth. Through this exploration, we uncover Khana, a new benchmark dataset for food image classification, segmentation, and retrieval of dishes from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian cuisine and offering around 131K images in the dataset spread across 80 labels, each with a resolution of 500x500 pixels. This paper describes the dataset creation process and evaluates state-of-the-art models on classification, segmentation, and retrieval as baselines. Khana bridges the gap between research and development by providing a comprehensive and challenging benchmark for researchers while also serving as a valuable resource for developers creating real-world applications that leverage the rich tapestry of Indian cuisine. Webpage: https://khana.omkar.xyz
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users</title>
<link>https://arxiv.org/abs/2509.06010</link>
<guid>https://arxiv.org/abs/2509.06010</guid>
<content:encoded><![CDATA[
arXiv:2509.06010v1 Announce Type: new 
Abstract: Visual Question Answering (VQA) holds great potential for assisting Blind and Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual impairments, BLV users often take blurry or poorly framed photos and face difficulty in articulating specific questions about what they cannot fully see. As a result, their visual questions are frequently ambiguous, and different users may interpret them in diverse ways. This leads to multiple valid answers, each grounded in different image regions-posing a mismatch with conventional VQA systems that assume a single answer and region. To bridge this gap, we present BLaVe-CoT, a VQA framework designed to reason about answer consistency in the face of ambiguity. Our method proposes diverse candidate answers using a LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer, and finally applies a chain-of-thought reasoning module to assess whether the answers refer to the same or different regions. Evaluated on the VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves more robust to the ambiguity and visual noise common in assistive settings. This work highlights the need for VQA systems that can adapt to real human uncertainty and provide inclusive support for BLV users. To foster further research and accessibility applications, we have made the code publicly available at https://github.com/Accecwan/BLaVe-CoT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection</title>
<link>https://arxiv.org/abs/2509.06011</link>
<guid>https://arxiv.org/abs/2509.06011</guid>
<content:encoded><![CDATA[
arXiv:2509.06011v1 Announce Type: new 
Abstract: Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology for applications involving Unmanned Aerial Vehicles (UAVs). However, the prevailing large-scale datasets for OVD pre-training are predominantly composed of ground-level, natural images. This creates a significant domain gap, causing models trained on them to exhibit a substantial drop in performance on UAV imagery. To address this limitation, we first propose a refined UAV-Label engine. Then we construct and introduce UAVDE-2M(contains over 2,000,000 instances and 1800 categories) and UAVCAP-15k(contains over 15,000 images). Furthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE) module and integrate it into the YOLO-World-v2 architecture. Finally, extensive experiments on the VisDrone and SIMD datasets verify the effectiveness of our proposed method for applications in UAV-based imagery and remote sensing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Micro-Expression Recognition via Fine-Grained Dynamic Perception</title>
<link>https://arxiv.org/abs/2509.06015</link>
<guid>https://arxiv.org/abs/2509.06015</guid>
<content:encoded><![CDATA[
arXiv:2509.06015v1 Announce Type: new 
Abstract: Facial micro-expression recognition (MER) is a challenging task, due to the transience, subtlety, and dynamics of micro-expressions (MEs). Most existing methods resort to hand-crafted features or deep networks, in which the former often additionally requires key frames, and the latter suffers from small-scale and low-diversity training data. In this paper, we develop a novel fine-grained dynamic perception (FDP) framework for MER. We propose to rank frame-level features of a sequence of raw frames in chronological order, in which the rank process encodes the dynamic information of both ME appearances and motions. Specifically, a novel local-global feature-aware transformer is proposed for frame representation learning. A rank scorer is further adopted to calculate rank scores of each frame-level feature. Afterwards, the rank features from rank scorer are pooled in temporal dimension to capture dynamic representation. Finally, the dynamic representation is shared by a MER module and a dynamic image construction module, in which the former predicts the ME category, and the latter uses an encoder-decoder structure to construct the dynamic image. The design of dynamic image construction task is beneficial for capturing facial subtle actions associated with MEs and alleviating the data scarcity issue. Extensive experiments show that our method (i) significantly outperforms the state-of-the-art MER methods, and (ii) works well for dynamic image construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11% over the previous best results in terms of F1-score on the CASME II, SAMM, CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at https://github.com/CYF-cuber/FDP.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</title>
<link>https://arxiv.org/abs/2509.06023</link>
<guid>https://arxiv.org/abs/2509.06023</guid>
<content:encoded><![CDATA[
arXiv:2509.06023v1 Announce Type: new 
Abstract: Visual-LiDAR odometry is a critical component for autonomous system localization, yet achieving high accuracy and strong robustness remains a challenge. Traditional approaches commonly struggle with sensor misalignment, fail to fully leverage temporal information, and require extensive manual tuning to handle diverse sensor configurations. To address these problems, we introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse spatial-temporal fusion to enhance accuracy and robustness. Our approach proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction and Update module that integrates temporally-predicted positions with current frame data, providing better initialization values for pose estimation and enhancing model's robustness against accumulative errors; and (3) a Temporal Clip Training strategy combined with a Collective Average Loss mechanism that aggregates losses across multiple frames, enabling global optimization and reducing the scale drift over long sequences. Extensive experiments on the KITTI and Argoverse Odometry dataset demonstrate the superiority of our proposed DVLO4D, which achieves state-of-the-art performance in terms of both pose accuracy and robustness. Additionally, our method has high efficiency, with an inference time of 82 ms, possessing the potential for the real-time deployment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Blood Report Images Using General Purpose Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06033</link>
<guid>https://arxiv.org/abs/2509.06033</guid>
<content:encoded><![CDATA[
arXiv:2509.06033v1 Announce Type: new 
Abstract: The reliable analysis of blood reports is important for health knowledge, but individuals often struggle with interpretation, leading to anxiety and overlooked issues. We explore the potential of general-purpose Vision-Language Models (VLMs) to address this challenge by automatically analyzing blood report images. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini 2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of 100 diverse blood report images. Each model was prompted with clinically relevant questions adapted to each blood report. The answers were then processed using Sentence-BERT to compare and evaluate how closely the models responded. The findings suggest that general-purpose VLMs are a practical and promising technology for developing patient-facing tools for preliminary blood report analysis. Their ability to provide clear interpretations directly from images can improve health literacy and reduce the limitations to understanding complex medical information. This work establishes a foundation for the future development of reliable and accessible AI-assisted healthcare applications. While results are encouraging, they should be interpreted cautiously given the limited dataset size.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
arXiv:2509.06035v1 Announce Type: new 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06040</link>
<guid>https://arxiv.org/abs/2509.06040</guid>
<content:encoded><![CDATA[
arXiv:2509.06040v1 Announce Type: new 
Abstract: Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities</title>
<link>https://arxiv.org/abs/2509.06041</link>
<guid>https://arxiv.org/abs/2509.06041</guid>
<content:encoded><![CDATA[
arXiv:2509.06041v1 Announce Type: new 
Abstract: Buoyancy-driven heat transfer in closed cavities serves as a canonical testbed for thermal design High-fidelity CFD modelling yields accurate thermal field solutions, yet its reliance on expert-crafted physics models, fine meshes, and intensive computation limits rapid iteration. Recent developments in data-driven modeling, especially Graph Neural Networks (GNNs), offer new alternatives for learning thermal-fluid behavior directly from simulation data, particularly on irregular mesh structures. However, conventional GNNs often struggle to capture long-range dependencies in high-resolution graph structures. To overcome this limitation, we propose a novel multi-stage GNN architecture that leverages hierarchical pooling and unpooling operations to progressively model global-to-local interactions across multiple spatial scales. We evaluate the proposed model on our newly developed CFD dataset simulating natural convection within a rectangular cavities with varying aspect ratios where the bottom wall is isothermal hot, the top wall is isothermal cold, and the two vertical walls are adiabatic. Experimental results demonstrate that the proposed model achieves higher predictive accuracy, improved training efficiency, and reduced long-term error accumulation compared to state-of-the-art (SOTA) GNN baselines. These findings underscore the potential of the proposed multi-stage GNN approach for modeling complex heat transfer in mesh-based fluid dynamics simulations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Home-made Diffusion Model from Scratch to Hatch</title>
<link>https://arxiv.org/abs/2509.06068</link>
<guid>https://arxiv.org/abs/2509.06068</guid>
<content:encoded><![CDATA[
arXiv:2509.06068v1 Announce Type: new 
Abstract: We introduce Home-made Diffusion Model (HDM), an efficient yet powerful text-to-image diffusion model optimized for training (and inferring) on consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality while maintaining a remarkably low training cost of $535-620 using four RTX5090 GPUs, representing a significant reduction in computational requirements compared to traditional approaches. Our key contributions include: (1) Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer (XUT), that employs cross-attention for skip connections, providing superior feature integration that leads to remarkable compositional consistency; (2) a comprehensive training recipe that incorporates TREAD acceleration, a novel shifted square crop strategy for efficient arbitrary aspect-ratio training, and progressive resolution scaling; and (3) an empirical demonstration that smaller models (343M parameters) with carefully crafted architectures can achieve high-quality results and emergent capabilities, such as intuitive camera control. Our work provides an alternative paradigm of scaling, demonstrating a viable path toward democratizing high-quality text-to-image generation for individual researchers and smaller organizations with limited computational resources.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization</title>
<link>https://arxiv.org/abs/2509.06082</link>
<guid>https://arxiv.org/abs/2509.06082</guid>
<content:encoded><![CDATA[
arXiv:2509.06082v1 Announce Type: new 
Abstract: In this work, we develop a novel technique for reconstructing images from projection-based nano- and microtomography. Our contribution focuses on enhancing reconstruction quality, particularly for specimen composed of homogeneous material phases connected by sharp edges. This is accomplished by training a neural network to identify edges within subpictures. The trained network is then integrated into a mathematical optimization model, to reduce artifacts from previous reconstructions. To this end, the optimization approach favors solutions according to the learned predictions, however may also determine alternative solutions if these are strongly supported by the raw data. Hence, our technique successfully incorporates knowledge about the homogeneity and presence of sharp edges in the sample and thereby eliminates blurriness. Our results on experimental datasets show significant enhancements in interface sharpness and material homogeneity compared to benchmark algorithms. Thus, our technique produces high-quality reconstructions, showcasing its potential for advancing tomographic imaging techniques.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.06096</link>
<guid>https://arxiv.org/abs/2509.06096</guid>
<content:encoded><![CDATA[
arXiv:2509.06096v1 Announce Type: new 
Abstract: Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&amp;G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology</title>
<link>https://arxiv.org/abs/2509.06105</link>
<guid>https://arxiv.org/abs/2509.06105</guid>
<content:encoded><![CDATA[
arXiv:2509.06105v1 Announce Type: new 
Abstract: Accurate analysis of pathological images is essential for automated tumor diagnosis but remains challenging due to high structural similarity and subtle morphological variations in tissue images. Current vision-language (VL) models often struggle to capture the complex reasoning required for interpreting structured pathological reports. To address these limitations, we propose PathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in hierarchical semantic understanding and compositional reasoning within the pathology domain. Results of this benchmark reveal that existing VL models fail to effectively model intricate cross-modal relationships, hence limiting their applicability in clinical setting. To overcome this, we further introduce a pathology-specific VL training scheme that generates enhanced and perturbed samples for multimodal contrastive learning. Experimental evaluations demonstrate that our approach achieves state-of-the-art performance on PathoHR-Bench and six additional pathology datasets, highlighting its effectiveness in fine-grained pathology representation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARDIE: clustering algorithm on relevant descriptors for image enhancement</title>
<link>https://arxiv.org/abs/2509.06116</link>
<guid>https://arxiv.org/abs/2509.06116</guid>
<content:encoded><![CDATA[
arXiv:2509.06116v1 Announce Type: new 
Abstract: Automatic image clustering is a cornerstone of computer vision, yet its application to image enhancement remains limited, primarily due to the difficulty of defining clusters that are meaningful for this specific task. To address this issue, we introduce CARDIE, an unsupervised algorithm that clusters images based on their color and luminosity content. In addition, we introduce a method to quantify the impact of image enhancement algorithms on luminance distribution and local variance. Using this method, we demonstrate that CARDIE produces clusters more relevant to image enhancement than those derived from semantic image attributes. Furthermore, we demonstrate that CARDIE clusters can be leveraged to resample image enhancement datasets, leading to improved performance for tone mapping and denoising algorithms. To encourage adoption and ensure reproducibility, we publicly release CARDIE code on our GitHub.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks</title>
<link>https://arxiv.org/abs/2509.06122</link>
<guid>https://arxiv.org/abs/2509.06122</guid>
<content:encoded><![CDATA[
arXiv:2509.06122v1 Announce Type: new 
Abstract: Multispectral and hyperspectral imagery are widely used in agriculture, environmental monitoring, and urban planning due to their complementary spatial and spectral characteristics. A fundamental trade-off persists: multispectral imagery offers high spatial but limited spectral resolution, while hyperspectral imagery provides rich spectra at lower spatial resolution. Prior hyperspectral generation approaches (e.g., pan-sharpening variants, matrix factorization, CNNs) often struggle to jointly preserve spatial detail and spectral fidelity. In response, we propose SpecSwin3D, a transformer-based model that generates hyperspectral imagery from multispectral inputs while preserving both spatial and spectral quality. Specifically, SpecSwin3D takes five multispectral bands as input and reconstructs 224 hyperspectral bands at the same spatial resolution. In addition, we observe that reconstruction errors grow for hyperspectral bands spectrally distant from the input bands. To address this, we introduce a cascade training strategy that progressively expands the spectral range to stabilize learning and improve fidelity. Moreover, we design an optimized band sequence that strategically repeats and orders the five selected multispectral bands to better capture pairwise relations within a 3D shifted-window transformer framework. Quantitatively, our model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96, outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by more than half. Beyond reconstruction, we further demonstrate the practical value of SpecSwin3D on two downstream tasks, including land use classification and burnt area segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving</title>
<link>https://arxiv.org/abs/2509.06142</link>
<guid>https://arxiv.org/abs/2509.06142</guid>
<content:encoded><![CDATA[
arXiv:2509.06142v1 Announce Type: new 
Abstract: The integration of AI with medical images enables the extraction of implicit image-derived biomarkers for a precise health assessment. Recently, retinal age, a biomarker predicted from fundus images, is a proven predictor of systemic disease risks, behavioral patterns, aging trajectory and even mortality. However, the capability to infer such sensitive biometric data raises significant privacy risks, where unauthorized use of fundus images could lead to bioinformation leakage, breaching individual privacy. In response, we formulate a new research problem of biometric privacy associated with medical images and propose RetinaGuard, a novel privacy-enhancing framework that employs a feature-level generative adversarial masking mechanism to obscure retinal age while preserving image visual quality and disease diagnostic utility. The framework further utilizes a novel multiple-to-one knowledge distillation strategy incorporating a retinal foundation model and diverse surrogate age encoders to enable a universal defense against black-box age prediction models. Comprehensive evaluations confirm that RetinaGuard successfully obfuscates retinal age prediction with minimal impact on image quality and pathological feature representation. RetinaGuard is also flexible for extension to other medical image derived biomarkers. RetinaGuard is also flexible for extension to other medical image biomarkers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniVerse-1: Unified Audio-Video Generation via Stitching of Experts</title>
<link>https://arxiv.org/abs/2509.06155</link>
<guid>https://arxiv.org/abs/2509.06155</guid>
<content:encoded><![CDATA[
arXiv:2509.06155v1 Announce Type: new 
Abstract: We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v1 Announce Type: new 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models</title>
<link>https://arxiv.org/abs/2509.06228</link>
<guid>https://arxiv.org/abs/2509.06228</guid>
<content:encoded><![CDATA[
arXiv:2509.06228v1 Announce Type: new 
Abstract: Bone fractures present a major global health challenge, often resulting in pain, reduced mobility, and productivity loss, particularly in low-resource settings where access to expert radiology services is limited. Conventional imaging methods suffer from high costs, radiation exposure, and dependency on specialized interpretation. To address this, we developed an AI-based solution for automated fracture detection from X-ray images using a custom Convolutional Neural Network (CNN) and benchmarked it against transfer learning models including EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on the publicly available FracAtlas dataset, comprising 4,083 anonymized musculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94 precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset. Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50) performed poorly in this specific setup, these results should be interpreted in light of class imbalance and data set limitations. This work highlights the promise of lightweight CNNs for detecting fractures in X-rays and underscores the importance of fair benchmarking, diverse datasets, and external validation for clinical translation
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Light-Weight Object Recognition for Real-Time Document Detection</title>
<link>https://arxiv.org/abs/2509.06246</link>
<guid>https://arxiv.org/abs/2509.06246</guid>
<content:encoded><![CDATA[
arXiv:2509.06246v1 Announce Type: new 
Abstract: Object Recognition and Document Skew Estimation have come a long way in terms of performance and efficiency. New models follow one of two directions: improving performance using larger models, and improving efficiency using smaller models. However, real-time document detection and rectification is a niche that is largely unexplored by the literature, yet it remains a vital step for automatic information retrieval from visual documents. In this work, we strive towards an efficient document detection pipeline that is satisfactory in terms of Optical Character Recognition (OCR) retrieval and faster than other available solutions. We adapt IWPOD-Net, a license plate detection network, and train it for detection on NBID, a synthetic ID card dataset. We experiment with data augmentation and cross-dataset validation with MIDV (another synthetic ID and passport document dataset) to find the optimal scenario for the model. Other methods from both the Object Recognition and Skew Estimation state-of-the-art are evaluated for comparison with our approach. We use each method to detect and rectify the document, which is then read by an OCR system. The OCR output is then evaluated using a novel OCR quality metric based on the Levenshtein distance. Since the end goal is to improve automatic information retrieval, we use the overall OCR quality as a performance metric. We observe that with a promising model, document rectification does not have to be perfect to attain state-of-the-art performance scores. We show that our model is smaller and more efficient than current state-of-the-art solutions while retaining a competitive OCR quality metric. All code is available at https://github.com/BOVIFOCR/iwpod-doc-corners.git
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</title>
<link>https://arxiv.org/abs/2509.06266</link>
<guid>https://arxiv.org/abs/2509.06266</guid>
<content:encoded><![CDATA[
arXiv:2509.06266v1 Announce Type: new 
Abstract: Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution</title>
<link>https://arxiv.org/abs/2509.06282</link>
<guid>https://arxiv.org/abs/2509.06282</guid>
<content:encoded><![CDATA[
arXiv:2509.06282v1 Announce Type: new 
Abstract: Skin health and disease resistance are closely linked to the skin barrier function, which protects against environmental factors and water loss. Two key physiological indicators can quantitatively represent this barrier function: skin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH and TEWL is valuable for the public to monitor skin conditions regularly, diagnose dermatological issues, and personalize their skincare regimens. However, these measurements are not easily accessible to general users unless they visit a dermatology clinic with specialized instruments. To tackle this problem, we propose a systematic solution to estimate SH and TEWL from selfie facial images remotely with smartphones. Our solution encompasses multiple stages, including SH/TEWL data collection, data preprocessing, and formulating a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression. Through experiments, we identified the annotation imbalance of the SH/TEWL data and proposed a symmetric-based contrastive regularization to reduce the model bias due to the imbalance effectively. This work is the first study to explore skin assessment from selfie facial images without physical measurements. It bridges the gap between computer vision and skin care research, enabling AI-driven accessible skin analysis for broader real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding</title>
<link>https://arxiv.org/abs/2509.06291</link>
<guid>https://arxiv.org/abs/2509.06291</guid>
<content:encoded><![CDATA[
arXiv:2509.06291v1 Announce Type: new 
Abstract: Visual Grounding (VG) aims to utilize given natural language queries to locate specific target objects within images. While current transformer-based approaches demonstrate strong localization performance in standard scene (i.e, scenarios without any novel objects), they exhibit notable limitations in open-vocabulary scene (i.e, both familiar and novel object categories during testing). These limitations primarily stem from three key factors: (1) imperfect alignment between visual and linguistic modalities, (2) insufficient cross-modal feature fusion, and (3) ineffective utilization of semantic prototype information. To overcome these challenges, we present Prototype-Aware Multimodal Learning (PAML), an innovative framework that systematically addresses these issues through several key components: First, we leverage ALBEF to establish robust cross-modal alignment during initial feature encoding. Subsequently, our Visual Discriminative Feature Encoder selectively enhances salient object representations while suppressing irrelevant visual context. The framework then incorporates a novel prototype discovering and inheriting mechanism that extracts and aggregates multi-neighbor semantic prototypes to facilitate open-vocabulary recognition. These enriched features undergo comprehensive multimodal integration through our Multi-stage Decoder before final bounding box regression. Extensive experiments across five benchmark datasets validate our approach, showing competitive performance in standard scene while achieving state-of-the-art results in open-vocabulary scene. Our code is available at https://github.com/plankXie/PAML.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.06306</link>
<guid>https://arxiv.org/abs/2509.06306</guid>
<content:encoded><![CDATA[
arXiv:2509.06306v1 Announce Type: new 
Abstract: Generalized Category Discovery (GCD) is an emerging and challenging open-world problem that has garnered increasing attention in recent years. Most existing GCD methods focus on discovering categories in static images. However, relying solely on static visual content is often insufficient to reliably discover novel categories. To bridge this gap, we extend the GCD problem to the video domain and introduce a new setting, termed Video-GCD. Thus, effectively integrating multi-perspective information across time is crucial for accurate Video-GCD. To tackle this challenge, we propose a novel Memory-guided Consistency-aware Contrastive Learning (MCCL) framework, which explicitly captures temporal-spatial cues and incorporates them into contrastive learning through a consistency-guided voting mechanism. MCCL consists of two core components: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided Representation Enhancement (MGRE). CACL exploits multiperspective temporal features to estimate consistency scores between unlabeled instances, which are then used to weight the contrastive loss accordingly. MGRE introduces a dual-level memory buffer that maintains both feature-level and logit-level representations, providing global context to enhance intra-class compactness and inter-class separability. This in turn refines the consistency estimation in CACL, forming a mutually reinforcing feedback loop between representation learning and consistency modeling. To facilitate a comprehensive evaluation, we construct a new and challenging Video-GCD benchmark, which includes action recognition and bird classification video datasets. Extensive experiments demonstrate that our method significantly outperforms competitive GCD approaches adapted from image-based settings, highlighting the importance of temporal information for discovering novel categories in videos. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text4Seg++: Advancing Image Segmentation via Generative Language Modeling</title>
<link>https://arxiv.org/abs/2509.06321</link>
<guid>https://arxiv.org/abs/2509.06321</guid>
<content:encoded><![CDATA[
arXiv:2509.06321v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks. However, effectively integrating image segmentation into these models remains a significant challenge. In this work, we propose a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. We first introduce image-wise semantic descriptors, a patch-aligned textual representation of segmentation masks that integrates naturally into the language modeling pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\times$, without compromising performance. Building upon this, our initial framework Text4Seg achieves strong segmentation performance across a wide range of vision tasks. To further improve granularity and compactness, we propose box-wise semantic descriptors, which localizes regions of interest using bounding boxes and represents region masks via structured mask tokens called semantic bricks. This leads to our refined model, Text4Seg++, which formulates segmentation as a next-brick prediction task, combining precision, scalability, and generative efficiency. Comprehensive experiments on natural and remote sensing datasets show that Text4Seg++ consistently outperforms state-of-the-art models across diverse benchmarks without any task-specific fine-tuning, while remaining compatible with existing MLLM backbones. Our work highlights the effectiveness, scalability, and generalizability of text-driven image segmentation within the MLLM framework.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap</title>
<link>https://arxiv.org/abs/2509.06329</link>
<guid>https://arxiv.org/abs/2509.06329</guid>
<content:encoded><![CDATA[
arXiv:2509.06329v1 Announce Type: new 
Abstract: The precise characterization of plant morphology provides valuable insights into plant environment interactions and genetic evolution. A key technology for extracting this information is 3D segmentation, which delineates individual plant organs from complex point clouds. Despite significant progress in general 3D computer vision domains, the adoption of 3D segmentation for plant phenotyping remains limited by three major challenges: i) the scarcity of large-scale annotated datasets, ii) technical difficulties in adapting advanced deep neural networks to plant point clouds, and iii) the lack of standardized benchmarks and evaluation protocols tailored to plant science. This review systematically addresses these barriers by: i) providing an overview of existing 3D plant datasets in the context of general 3D segmentation domains, ii) systematically summarizing deep learning-based methods for point cloud semantic and instance segmentation, iii) introducing Plant Segmentation Studio (PSS), an open-source framework for reproducible benchmarking, and iv) conducting extensive quantitative experiments to evaluate representative networks and sim-to-real learning strategies. Our findings highlight the efficacy of sparse convolutional backbones and transformer-based instance segmentation, while also emphasizing the complementary role of modeling-based and augmentation-based synthetic data generation for sim-to-real learning in reducing annotation demands. In general, this study bridges the gap between algorithmic advances and practical deployment, providing immediate tools for researchers and a roadmap for developing data-efficient and generalizable deep learning solutions in 3D plant phenotyping. Data and code are available at https://github.com/perrydoremi/PlantSegStudio.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users</title>
<link>https://arxiv.org/abs/2509.06331</link>
<guid>https://arxiv.org/abs/2509.06331</guid>
<content:encoded><![CDATA[
arXiv:2509.06331v1 Announce Type: new 
Abstract: Currency recognition systems often overlook usability and authenticity assessment, especially in low-resource environments where visually impaired users and offline validation are common. While existing methods focus on denomination classification, they typically ignore physical degradation and forgery, limiting their applicability in real-world conditions. This paper presents a unified framework for currency evaluation that integrates three modules: denomination classification using lightweight CNN models, damage quantification through a novel Unified Currency Damage Index (UCDI), and counterfeit detection using feature-based template matching. The dataset consists of over 82,000 annotated images spanning clean, damaged, and counterfeit notes. Our Custom_CNN model achieves high classification performance with low parameter count. The UCDI metric provides a continuous usability score based on binary mask loss, chromatic distortion, and structural feature loss. The counterfeit detection module demonstrates reliable identification of forged notes across varied imaging conditions. The framework supports real-time, on-device inference and addresses key deployment challenges in constrained environments. Results show that accurate, interpretable, and compact solutions can support inclusive currency evaluation in practical settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Camera-Based Detection of Vulnerable Road Users</title>
<link>https://arxiv.org/abs/2509.06333</link>
<guid>https://arxiv.org/abs/2509.06333</guid>
<content:encoded><![CDATA[
arXiv:2509.06333v1 Announce Type: new 
Abstract: Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists represent more than half of global traffic deaths, yet their detection remains challenging in poor lighting, adverse weather, and unbalanced data sets. This paper presents a multimodal detection framework that integrates RGB and thermal infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI, BDD100K, and Teledyne FLIR datasets, with class re-weighting and light augmentations to improve minority-class performance and robustness, experiments show that 640-pixel resolution and partial backbone freezing optimise accuracy and efficiency, while class-weighted losses enhance recall for rare VRUs. Results highlight that thermal models achieve the highest precision, and RGB-to-thermal augmentation boosts recall, demonstrating the potential of multimodal detection to improve VRU safety at intersections.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
<link>https://arxiv.org/abs/2509.06335</link>
<guid>https://arxiv.org/abs/2509.06335</guid>
<content:encoded><![CDATA[
arXiv:2509.06335v1 Announce Type: new 
Abstract: We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. While augmenting prompts with textual description of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object level information. To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart utilizing textual description of objects in the prompt. The gain generalizes across different models, datasets and video understanding tasks such as reasoning temporal localization and dense captioning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2509.06336</link>
<guid>https://arxiv.org/abs/2509.06336</guid>
<content:encoded><![CDATA[
arXiv:2509.06336v1 Announce Type: new 
Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: https://github.com/Elune001/MVP-FAS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study</title>
<link>https://arxiv.org/abs/2509.06351</link>
<guid>https://arxiv.org/abs/2509.06351</guid>
<content:encoded><![CDATA[
arXiv:2509.06351v1 Announce Type: new 
Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require quick, accurate care to be effectively treated. Traditional diagnostic pipelines require extensive preparation and rely on separate, individual evaluations on histological images and colonoscopy footage, introducing possible variability and inefficiencies. This pilot study proposes a unified deep learning network that uses convolutional neural networks (CN N s) to classify both histopathological slides and colonoscopy video frames in one pipeline. The pipeline integrates class-balancing learning, robust augmentation, and calibration methods to ensure accurate results. Static colon histology images were taken from the PathMNIST dataset, and the lower gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset. The CNN architecture used was ResNet-50. This study demonstrates an interpretable and reproducible diagnostic pipeline that unifies multiple diagnostic modalities to advance and ease the detection of colorectal diseases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification</title>
<link>https://arxiv.org/abs/2509.06367</link>
<guid>https://arxiv.org/abs/2509.06367</guid>
<content:encoded><![CDATA[
arXiv:2509.06367v1 Announce Type: new 
Abstract: Drought stress is a major threat to global crop productivity, making its early and precise detection essential for sustainable agricultural management. Traditional approaches, though useful, are often time-consuming and labor-intensive, which has motivated the adoption of deep learning methods. In recent years, Convolutional Neural Network (CNN) and Vision Transformer architectures have been widely explored for drought stress identification; however, these models generally rely on a large number of trainable parameters, restricting their use in resource-limited and real-time agricultural settings. To address this challenge, we propose a novel lightweight hybrid CNN framework inspired by ResNet, DenseNet, and MobileNet architectures. The framework achieves a remarkable 15-fold reduction in trainable parameters compared to conventional CNN and Vision Transformer models, while maintaining competitive accuracy. In addition, we introduce a machine unlearning mechanism based on a gradient norm-based influence function, which enables targeted removal of specific training data influence, thereby improving model adaptability. The method was evaluated on an aerial image dataset of potato fields with expert-annotated healthy and drought-stressed regions. Experimental results show that our framework achieves high accuracy while substantially lowering computational costs. These findings highlight its potential as a practical, scalable, and adaptive solution for drought stress monitoring in precision agriculture, particularly under resource-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Super Resolution Model is not Enough for Tackling Real-World Scenarios</title>
<link>https://arxiv.org/abs/2509.06387</link>
<guid>https://arxiv.org/abs/2509.06387</guid>
<content:encoded><![CDATA[
arXiv:2509.06387v1 Announce Type: new 
Abstract: Despite remarkable progress in Single Image Super-Resolution (SISR), traditional models often struggle to generalize across varying scale factors, limiting their real-world applicability. To address this, we propose a plug-in Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR models with the ability to perform arbitrary-scale SR. SAAM employs lightweight, scale-adaptive feature extraction and upsampling, incorporating the Simple parameter-free Attention Module (SimAM) for efficient guidance and gradient variance loss to enhance sharpness in image details. Our method integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet, HiT-SR, OverNet), delivering competitive or superior performance across a wide range of integer and non-integer scale factors. Extensive experiments on benchmark datasets demonstrate that our approach enables robust multi-scale upscaling with minimal computational overhead, offering a practical solution for real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery</title>
<link>https://arxiv.org/abs/2509.06396</link>
<guid>https://arxiv.org/abs/2509.06396</guid>
<content:encoded><![CDATA[
arXiv:2509.06396v1 Announce Type: new 
Abstract: Brain Metastases (BM) are a large contributor to mortality of patients with cancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored with Magnetic Resonance Imaging (MRI) at regular follow-up intervals according to treatment guidelines. Analyzing and quantifying this longitudinal imaging represents an intractable workload for clinicians. As a result, follow-up images are not annotated and merely assessed by observation. Response to treatment in longitudinal imaging is being studied, to better understand growth trajectories and ultimately predict treatment success or toxicity as early as possible. In this study, we implement an automated pipeline to curate a large longitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in 177 patients who were monitored for >360 days at approximately two-month intervals at Lausanne University Hospital (CHUV). We use a data-driven clustering to identify characteristic trajectories. In addition, we predict 12 months lesion-level response using classical as well as graph machine learning Graph Machine Learning (GML). Clustering revealed 5 dominant growth trajectories with distinct final response categories. Response prediction reaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first follow-up MRI with gradient boosting. Similarly, robust predictive performance of up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more flexibility with a single model for multiple input time-points configurations. Our results suggest potential automation and increased precision for the comprehensive assessment and prediction of BM response to SRS in longitudinal MRI. The proposed pipeline facilitates scalable data curation for the investigation of BM growth patterns, and lays the foundation for clinical decision support systems aiming at optimizing personalized care.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom</title>
<link>https://arxiv.org/abs/2509.06400</link>
<guid>https://arxiv.org/abs/2509.06400</guid>
<content:encoded><![CDATA[
arXiv:2509.06400v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results</title>
<link>https://arxiv.org/abs/2509.06413</link>
<guid>https://arxiv.org/abs/2509.06413</guid>
<content:encoded><![CDATA[
arXiv:2509.06413v1 Announce Type: new 
Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06415</link>
<guid>https://arxiv.org/abs/2509.06415</guid>
<content:encoded><![CDATA[
arXiv:2509.06415v1 Announce Type: new 
Abstract: Recent progress in vision-language models (VLMs) has led to impressive results in document understanding tasks, but their high computational demands remain a challenge. To mitigate the compute burdens, we propose a lightweight token pruning framework that filters out non-informative background regions from document images prior to VLM processing. A binary patch-level classifier removes non-text areas, and a max-pooling refinement step recovers fragmented text regions to enhance spatial coherence. Experiments on real-world document datasets demonstrate that our approach substantially lowers computational costs, while maintaining comparable accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM</title>
<link>https://arxiv.org/abs/2509.06422</link>
<guid>https://arxiv.org/abs/2509.06422</guid>
<content:encoded><![CDATA[
arXiv:2509.06422v1 Announce Type: new 
Abstract: Video camouflaged object detection (VCOD) is challenging due to dynamic environments. Existing methods face two main issues: (1) SAM-based methods struggle to separate camouflaged object edges due to model freezing, and (2) MLLM-based methods suffer from poor object separability as large language models merge foreground and background. To address these issues, we propose a novel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the separability of object edge details, we represent video sequences with temporal and spatial clues and perform feature fusion via LLM to increase information density. Next, multiple cues are generated through the dynamic foreground visual token scoring module and the prompt network to adaptively guide and fine-tune the SAM model, enabling it to adapt to subtle textures. To enhance the separability of objects and background, we propose a decoupled foreground-background learning strategy. By generating foreground and background cues separately and performing decoupled training, the visual token can effectively integrate foreground and background information independently, enabling SAM to more accurately segment camouflaged objects in the video. Experiments on the MoCA-Mask dataset show that Phantom-Insight achieves state-of-the-art performance across various metrics. Additionally, its ability to detect unseen camouflaged objects on the CAD2016 dataset highlights its strong generalization ability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection</title>
<link>https://arxiv.org/abs/2509.06427</link>
<guid>https://arxiv.org/abs/2509.06427</guid>
<content:encoded><![CDATA[
arXiv:2509.06427v1 Announce Type: new 
Abstract: Muzzle patterns are among the most effective biometric traits for cattle identification. Fast and accurate detection of the muzzle region as the region of interest is critical to automatic visual cattle identification.. Earlier approaches relied on manual detection, which is labor-intensive and inconsistent. Recently, automated methods using supervised models like YOLO have become popular for muzzle detection. Although effective, these methods require extensive annotated datasets and tend to be trained data-dependent, limiting their performance on new or unseen cattle. To address these limitations, this study proposes a zero-shot muzzle detection framework based on Grounding DINO, a vision-language model capable of detecting muzzles without any task-specific training or annotated data. This approach leverages natural language prompts to guide detection, enabling scalable and flexible muzzle localization across diverse breeds and environments. Our model achieves a mean Average Precision (mAP)@0.5 of 76.8\%, demonstrating promising performance without requiring annotated data. To our knowledge, this is the first research to provide a real-world, industry-oriented, and annotation-free solution for cattle muzzle detection. The framework offers a practical alternative to supervised methods, promising improved adaptability and ease of deployment in livestock monitoring applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment</title>
<link>https://arxiv.org/abs/2509.06442</link>
<guid>https://arxiv.org/abs/2509.06442</guid>
<content:encoded><![CDATA[
arXiv:2509.06442v1 Announce Type: new 
Abstract: Many super-resolution (SR) algorithms have been proposed to increase image resolution. However, full-reference (FR) image quality assessment (IQA) metrics for comparing and evaluating different SR algorithms are limited. In this work, we propose the Perception-oriented Bidirectional Attention Network (PBAN) for image SR FR-IQA, which is composed of three modules: an image encoder module, a perception-oriented bidirectional attention (PBA) module, and a quality prediction module. First, we encode the input images for feature representations. Inspired by the characteristics of the human visual system, we then construct the perception-oriented PBA module. Specifically, different from existing attention-based SR IQA methods, we conceive a Bidirectional Attention to bidirectionally construct visual attention to distortion, which is consistent with the generation and evaluation processes of SR images. To further guide the quality assessment towards the perception of distorted information, we propose Grouped Multi-scale Deformable Convolution, enabling the proposed method to adaptively perceive distortion. Moreover, we design Sub-information Excitation Convolution to direct visual perception to both sub-pixel and sub-channel attention. Finally, the quality prediction module is exploited to integrate quality-aware features and regress quality scores. Extensive experiments demonstrate that our proposed PBAN outperforms state-of-the-art quality assessment methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark</title>
<link>https://arxiv.org/abs/2509.06456</link>
<guid>https://arxiv.org/abs/2509.06456</guid>
<content:encoded><![CDATA[
arXiv:2509.06456v1 Announce Type: new 
Abstract: Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by the sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical lidar and a hybrid semi-solid-state lidar, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas. Then, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration. Extensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$, respectively, and improves the registration recall (RR) by $5.4\%$, which validates its effectiveness in achieving accurate cross-source registration.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.06459</link>
<guid>https://arxiv.org/abs/2509.06459</guid>
<content:encoded><![CDATA[
arXiv:2509.06459v1 Announce Type: new 
Abstract: Deep neural networks currently dominate many fields of the artificial intelligence landscape, achieving state-of-the-art results on numerous tasks while remaining hard to understand and exhibiting surprising weaknesses. An active area of research focuses on adversarial attacks, which aim to generate inputs that uncover these weaknesses. However, this proves challenging, especially in the black-box scenario where model details are inaccessible. This paper explores in detail the impact of such adversarial algorithms on ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101 datasets, we benchmark two novel black-box iterative adversarial algorithms based on affine transformations and genetic algorithms: 1) Affine Transformation Attack (ATA), an iterative algorithm maximizing our attack score function using random affine transformations, and 2) Affine Genetic Attack (AGA), a genetic algorithm that involves random noise and affine transformations. We evaluate the performance of the models in the algorithm parameter variation, data augmentation, and global and targeted attack configurations. We also compare our algorithms with two black-box adversarial algorithms, Pixle and Square Attack. Our experiments yield better results on the image classification task than similar methods in the literature, achieving an accuracy improvement of up to 8.82%. We provide noteworthy insights into successful adversarial defenses and attacks at both global and targeted levels, and demonstrate adversarial robustness through algorithm parameter variation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning</title>
<link>https://arxiv.org/abs/2509.06461</link>
<guid>https://arxiv.org/abs/2509.06461</guid>
<content:encoded><![CDATA[
arXiv:2509.06461v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical 3D Stomach Shape Model for Anatomical Analysis</title>
<link>https://arxiv.org/abs/2509.06464</link>
<guid>https://arxiv.org/abs/2509.06464</guid>
<content:encoded><![CDATA[
arXiv:2509.06464v1 Announce Type: new 
Abstract: Realistic and parameterized 3D models of human anatomy have become invaluable in research, diagnostics, and surgical planning. However, the development of detailed models for internal organs, such as the stomach, has been limited by data availability and methodological challenges. In this paper, we propose a novel pipeline for the generation of synthetic 3D stomach models, enabling the creation of anatomically diverse morphologies informed by established studies on stomach shape variability. Using this pipeline, we construct a dataset of synthetic stomachs. Building on this dataset, we develop a 3D statistical shape model of the stomach, trained to capture natural anatomical variability in a low-dimensional shape space. The model is further refined using CT meshes derived from publicly available datasets through a semi-supervised alignment process, enhancing its ability to generalize to unseen anatomical variations. We evaluated the model on a held-out test set of real stomach CT scans, demonstrating robust generalization and fit accuracy. We make the statistical shape model along with the synthetic dataset publicly available on GitLab: https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research. This work introduces the first statistical 3D shape model of the stomach, with applications ranging from surgical simulation and pre-operative planning to medical education and computational modeling. By combining synthetic data generation, parametric modeling, and real-world validation, our approach represents a significant advancement in organ modeling and opens new possibilities for personalized healthcare solutions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does DINOv3 Set a New Medical Vision Standard?</title>
<link>https://arxiv.org/abs/2509.06467</link>
<guid>https://arxiv.org/abs/2509.06467</guid>
<content:encoded><![CDATA[
arXiv:2509.06467v1 Announce Type: new 
Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2509.06482</link>
<guid>https://arxiv.org/abs/2509.06482</guid>
<content:encoded><![CDATA[
arXiv:2509.06482v1 Announce Type: new 
Abstract: Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</title>
<link>https://arxiv.org/abs/2509.06485</link>
<guid>https://arxiv.org/abs/2509.06485</guid>
<content:encoded><![CDATA[
arXiv:2509.06485v1 Announce Type: new 
Abstract: In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \textit{before} and \textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including "before" and "after" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</title>
<link>https://arxiv.org/abs/2509.06499</link>
<guid>https://arxiv.org/abs/2509.06499</guid>
<content:encoded><![CDATA[
arXiv:2509.06499v1 Announce Type: new 
Abstract: Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired "winning" (balanced preservation-compliance) and "losing" (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE's superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE's versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at https://github.com/KomJay520/TIDE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach</title>
<link>https://arxiv.org/abs/2509.06511</link>
<guid>https://arxiv.org/abs/2509.06511</guid>
<content:encoded><![CDATA[
arXiv:2509.06511v1 Announce Type: new 
Abstract: Accurate evaluation of the response of glioblastoma to therapy is crucial for clinical decision-making and patient management. The Response Assessment in Neuro-Oncology (RANO) criteria provide a standardized framework to assess patients' clinical response, but their application can be complex and subject to observer variability. This paper presents an automated method for classifying the intervention response from longitudinal MRI scans, developed to predict tumor response during therapy as part of the BraTS 2025 challenge. We propose a novel hybrid framework that combines deep learning derived feature extraction and an extensive set of radiomics and clinically chosen features. Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D regions of interest across four MRI modalities. These deep features are then fused with a rich set of more than 4800 radiomic and clinically driven features, including 3D radiomics of tumor growth and shrinkage masks, volumetric changes relative to the nadir, and tumor centroid shift. Using the fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a Macro F1 score of 0.50 in the 4-class response prediction task (Complete Response, Partial Response, Stable Disease, Progressive Disease). Our results highlight that synergizing learned image representations with domain-targeted radiomic features provides a robust and effective solution for automated treatment response assessment in neuro-oncology.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''</title>
<link>https://arxiv.org/abs/2509.06535</link>
<guid>https://arxiv.org/abs/2509.06535</guid>
<content:encoded><![CDATA[
arXiv:2509.06535v1 Announce Type: new 
Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking EfficientTAM on FMO datasets</title>
<link>https://arxiv.org/abs/2509.06536</link>
<guid>https://arxiv.org/abs/2509.06536</guid>
<content:encoded><![CDATA[
arXiv:2509.06536v1 Announce Type: new 
Abstract: Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval</title>
<link>https://arxiv.org/abs/2509.06566</link>
<guid>https://arxiv.org/abs/2509.06566</guid>
<content:encoded><![CDATA[
arXiv:2509.06566v1 Announce Type: new 
Abstract: The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural images matching the overall semantics and spatial layout of a free-hand sketch. Unlike prior work focused on architectural augmentations of retrieval models, we emphasize the inherent ambiguity and noise present in real-world sketches. This insight motivates a training objective that is explicitly designed to be robust to sketch variability. We show that with an appropriate combination of pre-training, encoder architecture, and loss formulation, it is possible to achieve state-of-the-art performance without the introduction of additional complexity. Extensive experiments on a challenging FS-COCO and widely-used SketchyCOCO datasets confirm the effectiveness of our approach and underline the critical role of training design in cross-modal retrieval tasks, as well as the need to improve the evaluation scenarios of scene-level SBIR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
<link>https://arxiv.org/abs/2509.06570</link>
<guid>https://arxiv.org/abs/2509.06570</guid>
<content:encoded><![CDATA[
arXiv:2509.06570v1 Announce Type: new 
Abstract: Existing open set recognition (OSR) methods are typically designed for static scenarios, where models aim to classify known classes and identify unknown ones within fixed scopes. This deviates from the expectation that the model should incrementally identify newly emerging unknown classes from continuous data streams and acquire corresponding knowledge. In such evolving scenarios, the discriminability of OSR decision boundaries is hard to maintain due to restricted access to former training data, causing severe inter-class confusion. To solve this problem, we propose retentive angular representation learning (RARL) for incremental open set recognition (IOSR). In RARL, unknown representations are encouraged to align around inactive prototypes within an angular space constructed under the equiangular tight frame, thereby mitigating excessive representation drift during knowledge updates. Specifically, we adopt a virtual-intrinsic interactive (VII) training strategy, which compacts known representations by enforcing clear inter-class margins through boundary-proximal virtual classes. Furthermore, a stratified rectification strategy is designed to refine decision boundaries, mitigating representation bias and feature space distortion caused by imbalances between old/new and positive/negative class samples. We conduct thorough evaluations on CIFAR100 and TinyImageNet datasets and establish a new benchmark for IOSR. Experimental results across various task setups demonstrate that the proposed method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating Condorcet Ordering for Vector-valued Mathematical Morphology</title>
<link>https://arxiv.org/abs/2509.06577</link>
<guid>https://arxiv.org/abs/2509.06577</guid>
<content:encoded><![CDATA[
arXiv:2509.06577v1 Announce Type: new 
Abstract: Mathematical morphology provides a nonlinear framework for image and spatial data processing and analysis. Although there have been many successful applications of mathematical morphology to vector-valued images, such as color and hyperspectral images, there is still no consensus on the most suitable vector ordering for constructing morphological operators. This paper addresses this issue by examining a reduced ordering approximating the Condorcet ranking derived from a set of vector orderings. Inspired by voting problems, the Condorcet ordering ranks elements from most to least voted, with voters representing different orderings. In this paper, we develop a machine learning approach that learns a reduced ordering that approximates the Condorcet ordering. Preliminary computational experiments confirm the effectiveness of learning the reduced mapping to define vector-valued morphological operators for color images.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</title>
<link>https://arxiv.org/abs/2509.06579</link>
<guid>https://arxiv.org/abs/2509.06579</guid>
<content:encoded><![CDATA[
arXiv:2509.06579v1 Announce Type: new 
Abstract: Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of trade in products derived from threatened species using machine learning and a smartphone</title>
<link>https://arxiv.org/abs/2509.06585</link>
<guid>https://arxiv.org/abs/2509.06585</guid>
<content:encoded><![CDATA[
arXiv:2509.06585v1 Announce Type: new 
Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now increasingly prevalent in digital marketplaces and social media. With the sheer volume of digital content, the need for automated methods to detect wildlife trade listings is growing. These methods are especially needed for the automatic identification of wildlife products, such as ivory. We developed machine learning-based object recognition models that can identify wildlife products within images and highlight them. The data consists of images of elephant, pangolin, and tiger products that were identified as being sold illegally or that were confiscated by authorities. Specifically, the wildlife products included elephant ivory and skins, pangolin scales, and claws (raw and crafted), and tiger skins and bones. We investigated various combinations of training strategies and two loss functions to identify the best model to use in the automatic detection of these wildlife products. Models were trained for each species while also developing a single model to identify products from all three species. The best model showed an overall accuracy of 84.2% with accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from elephants, pangolins, and tigers, respectively. We further demonstrate that the machine learning model can be made easily available to stakeholders, such as government authorities and law enforcement agencies, by developing a smartphone-based application that had an overall accuracy of 91.3%. The application can be used in real time to click images and help identify potentially prohibited products of target species. Thus, the proposed method is not only applicable for monitoring trade on the web but can also be used e.g. in physical markets for monitoring wildlife trade.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising</title>
<link>https://arxiv.org/abs/2509.06591</link>
<guid>https://arxiv.org/abs/2509.06591</guid>
<content:encoded><![CDATA[
arXiv:2509.06591v1 Announce Type: new 
Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework</title>
<link>https://arxiv.org/abs/2509.06625</link>
<guid>https://arxiv.org/abs/2509.06625</guid>
<content:encoded><![CDATA[
arXiv:2509.06625v1 Announce Type: new 
Abstract: Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery</title>
<link>https://arxiv.org/abs/2509.06660</link>
<guid>https://arxiv.org/abs/2509.06660</guid>
<content:encoded><![CDATA[
arXiv:2509.06660v1 Announce Type: new 
Abstract: High-throughput interpretation of robotically gathered seafloor visual imagery can increase the efficiency of marine monitoring and exploration. Although recent research has suggested that location metadata can enhance self-supervised feature learning (SSL), its benefits across different SSL strategies, models and seafloor image datasets are underexplored. This study evaluates the impact of location-based regularisation on six state-of-the-art SSL frameworks, which include Convolutional Neural Network (CNN) and Vision Transformer (ViT) models with varying latent-space dimensionality. Evaluation across three diverse seafloor image datasets finds that location-regularisation consistently improves downstream classification performance over standard SSL, with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for ViTs, respectively. While CNNs pretrained on generic datasets benefit from high-dimensional latent representations, dataset-optimised SSL achieves similar performance across the high (512) and low (128) dimensional latent representations. Location-regularised SSL improves CNN performance over pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and low-dimensional latent representations, respectively. For ViTs, high-dimensionality benefits both pre-trained and dataset-optimised SSL. Although location-regularisation improves SSL performance compared to standard SSL methods, pre-trained ViTs show strong generalisation, matching the best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$ and $0.795 \pm 0.077$, respectively. The findings highlight the value of location metadata for SSL regularisation, particularly when using low-dimensional latent representations, and demonstrate strong generalisation of high-dimensional ViTs for seafloor image analysis.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations</title>
<link>https://arxiv.org/abs/2509.06678</link>
<guid>https://arxiv.org/abs/2509.06678</guid>
<content:encoded><![CDATA[
arXiv:2509.06678v1 Announce Type: new 
Abstract: As long-endurance and seafloor-resident AUVs become more capable, there is an increasing need for extended, real-time interpretation of seafloor imagery to enable adaptive missions and optimise communication efficiency. Although offline image analysis methods are well established, they rely on access to complete datasets and human-labelled examples to manage the strong influence of environmental and operational conditions on seafloor image appearance-requirements that cannot be met in real-time settings. To address this, we introduce an online clustering framework (OCF) capable of interpreting seafloor imagery without supervision, which is designed to operate in real-time on continuous data streams in a scalable, adaptive, and self-consistent manner. The method enables the efficient review and consolidation of common patterns across the entire data history in constant time by identifying and maintaining a set of representative samples that capture the evolving feature distribution, supporting dynamic cluster merging and splitting without reprocessing the full image history. We evaluate the framework on three diverse seafloor image datasets, analysing the impact of different representative sampling strategies on both clustering accuracy and computational cost. The OCF achieves the highest average F1 score of 0.68 across the three datasets among all comparative online clustering approaches, with a standard deviation of 3% across three distinct survey trajectories, demonstrating its superior clustering capability and robustness to trajectory variation. In addition, it maintains consistently lower and bounded computational time as the data volume increases. These properties are beneficial for generating survey data summaries and supporting informative path planning in long-term, persistent autonomous marine exploration.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
<link>https://arxiv.org/abs/2509.06685</link>
<guid>https://arxiv.org/abs/2509.06685</guid>
<content:encoded><![CDATA[
arXiv:2509.06685v1 Announce Type: new 
Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring</title>
<link>https://arxiv.org/abs/2509.06690</link>
<guid>https://arxiv.org/abs/2509.06690</guid>
<content:encoded><![CDATA[
arXiv:2509.06690v1 Announce Type: new 
Abstract: Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment</title>
<link>https://arxiv.org/abs/2509.06693</link>
<guid>https://arxiv.org/abs/2509.06693</guid>
<content:encoded><![CDATA[
arXiv:2509.06693v1 Announce Type: new 
Abstract: Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal role in enhancing the performance of downstream anomaly segmentation, as it provides an effective means of expanding abnormal data. However, existing SIAS methods face several critical limitations: (i) the synthesized anomalies often lack intricate texture details and fail to align precisely with the surrounding background, and (ii) they struggle to generate fine-grained, pixel-level anomalies. To address these challenges, we propose Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed STAGE. STAGE introduces a novel anomaly inference strategy that incorporates clean background information as a prior to guide the denoising distribution, enabling the model to more effectively distinguish and highlight abnormal foregrounds. Furthermore, it employs a graded diffusion framework with an anomaly-only branch to explicitly record local anomalies during both the forward and reverse processes, ensuring that subtle anomalies are not overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA) strategy to progressively align the synthesized anomalies with the background, resulting in context-consistent and structurally coherent generations. Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE achieves state-of-the-art performance in SIAS, which in turn enhances downstream anomaly segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention</title>
<link>https://arxiv.org/abs/2509.06705</link>
<guid>https://arxiv.org/abs/2509.06705</guid>
<content:encoded><![CDATA[
arXiv:2509.06705v1 Announce Type: new 
Abstract: We present Cortex Synth, a novel end-to-end differentiable framework for joint 3D skeleton geometry and topology synthesis from single 2D images. Our architecture introduces three key innovations: (1) A hierarchical graph attention mechanism with multi-scale skeletal refinement, (2) Differentiable spectral topology optimization via Laplacian eigen decomposition, and (3) Adversarial geometric consistency training for pose structure alignment. The framework integrates four synergistic modules: a pseudo 3D point cloud generator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a novel Differentiable Graph Construction Network (DGCN). Our experiments demonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and 27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological errors by 42 percent compared to previous approaches. The model's end-to-end differentiability enables applications in robotic manipulation, medical imaging, and automated character rigging.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture</title>
<link>https://arxiv.org/abs/2509.06713</link>
<guid>https://arxiv.org/abs/2509.06713</guid>
<content:encoded><![CDATA[
arXiv:2509.06713v1 Announce Type: new 
Abstract: Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed model's performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training</title>
<link>https://arxiv.org/abs/2509.06723</link>
<guid>https://arxiv.org/abs/2509.06723</guid>
<content:encoded><![CDATA[
arXiv:2509.06723v1 Announce Type: new 
Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation</title>
<link>https://arxiv.org/abs/2509.06740</link>
<guid>https://arxiv.org/abs/2509.06740</guid>
<content:encoded><![CDATA[
arXiv:2509.06740v1 Announce Type: new 
Abstract: Histopathology image analysis is critical yet challenged by the demand of segmenting tissue regions and nuclei instances for tumor microenvironment and cellular morphology analysis. Existing studies focused on tissue semantic segmentation or nuclei instance segmentation separately, but ignored the inherent relationship between these two tasks, resulting in insufficient histopathology understanding. To address this issue, we propose a Co-Seg framework for collaborative tissue and nuclei segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing tissue and nuclei segmentation tasks to mutually enhance each other. To this end, we first devise a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and instance region prompts as prior constraints. Moreover, we design a mutual prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, collaboratively computing semantic and instance segmentation masks. Extensive experiments on the PUMA dataset demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the semantic, instance and panoptic segmentation of tumor tissues and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light</title>
<link>https://arxiv.org/abs/2509.06741</link>
<guid>https://arxiv.org/abs/2509.06741</guid>
<content:encoded><![CDATA[
arXiv:2509.06741v1 Announce Type: new 
Abstract: Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest environments for tasks such as environmental monitoring and search and rescue, which require safe navigation through dense foliage and precise data collection. Traditional sensing approaches, including passive multispectral and RGB imaging, suffer from latency, poor depth resolution, and strong dependence on ambient light - especially under forest canopies. In this work, we present a novel event spectroscopy system that simultaneously enables high-resolution, low-latency depth reconstruction and multispectral imaging using a single sensor. Depth is reconstructed using structured light, and by modulating the wavelength of the projected structured light, our system captures spectral information in controlled bands between 650 nm and 850 nm. We demonstrate up to $60\%$ improvement in RMSE over commercial depth sensors and validate the spectral accuracy against a reference spectrometer and commercial multispectral cameras, demonstrating comparable performance. A portable version limited to RGB (3 wavelengths) is used to collect real-world depth and spectral data from a Masoala Rainforest. We demonstrate the use of this prototype for color image reconstruction and material differentiation between leaves and branches using spectral and depth data. Our results show that adding depth (available at no extra effort with our setup) to material differentiation improves the accuracy by over $30\%$ compared to color-only method. Our system, tested in both lab and real-world rainforest environments, shows strong performance in depth estimation, RGB reconstruction, and material differentiation - paving the way for lightweight, integrated, and robust UAV perception and data collection in complex natural environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pothole Detection and Recognition based on Transfer Learning</title>
<link>https://arxiv.org/abs/2509.06750</link>
<guid>https://arxiv.org/abs/2509.06750</guid>
<content:encoded><![CDATA[
arXiv:2509.06750v1 Announce Type: new 
Abstract: With the rapid development of computer vision and machine learning, automated methods for pothole detection and recognition based on image and video data have received significant attention. It is of great significance for social development to conduct an in-depth analysis of road images through feature extraction, thereby achieving automatic identification of the pothole condition in new images. Consequently, this is the main issue addressed in this study. Based on preprocessing techniques such as standardization, normalization, and data augmentation applied to the collected raw dataset, we continuously improved the network model based on experimental results. Ultimately, we constructed a deep learning feature extraction network ResNet50-EfficientNet-RegNet model based on transfer learning. This model exhibits high classification accuracy and computational efficiency. In terms of model evaluation, this study employed a comparative evaluation approach by comparing the performance of the proposed transfer learning model with other models, including Random Forest, MLP, SVM, and LightGBM. The comparison analysis was conducted based on metrics such as Accuracy, Recall, Precision, F1-score, and FPS, to assess the classification performance of the transfer learning model proposed in this paper. The results demonstrate that our model exhibits high performance in terms of recognition speed and accuracy, surpassing the performance of other models. Through careful parameter selection and model optimization, our transfer learning model achieved a classification accuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89% (890/900) on the expanded test set.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Raw2Event: Converting Raw Frame Camera into Event Camera</title>
<link>https://arxiv.org/abs/2509.06767</link>
<guid>https://arxiv.org/abs/2509.06767</guid>
<content:encoded><![CDATA[
arXiv:2509.06767v1 Announce Type: new 
Abstract: Event cameras offer unique advantages such as high temporal resolution, low latency, and high dynamic range, making them more and more popular for vision tasks under challenging light conditions. However, their high cost, limited resolution, and lack of features such as autofocus hinder their broad adoption, particularly for early-stage development and prototyping. In this work, we present Raw2Event, a complete hardware-software system that enables real-time event generation from low-cost raw frame-based cameras. By leveraging direct access to raw Bayer data and bypassing traditional image signal processors (ISP), our system is able to utilize the full potential of camera hardware, delivering higher dynamic range, higher resolution, and more faithful output than RGB-based frame-to-event converters.
  Built upon the DVS-Voltmeter model, Raw2Event features a configurable simulation framework optimized for deployment on embedded platforms. We further design a data acquisition pipeline that supports synchronized recording of raw, RGB, and event streams, facilitating downstream evaluation and dataset creation. Experimental results show that Raw2Event can generate event streams closely resembling those from real event cameras, while benefiting from higher resolution and autofocus capabilities. The system also supports user-intuitive parameter tuning, enabling flexible adaptation to various application requirements. Finally, we deploy the system on a Raspberry Pi for real-time operation, providing a scalable and cost-effective solution for event-based vision research and early-stage system development.
  The codes are available online: https://anonymous.4open.science/r/raw2event-BFF2/README.md.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning</title>
<link>https://arxiv.org/abs/2509.06771</link>
<guid>https://arxiv.org/abs/2509.06771</guid>
<content:encoded><![CDATA[
arXiv:2509.06771v1 Announce Type: new 
Abstract: Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</title>
<link>https://arxiv.org/abs/2509.06781</link>
<guid>https://arxiv.org/abs/2509.06781</guid>
<content:encoded><![CDATA[
arXiv:2509.06781v1 Announce Type: new 
Abstract: This article presents UrbanTwin datasets - high-fidelity, realistic replicas of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I. Each UrbanTwin dataset contains 10K annotated frames corresponding to one of the public datasets. Annotations include 3D bounding boxes, instance segmentation labels, and tracking IDs for six object classes, along with semantic segmentation labels for nine classes. These datasets are synthesized using emulated lidar sensors within realistic digital twins, modeled based on surrounding geometry, road alignment at lane level, and the lane topology and vehicle movement patterns at intersections of the actual locations corresponding to each real dataset. Due to the precise digital twin modeling, the synthetic datasets are well aligned with their real counterparts, offering strong standalone and augmentative value for training deep learning models on tasks such as 3D object detection, tracking, and semantic and instance segmentation. We evaluate the alignment of the synthetic replicas through statistical and structural similarity analysis with real data, and further demonstrate their utility by training 3D object detection models solely on synthetic data and testing them on real, unseen data. The high similarity scores and improved detection performance, compared to the models trained on real data, indicate that the UrbanTwin datasets effectively enhance existing benchmark datasets by increasing sample size and scene diversity. In addition, the digital twins can be adapted to test custom scenarios by modifying the design and dynamics of the simulations. To our knowledge, these are the first digitally synthesized datasets that can replace in-domain real-world datasets for lidar perception tasks. UrbanTwin datasets are publicly available at https://dataverse.harvard.edu/dataverse/ucf-ut.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P3-SAM: Native 3D Part Segmentation</title>
<link>https://arxiv.org/abs/2509.06784</link>
<guid>https://arxiv.org/abs/2509.06784</guid>
<content:encoded><![CDATA[
arXiv:2509.06784v1 Announce Type: new 
Abstract: Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results</title>
<link>https://arxiv.org/abs/2509.06793</link>
<guid>https://arxiv.org/abs/2509.06793</guid>
<content:encoded><![CDATA[
arXiv:2509.06793v1 Announce Type: new 
Abstract: This paper presents a comprehensive review of the AIM 2025 High FPS Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions and final results. The objective of this challenge is to identify effective networks capable of producing clearer and visually compelling images in diverse and challenging conditions, by learning representative visual cues for complex aggregations of motion types. A total of 68 participants registered for the competition, and 9 teams ultimately submitted valid entries. This paper thoroughly evaluates the state-of-the-art advances in high-FPS single image motion deblurring, showcasing the significant progress in the field, while leveraging samples of the novel dataset, MIORe, that introduces challenging examples of movement patterns.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis</title>
<link>https://arxiv.org/abs/2509.06798</link>
<guid>https://arxiv.org/abs/2509.06798</guid>
<content:encoded><![CDATA[
arXiv:2509.06798v1 Announce Type: new 
Abstract: In the field of autonomous driving, sensor simulation is essential for generating rare and diverse scenarios that are difficult to capture in real-world environments. Current solutions fall into two categories: 1) CG-based methods, such as CARLA, which lack diversity and struggle to scale to the vast array of rare cases required for robust perception training; and 2) learning-based approaches, such as NeuSim, which are limited to specific object categories (vehicles) and require extensive multi-sensor data, hindering their applicability to generic objects. To address these limitations, we propose a scalable real2sim2real system that leverages 3D generation to automate asset mining, generation, and rare-case data synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIORe &amp; VAR-MIORe: Benchmarks to Push the Boundaries of Restoration</title>
<link>https://arxiv.org/abs/2509.06803</link>
<guid>https://arxiv.org/abs/2509.06803</guid>
<content:encoded><![CDATA[
arXiv:2509.06803v1 Announce Type: new 
Abstract: We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</title>
<link>https://arxiv.org/abs/2509.06818</link>
<guid>https://arxiv.org/abs/2509.06818</guid>
<content:encoded><![CDATA[
arXiv:2509.06818v1 Announce Type: new 
Abstract: Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With "multi-to-multi matching" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.06826</link>
<guid>https://arxiv.org/abs/2509.06826</guid>
<content:encoded><![CDATA[
arXiv:2509.06826v1 Announce Type: new 
Abstract: The rapid growth of visual content consumption across platforms necessitates automated video classification for age-suitability standards like the MPAA rating system (G, PG, PG-13, R). Traditional methods struggle with large labeled data requirements, poor generalization, and inefficient feature learning. To address these challenges, we employ contrastive learning for improved discrimination and adaptability, exploring three frameworks: Instance Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a Bahdanau attention mechanism, achieving state-of-the-art performance in the Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of 0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling, and attention mechanisms for dynamic frame prioritization, the model excels in fine-grained borderline distinctions, such as differentiating PG-13 and R-rated content. We evaluate the model's performance across various contrastive loss functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating the robustness of our proposed architecture. To ensure practical application, the model is deployed as a web application for real-time MPAA rating classification, offering an efficient solution for automated content compliance across streaming platforms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curia: A Multi-Modal Foundation Model for Radiology</title>
<link>https://arxiv.org/abs/2509.06830</link>
<guid>https://arxiv.org/abs/2509.06830</guid>
<content:encoded><![CDATA[
arXiv:2509.06830v1 Announce Type: new 
Abstract: AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis</title>
<link>https://arxiv.org/abs/2509.06831</link>
<guid>https://arxiv.org/abs/2509.06831</guid>
<content:encoded><![CDATA[
arXiv:2509.06831v1 Announce Type: new 
Abstract: We investigate how both the adaptation of a generic foundation model via transfer learning and the integration of complementary modalities from the operating room (OR) can support surgical data science. To this end, we use V-JEPA as the single-modality foundation of a multimodal model for minimally invasive surgery support. We analyze how the model's downstream performance can benefit (a) from finetuning on unlabeled surgical video data and (b) from providing additional time-resolved data streams from the OR in a multimodal setup.
  In an in-house dataset of liver surgery videos, we analyze the tasks of predicting hospital length of stay and postoperative complications. In videos of the public HeiCo dataset, we analyze the task of surgical phase recognition. As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on unlabeled, held-out videos to investigate its change in performance after domain adaptation. Following the idea of modular decision support networks, we integrate additional data streams from the OR by training a separate encoder to form a shared representation space with V-JEPA's embeddings.
  Our experiments show that finetuning on domain-specific data increases model performance. On the in-house data, integrating additional time-resolved data likewise benefits the model. On the HeiCo data, accuracy of the pretrained video-only, single-modality baseline setup is on par with the top-performing submissions of the EndoVis2017 challenge, while finetuning on domain-specific data increases accuracy further. Our results thus demonstrate how surgical data science can leverage public, generic foundation models. Likewise, they indicate the potential of domain adaptation and of integrating suitable complementary data streams from the OR. To support further research, we release our code and model weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset</title>
<link>https://arxiv.org/abs/2509.06835</link>
<guid>https://arxiv.org/abs/2509.06835</guid>
<content:encoded><![CDATA[
arXiv:2509.06835v1 Announce Type: new 
Abstract: Adversarial attacks pose significant threats to machine learning models by introducing carefully crafted perturbations that cause misclassification. While prior work has primarily focused on MNIST and similar datasets, this paper investigates the vulnerability of traffic sign classifiers using the LISA Traffic Sign dataset. We train a convolutional neural network to classify 47 different traffic signs and evaluate its robustness against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a sharp decline in classification accuracy as the perturbation magnitude increases, highlighting the models susceptibility to adversarial examples. This study lays the groundwork for future exploration into defense mechanisms tailored for real-world traffic sign recognition systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToonOut: Fine-tuned Background-Removal for Anime Characters</title>
<link>https://arxiv.org/abs/2509.06839</link>
<guid>https://arxiv.org/abs/2509.06839</guid>
<content:encoded><![CDATA[
arXiv:2509.06839v1 Announce Type: new 
Abstract: While state-of-the-art background removal models excel at realistic imagery, they frequently underperform in specialized domains such as anime-style content, where complex features like hair and transparency present unique challenges. To address this limitation, we collected and annotated a custom dataset of 1,228 high-quality anime images of characters and objects, and fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in marked improvements in background removal accuracy for anime-style images, increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric. We are open-sourcing the code, the fine-tuned model weights, as well as the dataset at: https://github.com/MatteoKartoon/BiRefNet.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice</title>
<link>https://arxiv.org/abs/2509.06854</link>
<guid>https://arxiv.org/abs/2509.06854</guid>
<content:encoded><![CDATA[
arXiv:2509.06854v1 Announce Type: new 
Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach</title>
<link>https://arxiv.org/abs/2509.06862</link>
<guid>https://arxiv.org/abs/2509.06862</guid>
<content:encoded><![CDATA[
arXiv:2509.06862v1 Announce Type: new 
Abstract: Non-rigid 3D mesh matching is a critical step in computer vision and computer graphics pipelines. We tackle matching meshes that contain topological artefacts which can break the assumption made by current approaches. While Functional Maps assume the deformation induced by the ground truth correspondences to be near-isometric, ARAP-like deformation-guided approaches assume the latter to be ARAP. Neither assumption holds in certain topological configurations of the input shapes. We are motivated by real-world scenarios such as per-frame multi-view reconstructions, often suffering from topological artefacts. To this end, we propose a topology-adaptive deformation model allowing changes in shape topology to align shape pairs under ARAP and bijective association constraints. Using this model, we jointly optimise for a template mesh with adequate topology and for its alignment with the shapes to be matched to extract correspondences. We show that, while not relying on any data-driven prior, our approach applies to highly non-isometric shapes and shapes with topological artefacts, including noisy per-frame multi-view reconstructions, even outperforming methods trained on large datasets in 3D alignment quality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition</title>
<link>https://arxiv.org/abs/2509.06868</link>
<guid>https://arxiv.org/abs/2509.06868</guid>
<content:encoded><![CDATA[
arXiv:2509.06868v1 Announce Type: new 
Abstract: Automatic License-Plate Recognition (ALPR) plays a pivotal role in Intelligent Transportation Systems (ITS) as a fundamental element of Smart Cities. However, due to its high variability, ALPR faces challenging issues more efficiently addressed by deep learning techniques. In this paper, a selective Generative Adversarial Network (GAN) is proposed for deblurring in the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once (YOLO)v5 object detection architectures for License-Plate Detection (LPD), and the integrated Character Segmentation (CS) and Character Recognition (CR) steps. The selective preprocessing bypasses unnecessary and sometimes counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high accuracy and low computing cost. As a result, YOLOv5 achieves a detection time of 0.026 seconds for both LP and CR detection stages, facilitating real-time applications with exceptionally rapid responsiveness. Moreover, the proposed model achieves accuracy rates of 95\% and 97\% in the LPD and CR detection phases, respectively. Furthermore, the inclusion of the Deblur-GAN pre-processor significantly improves detection accuracy by nearly 40\%, especially when encountering blurred License Plates (LPs).To train and test the learning components, we generated and publicly released our blur and ALPR datasets (using Iranian license plates as a use-case), which are more representative of close-to-real-life ad-hoc situations. The findings demonstrate that employing the state-of-the-art YOLO model results in excellent overall precision and detection time, making it well-suited for portable applications. Additionally, integrating the Deblur-GAN model as a preliminary processing step enhances the overall effectiveness of our comprehensive model, particularly when confronted with blurred scenes captured by the camera as input.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers</title>
<link>https://arxiv.org/abs/2509.06885</link>
<guid>https://arxiv.org/abs/2509.06885</guid>
<content:encoded><![CDATA[
arXiv:2509.06885v1 Announce Type: new 
Abstract: Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization</title>
<link>https://arxiv.org/abs/2509.06890</link>
<guid>https://arxiv.org/abs/2509.06890</guid>
<content:encoded><![CDATA[
arXiv:2509.06890v1 Announce Type: new 
Abstract: Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration</title>
<link>https://arxiv.org/abs/2509.06904</link>
<guid>https://arxiv.org/abs/2509.06904</guid>
<content:encoded><![CDATA[
arXiv:2509.06904v1 Announce Type: new 
Abstract: This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</title>
<link>https://arxiv.org/abs/2509.06907</link>
<guid>https://arxiv.org/abs/2509.06907</guid>
<content:encoded><![CDATA[
arXiv:2509.06907v1 Announce Type: new 
Abstract: Vision-driven field monitoring is central to digital agriculture, yet models built on general-domain pretrained backbones often fail to generalize across tasks, owing to the interaction of fine, variable canopy structures with fluctuating field conditions. We present FoMo4Wheat, one of the first crop-domain vision foundation model pretrained with self-supervision on ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5 million high-resolution images collected over a decade at 30 global sites, spanning >2,000 genotypes and >500 environmental conditions). This wheat-specific pretraining yields representations that are robust for wheat and transferable to other crops and weeds. Across ten in-field vision tasks at canopy and organ levels, FoMo4Wheat models consistently outperform state-of-the-art models pretrained on general-domain dataset. These results demonstrate the value of crop-specific foundation models for reliable in-field perception and chart a path toward a universal crop foundation model with cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is: https://fomo4wheat.phenix-lab.com/.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaving Reasoning for Better Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.06945</link>
<guid>https://arxiv.org/abs/2509.06945</guid>
<content:encoded><![CDATA[
arXiv:2509.06945v1 Announce Type: new 
Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title>
<link>https://arxiv.org/abs/2509.06956</link>
<guid>https://arxiv.org/abs/2509.06956</guid>
<content:encoded><![CDATA[
arXiv:2509.06956v1 Announce Type: new 
Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[
arXiv:2508.11849v2 Announce Type: cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory</title>
<link>https://arxiv.org/abs/2509.05314</link>
<guid>https://arxiv.org/abs/2509.05314</guid>
<content:encoded><![CDATA[
arXiv:2509.05314v1 Announce Type: cross 
Abstract: Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.05315</link>
<guid>https://arxiv.org/abs/2509.05315</guid>
<content:encoded><![CDATA[
arXiv:2509.05315v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-Wise Anomaly Detection in Directed Energy Deposition using High-Fidelity Fringe Projection Profilometry</title>
<link>https://arxiv.org/abs/2509.05327</link>
<guid>https://arxiv.org/abs/2509.05327</guid>
<content:encoded><![CDATA[
arXiv:2509.05327v1 Announce Type: cross 
Abstract: Directed energy deposition (DED), a metal additive manufacturing process, is highly susceptible to process-induced defects such as geometric deviations, lack of fusion, and poor surface finish. This work presents a build-height-synchronized fringe projection system for in-situ, layer-wise surface reconstruction of laser-DED components, achieving a reconstruction accuracy of ${\pm}$46 ${\mu}$m. From the reconstructed 3D morphology, two complementary geometry-based point cloud metrics are introduced: local point density, which highlights poor surface finish, and normal-change rate, which identifies lack-of-fusion features. These methods enable automated, annotation-free identification of common deposition anomalies directly from reconstructed surfaces, without the need for manual labeling. By directly linking geometric deviation to defect formation, the approach enables precise anomaly localization and advances the feasibility of closed-loop process control. This work establishes fringe projection as a practical tool for micrometer-scale monitoring in DED, bridging the gap between process signatures and part geometry for certifiable additive manufacturing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance</title>
<link>https://arxiv.org/abs/2509.05328</link>
<guid>https://arxiv.org/abs/2509.05328</guid>
<content:encoded><![CDATA[
arXiv:2509.05328v1 Announce Type: cross 
Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. To remedy this, most robust fine-tuning methods aim to preserve the pretrained weights, features, or logits. However, we find that these methods cannot always improve OOD robustness for different model architectures. This is due to the OOD robustness requiring the model function to produce stable prediction for input information of downstream tasks, while existing methods might serve as a poor proxy for the optimization in the function space. Based on this finding, we propose a novel regularization that constrains the distance of fine-tuning and pre-trained model in the function space with the simulated OOD samples, aiming to preserve the OOD robustness of the pre-trained model. Besides, to further enhance the OOD robustness capability of the fine-tuning model, we introduce an additional consistency regularization to promote stable predictions of perturbed samples. Extensive experiments demonstrate our approach could consistently improve both downstream task ID fine-tuning performance and OOD robustness across a variety of CLIP backbones, outperforming existing regularization-based robust fine-tuning methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Synthetic-to-Real Dehazing Method based on Domain Unification</title>
<link>https://arxiv.org/abs/2509.05374</link>
<guid>https://arxiv.org/abs/2509.05374</guid>
<content:encoded><![CDATA[
arXiv:2509.05374v1 Announce Type: cross 
Abstract: Due to distribution shift, the performance of deep learning-based method for image dehazing is adversely affected when applied to real-world hazy images. In this paper, we find that such deviation in dehazing task between real and synthetic domains may come from the imperfect collection of clean data. Owing to the complexity of the scene and the effect of depth, the collected clean data cannot strictly meet the ideal conditions, which makes the atmospheric physics model in the real domain inconsistent with that in the synthetic domain. For this reason, we come up with a synthetic-to-real dehazing method based on domain unification, which attempts to unify the relationship between the real and synthetic domain, thus to let the dehazing model more in line with the actual situation. Extensive experiments qualitatively and quantitatively demonstrate that the proposed dehazing method significantly outperforms state-of-the-art methods on real-world images.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation</title>
<link>https://arxiv.org/abs/2509.05469</link>
<guid>https://arxiv.org/abs/2509.05469</guid>
<content:encoded><![CDATA[
arXiv:2509.05469v1 Announce Type: cross 
Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
<link>https://arxiv.org/abs/2509.05584</link>
<guid>https://arxiv.org/abs/2509.05584</guid>
<content:encoded><![CDATA[
arXiv:2509.05584v1 Announce Type: cross 
Abstract: Foundation models face growing compute and memory bottlenecks, hindering deployment on resource-limited platforms. While compression techniques such as pruning and quantization are widely used, most rely on uniform heuristics that ignore architectural and runtime heterogeneity. Profiling tools expose per-layer latency, memory, and compute cost, yet are rarely integrated into automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic approach that uses large language models (LLMs) to automate compression via structured pruning and post-training dynamic quantization. Our modular multi-agent system reasons over static metrics (MACs, parameter counts) and dynamic signals (latency, memory) to design architecture-specific strategies. Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on smaller datasets), while quantization achieves up to 74% memory savings with <0.5% accuracy loss. Our quantization also yields consistent inference speedups of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo highlight the importance of LLM reasoning quality for iterative pruning. These results establish agentic systems as scalable solutions for profiling-guided model optimization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation</title>
<link>https://arxiv.org/abs/2509.05645</link>
<guid>https://arxiv.org/abs/2509.05645</guid>
<content:encoded><![CDATA[
arXiv:2509.05645v1 Announce Type: cross 
Abstract: Mars exploration requires precise and reliable terrain models to ensure safe rover navigation across its unpredictable and often hazardous landscapes. Stereoscopic vision serves a critical role in the rover's perception, allowing scene reconstruction by generating precise depth maps through stereo matching. State-of-the-art Martian planetary exploration uses traditional local block-matching, aggregates cost over square windows, and refines disparities via smoothness constraints. However, this method often struggles with low-texture images, occlusion, and repetitive patterns because it considers only limited neighbouring pixels and lacks a wider understanding of scene context. This paper uses Semi-Global Matching (SGM) with superpixel-based refinement to mitigate the inherent block artefacts and recover lost details. The approach balances the efficiency and accuracy of SGM and adds context-aware segmentation to support more coherent depth inference. The proposed method has been evaluated in three datasets with successful results: In a Mars analogue, the terrain maps obtained show improved structural consistency, particularly in sloped or occlusion-prone regions. Large gaps behind rocks, which are common in raw disparity outputs, are reduced, and surface details like small rocks and edges are captured more accurately. Another two datasets, evaluated to test the method's general robustness and adaptability, show more precise disparity maps and more consistent terrain models, better suited for the demands of autonomous navigation on Mars, and competitive accuracy across both non-occluded and full-image error metrics. This paper outlines the entire terrain modelling process, from finding corresponding features to generating the final 2D navigation maps, offering a complete pipeline suitable for integration in future planetary exploration missions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.05714</link>
<guid>https://arxiv.org/abs/2509.05714</guid>
<content:encoded><![CDATA[
arXiv:2509.05714v1 Announce Type: cross 
Abstract: Knowledge editing enables multimodal large language models (MLLMs) to efficiently update outdated or incorrect information. However, existing benchmarks primarily emphasize cognitive-level modifications while lacking a focus on deeper meta-cognitive processes. To bridge this gap, we introduce CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge editing abilities across three levels: (1) Counterfactual-Driven Editing, assessing self-awareness of knowledge correctness changes; (2) Boundary Constraint Editing, ensuring appropriate generalization without unintended interference; and (3) Noise-Robust Editing, promoting reflective evaluation of uncertain information. To advance meta-cognitive editing, we propose MIND (Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that constructs a meta-knowledge memory for self-awareness, employs game-theoretic interactions to monitor knowledge activation, and incorporates label refinement for noise-robust updates. Extensive experiments show that MIND significantly outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics</title>
<link>https://arxiv.org/abs/2509.05753</link>
<guid>https://arxiv.org/abs/2509.05753</guid>
<content:encoded><![CDATA[
arXiv:2509.05753v1 Announce Type: cross 
Abstract: The rise of synthetic media has blurred the boundary between reality and fabrication under the evolving power of artificial intelligence, fueling an infodemic that erodes public trust in cyberspace. For digital imagery, a multitude of editing applications further complicates the forensic analysis, including semantic edits that alter content, photometric adjustments that recalibrate colour characteristics, and geometric projections that reshape viewpoints. Collectively, these transformations manipulate and control perceptual interpretation of digital imagery. This susceptibility calls for forensic enquiry into reconstructing the chain of events, thereby revealing deeper evidential insight into the presence or absence of criminal intent. This study seeks to address an inverse problem of tracing the underlying generation chain that gives rise to the observed synthetic media. A tell-tale watermarking system is developed for explanatory reasoning over the nature and extent of transformations across the lifecycle of synthetic media. Tell-tale watermarks are tailored to different classes of transformations, responding in a manner that is neither strictly robust nor fragile but instead interpretable. These watermarks function as reference clues that evolve under the same transformation dynamics as the carrier media, leaving interpretable traces when subjected to transformations. Explanatory reasoning is then performed to infer the most plausible account across the combinatorial parameter space of composite transformations. Experimental evaluations demonstrate the validity of tell-tale watermarking with respect to fidelity, synchronicity and traceability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Tumor Detection Through Diverse CNN Architectures in IoT Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and Fully Connected CNN</title>
<link>https://arxiv.org/abs/2509.05821</link>
<guid>https://arxiv.org/abs/2509.05821</guid>
<content:encoded><![CDATA[
arXiv:2509.05821v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-powered deep learning has advanced brain tumor diagnosis in Internet of Things (IoT)-healthcare systems, achieving high accuracy with large datasets. Brain health is critical to human life, and accurate diagnosis is essential for effective treatment. Magnetic Resonance Imaging (MRI) provides key data for brain tumor detection, serving as a major source of big data for AI-driven image classification. In this study, we classified glioma, meningioma, and pituitary tumors from MRI images using Region-based Convolutional Neural Network (R-CNN) and UNet architectures. We also applied Convolutional Neural Networks (CNN) and CNN-based transfer learning models such as Inception-V3, EfficientNetB4, and VGG19. Model performance was assessed using F-score, recall, precision, and accuracy. The Fast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5% Area Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN, UNet, and transfer learning enables earlier diagnosis and more effective treatment in IoT-healthcare systems, improving patient outcomes. IoT devices such as wearable monitors and smart imaging systems continuously collect real-time data, which AI algorithms analyze to provide immediate insights for timely interventions and personalized care. For external cohort cross-dataset validation, EfficientNetB2 achieved the strongest performance among fine-tuned EfficientNet models, with 92.11% precision, 92.11% recall/sensitivity, 95.96% specificity, 92.02% F1-score, and 92.23% accuracy. These findings underscore the robustness and reliability of AI models in handling diverse datasets, reinforcing their potential to enhance brain tumor classification and patient care in IoT healthcare environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of Conformal Prediction in Capturing Aleatoric Uncertainty</title>
<link>https://arxiv.org/abs/2509.05826</link>
<guid>https://arxiv.org/abs/2509.05826</guid>
<content:encoded><![CDATA[
arXiv:2509.05826v1 Announce Type: cross 
Abstract: Conformal prediction is a model-agnostic approach to generating prediction sets that cover the true class with a high probability. Although its prediction set size is expected to capture aleatoric uncertainty, there is a lack of evidence regarding its effectiveness. The literature presents that prediction set size can upper-bound aleatoric uncertainty or that prediction sets are larger for difficult instances and smaller for easy ones, but a validation of this attribute of conformal predictors is missing. This work investigates how effectively conformal predictors quantify aleatoric uncertainty, specifically the inherent ambiguity in datasets caused by overlapping classes. We perform this by measuring the correlation between prediction set sizes and the number of distinct labels assigned by human annotators per instance. We further assess the similarity between prediction sets and human-provided annotations. We use three conformal prediction approaches to generate prediction sets for eight deep learning models trained on four datasets. The datasets contain annotations from multiple human annotators (ranging from five to fifty participants) per instance, enabling the identification of class overlap. We show that the vast majority of the conformal prediction outputs show a very weak to weak correlation with human annotations, with only a few showing moderate correlation. These findings underscore the necessity of critically reassessing the prediction sets generated using conformal predictors. While they can provide a higher coverage of the true classes, their capability in capturing aleatoric uncertainty remains limited.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems</title>
<link>https://arxiv.org/abs/2509.05923</link>
<guid>https://arxiv.org/abs/2509.05923</guid>
<content:encoded><![CDATA[
arXiv:2509.05923v1 Announce Type: cross 
Abstract: The bioinspired event camera, distinguished by its exceptional temporal resolution, high dynamic range, and low power consumption, has been extensively studied in recent years for motion estimation, robotic perception, and object detection. In ego-motion estimation, the visual-inertial setup is commonly adopted due to complementary characteristics between sensors (e.g., scale perception and low drift). For optimal event-based visual-inertial fusion, accurate spatiotemporal (extrinsic and temporal) calibration is required. In this work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator for event-based visual-inertial systems, utilizing the widely used circle grid board. Building upon the grid pattern recognition and tracking methods in eKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and efficient initialization, where all parameters in the estimator would be accurately recovered. Subsequently, a continuous-time-based batch optimization is conducted to refine the initialized parameters toward better states. The results of extensive real-world experiments show that eKalibr-Inertial can achieve accurate event-based visual-inertial spatiotemporal calibration. The implementation of eKalibr-Inertial is open-sourced at (https://github.com/Unsigned-Long/eKalibr) to benefit the research community.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</title>
<link>https://arxiv.org/abs/2509.05978</link>
<guid>https://arxiv.org/abs/2509.05978</guid>
<content:encoded><![CDATA[
arXiv:2509.05978v1 Announce Type: cross 
Abstract: Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brain's three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</title>
<link>https://arxiv.org/abs/2509.06079</link>
<guid>https://arxiv.org/abs/2509.06079</guid>
<content:encoded><![CDATA[
arXiv:2509.06079v1 Announce Type: cross 
Abstract: Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes</title>
<link>https://arxiv.org/abs/2509.06159</link>
<guid>https://arxiv.org/abs/2509.06159</guid>
<content:encoded><![CDATA[
arXiv:2509.06159v1 Announce Type: cross 
Abstract: The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)</title>
<link>https://arxiv.org/abs/2509.06191</link>
<guid>https://arxiv.org/abs/2509.06191</guid>
<content:encoded><![CDATA[
arXiv:2509.06191v1 Announce Type: cross 
Abstract: Recent 3D generative models, which are capable of generating full object shapes from just a few images, now open up new opportunities in robotics. In this work, we show that 3D generative models can be used to augment a dataset from a single real-world demonstration, after which an omnidirectional policy can be learned within this imagined dataset. We found that this enables a robot to perform a task when initialised from states very far from those observed during the demonstration, including starting from the opposite side of the object relative to the real-world demonstration, significantly reducing the number of demonstrations required for policy learning. Through several real-world experiments across tasks such as grasping objects, opening a drawer, and placing trash into a bin, we study these omnidirectional policies by investigating the effect of various design choices on policy behaviour, and we show superior performance to recent baselines which use alternative methods for data augmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</title>
<link>https://arxiv.org/abs/2509.06233</link>
<guid>https://arxiv.org/abs/2509.06233</guid>
<content:encoded><![CDATA[
arXiv:2509.06233v1 Announce Type: cross 
Abstract: Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data contraints. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for robotics manipulation, significantly enhancing LLMs' capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</title>
<link>https://arxiv.org/abs/2509.06314</link>
<guid>https://arxiv.org/abs/2509.06314</guid>
<content:encoded><![CDATA[
arXiv:2509.06314v1 Announce Type: cross 
Abstract: A central challenge in representation learning is constructing latent embeddings that are both expressive and efficient. In practice, deep networks often produce redundant latent spaces where multiple coordinates encode overlapping information, reducing effective capacity and hindering generalization. Standard metrics such as accuracy or reconstruction loss provide only indirect evidence of such redundancy and cannot isolate it as a failure mode. We introduce a redundancy index, denoted rho(C), that directly quantifies inter-dimensional dependencies by analyzing coupling matrices derived from latent representations and comparing their off-diagonal statistics against a normal distribution via energy distance. The result is a compact, interpretable, and statistically grounded measure of representational quality. We validate rho(C) across discriminative and generative settings on MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple architectures and hyperparameter optimization strategies. Empirically, low rho(C) reliably predicts high classification accuracy or low reconstruction error, while elevated redundancy is associated with performance collapse. Estimator reliability grows with latent dimension, yielding natural lower bounds for reliable analysis. We further show that Tree-structured Parzen Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C) can guide neural architecture search and serve as a redundancy-aware regularization target. By exposing redundancy as a universal bottleneck across models and tasks, rho(C) offers both a theoretical lens and a practical tool for evaluating and improving the efficiency of learned representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal-Based Malware Classification Using 1D CNNs</title>
<link>https://arxiv.org/abs/2509.06548</link>
<guid>https://arxiv.org/abs/2509.06548</guid>
<content:encoded><![CDATA[
arXiv:2509.06548v1 Announce Type: cross 
Abstract: Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing</title>
<link>https://arxiv.org/abs/2509.06552</link>
<guid>https://arxiv.org/abs/2509.06552</guid>
<content:encoded><![CDATA[
arXiv:2509.06552v1 Announce Type: cross 
Abstract: The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning</title>
<link>https://arxiv.org/abs/2509.06553</link>
<guid>https://arxiv.org/abs/2509.06553</guid>
<content:encoded><![CDATA[
arXiv:2509.06553v1 Announce Type: cross 
Abstract: Objectives: Federated learning (FL) may mitigate privacy constraints, heterogeneous data quality, and inconsistent labeling in dental diagnostic AI. We compared FL with centralized (CL) and local learning (LL) for tooth segmentation in panoramic radiographs across multiple data corruption scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six institutions across four settings: baseline (unaltered data); label manipulation (dilated/missing annotations); image-quality manipulation (additive Gaussian noise); and exclusion of a faulty client with corrupted data. FL was implemented via the Flower AI framework. Per-client training- and validation-loss trajectories were monitored for anomaly detection and a set of metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set. From these metrics significance results were reported through Wilcoxon signed-rank test. CL and LL served as comparators. Results: Baseline: FL achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at 0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777). Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD: 1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD: 1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD: 1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD: 1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and outperforms LL across corruption scenarios while preserving privacy. Per-client loss trajectories provide an effective anomaly-detection mechanism and support FL as a practical, privacy-preserving approach for scalable clinical AI deployment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method</title>
<link>https://arxiv.org/abs/2509.06592</link>
<guid>https://arxiv.org/abs/2509.06592</guid>
<content:encoded><![CDATA[
arXiv:2509.06592v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI) is an invaluable tool for clinical and research applications. Yet, variations in scanners and acquisition parameters cause inconsistencies in image contrast, hindering data comparability and reproducibility across datasets and clinical studies. Existing scanner harmonization methods, designed to address this challenge, face limitations, such as requiring traveling subjects or struggling to generalize to unseen domains. We propose a novel approach using a conditioned diffusion autoencoder with a contrastive loss and domain-agnostic contrast augmentation to harmonize MR images across scanners while preserving subject-specific anatomy. Our method enables brain MRI synthesis from a single reference image. It outperforms baseline techniques, achieving a +7% PSNR improvement on a traveling subjects dataset and +18% improvement on age regression in unseen. Our model provides robust, effective harmonization of brain MRIs to target scanners without requiring fine-tuning. This advancement promises to enhance comparability, reproducibility, and generalizability in multi-site and longitudinal clinical studies, ultimately contributing to improved healthcare outcomes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans</title>
<link>https://arxiv.org/abs/2509.06607</link>
<guid>https://arxiv.org/abs/2509.06607</guid>
<content:encoded><![CDATA[
arXiv:2509.06607v1 Announce Type: cross 
Abstract: Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses.
  We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to "upgrade" existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained and more realistic model of human articulation. The model, code, and data are available for research at https://skel.is.tue.mpg.de..
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations</title>
<link>https://arxiv.org/abs/2509.06615</link>
<guid>https://arxiv.org/abs/2509.06615</guid>
<content:encoded><![CDATA[
arXiv:2509.06615v1 Announce Type: cross 
Abstract: In environments where visual sensors falter, in-air sonar provides a reliable alternative for autonomous systems. While previous research has successfully classified individual acoustic landmarks, this paper takes a step towards increasing information capacity by introducing reflector constellations as encoded tags. Our primary contribution is a multi-label Convolutional Neural Network (CNN) designed to simultaneously identify multiple, closely spaced reflectors from a single in-air 3D sonar measurement. Our initial findings on a small dataset confirm the feasibility of this approach, validating the ability to decode these complex acoustic patterns. Secondly, we investigated using adaptive beamforming with null-steering to isolate individual reflectors for single-label classification. Finally, we discuss the experimental results and limitations, offering key insights and future directions for developing acoustic landmark systems with significantly increased information entropy and their accurate and robust detection and classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis</title>
<link>https://arxiv.org/abs/2509.06617</link>
<guid>https://arxiv.org/abs/2509.06617</guid>
<content:encoded><![CDATA[
arXiv:2509.06617v1 Announce Type: cross 
Abstract: Vision foundation models like DINOv2 demonstrate remarkable potential in medical imaging despite their origin in natural image domains. However, their design inherently works best for uni-modal image analysis, limiting their effectiveness for multi-modal imaging tasks that are common in many medical fields, such as neurology and oncology. While supervised models perform well in this setting, they fail to leverage unlabeled datasets and struggle with missing modalities, a frequent challenge in clinical settings. To bridge these gaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the pre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our approach incorporates multi-modal patch embeddings, enabling vision foundation models to effectively process multi-modal imaging data. To address missing modalities, we employ full-modality masking, which encourages the model to learn robust cross-modality relationships. Furthermore, we leverage semi-supervised learning to harness large unlabeled datasets, enhancing both the accuracy and reliability of medical predictions. Applied to glioma subtype classification from multi-sequence brain MRI, our method achieves a Matthews Correlation Coefficient (MCC) of 0.6 on an external test set, surpassing state-of-the-art supervised approaches by +11.1%. Our work establishes a scalable and robust solution for multi-modal medical imaging tasks, leveraging powerful vision foundation models pre-trained on natural images while addressing real-world clinical challenges such as missing data and limited annotations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaDA-VLA: Vision Language Diffusion Action Models</title>
<link>https://arxiv.org/abs/2509.06932</link>
<guid>https://arxiv.org/abs/2509.06932</guid>
<content:encoded><![CDATA[
arXiv:2509.06932v1 Announce Type: cross 
Abstract: The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</title>
<link>https://arxiv.org/abs/2509.06950</link>
<guid>https://arxiv.org/abs/2509.06950</guid>
<content:encoded><![CDATA[
arXiv:2509.06950v1 Announce Type: cross 
Abstract: Large transformer-based models have made significant progress in generalizable novel view synthesis (NVS) from sparse input views, generating novel viewpoints without the need for test-time optimization. However, these models are constrained by the limited diversity of publicly available scene datasets, making most real-world (in-the-wild) scenes out-of-distribution. To overcome this, we incorporate synthetic training data generated from diffusion models, which improves generalization across unseen domains. While synthetic data offers scalability, we identify artifacts introduced during data generation as a key bottleneck affecting reconstruction quality. To address this, we propose a token disentanglement process within the transformer architecture, enhancing feature separation and ensuring more effective learning. This refinement not only improves reconstruction quality over standard transformers but also enables scalable training with synthetic data. As a result, our method outperforms existing models on both in-dataset and cross-dataset evaluations, achieving state-of-the-art results across multiple benchmarks while significantly reducing computational costs. Project page: https://scaling3dnvs.github.io/
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions</title>
<link>https://arxiv.org/abs/2509.06951</link>
<guid>https://arxiv.org/abs/2509.06951</guid>
<content:encoded><![CDATA[
arXiv:2509.06951v1 Announce Type: cross 
Abstract: Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
<link>https://arxiv.org/abs/2509.06953</link>
<guid>https://arxiv.org/abs/2509.06953</guid>
<content:encoded><![CDATA[
arXiv:2509.06953v1 Announce Type: cross 
Abstract: Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Machines Imitate Humans? Integrative Turing-like tests for Language and Vision Demonstrate a Narrowing Gap</title>
<link>https://arxiv.org/abs/2211.13087</link>
<guid>https://arxiv.org/abs/2211.13087</guid>
<content:encoded><![CDATA[
arXiv:2211.13087v3 Announce Type: replace 
Abstract: As AI becomes increasingly embedded in daily life, ascertaining whether an agent is human is critical. We systematically benchmark AI's ability to imitate humans in three language tasks (image captioning, word association, conversation) and three vision tasks (color estimation, object detection, attention prediction), collecting data from 636 humans and 37 AI agents. Next, we conducted 72,191 Turing-like tests with 1,916 human judges and 10 AI judges. Current AIs are approaching the ability to convincingly impersonate humans and deceive human judges in both language and vision. Even simple AI judges outperformed humans in distinguishing AI from human responses. Imitation ability showed minimal correlation with conventional AI performance metrics, suggesting that passing as human is an important independent evaluation criterion. The large-scale Turing datasets and metrics introduced here offer valuable benchmarks for assessing human-likeness in AI and highlight the importance of rigorous, quantitative imitation tests for AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADIR: Adaptive Diffusion for Image Reconstruction</title>
<link>https://arxiv.org/abs/2212.03221</link>
<guid>https://arxiv.org/abs/2212.03221</guid>
<content:encoded><![CDATA[
arXiv:2212.03221v2 Announce Type: replace 
Abstract: Denoising diffusion models have recently achieved remarkable success in image generation, capturing rich information about natural image statistics. This makes them highly promising for image reconstruction, where the goal is to recover a clean image from a degraded observation. In this work, we introduce a conditional sampling framework that leverages the powerful priors learned by diffusion models while enforcing consistency with the available measurements. To further adapt pre-trained diffusion models to the specific degradation at hand, we propose a novel fine-tuning strategy. In particular, we employ LoRA-based adaptation using images that are semantically and visually similar to the degraded input, efficiently retrieved from a large and diverse dataset via an off-the-shelf vision-language model. We evaluate our approach on two leading publicly available diffusion models--Stable Diffusion and Guided Diffusion--and demonstrate that our method, termed Adaptive Diffusion for Image Reconstruction (ADIR), yields substantial improvements across a range of image reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The GOOSE Dataset for Perception in Unstructured Environments</title>
<link>https://arxiv.org/abs/2310.16788</link>
<guid>https://arxiv.org/abs/2310.16788</guid>
<content:encoded><![CDATA[
arXiv:2310.16788v2 Announce Type: replace 
Abstract: The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10 000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeSAM: Prompt-In-the-Loop Distillation for SAM</title>
<link>https://arxiv.org/abs/2312.06660</link>
<guid>https://arxiv.org/abs/2312.06660</guid>
<content:encoded><![CDATA[
arXiv:2312.06660v3 Announce Type: replace 
Abstract: This paper presents EdgeSAM, an accelerated variant of the Segment Anything Model (SAM), optimized for efficient execution on edge devices with minimal compromise in performance. Our approach involves distilling the original ViT-based SAM image encoder into a purely CNN-based architecture, better suited for edge devices. We carefully benchmark various distillation strategies and demonstrate that task-agnostic encoder distillation fails to capture the full knowledge embodied in SAM. To overcome this bottleneck, we include both the prompt encoder and mask decoder in the distillation process, with box and point prompts in the loop, so that the distilled model can accurately capture the intricate dynamics between user input and mask generation. To mitigate dataset bias issues stemming from point prompt distillation, we incorporate a lightweight module within the encoder. As a result, EdgeSAM achieves a 37-fold speed increase compared to the original SAM, and it also outperforms MobileSAM/EfficientSAM, being over 7 times as fast when deployed on edge devices while enhancing the mIoUs on COCO and LVIS by 2.3/1.5 and 3.1/1.6, respectively. It is also the first SAM variant that can run at over 30 FPS on an iPhone 14. Code and demo are available at https://www.mmlab-ntu.com/project/edgesam.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-SDM: Language-Driven Hierarchical Species Distribution Modeling</title>
<link>https://arxiv.org/abs/2312.08334</link>
<guid>https://arxiv.org/abs/2312.08334</guid>
<content:encoded><![CDATA[
arXiv:2312.08334v2 Announce Type: replace 
Abstract: We focus on species distribution modeling using global-scale presence-only data, leveraging geographical and environmental features to map species ranges, as in previous studies. However, we innovate by integrating taxonomic classification into our approach. Specifically, we propose using a large language model to extract a latent representation of the taxonomic classification from a textual prompt. This allows us to map the range of any taxonomic rank, including unseen species, without additional supervision. We also present a new proximity-aware evaluation metric, suitable for evaluating species distribution models, which addresses critical shortcomings of traditional metrics. We evaluated our model for species range prediction, zero-shot prediction, and geo-feature regression and found that it outperforms several state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Osprey: Pixel Understanding with Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2312.10032</link>
<guid>https://arxiv.org/abs/2312.10032</guid>
<content:encoded><![CDATA[
arXiv:2312.10032v4 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have recently achieved impressive general-purpose vision-language capabilities through visual instruction tuning. However, current MLLMs primarily focus on image-level or box-level understanding, falling short in achieving fine-grained vision-language alignment at pixel level. Besides, the lack of mask-based instruction data limits their advancements. In this paper, we propose Osprey, a mask-text instruction tuning approach, to extend MLLMs by incorporating fine-grained mask regions into language instruction, aiming at achieving pixel-wise visual understanding. To achieve this goal, we first meticulously curate a mask-based region-text dataset with 724K samples, and then design a vision-language model by injecting pixel-level representation into LLM. Specifically, Osprey adopts a convolutional CLIP backbone as the vision encoder and employs a mask-aware visual extractor to extract precise visual mask features from high resolution input. Experimental results demonstrate Osprey's superiority in various region understanding tasks, showcasing its new capability for pixel-level instruction tuning. In particular, Osprey can be integrated with Segment Anything Model (SAM) seamlessly to obtain multi-granularity semantics. The source code, dataset and demo can be found at https://github.com/CircleRadon/Osprey.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2403.01489</link>
<guid>https://arxiv.org/abs/2403.01489</guid>
<content:encoded><![CDATA[
arXiv:2403.01489v2 Announce Type: replace 
Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing</title>
<link>https://arxiv.org/abs/2407.04180</link>
<guid>https://arxiv.org/abs/2407.04180</guid>
<content:encoded><![CDATA[
arXiv:2407.04180v3 Announce Type: replace 
Abstract: G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently, there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present Slice-100K, a first-of-its-kind dataset of over 100,000 G-code files, along with their tessellated CAD model, LVIS (Large Vocabulary Instance Segmentation) categories, geometric properties, and renderings. We build our dataset from triangulated meshes derived from Objaverse-XL and Thingi10K datasets. We demonstrate the utility of this dataset by finetuning GPT-2 on a subset of the dataset for G-code translation from a legacy G-code format (Sailfish) to a more modern, widely used format (Marlin). Our dataset can be found at https://github.com/idealab-isu/Slice-100K. Slice-100K will be the first step in developing a multimodal foundation model for digital manufacturing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining</title>
<link>https://arxiv.org/abs/2408.10906</link>
<guid>https://arxiv.org/abs/2408.10906</guid>
<content:encoded><![CDATA[
arXiv:2408.10906v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build ShapeSplat, a large-scale dataset of 3DGS using the commonly used ShapeNet, ModelNet and Objaverse datasets. Our dataset ShapeSplat consists of 206K objects spanning over 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 3.8 GPU years on a TITAN XP GPU.
  We utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce Gaussian-MAE, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Transformers for Improved Image Retrieval</title>
<link>https://arxiv.org/abs/2409.01082</link>
<guid>https://arxiv.org/abs/2409.01082</guid>
<content:encoded><![CDATA[
arXiv:2409.01082v2 Announce Type: replace 
Abstract: We introduce the Evidential Transformer, an uncertainty-driven transformer model for improved and robust image retrieval. In this paper, we make several contributions to content-based image retrieval (CBIR). We incorporate probabilistic methods into image retrieval, achieving robust and reliable results, with evidential classification surpassing traditional training based on multiclass classification as a baseline for deep metric learning. Furthermore, we improve the state-of-the-art retrieval results on several datasets by leveraging the Global Context Vision Transformer (GC ViT) architecture. Our experimental results consistently demonstrate the reliability of our approach, setting a new benchmark in CBIR in all test settings on the Stanford Online Products (SOP) and CUB-200-2011 datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</title>
<link>https://arxiv.org/abs/2410.09374</link>
<guid>https://arxiv.org/abs/2410.09374</guid>
<content:encoded><![CDATA[
arXiv:2410.09374v4 Announce Type: replace 
Abstract: Event-based visual odometry is a specific branch of visual Simultaneous Localization and Mapping (SLAM) techniques, which aims at solving tracking and mapping subproblems (typically in parallel), by exploiting the special working principles of neuromorphic (i.e., event-based) cameras. Due to the motion-dependent nature of event data, explicit data association (i.e., feature matching) under large-baseline view-point changes is difficult to establish, making direct methods a more rational choice. However, state-of-the-art direct methods are limited by the high computational complexity of the mapping sub-problem and the degeneracy of camera pose tracking in certain degrees of freedom (DoF) in rotation. In this paper, we tackle these issues by building an event-based stereo visual-inertial odometry system on top of a direct pipeline. Specifically, to speed up the mapping operation, we propose an efficient strategy for sampling contour points according to the local dynamics of events. The mapping performance is also improved in terms of structure completeness and local smoothness by merging the temporal stereo and static stereo results. To circumvent the degeneracy of camera pose tracking in recovering the pitch and yaw components of general 6-DoF motion, we introduce IMU measurements as motion priors via pre-integration. To this end, a compact back-end is proposed for continuously updating the IMU bias and predicting the linear velocity, enabling an accurate motion prediction for camera pose tracking. The resulting system scales well with modern high-resolution event cameras and leads to better global positioning accuracy in large-scale outdoor environments. Extensive evaluations on five publicly available datasets featuring different resolutions and scenarios justify the superior performance of the proposed system against five state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative World Explorer</title>
<link>https://arxiv.org/abs/2411.11844</link>
<guid>https://arxiv.org/abs/2411.11844</guid>
<content:encoded><![CDATA[
arXiv:2411.11844v3 Announce Type: replace 
Abstract: Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. In contrast, humans can $\textit{imagine}$ unseen parts of the world through a mental exploration and $\textit{revise}$ their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the $\textit{Generative World Explorer (Genex)}$, an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) $\textit{Genex}$ can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagram-Driven Course Questions Generation</title>
<link>https://arxiv.org/abs/2411.17771</link>
<guid>https://arxiv.org/abs/2411.17771</guid>
<content:encoded><![CDATA[
arXiv:2411.17771v5 Announce Type: replace 
Abstract: Visual Question Generation (VQG) research focuses predominantly on natural images while neglecting the diagram, which is a critical component in educational materials. To meet the needs of pedagogical assessment, we propose the Diagram-Driven Course Questions Generation (DDCQG) task and construct DiagramQG, a comprehensive dataset with 15,720 diagrams and 25,798 questions across 37 subjects and 371 courses. Our approach employs course and input text constraints to generate course-relevant questions about specific diagram elements. We reveal three challenges of DDCQG: domain-specific knowledge requirements across courses, long-tail distribution in course coverage, and high information density in diagrams. To address these, we propose the Hierarchical Knowledge Integration framework (HKI-DDCQG), which utilizes trainable CLIP for identifying relevant diagram patches, leverages frozen vision-language models for knowledge extraction, and generates questions with trainable T5. Experiments demonstrate that HKI-DDCQG outperforms existing models on DiagramQG while maintaining strong generalizability across natural image datasets, establishing a strong baseline for DDCQG.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?</title>
<link>https://arxiv.org/abs/2412.00102</link>
<guid>https://arxiv.org/abs/2412.00102</guid>
<content:encoded><![CDATA[
arXiv:2412.00102v2 Announce Type: replace 
Abstract: Multi-modal Large Language Models (MLLMs) are gaining significant attention for their ability to process multi-modal data, providing enhanced contextual understanding of complex problems. MLLMs have demonstrated exceptional capabilities in tasks such as Visual Question Answering (VQA); however, they often struggle with fundamental engineering problems, and there is a scarcity of specialized datasets for training on topics like digital electronics. To address this gap, we propose a benchmark dataset called ElectroVizQA specifically designed to evaluate MLLMs' performance on digital electronic circuit problems commonly found in undergraduate curricula. This dataset, the first of its kind tailored for the VQA task in digital electronics, comprises approximately 626 visual questions, offering a comprehensive overview of digital electronics topics. This paper rigorously assesses the extent to which MLLMs can understand and solve digital electronic circuit questions, providing insights into their capabilities and limitations within this specialized domain. By introducing this benchmark dataset, we aim to motivate further research and development in the application of MLLMs to engineering education, ultimately bridging the performance gap and enhancing the efficacy of these models in technical fields.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TASR: Timestep-Aware Diffusion Model for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2412.03355</link>
<guid>https://arxiv.org/abs/2412.03355</guid>
<content:encoded><![CDATA[
arXiv:2412.03355v2 Announce Type: replace 
Abstract: Diffusion models have recently achieved outstanding results in the field of image super-resolution. These methods typically inject low-resolution (LR) images via ControlNet.In this paper, we first explore the temporal dynamics of information infusion through ControlNet, revealing that the input from LR images predominantly influences the initial stages of the denoising process. Leveraging this insight, we introduce a novel timestep-aware diffusion model that adaptively integrates features from both ControlNet and the pre-trained Stable Diffusion (SD). Our method enhances the transmission of LR information in the early stages of diffusion to guarantee image fidelity and stimulates the generation ability of the SD model itself more in the later stages to enhance the detail of generated images. To train this method, we propose a timestep-aware training strategy that adopts distinct losses at varying timesteps and acts on disparate modules. Experiments on benchmark datasets demonstrate the effectiveness of our method. Code: https://github.com/SleepyLin/TASR
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NF3DM: Combining Neural Fields and Deformation Models for 3D Non-Rigid Motion Reconstruction</title>
<link>https://arxiv.org/abs/2412.08511</link>
<guid>https://arxiv.org/abs/2412.08511</guid>
<content:encoded><![CDATA[
arXiv:2412.08511v2 Announce Type: replace 
Abstract: We introduce a novel, data-driven approach for reconstructing temporally coherent 3D motion from unstructured and potentially partial observations of non-rigidly deforming shapes. Our goal is to achieve high-fidelity motion reconstructions for shapes that undergo near-isometric deformations, such as humans wearing loose clothing. The key novelty of our work lies in its ability to combine implicit shape representations with explicit mesh-based deformation models, enabling detailed and temporally coherent motion reconstructions without relying on parametric shape models or decoupling shape and motion. Each frame is represented as a neural field decoded from a feature space where observations over time are fused, hence preserving geometric details present in the input data. Temporal coherence is enforced with a near-isometric deformation constraint between adjacent frames that applies to the underlying surface in the neural field. Our method outperforms state-of-the-art approaches, as demonstrated by its application to human and animal motion sequences reconstructed from monocular depth videos.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-enhanced Cardiac Anatomy Segmentation via an Insertable Temporal Attention Module</title>
<link>https://arxiv.org/abs/2501.14929</link>
<guid>https://arxiv.org/abs/2501.14929</guid>
<content:encoded><![CDATA[
arXiv:2501.14929v2 Announce Type: replace 
Abstract: Cardiac anatomy segmentation is useful for clinical assessment of cardiac morphology to inform diagnosis and intervention. Deep learning (DL), especially with motion information, has improved segmentation accuracy. However, existing techniques for motion enhancement are not yet optimal, and they have high computational costs due to increased dimensionality or reduced robustness due to suboptimal approaches that use non-DL motion registration, non-attention models, or single-headed attention. They further have limited adaptability and are inconvenient for incorporation into existing networks where motion awareness is desired. Here, we propose a novel, computationally efficient Temporal Attention Module (TAM) that offers robust motion enhancement, modeled as a small, multi-headed, cross-temporal attention module. TAM's uniqueness is that it is a lightweight, plug-and-play module that can be inserted into a broad range of segmentation networks (CNN-based, Transformer-based, or hybrid) for motion enhancement without requiring substantial changes in the network's backbone. This feature enables high adaptability and ease of integration for enhancing both existing and future networks. Extensive experiments on multiple 2D and 3D cardiac ultrasound and MRI datasets confirm that TAM consistently improves segmentation across a range of networks while maintaining computational efficiency and improving on currently reported performance. The evidence demonstrates that it is a robust, generalizable solution for motion-awareness enhancement that is scalable (such as from 2D to 3D).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16714</link>
<guid>https://arxiv.org/abs/2501.16714</guid>
<content:encoded><![CDATA[
arXiv:2501.16714v2 Announce Type: replace 
Abstract: Motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips with the same motion concept. To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearances. Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of DM. Typical previous works explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models, e.g., learning a motion LoRA, using latent noise residuals, etc. While those methods can encode the motion concept, they also inevitably encode the appearance in the reference videos, resulting in weakened appearance generation capability. In this paper, we follow the typical way to learn a motion LoRA to encode the motion concept, but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH). Specifically, we assume that in the temporal attention module, the pretrained Value embeddings are sufficient to serve as basic components needed by producing a new motion. Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion. Further, in AH, we alter the starting point of each skip connection in U-Net from the output of each temporal attention module to the output of each spatial attention module. Extensive experiments demonstrate that compared to previous works, our method can generate videos with appearance more aligned with the text descriptions and motion more consistent with the reference videos.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAAGC: Feature Augmentation on Adaptive Geodesic Curve Based on the shape space theory</title>
<link>https://arxiv.org/abs/2501.18619</link>
<guid>https://arxiv.org/abs/2501.18619</guid>
<content:encoded><![CDATA[
arXiv:2501.18619v2 Announce Type: replace 
Abstract: Deep learning models have been widely applied across various domains and industries. However, many fields still face challenges due to limited and insufficient data. This paper proposes a Feature Augmentation on Adaptive Geodesic Curve (FAAGC) method in the pre-shape space to increase data. In the pre-shape space, objects with identical shapes lie on a great circle. Thus, we project deep model representations into the pre-shape space and construct a geodesic curve, i.e., an arc of a great circle, for each class. Feature augmentation is then performed by sampling along these geodesic paths. Extensive experiments demonstrate that FAAGC improves classification accuracy under data-scarce conditions and generalizes well across various feature types.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis</title>
<link>https://arxiv.org/abs/2502.06681</link>
<guid>https://arxiv.org/abs/2502.06681</guid>
<content:encoded><![CDATA[
arXiv:2502.06681v2 Announce Type: replace 
Abstract: Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across cameras, locations, and time. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust systems that handle long-term variations caused by clothing and physical changes. We present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset designed for video-based long-term person Re-ID. CHIRLA was recorded over seven months in four connected indoor environments using seven strategically placed cameras, capturing realistic movements with substantial clothing and appearance variability. The dataset includes 22 individuals, more than five hours of video, and about 1M bounding boxes with identity annotations obtained through semi-automatic labeling. We also define benchmark protocols for person tracking and Re-ID, covering diverse and challenging scenarios such as occlusion, reappearance, and multi-camera conditions. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios. The benchmark code is publicly available at: https://github.com/bdager/CHIRLA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Sees Your Location, But With A Bias Toward The Wealthy World</title>
<link>https://arxiv.org/abs/2502.11163</link>
<guid>https://arxiv.org/abs/2502.11163</guid>
<content:encoded><![CDATA[
arXiv:2502.11163v3 Announce Type: replace 
Abstract: Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, VLMs still show regional biases in this task. To systematically evaluate these issues, we introduce a benchmark consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to 53.8% accuracy in city prediction, they exhibit significant biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed (-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of frequently over-predicting certain locations remain. For instance, they consistently predict Sydney for images taken in Australia, shown by the low entropy scores for these countries. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at https://github.com/uscnlp-lime/FairLocator.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Reverse Classification Accuracy: Efficient Estimation of Segmentation Quality without Ground-Truth</title>
<link>https://arxiv.org/abs/2503.04522</link>
<guid>https://arxiv.org/abs/2503.04522</guid>
<content:encoded><![CDATA[
arXiv:2503.04522v2 Announce Type: replace 
Abstract: Assessing the quality of automatic image segmentation is crucial in clinical practice, but often very challenging due to the limited availability of ground truth annotations. In this paper, we introduce In-Context Reverse Classification Accuracy (In-Context RCA), a novel framework for automatically estimating segmentation quality in the absence of ground-truth annotations. By leveraging recent in-context learning segmentation models and incorporating retrieval-augmentation techniques to select the most relevant reference images, our approach enables efficient quality estimation with minimal reference data. Validated across diverse medical imaging modalities, our method demonstrates robust performance and computational efficiency, offering a promising solution for automated quality control in clinical workflows, where fast and reliable segmentation assessment is essential. The code is available at https://github.com/mcosarinsky/In-Context-RCA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.05689</link>
<guid>https://arxiv.org/abs/2503.05689</guid>
<content:encoded><![CDATA[
arXiv:2503.05689v5 Announce Type: replace 
Abstract: We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the Navsim\cite{Dauner2024_navsim}, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition</title>
<link>https://arxiv.org/abs/2503.06220</link>
<guid>https://arxiv.org/abs/2503.06220</guid>
<content:encoded><![CDATA[
arXiv:2503.06220v3 Announce Type: replace 
Abstract: With the rise of real-world human-AI interaction applications, such as AI assistants, the need for Streaming Video Dialogue is critical. To address this need, we introduce StreamMind, a video LLM framework that achieves ultra-FPS streaming video processing (100 fps on a single A100) and enables proactive, always-on responses in real time, without explicit user intervention.
  To solve the key challenge of the contradiction between linear video streaming speed and quadratic transformer computation cost, we propose a novel perception-cognition interleaving paradigm named ''event-gated LLM invocation'', in contrast to the existing per-time-step LLM invocation. By introducing a Cognition Gate network between the video encoder and the LLM, LLM is only invoked when relevant events occur. To realize the event feature extraction with constant cost, we propose Event-Preserving Feature Extractor (EPFE) based on state-space method, generating a single perception token for spatiotemporal features. These techniques enable the video LLM with full-FPS perception and real-time cognition response.
  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard offline benchmarks, demonstrate state-of-the-art performance in both model capability and real-time efficiency, paving the way for ultra-high-FPS applications, such as Game AI and interactive media. The code and data is available at https://aka.ms/StreamMind.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models</title>
<link>https://arxiv.org/abs/2503.07575</link>
<guid>https://arxiv.org/abs/2503.07575</guid>
<content:encoded><![CDATA[
arXiv:2503.07575v3 Announce Type: replace 
Abstract: This research investigates both explicit and implicit social biases exhibited by Vision-Language Models (VLMs). The key distinction between these bias types lies in the level of awareness: explicit bias refers to conscious, intentional biases, while implicit bias operates subconsciously. To analyze explicit bias, we directly pose questions to VLMs related to gender and racial differences: (1) Multiple-choice questions based on a given image (e.g., "What is the education level of the person in the image?") (2) Yes-No comparisons using two images (e.g., "Is the person in the first image more educated than the person in the second image?") For implicit bias, we design tasks where VLMs assist users but reveal biases through their responses: (1) Image description tasks: Models are asked to describe individuals in images, and we analyze disparities in textual cues across demographic groups. (2) Form completion tasks: Models draft a personal information collection form with 20 attributes, and we examine correlations among selected attributes for potential biases. We evaluate Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data are publicly available at https://github.com/uscnlp-lime/VisBias.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2503.13111</link>
<guid>https://arxiv.org/abs/2503.13111</guid>
<content:encoded><![CDATA[
arXiv:2503.13111v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images</title>
<link>https://arxiv.org/abs/2503.16376</link>
<guid>https://arxiv.org/abs/2503.16376</guid>
<content:encoded><![CDATA[
arXiv:2503.16376v2 Announce Type: replace 
Abstract: The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation</title>
<link>https://arxiv.org/abs/2503.23212</link>
<guid>https://arxiv.org/abs/2503.23212</guid>
<content:encoded><![CDATA[
arXiv:2503.23212v3 Announce Type: replace 
Abstract: While convolutional neural networks (CNNs) have come to match and exceed human performance in many settings, the tasks these models optimize for are largely constrained to the level of individual objects, such as classification and captioning. Humans remain vastly superior to CNNs in visual tasks involving relations, including the ability to identify two objects as `same' or `different'. A number of studies have shown that while CNNs can be coaxed into learning the same-different relation in some settings, they tend to generalize poorly to other instances of this relation. In this work we show that the same CNN architectures that fail to generalize the same-different relation with conventional training are able to succeed when trained via meta-learning, which explicitly encourages abstraction and generalization across tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGGarment: Fine-Grained Garment Generation for Controllable Fashion Design</title>
<link>https://arxiv.org/abs/2504.13176</link>
<guid>https://arxiv.org/abs/2504.13176</guid>
<content:encoded><![CDATA[
arXiv:2504.13176v2 Announce Type: replace 
Abstract: This paper presents IMAGGarment, a fine-grained garment generation (FGG) framework that enables high-fidelity garment synthesis with precise control over silhouette, color, and logo placement. Unlike existing methods that are limited to single-condition inputs, IMAGGarment addresses the challenges of multi-conditional controllability in personalized fashion design and digital apparel applications. Specifically, IMAGGarment employs a two-stage training strategy to separately model global appearance and local details, while enabling unified and controllable generation through end-to-end inference. In the first stage, we propose a global appearance model that jointly encodes silhouette and color using a mixed attention module and a color adapter. In the second stage, we present a local enhancement model with an adaptive appearance-aware module to inject user-defined logos and spatial constraints, enabling accurate placement and visual consistency. To support this task, we release GarmentBench, a large-scale dataset comprising over 180K garment samples paired with multi-level design conditions, including sketches, color references, logo placements, and textual prompts. Extensive experiments demonstrate that our method outperforms existing baselines, achieving superior structural stability, color fidelity, and local controllability performance. Code, models, and datasets are publicly available at https://github.com/muzishen/IMAGGarment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Hyperspectral Images Into Chemical Maps: A Novel End-to-End Deep Learning Approach</title>
<link>https://arxiv.org/abs/2504.14131</link>
<guid>https://arxiv.org/abs/2504.14131</guid>
<content:encoded><![CDATA[
arXiv:2504.14131v4 Announce Type: replace 
Abstract: Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. The U-Net is compared with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WMKA-Net: A Weighted Multi-Kernel Attention Network for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2504.14888</link>
<guid>https://arxiv.org/abs/2504.14888</guid>
<content:encoded><![CDATA[
arXiv:2504.14888v3 Announce Type: replace 
Abstract: Retinal vessel segmentation is crucial for intelligent ophthalmic diagnosis, yet it faces three major challenges: insufficient multi-scale feature fusion, disruption of contextual continuity, and noise interference. This study proposes a dual-stage solution to address these issues. The first stage employs a Reversible Multi-Scale Fusion Module (RMS) that uses hierarchical adaptive convolution to dynamically merge cross-scale features from capillaries to main vessels, self-adaptively calibrating feature biases. The second stage introduces a Vascular-Oriented Attention Mechanism, which models long-distance vascular continuity through an axial pathway and enhances the capture of topological key nodes, such as bifurcation points, via a dedicated bifurcation attention pathway. The synergistic operation of these two pathways effectively restores the continuity of vascular structures and improves the segmentation accuracy of complex vascular networks. Systematic experiments on the DRIVE, STARE, and CHASE-DB1 datasets demonstrate that WMKA-Net achieves an accuracy of 0.9909, sensitivity of 0.9198, and specificity of 0.9953, significantly outperforming existing methods. This model provides an efficient, precise, and robust intelligent solution for the early screening of diabetic retinopathy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Partially Relevant Video Retrieval through Inter- and Intra-Sample Analysis with Coherence Prediction</title>
<link>https://arxiv.org/abs/2504.19637</link>
<guid>https://arxiv.org/abs/2504.19637</guid>
<content:encoded><![CDATA[
arXiv:2504.19637v2 Announce Type: replace 
Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video that is partially relevant to the text query. The primary challenge in PRVR arises from the semantic asymmetry between textual and visual modalities, as videos often contain substantial content irrelevant to the query. Existing methods coarsely align paired videos and text queries to construct the semantic space, neglecting the critical cross-modal dual nature inherent in this task: inter-sample correlation and intra-sample redundancy. To this end, we propose a novel PRVR framework to systematically exploit these two characteristics. Our framework consists of three core modules. First, the Inter Correlation Enhancement (ICE) module captures inter-sample correlation by identifying semantically similar yet unpaired text queries and video moments, combining them to form pseudo-positive pairs for more robust semantic space construction. Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample redundancy by mining redundant moment features and distinguishing them from query-relevant moments, encouraging the model to learn more discriminative representations. Finally, to reinforce these modules, we introduce the Temporal Coherence Prediction (TCP) module, which enhances temporal structure learning by training the model to predict the original temporal order of randomly shuffled video frames and moments. Extensive experiments demonstrate the superiority of our approach compared to prior methods, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models</title>
<link>https://arxiv.org/abs/2505.03569</link>
<guid>https://arxiv.org/abs/2505.03569</guid>
<content:encoded><![CDATA[
arXiv:2505.03569v2 Announce Type: replace 
Abstract: Backgrounds in images play a major role in contributing to spurious correlations among different data points. Owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. In this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. To better illustrate our findings, we propose a synthetic dataset derived from ImageNet-1k, Hard-Spurious-ImageNet, which contains images with various backgrounds, object positions, and object sizes. By evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (ROI) to image ratio is small and the object is far from the center of the image. Moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change. The dataset and implementation code are available at https://github.com/Mishalfatima/Corner_Cases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
arXiv:2505.11454v4 Announce Type: replace 
Abstract: Large multimodal models (LMMs) have been widely tested on tasks like visual question answering (VQA), image captioning, and grounding, but lack rigorous evaluation for alignment with human-centered (HC) values such as fairness, ethics, and inclusivity. To address this gap, we introduce \textbf{HumaniBench}, a novel benchmark of 32,000 real-world image-question pairs and an evaluation suite. Labels are generated via an AI-assisted pipeline and validated by experts. HumaniBench assesses LMMs across seven key alignment principles: fairness, ethics, empathy, inclusivity, reasoning, robustness, and multilinguality, through diverse open-ended and closed-ended VQA tasks. Grounded in AI ethics and real-world needs, these principles provide a holistic lens for societal impact. Benchmarking results on different LMM shows that proprietary models generally lead in reasoning, fairness, and multilinguality, while open-source models excel in robustness and grounding. Most models struggle to balance accuracy with ethical and inclusive behavior. Techniques like Chain-of-Thought prompting and test-time scaling improve alignment. As the first benchmark tailored for HC alignment, HumaniBench offers a rigorous testbed to diagnose limitations, and promote responsible LMM development. All data and code are publicly available for reproducibility.
  Keywords: HumaniBench, vision-language models, responsible AI benchmark, AI alignment evaluation, AI ethics assessment, fairness in AI models, visual question answering (VQA) benchmark, image captioning evaluation, visual grounding tasks, trustworthy AI models, Chain-of-Thought prompting, test-time scaling, ethical AI development tools.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Rotation Averaging Fast and Robust with Anisotropic Coordinate Descent</title>
<link>https://arxiv.org/abs/2506.01940</link>
<guid>https://arxiv.org/abs/2506.01940</guid>
<content:encoded><![CDATA[
arXiv:2506.01940v2 Announce Type: replace 
Abstract: Anisotropic rotation averaging has recently been explored as a natural extension of respective isotropic methods. In the anisotropic formulation, uncertainties of the estimated relative rotations -- obtained via standard two-view optimization -- are propagated to the optimization of absolute rotations. The resulting semidefinite relaxations are able to recover global minima but scale poorly with the problem size. Local methods are fast and also admit robust estimation but are sensitive to initialization. They usually employ minimum spanning trees and therefore suffer from drift accumulation and can get trapped in poor local minima. In this paper, we attempt to bridge the gap between optimality, robustness and efficiency of anisotropic rotation averaging. We analyze a family of block coordinate descent methods initially proposed to optimize the standard chordal distances, and derive a much simpler formulation and an anisotropic extension obtaining a fast general solver. We integrate this solver into the extended anisotropic large-scale robust rotation averaging pipeline. The resulting algorithm achieves state-of-the-art performance on public structure-from-motion datasets. Project page: https://ylochman.github.io/acd
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2506.14096</link>
<guid>https://arxiv.org/abs/2506.14096</guid>
<content:encoded><![CDATA[
arXiv:2506.14096v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2506.21109</link>
<guid>https://arxiv.org/abs/2506.21109</guid>
<content:encoded><![CDATA[
arXiv:2506.21109v2 Announce Type: replace 
Abstract: Remote sensing change detection is essential for monitoring urban expansion, disaster assessment, and resource management, offering timely, accurate, and large-scale insights into dynamic landscape transformations. While deep learning has revolutionized change detection, the increasing complexity and computational demands of modern models have not necessarily translated into significant accuracy gains. Instead of following this trend, this study explores a more efficient approach, focusing on lightweight models that maintain high accuracy while minimizing resource consumption, which is an essential requirement for on-satellite processing. To this end, we propose FlickCD, which means quick flick then get great results, pushing the boundaries of the performance-resource trade-off. FlickCD introduces an Enhanced Difference Module (EDM) to amplify critical feature differences between temporal phases while suppressing irrelevant variations such as lighting and weather changes, thereby reducing computational costs in the subsequent change decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion Blocks, leveraging Shifted Window Self-Attention (SWSA) and Efficient Global Self-Attention (EGSA) to effectively capture semantic information at multiple scales, preserving both coarse- and fine-grained changes. Extensive experiments on four benchmark datasets demonstrate that FlickCD reduces computational and storage overheads by more than an order of magnitude while achieving state-of-the-art (SOTA) performance or incurring only a minor (<1% F1) accuracy trade-off. The implementation code is publicly available at https://github.com/xulsh8/FlickCD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</title>
<link>https://arxiv.org/abs/2506.22146</link>
<guid>https://arxiv.org/abs/2506.22146</guid>
<content:encoded><![CDATA[
arXiv:2506.22146v3 Announce Type: replace 
Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces VISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective intervention: augmenting visual inputs with low-level spatial structures and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, VISER improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoreMark: Toward Robust and Universal Text Watermarking Technique</title>
<link>https://arxiv.org/abs/2506.23066</link>
<guid>https://arxiv.org/abs/2506.23066</guid>
<content:encoded><![CDATA[
arXiv:2506.23066v2 Announce Type: replace 
Abstract: Text watermarking schemes have gained considerable attention in recent years, yet still face critical challenges in achieving simultaneous robustness, generalizability, and imperceptibility. This paper introduces a new embedding paradigm,termed CORE, which comprises several consecutively aligned black pixel segments. Its key innovation lies in its inherent noise resistance during transmission and broad applicability across languages and fonts. Based on the CORE, we present a text watermarking framework named CoreMark. Specifically, CoreMark first dynamically extracts COREs from characters. Then, the characters with stronger robustness are selected according to the lengths of COREs. By modifying the thickness of the CORE, the hidden data is embedded into the selected characters without causing significant visual distortions. Moreover, a general plug-and-play embedding strength modulator is proposed, which can adaptively enhance the robustness for small font sizes by adjusting the embedding strength according to the font size. Experimental evaluation indicates that CoreMark demonstrates outstanding generalizability across multiple languages and fonts. Compared to existing methods, CoreMark achieves significant improvements in resisting screenshot, print-scan, and print camera attacks, while maintaining satisfactory imperceptibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model</title>
<link>https://arxiv.org/abs/2507.03302</link>
<guid>https://arxiv.org/abs/2507.03302</guid>
<content:encoded><![CDATA[
arXiv:2507.03302v2 Announce Type: replace 
Abstract: In semi-supervised semantic segmentation, existing studies have shown promising results in academic settings with controlled splits of benchmark datasets. However, the potential benefits of leveraging significantly larger sets of unlabeled images remain unexplored. In real-world scenarios, abundant unlabeled images are often available from online sources (web-scraped images) or large-scale datasets. However, these images may have different distributions from those of the target dataset, a situation known as out-of-distribution (OOD). Using these images as unlabeled data in semi-supervised learning can lead to inaccurate pseudo-labels, potentially misguiding network training. In this paper, we propose a new semi-supervised semantic segmentation framework with an open-vocabulary segmentation model (SemiOVS) to effectively utilize unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets demonstrate two key findings: (1) using additional unlabeled images improves the performance of semi-supervised learners in scenarios with few labels, and (2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD images leads to substantial performance gains. In particular, SemiOVS outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU, respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art performance. These findings demonstrate that our approach effectively utilizes abundant unlabeled OOD images for semantic segmentation tasks. We hope this work can inspire future research and real-world applications. The code is available at https://github.com/wooseok-shin/SemiOVS
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles</title>
<link>https://arxiv.org/abs/2507.04139</link>
<guid>https://arxiv.org/abs/2507.04139</guid>
<content:encoded><![CDATA[
arXiv:2507.04139v2 Announce Type: replace 
Abstract: Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepShade: Enable Shade Simulation by Text-conditioned Image Generation</title>
<link>https://arxiv.org/abs/2507.12103</link>
<guid>https://arxiv.org/abs/2507.12103</guid>
<content:encoded><![CDATA[
arXiv:2507.12103v4 Announce Type: replace 
Abstract: Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model</title>
<link>https://arxiv.org/abs/2507.13087</link>
<guid>https://arxiv.org/abs/2507.13087</guid>
<content:encoded><![CDATA[
arXiv:2507.13087v2 Announce Type: replace 
Abstract: Annotation variability remains a substantial challenge in medical image segmentation, stemming from ambiguous imaging boundaries and diverse clinical expertise. Traditional deep learning methods producing single deterministic segmentation predictions often fail to capture these annotator biases. Although recent studies have explored multi-rater segmentation, existing methods typically focus on a single perspective -- either generating a probabilistic ``gold standard'' consensus or preserving expert-specific preferences -- thus struggling to provide a more omni view. In this study, we propose DiffOSeg, a two-stage diffusion-based framework, which aims to simultaneously achieve both consensus-driven (combining all experts' opinions) and preference-driven (reflecting experts' individual assessments) segmentation. Stage I establishes population consensus through a probabilistic consensus strategy, while Stage II captures expert-specific preference via adaptive prompts. Demonstrated on two public datasets (LIDC-IDRI and NPC-170), our model outperforms existing state-of-the-art methods across all evaluated metrics. Source code is available at https://github.com/string-ellipses/DiffOSeg .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2</title>
<link>https://arxiv.org/abs/2507.14613</link>
<guid>https://arxiv.org/abs/2507.14613</guid>
<content:encoded><![CDATA[
arXiv:2507.14613v2 Announce Type: replace 
Abstract: Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs</title>
<link>https://arxiv.org/abs/2507.16193</link>
<guid>https://arxiv.org/abs/2507.16193</guid>
<content:encoded><![CDATA[
arXiv:2507.16193v2 Announce Type: replace 
Abstract: The rapid advancement of Text-guided Image Editing (TIE) enables image modifications through text prompts. However, current TIE models still struggle to balance image quality, editing alignment, and consistency with the original image, limiting their practical applications. Existing TIE evaluation benchmarks and metrics have limitations on scale or alignment with human perception. To this end, we introduce EBench-18K, the first large-scale image Editing Benchmark including 18K edited images with fine-grained human preference annotations for evaluating TIE. Specifically, EBench-18K includes 1,080 source images with corresponding editing prompts across 21 tasks, 18K+ edited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion scores (MOSs) assessed from three evaluation dimensions, and 18K+ question-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs to assess edited images, while the evaluation results, in turn, provide insights into assessing the alignment between the LMMs' understanding ability and human preferences. Then, we propose LMM4Edit, a LMM-based metric for evaluating image Editing models from perceptual quality, editing alignment, attribute preservation, and task-specific QA accuracy in an all-in-one manner. Extensive experiments show that LMM4Edit achieves outstanding performance and aligns well with human preference. Zero-shot validation on the other datasets also shows the generalization ability of our model. The dataset and code are available at https://github.com/IntMeGroup/LMM4Edit.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualization without a Legend</title>
<link>https://arxiv.org/abs/2507.20632</link>
<guid>https://arxiv.org/abs/2507.20632</guid>
<content:encoded><![CDATA[
arXiv:2507.20632v2 Announce Type: replace 
Abstract: Recovering a continuous colormap from a single 2D scalar field visualization can be quite challenging, especially in the absence of a corresponding color legend. In this paper, we propose a novel colormap recovery approach that extracts the colormap from a color-encoded 2D scalar field visualization by simultaneously predicting the colormap and underlying data using a decoupling-and-reconstruction strategy. Our approach first separates the input visualization into colormap and data using a decoupling module, then reconstructs the visualization with a differentiable color-mapping module. To guide this process, we design a reconstruction loss between the input and reconstructed visualizations, which serves both as a constraint to ensure strong correlation between colormap and data during training, and as a self-supervised optimizer for fine-tuning the predicted colormap of unseen visualizations during inferencing. To ensure smoothness and correct color ordering in the extracted colormap, we introduce a compact colormap representation using cubic B-spline curves and an associated color order loss. We evaluate our method quantitatively and qualitatively on a synthetic dataset and a collection of real-world visualizations from the VIS30K dataset. Additionally, we demonstrate its utility in two prototype applications -- colormap adjustment and colormap transfer -- and explore its generalization to visualizations with color legends and ones encoded using discrete color palettes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Image Similarity Metric for Scene Composition Structure</title>
<link>https://arxiv.org/abs/2508.05037</link>
<guid>https://arxiv.org/abs/2508.05037</guid>
<content:encoded><![CDATA[
arXiv:2508.05037v3 Announce Type: replace 
Abstract: The rapid advancement of generative AI models necessitates novel methods for evaluating image quality that extend beyond human perception. A critical concern for these models is the preservation of an image's underlying Scene Composition Structure (SCS), which defines the geometric relationships among objects and the background, their relative positions, sizes, orientations, etc. Maintaining SCS integrity is paramount for ensuring faithful and structurally accurate GenAI outputs. Traditional image similarity metrics often fall short in assessing SCS. Pixel-level approaches are overly sensitive to minor visual noise, while perception-based metrics prioritize human aesthetic appeal, neither adequately capturing structural fidelity. Furthermore, recent neural-network-based metrics introduce training overheads and potential generalization issues. We introduce the SCS Similarity Index Measure (SCSSIM), a novel, analytical, and training-free metric that quantifies SCS preservation by exploiting statistical measures derived from the Cuboidal hierarchical partitioning of images, robustly capturing non-object-based structural relationships. Our experiments demonstrate SCSSIM's high invariance to non-compositional distortions, accurately reflecting unchanged SCS. Conversely, it shows a strong monotonic decrease for compositional distortions, precisely indicating when SCS has been altered. Compared to existing metrics, SCSSIM exhibits superior properties for structural evaluation, making it an invaluable tool for developing and evaluating generative models, ensuring the integrity of scene composition. See \href{https://github.com/RedwanPlague/scssim}{code}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2508.05264</link>
<guid>https://arxiv.org/abs/2508.05264</guid>
<content:encoded><![CDATA[
arXiv:2508.05264v2 Announce Type: replace 
Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.06869</link>
<guid>https://arxiv.org/abs/2508.06869</guid>
<content:encoded><![CDATA[
arXiv:2508.06869v2 Announce Type: replace 
Abstract: Long video understanding presents a significant challenge to multimodal large language models (MLLMs) primarily due to the immense data scale. A critical and widely adopted strategy for making this task computationally tractable is keyframe retrieval, which seeks to identify a sparse set of video frames that are most salient to a given textual query. However, the efficacy of this approach is hindered by weak multimodal alignment between textual queries and visual content and fails to capture the complex temporal semantic information required for precise reasoning. To address this, we propose Visual-Subtitle Integeration(VSI), a multimodal keyframe search method that integrates subtitles, timestamps, and scene boundaries into a unified multimodal search process. The proposed method captures the visual information of video frames as well as the complementary textual information through a dual-stream search mechanism by Video Search Stream as well as Subtitle Match Stream, respectively, and improves the keyframe search accuracy through the interaction of the two search streams. Experimental results show that VSI achieve 40.00% key frame localization accuracy on the text-relevant subset of LongVideoBench and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive baselines by 20.35% and 15.79%, respectively. Furthermore, on the LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA tasks, demonstrating the robustness and generalizability of the proposed multimodal search strategy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical Autoregressive Model for Robotic Manipulation without Action Pretraining</title>
<link>https://arxiv.org/abs/2508.09822</link>
<guid>https://arxiv.org/abs/2508.09822</guid>
<content:encoded><![CDATA[
arXiv:2508.09822v4 Announce Type: replace 
Abstract: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages</title>
<link>https://arxiv.org/abs/2508.11854</link>
<guid>https://arxiv.org/abs/2508.11854</guid>
<content:encoded><![CDATA[
arXiv:2508.11854v2 Announce Type: replace 
Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Discrepancy-aware Detector for Image Forgery Identification</title>
<link>https://arxiv.org/abs/2508.12341</link>
<guid>https://arxiv.org/abs/2508.12341</guid>
<content:encoded><![CDATA[
arXiv:2508.12341v2 Announce Type: replace 
Abstract: With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language</title>
<link>https://arxiv.org/abs/2508.12543</link>
<guid>https://arxiv.org/abs/2508.12543</guid>
<content:encoded><![CDATA[
arXiv:2508.12543v2 Announce Type: replace 
Abstract: The rapid advancement of generative models has intensified the challenge of detecting and interpreting visual forgeries, necessitating robust frameworks for image forgery detection while providing reasoning as well as localization. While existing works approach this problem using supervised training for specific manipulation or anomaly detection in the embedding space, generalization across domains remains a challenge. We frame this problem of forgery detection as a prompt-driven visual reasoning task, leveraging the semantic alignment capabilities of large vision-language models. We propose a framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through Aligned Language), that incorporates generalized guidelines. We propose two tangential approaches - (1) Holistic Scene-level Evaluation that relies on the physics, semantics, perspective, and realism of the image as a whole and (2) Region-wise anomaly detection that splits the image into multiple regions and analyzes each of them. We conduct experiments over datasets from different domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language Models against competitive baselines and analyze the reasoning provided by them.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D Visual Pre-training for Robot Learning</title>
<link>https://arxiv.org/abs/2508.17230</link>
<guid>https://arxiv.org/abs/2508.17230</guid>
<content:encoded><![CDATA[
arXiv:2508.17230v2 Announce Type: replace 
Abstract: General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d-visual-pretraining.github.io/
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Label-Efficient Deep Waste Detection</title>
<link>https://arxiv.org/abs/2508.18799</link>
<guid>https://arxiv.org/abs/2508.18799</guid>
<content:encoded><![CDATA[
arXiv:2508.18799v2 Announce Type: replace 
Abstract: Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions. Our code is available at: https://github.com/h-abid97/robust-waste-detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning</title>
<link>https://arxiv.org/abs/2508.19769</link>
<guid>https://arxiv.org/abs/2508.19769</guid>
<content:encoded><![CDATA[
arXiv:2508.19769v2 Announce Type: replace 
Abstract: Multimodal learning has significantly enhanced machine learning performance but still faces numerous challenges and limitations. Imbalanced multimodal learning is one of the problems extensively studied in recent works and is typically mitigated by modulating the learning of each modality. However, we find that these methods typically hinder the dominant modality's learning to promote weaker modalities, which affects overall multimodal performance. We analyze the cause of this issue and highlight a commonly overlooked problem: optimization bias within networks. To address this, we propose Adaptive Intra-Network Modulation (AIM) to improve balanced modality learning. AIM accounts for differences in optimization state across parameters and depths within the network during modulation, achieving balanced multimodal learning without hindering either dominant or weak modalities for the first time. Specifically, AIM decouples the dominant modality's under-optimized parameters into Auxiliary Blocks and encourages reliance on these performance-degraded blocks for joint training with weaker modalities. This approach effectively prevents suppression of weaker modalities while enabling targeted optimization of under-optimized parameters to improve the dominant modality. Additionally, AIM assesses modality imbalance level across network depths and adaptively adjusts modulation strength at each depth. Experimental results demonstrate that AIM outperforms state-of-the-art imbalanced modality learning methods across multiple benchmarks and exhibits strong generalizability across different backbones, fusion strategies, and optimizers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoisyNN: Exploring the Impact of Information Entropy Change in Learning Systems</title>
<link>https://arxiv.org/abs/2309.10625</link>
<guid>https://arxiv.org/abs/2309.10625</guid>
<content:encoded><![CDATA[
arXiv:2309.10625v5 Announce Type: replace-cross 
Abstract: We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$\%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation</title>
<link>https://arxiv.org/abs/2311.01766</link>
<guid>https://arxiv.org/abs/2311.01766</guid>
<content:encoded><![CDATA[
arXiv:2311.01766v5 Announce Type: replace-cross 
Abstract: Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale dataset demonstrated that our proposed method outperformed the state-of-the-art baselines, with the best model achieving a performance gain of 3.2% in accuracy. The source code and checkpoints are publicly available at https://github.com/yx3266/SEN.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-Aware Data Augmentation for Cardiac MRI using Text-Conditioned Diffusion Models</title>
<link>https://arxiv.org/abs/2403.19508</link>
<guid>https://arxiv.org/abs/2403.19508</guid>
<content:encoded><![CDATA[
arXiv:2403.19508v2 Announce Type: replace-cross 
Abstract: While deep learning holds great promise for disease diagnosis and prognosis in cardiac magnetic resonance imaging, its progress is often constrained by highly imbalanced and biased training datasets. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index (BMI), and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks. We assess our method using a large-cohort study from the UK Biobank by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of diagnosed female patients or individuals with normal BMI level suffering from heart failure. This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks. Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments. Our code is available at https://github.com/faildeny/debiasing-cardiac-mri.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIBESegmentator: Full Body MRI Segmentation for the NAKO and UK Biobank</title>
<link>https://arxiv.org/abs/2406.00125</link>
<guid>https://arxiv.org/abs/2406.00125</guid>
<content:encoded><![CDATA[
arXiv:2406.00125v4 Announce Type: replace-cross 
Abstract: Objectives: To present a publicly available deep learning-based torso segmentation model that provides comprehensive voxel-wise coverage, including delineations that extend to the boundaries of anatomical compartments. Materials and Methods: We extracted preliminary segmentations from TotalSegmentator, spine, and body composition models for Magnetic Resonance Tomography (MR) images, then improved them iteratively and retrained an nnUNet model. Using a random retrospective subset of German National Cohort (NAKO), UK Biobank, internal MR and Computed Tomography (CT) data (Training: 2897 series from 626 subjects, 290 female; mean age 53+-16; 3-fold-cross validation (20% hold-out). Internal testing 36 series from 12 subjects, 6 male; mean age 60+-11), we segmented 71 structures in torso MR and 72 in CT images: 20 organs, 10 muscles, 19 vessels, 16 bones, ribs in CT, intervertebral discs, spinal cord, spinal canal and body composition (subcutaneous fat, unclassified muscles and visceral fat). For external validation, we used existing automatic organ segmentations, independent ground truth segmentations on gradient echo images, and the Amos data. We used non-parametric bootstrapping for confidence intervals and Wilcoxon rank-sum test for computing statistical significance. Results: We achieved an average Dice score of 0.90+-0.06 on our internal gradient echo test set, which included 71 semantic segmentation labels. Our model ties with the best model on Amos with a Dice of 0,81+-0.14, while having a larger field of view and a considerably higher number structures included. Conclusion: Our work presents a publicly available full-torso segmentation model for MRI and CT images that classifies almost all subject voxels to date.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale Transformer Approach</title>
<link>https://arxiv.org/abs/2408.04290</link>
<guid>https://arxiv.org/abs/2408.04290</guid>
<content:encoded><![CDATA[
arXiv:2408.04290v5 Announce Type: replace-cross 
Abstract: Pneumonia, a prevalent respiratory infection, remains a leading cause of morbidity and mortality worldwide, particularly among vulnerable populations. Chest X-rays serve as a primary tool for pneumonia detection; however, variations in imaging conditions and subtle visual indicators complicate consistent interpretation. Automated tools can enhance traditional methods by improving diagnostic reliability and supporting clinical decision-making. In this study, we propose a novel multi-scale transformer approach for pneumonia detection that integrates lung segmentation and classification into a unified framework. Our method introduces a lightweight transformer-enhanced TransUNet for precise lung segmentation, achieving a Dice score of 95.68% on the "Chest X-ray Masks and Labels" dataset with fewer parameters than traditional transformers. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, which are then processed through a modified transformer module to enhance pneumonia detection. This integration of multi-scale feature extraction and lightweight transformer modules ensures robust performance, making our method suitable for resource-constrained clinical environments. Our approach achieves 93.75% accuracy on the "Kermany" dataset and 96.04% accuracy on the "Cohen" dataset, outperforming existing methods while maintaining computational efficiency. This work demonstrates the potential of multi-scale transformer architectures to improve pneumonia diagnosis, offering a scalable and accurate solution to global healthcare challenges. https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILeSiA: Interactive Learning of Robot Situational Awareness from Camera Input</title>
<link>https://arxiv.org/abs/2409.20173</link>
<guid>https://arxiv.org/abs/2409.20173</guid>
<content:encoded><![CDATA[
arXiv:2409.20173v3 Announce Type: replace-cross 
Abstract: Learning from demonstration is a promising approach for teaching robots new skills. However, a central challenge in the execution of acquired skills is the ability to recognize faults and prevent failures. This is essential because demonstrations typically cover only a limited set of scenarios and often only the successful ones. During task execution, unforeseen situations may arise, such as changes in the robot's environment or interaction with human operators. To recognize such situations, this paper focuses on teaching the robot situational awareness by using a camera input and labeling frames as safe or risky. We train a Gaussian Process (GP) regression model fed by a low-dimensional latent space representation of the input images. The model outputs a continuous risk score ranging from zero to one, quantifying the degree of risk at each timestep. This allows for pausing task execution in unsafe situations and directly adding new training data, labeled by the human user. Our experiments on a robotic manipulator show that the proposed method can reliably detect both known and novel faults using only a single example for each new fault. In contrast, a standard multi-layer perceptron (MLP) performs well only on faults it has encountered during training. Our method enables the next generation of cobots to be rapidly deployed with easy-to-set-up, vision-based risk assessment, proactively safeguarding humans and detecting misaligned parts or missing objects before failures occur. We provide all the code and data required to reproduce our experiments at imitrob.ciirc.cvut.cz/publications/ilesia.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration</title>
<link>https://arxiv.org/abs/2410.02006</link>
<guid>https://arxiv.org/abs/2410.02006</guid>
<content:encoded><![CDATA[
arXiv:2410.02006v2 Announce Type: replace-cross 
Abstract: Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
<link>https://arxiv.org/abs/2411.01579</link>
<guid>https://arxiv.org/abs/2411.01579</guid>
<content:encoded><![CDATA[
arXiv:2411.01579v3 Announce Type: replace-cross 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Sim2Real Gap: Vision Encoder Pre-Training for Visuomotor Policy Transfer</title>
<link>https://arxiv.org/abs/2501.16389</link>
<guid>https://arxiv.org/abs/2501.16389</guid>
<content:encoded><![CDATA[
arXiv:2501.16389v2 Announce Type: replace-cross 
Abstract: Simulation offers a scalable and efficient alternative to real-world data collection for learning visuomotor robotic policies. However, the simulation-to-reality, or Sim2Real distribution shift -- introduced by employing simulation-trained policies in real-world environments -- frequently prevents successful policy transfer. We present an offline framework to evaluate the performance of using large-scale pre-trained vision encoders to address the Sim2Real gap. We examine a diverse collection of encoders, assessing their ability to extract features necessary for robot control (Action Score) while remaining invariant to task-irrelevant environmental variations (Domain Invariance Score). Evaluating 23 encoders, we reveal patterns across architectures, pre-training datasets, and parameter scales. Our findings show that manipulation-pretrained encoders consistently achieve higher Action Scores, CNN-based encoders demonstrate stronger domain invariance than ViTs, and the best-performing models combine both properties, underscoring DIS and AS as complementary predictors of Sim2Real transferability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Alignment of Unconditioned Action Prior for Language-conditioned Pick and Place in Clutter</title>
<link>https://arxiv.org/abs/2503.09423</link>
<guid>https://arxiv.org/abs/2503.09423</guid>
<content:encoded><![CDATA[
arXiv:2503.09423v3 Announce Type: replace-cross 
Abstract: We study the task of language-conditioned pick and place in clutter, where a robot should grasp a target object in open clutter and move it to a specified place. Some approaches learn end-to-end policies with features from vision foundation models, requiring large datasets. Others combine foundation models in a zero-shot setting, suffering from cascading errors. In addition, they primarily leverage vision and language foundation models, focusing less on action priors. In this paper, we aim to develop an effective policy by integrating foundation priors from vision, language, and action. We propose A$^2$, an action prior alignment method that aligns unconditioned action priors with 3D vision-language priors by learning one attention layer. The alignment formulation enables our policy to train with less data and preserve zero-shot generalization capabilities. We show that a shared policy for both pick and place actions enhances the performance for each task, and introduce a policy adaptation scheme to accommodate the multi-modal nature of actions. Extensive experiments in simulation and the real-world show that our policy achieves higher task success rates with fewer steps for both pick and place tasks in clutter, effectively generalizing to unseen objects and language instructions. Videos and codes are available at https://xukechun.github.io/papers/A2.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Latent Fusion of ECG Leads for Early Assessment of Pulmonary Hypertension</title>
<link>https://arxiv.org/abs/2503.13470</link>
<guid>https://arxiv.org/abs/2503.13470</guid>
<content:encoded><![CDATA[
arXiv:2503.13470v2 Announce Type: replace-cross 
Abstract: Recent advancements in early assessment of pulmonary hypertension (PH) primarily focus on applying machine learning methods to centralized diagnostic modalities, such as 12-lead electrocardiogram (12L-ECG). Despite their potential, these approaches fall short in decentralized clinical settings, e.g., point-of-care and general practice, where handheld 6-lead ECG (6L-ECG) can offer an alternative but is limited by the scarcity of labeled data for developing reliable models. To address this, we propose a lead-specific electrocardiogram multimodal variational autoencoder (\textsc{LS-EMVAE}), which incorporates a hierarchical modality expert (HiME) fusion mechanism and a latent representation alignment loss. HiME combines mixture-of-experts and product-of-experts to enable flexible, adaptive latent fusion, while the alignment loss improves coherence among lead-specific and shared representations. To alleviate data scarcity and enhance representation learning, we adopt a transfer learning strategy: the model is first pre-trained on a large unlabeled 12L-ECG dataset and then fine-tuned on smaller task-specific labeled 6L-ECG datasets. We validate \textsc{LS-EMVAE} across two retrospective cohorts in a 6L-ECG setting: 892 subjects from the ASPIRE registry for (1) PH detection and (2) phenotyping pre-/post-capillary PH, and 16,416 subjects from UK Biobank for (3) predicting elevated pulmonary atrial wedge pressure, where it consistently outperforms unimodal and multimodal baseline methods and demonstrates strong generalizability and interpretability. The code is available at https://github.com/Shef-AIRE/LS-EMVAE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.19713</link>
<guid>https://arxiv.org/abs/2503.19713</guid>
<content:encoded><![CDATA[
arXiv:2503.19713v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Semi-SD, a novel metric depth estimation framework tailored for surrounding cameras equipment in autonomous driving. In this work, the input data consists of adjacent surrounding frames and camera parameters. We propose a unified spatial-temporal-semantic fusion module to construct the visual fused features. Cross-attention components for surrounding cameras and adjacent frames are utilized to focus on metric scale information refinement and temporal feature matching. Building on this, we propose a pose estimation framework using surrounding cameras, their corresponding estimated depths, and extrinsic parameters, which effectively address the scale ambiguity in multi-camera setups. Moreover, semantic world model and monocular depth estimation world model are integrated to supervised the depth estimation, which improve the quality of depth estimation. We evaluate our algorithm on DDAD and nuScenes datasets, and the results demonstrate that our method achieves state-of-the-art performance in terms of surrounding camera based depth estimation quality. The source code will be available on https://github.com/xieyuser/Semi-SD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy</title>
<link>https://arxiv.org/abs/2503.19757</link>
<guid>https://arxiv.org/abs/2503.19757</guid>
<content:encoded><![CDATA[
arXiv:2503.19757v2 Announce Type: replace-cross 
Abstract: While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2505.08299</link>
<guid>https://arxiv.org/abs/2505.08299</guid>
<content:encoded><![CDATA[
arXiv:2505.08299v2 Announce Type: replace-cross 
Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\% parameter reduction while retaining over 95\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content Generation Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.10993</link>
<guid>https://arxiv.org/abs/2505.10993</guid>
<content:encoded><![CDATA[
arXiv:2505.10993v2 Announce Type: replace-cross 
Abstract: Content generation modeling has emerged as a promising direction in computational pathology, offering capabilities such as data-efficient learning, synthetic data augmentation, and task-oriented generation across diverse diagnostic tasks. This review provides a comprehensive synthesis of recent progress in the field, organized into four key domains: image generation, text generation, molecular profile-morphology generation, and other specialized generation applications. By analyzing over 150 representative studies, we trace the evolution of content generation architectures -- from early generative adversarial networks to recent advances in diffusion models and generative vision-language models. We further examine the datasets and evaluation protocols commonly used in this domain and highlight ongoing limitations, including challenges in generating high-fidelity whole slide images, clinical interpretability, and concerns related to the ethical and legal implications of synthetic data. The review concludes with a discussion of open challenges and prospective research directions, with an emphasis on developing integrated and clinically deployable generation systems. This work aims to provide a foundational reference for researchers and practitioners developing content generation models in computational pathology.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised cell segmentation by fast Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.18902</link>
<guid>https://arxiv.org/abs/2505.18902</guid>
<content:encoded><![CDATA[
arXiv:2505.18902v2 Announce Type: replace-cross 
Abstract: Cell boundary information is crucial for analyzing cell behaviors from time-lapse microscopy videos. Existing supervised cell segmentation tools, such as ImageJ, require tuning various parameters and rely on restrictive assumptions about the shape of the objects. While recent supervised segmentation tools based on convolutional neural networks enhance accuracy, they depend on high-quality labeled images, making them unsuitable for segmenting new types of objects not in the database. We developed a novel unsupervised cell segmentation algorithm based on fast Gaussian processes for noisy microscopy images without the need for parameter tuning or restrictive assumptions about the shape of the object. We derived robust thresholding criteria adaptive for heterogeneous images containing distinct brightness at different parts to separate objects from the background, and employed watershed segmentation to distinguish touching cell objects. Both simulated studies and real-data analysis of large microscopy images demonstrate the scalability and accuracy of our approach compared with the alternatives.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards</title>
<link>https://arxiv.org/abs/2506.07963</link>
<guid>https://arxiv.org/abs/2506.07963</guid>
<content:encoded><![CDATA[
arXiv:2506.07963v3 Announce Type: replace-cross 
Abstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \textbf{SUDER} (\textbf{S}elf-improving \textbf{U}nified LMMs with \textbf{D}ual s\textbf{E}lf-\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRO: Projection Domain Synthesis for CT Imaging</title>
<link>https://arxiv.org/abs/2506.13443</link>
<guid>https://arxiv.org/abs/2506.13443</guid>
<content:encoded><![CDATA[
arXiv:2506.13443v3 Announce Type: replace-cross 
Abstract: Synthetic CT projection data is crucial for advancing imaging research, yet its generation remains challenging. Current image domain methods are limited as they cannot simulate the physical acquisition process or utilize the complete statistical information present in projection data, restricting their utility and fidelity. In this work, we present PRO, a projection domain synthesis foundation model for CT imaging. To the best of our knowledge, this is the first study that performs CT synthesis in the projection domain. Unlike previous approaches that operate in the image domain, PRO learns rich structural representations from projection data and leverages anatomical text prompts for controllable synthesis. Projection data generation models can utilize complete measurement signals and simulate the physical processes of scanning, including material attenuation characteristics, beam hardening, scattering, and projection geometry, and support research on downstream imaging tasks. Moreover, PRO functions as a foundation model, capable of generalizing across diverse downstream tasks by adjusting its generative behavior via prompt inputs. Experimental results demonstrated that incorporating our synthesized data significantly improves performance across multiple downstream tasks, including low-dose and sparse-view reconstruction. These findings underscore the versatility and scalability of PRO in data generation for various CT applications. These results highlight the potential of projection domain synthesis as a powerful tool for data augmentation and robust CT imaging. Our source code is publicly available at: https://github.com/yqx7150/PRO.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder</title>
<link>https://arxiv.org/abs/2508.02431</link>
<guid>https://arxiv.org/abs/2508.02431</guid>
<content:encoded><![CDATA[
arXiv:2508.02431v3 Announce Type: replace-cross 
Abstract: Identifying actionable driver mutations in non-small cell lung cancer (NSCLC) can impact treatment decisions and significantly improve patient outcomes. Despite guideline recommendations, broader adoption of genetic testing remains challenging due to limited availability and lengthy turnaround times. Machine Learning (ML) methods for Computational Pathology (CPath) offer a potential solution; however, research often focuses on only one or two common mutations, limiting the clinical value of these tools and the pool of patients who can benefit from them. This study evaluates various Multiple Instance Learning (MIL) techniques to detect six key actionable NSCLC driver mutations: ALK, BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric Transformer Decoder model that employs queries and key-values of varying dimensions to maintain a low query dimensionality. This approach efficiently extracts information from patch embeddings and minimizes overfitting risks, proving highly adaptable to the MIL setting. Moreover, we present a method to directly utilize tissue type in the model, addressing a typical MIL limitation where either all regions or only some specific regions are analyzed, neglecting biological relevance. Our method outperforms top MIL models by an average of 3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving ML-based tests closer to being practical alternatives to standard genetic testing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hessian-Based Lightweight Neural Network HessNet for State-of-the-Art Brain Vessel Segmentation on a Minimal Training Dataset</title>
<link>https://arxiv.org/abs/2508.15660</link>
<guid>https://arxiv.org/abs/2508.15660</guid>
<content:encoded><![CDATA[
arXiv:2508.15660v3 Announce Type: replace-cross 
Abstract: Accurate segmentation of blood vessels in brain magnetic resonance angiography (MRA) is essential for successful surgical procedures, such as aneurysm repair or bypass surgery. Currently, annotation is primarily performed through manual segmentation or classical methods, such as the Frangi filter, which often lack sufficient accuracy. Neural networks have emerged as powerful tools for medical image segmentation, but their development depends on well-annotated training datasets. However, there is a notable lack of publicly available MRA datasets with detailed brain vessel annotations. To address this gap, we propose a novel semi-supervised learning lightweight neural network with Hessian matrices on board for 3D segmentation of complex structures such as tubular structures, which we named HessNet. The solution is a Hessian-based neural network with only 6000 parameters. HessNet can run on the CPU and significantly reduces the resource requirements for training neural networks. The accuracy of vessel segmentation on a minimal training dataset reaches state-of-the-art results. It helps us create a large, semi-manually annotated brain vessel dataset of brain MRA images based on the IXI dataset (annotated 200 images). Annotation was performed by three experts under the supervision of three neurovascular surgeons after applying HessNet. It provides high accuracy of vessel segmentation and allows experts to focus only on the most complex important cases. The dataset is available at https://git.scinalytics.com/terilat/VesselDatasetPartly.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Landscape of Non-Equilibrium Memories with Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2508.15726</link>
<guid>https://arxiv.org/abs/2508.15726</guid>
<content:encoded><![CDATA[
arXiv:2508.15726v2 Announce Type: replace-cross 
Abstract: We investigate the landscape of many-body memories: families of local non-equilibrium dynamics that retain information about their initial conditions for thermodynamically long time scales, even in the presence of arbitrary perturbations. In two dimensions, the only well-studied memory is Toom's rule. Using a combination of rigorous proofs and machine learning methods, we show that the landscape of 2D memories is in fact quite vast. We discover memories that correct errors in ways qualitatively distinct from Toom's rule, have ordered phases stabilized by fluctuations, and preserve information only in the presence of noise. Taken together, our results show that physical systems can perform robust information storage in many distinct ways, and demonstrate that the physics of many-body memories is richer than previously realized. Interactive visualizations of the dynamics studied in this work are available at https://memorynca.github.io/2D.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis</title>
<link>https://arxiv.org/abs/2508.18597</link>
<guid>https://arxiv.org/abs/2508.18597</guid>
<content:encoded><![CDATA[
arXiv:2508.18597v2 Announce Type: replace-cross 
Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor scenes across multiple room types. The model introduces a scene layout representation combining a top-down semantic map and attributes for each object. Unlike prior approaches, which cannot condition on architectural constraints, SemLayoutDiff employs a categorical diffusion model capable of conditioning scene synthesis explicitly on room masks. It first generates a coherent semantic map, followed by a cross-attention-based network to predict furniture placements that respect the synthesized layout. Our method also accounts for architectural elements such as doors and windows, ensuring that generated furniture arrangements remain practical and unobstructed. Experiments on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent, realistic, and varied scenes, outperforming previous methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
arXiv:2508.19026v2 Announce Type: replace-cross 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge discovery, multimodal content, automated article generation, self-reflection mechanism, benchmark evaluation 

Summary: 
WikiAutoGen is a new system designed for automated generation of Wikipedia-style articles that incorporate both text and images to enhance informativeness and engagement. The system improves factual accuracy and comprehensiveness by critically assessing retrieved content from various perspectives. A benchmark called WikiSeek has been introduced to evaluate the system's performance on challenging topics, consisting of Wikipedia articles paired with textual and image-based representations. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on the WikiSeek benchmark, producing more accurate, coherent, and visually enriched articles. The code and examples for WikiAutoGen are available at https://wikiautogen.github.io/ <br /><br />Summary: <div>
arXiv:2503.19065v3 Announce Type: replace 
Abstract: Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. Our code and examples are available at https://wikiautogen.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facial Emotion Recognition does not detect feeling unsafe in automated driving</title>
<link>https://arxiv.org/abs/2509.04490</link>
<guid>https://arxiv.org/abs/2509.04490</guid>
<content:encoded><![CDATA[
<div> perceived risk, automated vehicles, trust, driving simulator, facial expression

Summary:
The study investigated perceived risk in automated vehicles by conducting an experiment with 32 participants using a driving simulator. Different driving styles were tested, and the introduction of a crossing pedestrian was also examined. Results showed that discomfort was associated with perceived risk during cornering and braking, with the dynamic driving style inducing stronger discomfort. Facial expression analysis revealed that most participants did not show detectable reactions to critical events, indicating that it may not be a reliable method for assessing perceived risk. A neural network model using vehicle motion and skin conductance was implemented to predict perceived risk, showing promising results in correlating with reported discomfort. This suggests the potential for objective perceived risk assessment in automated vehicles, reducing subjective bias and pointing towards future research directions. 

<br /><br />Summary: <div>
arXiv:2509.04490v1 Announce Type: new 
Abstract: Trust and perceived safety play a crucial role in the public acceptance of automated vehicles. To understand perceived risk, an experiment was conducted using a driving simulator under two automated driving styles and optionally introducing a crossing pedestrian. Data was collected from 32 participants, consisting of continuous subjective comfort ratings, motion, webcam footage for facial expression, skin conductance, heart rate, and eye tracking. The continuous subjective perceived risk ratings showed significant discomfort associated with perceived risk during cornering and braking followed by relief or even positive comfort on continuing the ride. The dynamic driving style induced a stronger discomfort as compared to the calm driving style. The crossing pedestrian did not affect discomfort with the calm driving style but doubled the comfort decrement with the dynamic driving style. This illustrates the importance of consequences of critical interactions in risk perception. Facial expression was successfully analyzed for 24 participants but most (15/24) did not show any detectable facial reaction to the critical event. Among the 9 participants who did, 8 showed a Happy expression, and only 4 showed a Surprise expression. Fear was never dominant. This indicates that facial expression recognition is not a reliable method for assessing perceived risk in automated vehicles. To predict perceived risk a neural network model was implemented using vehicle motion and skin conductance. The model correlated well with reported perceived risk, demonstrating its potential for objective perceived risk assessment in automated vehicles, reducing subjective bias and highlighting areas for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting</title>
<link>https://arxiv.org/abs/2509.04545</link>
<guid>https://arxiv.org/abs/2509.04545</guid>
<content:encoded><![CDATA[
<div> keywords: text-to-image, PromptEnhancer, Chain-of-Thought rewriter, AlignEvaluator, image-text alignment

Summary:
PromptEnhancer introduces a novel prompt rewriting framework to enhance text-to-image models by improving attribute binding, negation, and compositional relationships in generated images. The framework, which includes a Chain-of-Thought rewriter trained through reinforcement learning guided by an AlignEvaluator, helps address the mismatch between user prompts and model outputs. By optimizing the CoT rewriter based on a taxonomy of 24 key points derived from T2I failure modes, PromptEnhancer improves image-text alignment in the HunyuanImage 2.1 model. The AlignEvaluator provides explicit feedback to enhance prompt generation, resulting in more precise interpretations by T2I models. Additionally, a human preference benchmark is introduced to support further research in this area.

<br /><br />Summary: <div>
arXiv:2509.04545v1 Announce Type: new 
Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model</title>
<link>https://arxiv.org/abs/2509.04548</link>
<guid>https://arxiv.org/abs/2509.04548</guid>
<content:encoded><![CDATA[
<div> Keywords: UniPic2-SD3.5M-Kontext, multimodal models, image generation, editing capabilities, reinforcement strategy

Summary:
UniPic2-SD3.5M-Kontext is a 2B-parameter DiT model based on SD3.5-Medium, achieving state-of-the-art image generation and editing in a unified multimodal framework. The model undergoes architectural modifications and large-scale pre-training on high-quality data to enable joint text-to-image generation and editing. A Progressive Dual-Task Reinforcement strategy (PDTR) is introduced to enhance instruction following and editing consistency, with reinforcement phases for different tasks proving mutually beneficial. Despite having fewer generation parameters than models like BAGEL and Flux-Kontext, UniPic2-SD3.5M-Kontext demonstrates superior image generation and editing capabilities. Through joint training with Qwen2.5-VL-7B, a unified multimodal model, UniPic2-Metaquery, is launched to integrate understanding, generation, and editing tasks, showing top-tier performance. This successful training paradigm, labeled as Skywork UniPic 2.0, highlights the efficiency and generalizability of the proposed approach. 

<br /><br />Summary: UniPic2-SD3.5M-Kontext excels in image generation and editing within a multimodal framework, leveraging a novel reinforcement strategy and achieving top-tier performance through joint training and integration of tasks. <div>
arXiv:2509.04548v1 Announce Type: new 
Abstract: Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping</title>
<link>https://arxiv.org/abs/2509.04582</link>
<guid>https://arxiv.org/abs/2509.04582</guid>
<content:encoded><![CDATA[
<div> framework, drag-based editing, bidirectional warping, image inpainting, real-time performance

Summary:
Inpaint4Drag introduces a novel framework for drag-based image editing, combining bidirectional warping and image inpainting to achieve precise control and real-time performance. Inspired by elastic object deformation, the method treats image regions as deformable materials, maintaining natural shape under user manipulation. The approach enables real-time warping previews and efficient inpainting at high resolution, significantly improving the interaction experience compared to existing methods. By transforming drag inputs into standard inpainting formats, Inpaint4Drag can adapt to any inpainting model without architecture modifications, benefiting from future improvements in inpainting technology. Extensive experiments showcase superior visual quality and precise control, making Inpaint4Drag a versatile and efficient tool for intuitive image manipulation. <div>
arXiv:2509.04582v1 Announce Type: new 
Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models</title>
<link>https://arxiv.org/abs/2509.04597</link>
<guid>https://arxiv.org/abs/2509.04597</guid>
<content:encoded><![CDATA[
<div> Defense, Object Detection, Adversarial Patch Attacks, DISPATCH, Generative Models<br />
Summary:<br />
- Object detection is crucial for applications like security monitoring, but current detectors are vulnerable to adversarial patch attacks.
- DISPATCH is a novel defense framework that uses diffusion models to regenerate and rectify images, disarming attack effects and preserving image integrity.
- Unlike other methods, DISPATCH does not require prior knowledge of existing patches and is attack-agnostic.
- Experiments show DISPATCH outperforms existing defenses, achieving high mAP.5 scores on hiding and creating attacks.
- DISPATCH also exhibits strong robustness against adaptive attacks, making it a practical and reliable defense for object detection systems. <br /> <div>
arXiv:2509.04597v1 Announce Type: new 
Abstract: Object detection is fundamental to various real-world applications, such as security monitoring and surveillance video analysis. Despite their advancements, state-of-theart object detectors are still vulnerable to adversarial patch attacks, which can be easily applied to real-world objects to either conceal actual items or create non-existent ones, leading to severe consequences. Given the current diversity of adversarial patch attacks and potential unknown threats, an ideal defense method should be effective, generalizable, and robust against adaptive attacks. In this work, we introduce DISPATCH, the first diffusion-based defense framework for object detection. Unlike previous works that aim to "detect and remove" adversarial patches, DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative models to disarm attack effects while preserving the integrity of the input image. Specifically, we utilize the in-distribution generative power of diffusion models to regenerate the entire image, aligning it with benign data. A rectification process is then employed to identify and replace adversarial regions with their regenerated benign counterparts. DISPATCH is attack-agnostic and requires no prior knowledge of the existing patches. Extensive experiments across multiple detectors and attacks demonstrate that DISPATCH consistently outperforms state-of-the-art defenses on both hiding attacks and creating attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and lowering the attack success rate to 24.8% on untargeted creating attacks. Moreover, it maintains strong robustness against adaptive attacks, making it a practical and reliable defense for object detection systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human</title>
<link>https://arxiv.org/abs/2509.04600</link>
<guid>https://arxiv.org/abs/2509.04600</guid>
<content:encoded><![CDATA[
<div> Keywords: Global human motion reconstruction, monocular videos, camera orientation information, camera translation cues, trajectory reconstruction 

Summary:
WATCH is a unified framework designed for global human motion reconstruction from in-the-wild monocular videos. It addresses the challenges of accurately mapping human poses from camera to world coordinates by introducing an analytical heading angle decomposition technique for efficient and extensible processing. WATCH also integrates camera trajectory information inspired by world models to effectively leverage camera translation cues. Through experiments on in-the-wild benchmarks, WATCH demonstrates state-of-the-art performance in end-to-end trajectory reconstruction. This approach highlights the importance of jointly modeling camera-human motion relationships and provides insights for improving camera translation integration in global human motion reconstruction. The code for WATCH will be made publicly available. 

Summary: <div>
arXiv:2509.04600v1 Announce Type: new 
Abstract: Global human motion reconstruction from in-the-wild monocular videos is increasingly demanded across VR, graphics, and robotics applications, yet requires accurate mapping of human poses from camera to world coordinates-a task challenged by depth ambiguity, motion ambiguity, and the entanglement between camera and human movements. While human-motion-centric approaches excel in preserving motion details and physical plausibility, they suffer from two critical limitations: insufficient exploitation of camera orientation information and ineffective integration of camera translation cues. We present WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and Human), a unified framework addressing both challenges. Our approach introduces an analytical heading angle decomposition technique that offers superior efficiency and extensibility compared to existing geometric methods. Additionally, we design a camera trajectory integration mechanism inspired by world models, providing an effective pathway for leveraging camera translation information beyond naive hard-decoding approaches. Through experiments on in-the-wild benchmarks, WATCH achieves state-of-the-art performance in end-to-end trajectory reconstruction. Our work demonstrates the effectiveness of jointly modeling camera-human motion relationships and offers new insights for addressing the long-standing challenge of camera translation integration in global human motion reconstruction. The code will be available publicly.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning</title>
<link>https://arxiv.org/abs/2509.04602</link>
<guid>https://arxiv.org/abs/2509.04602</guid>
<content:encoded><![CDATA[
<div> Sali4Vid, dense video captioning, timestamp supervision, video frames, scene transitions<br />
Summary:<br />
Sali4Vid is a saliency-aware framework for dense video captioning that addresses limitations in existing end-to-end models. It introduces Saliency-aware Video Reweighting to convert timestamp annotations into frame importance weights based on saliency, and Semantic-based Adaptive Caption Retrieval to segment videos by frame similarity, capturing scene transitions. By improving video weighting and retrieval simultaneously, Sali4Vid achieves state-of-the-art results on YouCook2 and ViTT datasets. This approach demonstrates the benefits of considering both timestamp annotations and scene transitions in dense video captioning tasks. <br /><br />Summary: <div>
arXiv:2509.04602v1 Announce Type: new 
Abstract: Dense video captioning aims to temporally localize events in video and generate captions for each event. While recent works propose end-to-end models, they suffer from two limitations: (1) applying timestamp supervision only to text while treating all video frames equally, and (2) retrieving captions from fixed-size video chunks, overlooking scene transitions. To address these, we propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce Saliency-aware Video Reweighting, which converts timestamp annotations into sigmoid-based frame importance weights, and Semantic-based Adaptive Caption Retrieval, which segments videos by frame similarity to capture scene transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art results on YouCook2 and ViTT, demonstrating the benefit of jointly improving video weighting and retrieval for dense video captioning
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis</title>
<link>https://arxiv.org/abs/2509.04624</link>
<guid>https://arxiv.org/abs/2509.04624</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic surveillance, unmanned aerial vehicle, vehicle detection, classification, urban mobility

Summary:
The paper introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance system for accurate vehicle detection, classification, tracking, and behavioral analysis in urban environments. Leveraging various techniques such as multi-scale template matching and Kalman filtering, the system achieves high detection precision, classification of vehicle types, and automatic detection of traffic violations. It supports origin-destination tracking, vehicle count visualization, congestion modeling, and entry-exit trajectory profiling. The system also estimates vehicle density, logs movement direction, and provides comprehensive urban mobility analytics. The experimental results demonstrate the system's scalability, accuracy, and practical relevance, showcasing its potential as a next-generation smart cities traffic monitoring solution that is enforcement-aware and infrastructure-independent.<br /><br />Summary: <div>
arXiv:2509.04624v1 Announce Type: new 
Abstract: Traffic congestion and violations pose significant challenges for urban mobility and road safety. Traditional traffic monitoring systems, such as fixed cameras and sensor-based methods, are often constrained by limited coverage, low adaptability, and poor scalability. To address these challenges, this paper introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance system capable of accurate vehicle detection, classification, tracking, and behavioral analysis in real-world, unconstrained urban environments. The system leverages multi-scale and multi-angle template matching, Kalman filtering, and homography-based calibration to process aerial video data collected from altitudes of approximately 200 meters. A case study in urban area demonstrates robust performance, achieving a detection precision of 91.8%, an F1-score of 90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively. Beyond precise detection, the system classifies five vehicle types and automatically detects critical traffic violations, including unsafe lane changes, illegal double parking, and crosswalk obstructions, through the fusion of geofencing, motion filtering, and trajectory deviation analysis. The integrated analytics module supports origin-destination tracking, vehicle count visualization, inter-class correlation analysis, and heatmap-based congestion modeling. Additionally, the system enables entry-exit trajectory profiling, vehicle density estimation across road segments, and movement direction logging, supporting comprehensive multi-scale urban mobility analytics. Experimental results confirms the system's scalability, accuracy, and practical relevance, highlighting its potential as an enforcement-aware, infrastructure-independent traffic monitoring solution for next-generation smart cities.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</title>
<link>https://arxiv.org/abs/2509.04669</link>
<guid>https://arxiv.org/abs/2509.04669</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, State Space Models, Convolutional Neural Networks, VCMamba, ImageNet-1K, ADE20K

Summary:
VCMamba is introduced as a vision backbone that combines the strengths of CNNs and multi-directional Mamba SSMs to bridge the gap in capturing local and global features effectively. It utilizes a convolutional stem and hierarchical structure with convolutional blocks for local feature extraction, followed by multi-directional Mamba blocks for modeling long-range dependencies and global context. The hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. In experiments on ImageNet-1K classification, VCMamba-B achieves 82.6% top-1 accuracy, outperforming existing models with fewer parameters. Additionally, on ADE20K semantic segmentation, VCMamba-B obtains 47.1 mIoU, surpassing other models with a reduced parameter count. The code for VCMamba is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2509.04669v1 Announce Type: new 
Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guideline-Consistent Segmentation via Multi-Agent Refinement</title>
<link>https://arxiv.org/abs/2509.04687</link>
<guid>https://arxiv.org/abs/2509.04687</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic segmentation, vision-language models, reinforcement learning, guideline adherence, generalization

Summary: 
This paper introduces a novel framework for semantic segmentation in real-world applications that emphasizes strict adherence to textual labeling guidelines. The proposed multi-agent, training-free framework coordinates vision-language models in an iterative Worker-Supervisor refinement architecture. The Worker performs segmentation, the Supervisor provides feedback based on guidelines, and a reinforcement learning stop policy ensures guideline-consistent masks while optimizing resource use. The framework outperforms state-of-the-art baselines on the Waymo and ReasonSeg datasets, showcasing strong generalization and instruction adherence. This approach alleviates the need for expensive task-specific retraining and accommodates evolving labeling guidelines efficiently. By combining the strengths of vision-language models and reinforcement learning, the framework enables accurate semantic segmentation while maintaining compliance with complex and lengthy textual labeling guidelines. <div>
arXiv:2509.04687v1 Announce Type: new 
Abstract: Semantic segmentation in real-world applications often requires not only accurate masks but also strict adherence to textual labeling guidelines. These guidelines are typically complex and long, and both human and automated labeling often fail to follow them faithfully. Traditional approaches depend on expensive task-specific retraining that must be repeated as the guidelines evolve. Although recent open-vocabulary segmentation methods excel with simple prompts, they often fail when confronted with sets of paragraph-length guidelines that specify intricate segmentation rules. To address this, we introduce a multi-agent, training-free framework that coordinates general-purpose vision-language models within an iterative Worker-Supervisor refinement architecture. The Worker performs the segmentation, the Supervisor critiques it against the retrieved guidelines, and a lightweight reinforcement learning stop policy decides when to terminate the loop, ensuring guideline-consistent masks while balancing resource use. Evaluated on the Waymo and ReasonSeg datasets, our method notably outperforms state-of-the-art baselines, demonstrating strong generalization and instruction adherence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation for Different Sensor Configurations in 3D Object Detection</title>
<link>https://arxiv.org/abs/2509.04711</link>
<guid>https://arxiv.org/abs/2509.04711</guid>
<content:encoded><![CDATA[
<div> fine-tuning, domain adaptation, 3D object detection, LiDAR, sensor configurations 
Summary:<br />
This paper addresses the challenge of adapting 3D object detection models across different sensor configurations in autonomous driving. It introduces two techniques, Downstream Fine-tuning and Partial Layer Fine-tuning, to improve generalization across diverse vehicle platforms. By leveraging paired datasets with multiple sensor configurations, the study demonstrates that joint training with these techniques outperforms naive joint training for each configuration. The research highlights the importance of considering shifts in point cloud distribution when deploying models trained on one sensor configuration to another. The proposed methods offer practical and scalable solutions for enhancing the accuracy of 3D object detection models in autonomous driving scenarios. 
<br /><br /> <div>
arXiv:2509.04711v1 Announce Type: new 
Abstract: Recent advances in autonomous driving have underscored the importance of accurate 3D object detection, with LiDAR playing a central role due to its robustness under diverse visibility conditions. However, different vehicle platforms often deploy distinct sensor configurations, causing performance degradation when models trained on one configuration are applied to another because of shifts in the point cloud distribution. Prior work on multi-dataset training and domain adaptation for 3D object detection has largely addressed environmental domain gaps and density variation within a single LiDAR; in contrast, the domain gap for different sensor configurations remains largely unexplored. In this work, we address domain adaptation across different sensor configurations in 3D object detection. We propose two techniques: Downstream Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and Partial Layer Fine-tuning (updating only a subset of layers to improve cross-configuration generalization). Using paired datasets collected in the same geographic region with multiple sensor configurations, we show that joint training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently outperforms naive joint training for each configuration. Our findings provide a practical and scalable solution for adapting 3D object detection models to the diverse vehicle platforms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CD-Mamba: Cloud detection with long-range spatial dependency modeling</title>
<link>https://arxiv.org/abs/2509.04729</link>
<guid>https://arxiv.org/abs/2509.04729</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, cloud detection, convolutional neural networks, Mamba, spatial dependencies

Summary:
CD-Mamba is a hybrid model that combines convolutional neural networks and Mamba's state-space modeling to enhance cloud detection in remote sensing images. By integrating local spatial relations and long-range dependencies, CD-Mamba effectively captures pixel-wise textural details and extensive patch-wise dependencies for accurate cloud detection across various spatial scales. The model addresses the challenges posed by cloud cover in remote sensing images by simultaneously considering short-range spatial redundancies and long-range atmospheric similarities among cloud patches. Experimental results demonstrate the superior performance of CD-Mamba compared to existing methods, showcasing its ability to manage pixel-wise interactions and patch-wise dependencies concurrently for improved detection accuracy. <div>
arXiv:2509.04729v1 Announce Type: new 
Abstract: Remote sensing images are frequently obscured by cloud cover, posing significant challenges to data integrity and reliability. Effective cloud detection requires addressing both short-range spatial redundancies and long-range atmospheric similarities among cloud patches. Convolutional neural networks are effective at capturing local spatial dependencies, while Mamba has strong capabilities in modeling long-range dependencies. To fully leverage both local spatial relations and long-range dependencies, we propose CD-Mamba, a hybrid model that integrates convolution and Mamba's state-space modeling into a unified cloud detection network. CD-Mamba is designed to comprehensively capture pixelwise textural details and long term patchwise dependencies for cloud detection. This design enables CD-Mamba to manage both pixel-wise interactions and extensive patch-wise dependencies simultaneously, improving detection accuracy across diverse spatial scales. Extensive experiments validate the effectiveness of CD-Mamba and demonstrate its superior performance over existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.04732</link>
<guid>https://arxiv.org/abs/2509.04732</guid>
<content:encoded><![CDATA[
<div> Keywords: versatile medical image segmentation, partially labeled datasets, task consistency training, class imbalance, uncertainty-weighted loss<br />
<br />
Summary: <br />
The study introduces a Task Consistency Training (TCT) framework for versatile medical image segmentation on partially labeled datasets. TCT aims to address class imbalance without the need for extra models by enforcing consistency between main segmentation and auxiliary task predictions. This allows for effective utilization of unlabeled anatomical structures while minimizing error propagation from low-consistency data through a filtering strategy. Additionally, a unified auxiliary uncertainty-weighted loss (UAUWL) is proposed to prevent segmentation quality declines due to task dominance. The approach is validated through extensive experiments on eight abdominal datasets from diverse clinical sites, demonstrating its effectiveness in improving segmentation accuracy and addressing class imbalance issues in versatile medical image segmentation tasks. <div>
arXiv:2509.04732v1 Announce Type: new 
Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of multiple classes, while obtaining full annotations for all classes is often impractical due to the time and labor required. Leveraging partially labeled datasets (PLDs) presents a promising alternative; however, current VMIS approaches face significant class imbalance due to the unequal category distribution in PLDs. Existing methods attempt to address this by generating pseudo-full labels. Nevertheless, these typically require additional models and often result in potential performance degradation from label noise. In this work, we introduce a Task Consistency Training (TCT) framework to address class imbalance without requiring extra models. TCT includes a backbone network with a main segmentation head (MSH) for multi-channel predictions and multiple auxiliary task heads (ATHs) for task-specific predictions. By enforcing a consistency constraint between the MSH and ATH predictions, TCT effectively utilizes unlabeled anatomical structures. To avoid error propagation from low-consistency, potentially noisy data, we propose a filtering strategy to exclude such data. Additionally, we introduce a unified auxiliary uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines caused by the dominance of specific tasks. Extensive experiments on eight abdominal datasets from diverse clinical sites demonstrate our approach's effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
<link>https://arxiv.org/abs/2509.04735</link>
<guid>https://arxiv.org/abs/2509.04735</guid>
<content:encoded><![CDATA[
<div> Keywords: vision models, uncertainty quantification, autonomous driving, segmentation, uncertainty-aware training 

Summary: 
In this study, the authors address the limitations of current vision foundation models in adverse weather conditions for autonomous driving. They propose two approaches to enhance segmentation robustness: a multi-step finetuning procedure for the Segment Anything Model 2 (SAM2) that incorporates uncertainty metrics into the loss function, and the adaptation of the Uncertainty-Aware Adapter (UAT) for driving contexts. The experiments conducted on various driving datasets demonstrate that the UAT-SAM model outperforms the standard SAM model in extreme weather conditions, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. The findings highlight the importance of explicit uncertainty modeling in ensuring safety for autonomous driving in challenging environments. 

<br /><br />Summary: <div>
arXiv:2509.04735v1 Announce Type: new 
Abstract: Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches</title>
<link>https://arxiv.org/abs/2509.04736</link>
<guid>https://arxiv.org/abs/2509.04736</guid>
<content:encoded><![CDATA[
<div> sensor data preprocessing, activity recognition, smartwatches, on-device, multimodal<br />
<br />
Summary: <br />
The article introduces WatchHAR, a Human Activity Recognition (HAR) system that operates entirely on smartwatches, addressing privacy and latency issues by processing data internally. WatchHAR is optimized to achieve compounding performance gains, with an end-to-end trainable module that speeds up processing by 5 times while maintaining over 90% accuracy across 25 activity classes. It outperforms existing models for event detection and activity classification, with processing times of 9.3 ms for event detection and 11.8 ms for multimodal classification. This research enhances on-device activity recognition, making smartwatches capable of standalone, privacy-aware, and minimally-invasive continuous activity tracking. <div>
arXiv:2509.04736v1 Announce Type: new 
Abstract: Despite advances in practical and multimodal fine-grained Human Activity Recognition (HAR), a system that runs entirely on smartwatches in unconstrained environments remains elusive. We present WatchHAR, an audio and inertial-based HAR system that operates fully on smartwatches, addressing privacy and latency issues associated with external data processing. By optimizing each component of the pipeline, WatchHAR achieves compounding performance gains. We introduce a novel architecture that unifies sensor data preprocessing and inference into an end-to-end trainable module, achieving 5x faster processing while maintaining over 90% accuracy across more than 25 activity classes. WatchHAR outperforms state-of-the-art models for event detection and activity classification while running directly on the smartwatch, achieving 9.3 ms processing time for activity event detection and 11.8 ms for multimodal activity classification. This research advances on-device activity recognition, realizing smartwatches' potential as standalone, privacy-aware, and minimally-invasive continuous activity tracking devices.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery</title>
<link>https://arxiv.org/abs/2509.04757</link>
<guid>https://arxiv.org/abs/2509.04757</guid>
<content:encoded><![CDATA[
<div> Keywords: MCANet, multi-scale representation, spatial attention, post-hurricane damage assessment, image classification

Summary:
MCANet is proposed as a multi-label classification framework for post-hurricane damage assessment. It utilizes a Res2Net-based hierarchical backbone to capture multi-scale spatial features and a multi-head class-specific residual attention module for enhanced discrimination. Each attention branch focuses on different spatial granularities to balance local detail and global context. Evaluation on the RescueNet dataset shows MCANet outperforms various existing models with a mean average precision of 91.75%, improving to 92.35% with eight attention heads. The model's ability to localize damage-relevant regions is confirmed through class activation mapping, providing interpretability. The outputs from MCANet can inform post-disaster risk mapping, emergency routing, and disaster response. Future work may involve integrating disaster-specific knowledge graphs and multimodal large language models to enhance adaptability to unseen disasters and improve semantic understanding for decision-making.<br /><br />Summary: MCANet, a multi-label classification framework, effectively assesses post-hurricane damage by capturing multi-scale spatial features and enhancing discrimination through attention mechanisms. Its superior performance, interpretability, and potential applications make it a valuable tool for disaster response and recovery efforts. Future enhancements could further improve adaptability and semantic understanding for comprehensive decision-making in disaster scenarios. <div>
arXiv:2509.04757v1 Announce Type: new 
Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster response and recovery. Yet existing CNN-based methods struggle to capture multi-scale spatial features and to distinguish visually similar or co-occurring damage types. To address these issues, we propose MCANet, a multi-label classification framework that learns multi-scale representations and adaptively attends to spatially relevant regions for each damage category. MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context across scales and a multi-head class-specific residual attention module to enhance discrimination. Each attention branch focuses on different spatial granularities, balancing local detail with global context. We evaluate MCANet on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael. MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet, Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads, performance further improves to 92.35%, boosting average precision for challenging classes such as Road Blocked by over 6%. Class activation mapping confirms MCANet's ability to localize damage-relevant regions, supporting interpretability. Outputs from MCANet can inform post-disaster risk mapping, emergency routing, and digital twin-based disaster response. Future work could integrate disaster-specific knowledge graphs and multimodal large language models to improve adaptability to unseen disasters and enrich semantic understanding for real-world decision-making.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Group Detection using VLM-augmented Temporal Groupness Graph</title>
<link>https://arxiv.org/abs/2509.04758</link>
<guid>https://arxiv.org/abs/2509.04758</guid>
<content:encoded><![CDATA[
<div> Vision-Language Model, group detection, dynamic human group, video, global optimization
Summary:
The paper introduces a novel approach for dynamic human group detection in videos, considering both local appearance features of individuals within the group and the global scene context. This method utilizes a Vision-Language Model (VLM) to extract local and global appearance features in each frame. Unlike previous methods that assume group structures remain static, this approach accounts for dynamically changing groups through global optimization using a graph and groupness probabilities estimated by groupness-augmented CLIP features. Experimental results demonstrate superior performance compared to existing group detection techniques on public datasets. The code for this method is available on GitHub for reference and further exploration. 

Summary: <div>
arXiv:2509.04758v1 Announce Type: new 
Abstract: This paper proposes dynamic human group detection in videos. For detecting complex groups, not only the local appearance features of in-group members but also the global context of the scene are important. Such local and global appearance features in each frame are extracted using a Vision-Language Model (VLM) augmented for group detection in our method. For further improvement, the group structure should be consistent over time. While previous methods are stabilized on the assumption that groups are not changed in a video, our method detects dynamically changing groups by global optimization using a graph with all frames' groupness probabilities estimated by our groupness-augmented CLIP features. Our experimental results demonstrate that our method outperforms state-of-the-art group detection methods on public datasets. Code: https://github.com/irajisamurai/VLM-GroupDetection.git
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</title>
<link>https://arxiv.org/abs/2509.04772</link>
<guid>https://arxiv.org/abs/2509.04772</guid>
<content:encoded><![CDATA[
<div> FloodVision, zero-shot framework, depth estimation, computer vision, GPT-4o<br />
Summary: 
FloodVision is introduced as a framework for accurate floodwater depth estimation, crucial for emergency response. It combines the GPT-4o vision-language model with a domain knowledge graph encoding real-world object dimensions. It dynamically identifies visible reference objects, retrieves heights from the knowledge graph, estimates submergence ratios, and filters outliers to compute depth values. Evaluated on MyCoast New York images, it achieves an 8.17 cm mean absolute error, outperforming CNN-based methods. The system generalizes well across various scenes, operates in near real-time, and is suitable for integration into digital twin platforms and citizen-reporting apps for smart city flood resilience. <br /><br />Summary: <div>
arXiv:2509.04772v1 Announce Type: new 
Abstract: Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval</title>
<link>https://arxiv.org/abs/2509.04773</link>
<guid>https://arxiv.org/abs/2509.04773</guid>
<content:encoded><![CDATA[
<div> Hybrid-Tower Framework, Text-to-Video Retrieval, CLIP-based approaches, Pseudo-query Generation, Fine-grained Interaction<br />
<br />
Summary: 
The study introduces a novel Hybrid-Tower framework for Text-to-Video Retrieval (T2VR) that combines the strengths of Two-Tower and Single-Tower frameworks. The proposed method, PIG, employs Fine-grained Pseudo-query Interaction and Generation to generate pseudo-queries for videos, facilitating effective interaction between video and textual features. This approach improves retrieval performance by 1.6% to 3.9% in R@1 across five benchmark datasets. PIG achieves high efficiency by introducing no extra overhead during inference, maintaining a performance level comparable to Two-Tower models. The Hybrid-Tower framework shows promise in balancing effectiveness and efficiency in text-to-video retrieval tasks. <br /><br /> <div>
arXiv:2509.04773v1 Announce Type: new 
Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by textual queries with the same semantic meanings. Recent CLIP-based approaches have explored two frameworks: Two-Tower versus Single-Tower framework, yet the former suffers from low effectiveness, while the latter suffers from low efficiency. In this study, we explore a new Hybrid-Tower framework that can hybridize the advantages of the Two-Tower and Single-Tower framework, achieving high effectiveness and efficiency simultaneously. We propose a novel hybrid method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG, which includes a new pseudo-query generator designed to generate a pseudo-query for each video. This enables the video feature and the textual features of pseudo-query to interact in a fine-grained manner, similar to the Single-Tower approaches to hold high effectiveness, even before the real textual query is received. Simultaneously, our method introduces no additional storage or computational overhead compared to the Two-Tower framework during the inference stage, thus maintaining high efficiency. Extensive experiments on five commonly used text-video retrieval benchmarks demonstrate that our method achieves a significant improvement over the baseline, with an increase of $1.6\% \sim 3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower models while achieving near state-of-the-art performance, highlighting the advantages of the Hybrid-Tower framework.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data</title>
<link>https://arxiv.org/abs/2509.04775</link>
<guid>https://arxiv.org/abs/2509.04775</guid>
<content:encoded><![CDATA[
<div> feature matching algorithms, lunar exploration, image registration, preprocessing pipeline, SuperGlue <br />
Summary: <br />
Accurate image registration is crucial for lunar exploration, allowing for surface mapping, resource localization, and mission planning. The study evaluates five feature matching algorithms, including SuperGlue, for aligning data from different lunar sensors. A proposed preprocessing pipeline involves georeferencing, resolution alignment, intensity normalization, and image enhancements to improve registration accuracy. SuperGlue consistently outperforms other algorithms in terms of both accuracy and speed. Classical methods like SIFT and AKAZE perform well in equatorial regions but struggle in polar lighting conditions. The findings emphasize the significance of preprocessing techniques and deep learning-based approaches in achieving robust lunar image registration across varying environmental conditions. <br /> <div>
arXiv:2509.04775v1 Announce Type: new 
Abstract: Accurate image registration is critical for lunar exploration, enabling surface mapping, resource localization, and mission planning. Aligning data from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera, Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer), and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya mission) -- is challenging due to differences in resolution, illumination, and sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT, AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using cross-modality image pairs from equatorial and polar regions. A preprocessing pipeline is proposed, including georeferencing, resolution alignment, intensity normalization, and enhancements like adaptive histogram equalization, principal component analysis, and shadow correction. SuperGlue consistently yields the lowest root mean square error and fastest runtimes. Classical methods such as SIFT and AKAZE perform well near the equator but degrade under polar lighting. The results highlight the importance of preprocessing and learning-based approaches for robust lunar image registration across diverse conditions.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images</title>
<link>https://arxiv.org/abs/2509.04800</link>
<guid>https://arxiv.org/abs/2509.04800</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, Skin disease classification, Mobile-acquired datasets, Gradient-weighted Class Activation Mapping, Interpretability<br />
Summary:<br />
- Skin diseases are widespread, but diagnosing them can be challenging in low-resource settings.<br />
- Deep learning models, particularly Transformer architectures like Swin Transformer, show promise in accurately classifying skin diseases from mobile-acquired images.<br />
- A large dataset with over 50 skin disease categories captured with mobile devices was curated to reflect real-world conditions.<br />
- Transformer models outperformed traditional CNNs in capturing global contextual features for improved classification accuracy.<br />
- Incorporating Gradient-weighted Class Activation Mapping (Grad-CAM) enhanced interpretability by highlighting clinically relevant regions in model predictions. <br /> <div>
arXiv:2509.04800v1 Announce Type: new 
Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet conventional diagnostic methods are often costly, complex, and unavailable in low-resource settings. Automated classification using deep learning has emerged as a promising alternative, but existing studies are mostly limited to dermoscopic datasets and a narrow range of disease classes. In this work, we curate a large dataset of over 50 skin disease categories captured with mobile devices, making it more representative of real-world conditions. We evaluate multiple convolutional neural networks and Transformer-based architectures, demonstrating that Transformer models, particularly the Swin Transformer, achieve superior performance by effectively capturing global contextual features. To enhance interpretability, we incorporate Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights clinically relevant regions and provides transparency in model predictions. Our results underscore the potential of Transformer-based approaches for mobile-acquired skin lesion classification, paving the way toward accessible AI-assisted dermatological screening and early diagnosis in resource-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.04816</link>
<guid>https://arxiv.org/abs/2509.04816</guid>
<content:encoded><![CDATA[
<div> Keywords: predictive uncertainty, mixture of experts, semantic segmentation, calibration, gating network

Summary:<br />
- The study focuses on estimating predictive uncertainty in computer vision models for safety-critical applications like traffic scene perception.
- Mixture of Experts (MoE) is proposed as an efficient method to quantify uncertainty without architectural modifications, offering well-calibrated uncertainty estimates.
- Three methods - predictive entropy, mutual information, and expert variance - are examined for uncertainty estimation in an MoE with two experts trained on the A2D2 dataset.
- MoEs outperform ensembles in terms of reliability metrics under out-of-distribution data, showcasing the effectiveness of MoEs in uncertainty estimation.
- Simple gating mechanisms in MoEs, such as gate entropy, lead to better calibration of routing uncertainty estimates compared to more complex approaches. Increasing the number of experts in MoEs further improves uncertainty calibration, as demonstrated in experiments on the Cityscapes dataset. <div>
arXiv:2509.04816v1 Announce Type: new 
Abstract: Estimating accurate and well-calibrated predictive uncertainty is important for enhancing the reliability of computer vision models, especially in safety-critical applications like traffic scene perception. While ensemble methods are commonly used to quantify uncertainty by combining multiple models, a mixture of experts (MoE) offers an efficient alternative by leveraging a gating network to dynamically weight expert predictions based on the input. Building on the promising use of MoEs for semantic segmentation in our previous works, we show that well-calibrated predictive uncertainty estimates can be extracted from MoEs without architectural modifications. We investigate three methods to extract predictive uncertainty estimates: predictive entropy, mutual information, and expert variance. We evaluate these methods for an MoE with two experts trained on a semantical split of the A2D2 dataset. Our results show that MoEs yield more reliable uncertainty estimates than ensembles in terms of conditional correctness metrics under out-of-distribution (OOD) data. Additionally, we evaluate routing uncertainty computed via gate entropy and find that simple gating mechanisms lead to better calibration of routing uncertainty estimates than more complex classwise gates. Finally, our experiments on the Cityscapes dataset suggest that increasing the number of experts can further enhance uncertainty calibration. Our code is available at https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution</title>
<link>https://arxiv.org/abs/2509.04824</link>
<guid>https://arxiv.org/abs/2509.04824</guid>
<content:encoded><![CDATA[
<div> Subspace Simple Scanning, feature extraction, dual-stage modeling strategy, Mamba-Transformer framework, light field image super-resolution

Summary:
The article introduces a new approach for optimizing light field image super-resolution (LFSR) using Mamba-based methods. The proposed Subspace Simple Scanning (Sub-SS) strategy improves feature extraction efficiency by reducing redundancy in complex LF data. A dual-stage modeling strategy enhances the exploration of non-local spatial-angular correlations by incorporating spatial-angular and disparity information extraction in different stages. The Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) and Epipolar Plane Mamba Block (EPMB) combined with the Epipolar Plane Transformer Block (EPTB) facilitate shallow and deep feature extraction for spatial-angular and epipolar domains. The resulting hybrid Mamba-Transformer framework, LFMT, outperforms existing LFSR methods by achieving significant performance improvements with low computational complexity on various LF datasets. <div>
arXiv:2509.04824v1 Announce Type: new 
Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
<link>https://arxiv.org/abs/2509.04833</link>
<guid>https://arxiv.org/abs/2509.04833</guid>
<content:encoded><![CDATA[
<div> Keywords: visual grounding, proposal-based framework, end-to-end, referential object comprehension, multi-granularity discrimination

Summary: 
PropVG is introduced as an end-to-end proposal-based framework that integrates foreground object proposal generation with referential object comprehension. It incorporates a Contrastive-based Refer Scoring (CRS) module that utilizes contrastive learning at both sentence and word levels to enhance the understanding and differentiation of referred objects. A Multi-granularity Target Discrimination (MTD) module is designed to fuse object- and semantic-level information for improved recognition of absent targets. Experimental results on multiple benchmarks showcase the effectiveness of PropVG in visual grounding tasks. The framework addresses limitations of existing methods by considering both referred and prospective targets for supervision, as well as incorporating multi-granularity discrimination techniques. The codes and models of PropVG are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2509.04833v1 Announce Type: new 
Abstract: Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution</title>
<link>https://arxiv.org/abs/2509.04834</link>
<guid>https://arxiv.org/abs/2509.04834</guid>
<content:encoded><![CDATA[
<div> scramjet engines, combustion dynamics, visual analytics, flow field data, high-speed propulsion <br />
Summary: 
TemporalFlowViz introduces a parameter-aware visual analytics workflow for analyzing temporal flow fields from scramjet combustion simulations. Leveraging Vision Transformers to extract embeddings and clustering techniques to uncover latent modes, the system tracks simulation evolution over time. Domain specialists annotate cluster centroids to provide contextual prompts for a vision-language model to generate natural-language summaries, supporting in-depth analysis. The system also allows parameter-based filtering, case retrieval, and multi-view exploration. Through expert-informed case studies and feedback, TemporalFlowViz enhances hypothesis generation, supports pattern discovery, and aids knowledge discovery in large-scale scramjet combustion analysis. <div>
arXiv:2509.04834v1 Announce Type: new 
Abstract: Understanding the complex combustion dynamics within scramjet engines is critical for advancing high-speed propulsion technologies. However, the large scale and high dimensionality of simulation-generated temporal flow field data present significant challenges for visual interpretation, feature differentiation, and cross-case comparison. In this paper, we present TemporalFlowViz, a parameter-aware visual analytics workflow and system designed to support expert-driven clustering, visualization, and interpretation of temporal flow fields from scramjet combustion simulations. Our approach leverages hundreds of simulated combustion cases with varying initial conditions, each producing time-sequenced flow field images. We use pretrained Vision Transformers to extract high-dimensional embeddings from these frames, apply dimensionality reduction and density-based clustering to uncover latent combustion modes, and construct temporal trajectories in the embedding space to track the evolution of each simulation over time. To bridge the gap between latent representations and expert reasoning, domain specialists annotate representative cluster centroids with descriptive labels. These annotations are used as contextual prompts for a vision-language model, which generates natural-language summaries for individual frames and full simulation cases. The system also supports parameter-based filtering, similarity-based case retrieval, and coordinated multi-view exploration to facilitate in-depth analysis. We demonstrate the effectiveness of TemporalFlowViz through two expert-informed case studies and expert feedback, showing TemporalFlowViz enhances hypothesis generation, supports interpretable pattern discovery, and enhances knowledge discovery in large-scale scramjet combustion analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations</title>
<link>https://arxiv.org/abs/2509.04848</link>
<guid>https://arxiv.org/abs/2509.04848</guid>
<content:encoded><![CDATA[
<div> Keywords: High-throughput 3D quantitative phase imaging, flow cytometry, label-free, volumetric characterization, OmniFHT

Summary:
OmniFHT is a new framework introduced for high-throughput 3D quantitative phase imaging in flow cytometry, enabling label-free characterization of individual cells as they flow through microfluidic channels. Unlike current methods that assume uniform single-axis rotation of cells, OmniFHT allows for arbitrary cell geometries and multi-axis rotations by jointly optimizing rotational trajectory and volumetric structure. This framework leverages the Fourier diffraction theorem and implicit neural representations to accurately reconstruct cell populations without the need for known poses at each frame. OmniFHT supports reconstruction from sparsely sampled projections and restricted angular coverage, providing high-fidelity results with minimal viewing angles. This advancement enables in situ, high-throughput tomographic imaging of flowing cell populations, allowing for scalable and unbiased morphometric analysis in flow cytometry platforms.<br /><br />Summary: <div>
arXiv:2509.04848v1 Announce Type: new 
Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables label-free, volumetric characterization of individual cells by reconstructing their refractive index (RI) distributions from multiple viewing angles during flow through microfluidic channels. However, current imaging methods assume that cells undergo uniform, single-axis rotation, which require their poses to be known at each frame. This assumption restricts applicability to near-spherical cells and prevents accurate imaging of irregularly shaped cells with complex rotations. As a result, only a subset of the cellular population can be analyzed, limiting the ability of flow-based assays to perform robust statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction framework that leverages the Fourier diffraction theorem and implicit neural representations (INRs) for high-throughput flow cytometry tomographic imaging. By jointly optimizing each cell's unknown rotational trajectory and volumetric structure under weak scattering assumptions, OmniFHT supports arbitrary cell geometries and multi-axis rotations. Its continuous representation also allows accurate reconstruction from sparsely sampled projections and restricted angular coverage, producing high-fidelity results with as few as 10 views or only 120 degrees of angular range. OmniFHT enables, for the first time, in situ, high-throughput tomographic imaging of entire flowing cell populations, providing a scalable and unbiased solution for label-free morphometric analysis in flow cytometry platforms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus</title>
<link>https://arxiv.org/abs/2509.04859</link>
<guid>https://arxiv.org/abs/2509.04859</guid>
<content:encoded><![CDATA[
<div> Keywords: Mobile reconstruction, autonomous aerial robotics, Gaussian Splatting, semantic editing, novel view synthesis

Summary:<br /><br />Mobile reconstruction for autonomous aerial robotics is crucial for applications like disaster response. The focus is on points of interest (PoIs) rather than full scene optimization. Gaussian Splatting (GS) combined with semantic editing provides high-quality 3D reconstruction. The proposed CoRe-GS approach generates a coarse segmentation-ready scene using semantic GS, followed by color-based effective filtering for object isolation. This reduces training time by about a quarter compared to full training cycles for semantic GS. Evaluation on SCRREAM and NeRDS 360 datasets shows reduced runtime and improved novel view synthesis quality. <div>
arXiv:2509.04859v1 Announce Type: new 
Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential for critical applications such as tele-guidance and disaster response. These tasks demand both accurate 3D reconstruction and fast scene processing. Instead of reconstructing the entire scene in detail, it is often more efficient to focus on specific objects, i.e., points of interest (PoIs). Mobile robots equipped with advanced sensing can usually detect these early during data acquisition or preliminary analysis, reducing the need for full-scene optimization. Gaussian Splatting (GS) has recently shown promise in delivering high-quality novel view synthesis and 3D representation by an incremental learning process. Extending GS with scene editing, semantics adds useful per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training cycle is completed, reducing the overall training time. Moreover, the semantically relevant area, the PoI, is usually already known during capturing. To balance high-quality reconstruction with reduced training time, we propose CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS and then refine it for the semantic object using our novel color-based effective filtering for effective object isolation. This is speeding up the training process to be about a quarter less than a full training cycle for semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world, outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher novel-view-synthesis quality.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning</title>
<link>https://arxiv.org/abs/2509.04886</link>
<guid>https://arxiv.org/abs/2509.04886</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryoablation, Prostate cancer, Reinforcement learning, Markov decision process, Optimal policy <br /> 
<br />
Summary: 
Cryoablation is a minimally invasive treatment for prostate cancer that aims to destroy cancerous tissue while sparing healthy structures. Current manual planning methods for cryoprobe placements can be time-consuming and variable in quality. In response to these challenges, Cryo-RL introduces a reinforcement learning framework to automate the planning process. By modeling cryoablation planning as a Markov decision process, the agent learns an optimal policy for cryoprobe placement based on tumour coverage. Through simulated environments reflecting clinical constraints, the agent achieved significant improvements in treatment planning compared to automated baselines. This approach demonstrates the potential for reinforcement learning to deliver efficient, reproducible, and clinically viable cryoablation plans with enhanced precision and reduced planning time. <div>
arXiv:2509.04886v1 Announce Type: new 
Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer that destroys malignant tissue during de-freezing, while sparing surrounding healthy structures. Its success depends on accurate preoperative planning of cryoprobe placements to fully cover the tumour and avoid critical anatomy. This planning is currently manual, expertise-dependent, and time-consuming, leading to variability in treatment quality and limited scalability. In this work, we introduce Cryo-RL, a reinforcement learning framework that models cryoablation planning as a Markov decision process and learns an optimal policy for cryoprobe placement. Within a simulated environment that models clinical constraints and stochastic intraoperative variability, an agent sequentially selects cryoprobe positions and ice sphere diameters. Guided by a reward function based on tumour coverage, this agent learns a cryoablation strategy that leads to optimal cryoprobe placements without the need for any manually-designed plans. Evaluated on 583 retrospective prostate cancer cases, Cryo-RL achieved over 8 percentage-point Dice improvements compared with the best automated baselines, based on geometric optimisation, and matched human expert performance while requiring substantially less planning time. These results highlight the potential of reinforcement learning to deliver clinically viable, reproducible, and efficient cryoablation plans.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
<link>https://arxiv.org/abs/2509.04889</link>
<guid>https://arxiv.org/abs/2509.04889</guid>
<content:encoded><![CDATA[
<div> fear prediction, computer vision models, transfer learning, explainability, dataset size<br />
<br />
Summary:<br />
Advances in computer vision have paved the way for clinical applications like computerized exposure therapy. In this study, pretrained computer vision models were utilized to predict fear levels from spider-related images. The models, adapted using transfer learning, demonstrated an average mean absolute error between 10.1 and 11.0 when predicting human fear ratings. It was found that reducing the dataset size significantly impacted model performance, with no substantial gains observed with further increases. Explainability assessments confirmed that the models' predictions were based on spider-related features. A category-wise error analysis highlighted specific visual conditions, such as distant views and artificial/painted spiders, associated with higher errors. These results underscore the potential of explainable computer vision models in predicting fear ratings and stress the significance of model explainability and sufficient dataset size for the development of effective emotion-aware therapeutic technologies. <br /> <div>
arXiv:2509.04889v1 Announce Type: new 
Abstract: Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynGen-Vision: Synthetic Data Generation for training industrial vision models</title>
<link>https://arxiv.org/abs/2509.04894</link>
<guid>https://arxiv.org/abs/2509.04894</guid>
<content:encoded><![CDATA[
<div> approach, synthetic data, computer vision, industrial wear and tear detection, rust conditions 

Summary: 
An approach to generating synthetic data for training computer vision models for industrial wear and tear detection is proposed. The method combines a vision language model with a 3D simulation and rendering engine to create data for various rust conditions. By training a CV model for rust detection using the synthetic dataset, the approach achieved a mAP50 score of 0.87 when tested on real images of rusted industrial objects. This customizable approach can be extended to other wear and tear detection scenarios, offering a cost-effective and efficient solution for training models in industries where data curation is challenging. <div>
arXiv:2509.04894v1 Announce Type: new 
Abstract: We propose an approach to generate synthetic data to train computer vision (CV) models for industrial wear and tear detection. Wear and tear detection is an important CV problem for predictive maintenance tasks in any industry. However, data curation for training such models is expensive and time-consuming due to the unavailability of datasets for different wear and tear scenarios. Our approach employs a vision language model along with a 3D simulation and rendering engine to generate synthetic data for varying rust conditions. We evaluate our approach by training a CV model for rust detection using the generated dataset and tested the trained model on real images of rusted industrial objects. The model trained with the synthetic data generated by our approach, outperforms the other approaches with a mAP50 score of 0.87. The approach is customizable and can be easily extended to other industrial wear and tear detection scenarios
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting</title>
<link>https://arxiv.org/abs/2509.04895</link>
<guid>https://arxiv.org/abs/2509.04895</guid>
<content:encoded><![CDATA[
<div> Keywords: sebocytes, lipid droplets, image analysis, multiple instance learning, attention mechanism

Summary: 
Sebocytes are lipid-secreting cells with intracellular lipid droplets, making quantification important. Manual counting is tedious, leading to the development of automated solutions. A new attention-based multiple instance learning (MIL) framework for analyzing sebocyte images was introduced. Nile Red-stained images were annotated into 14 classes based on droplet counts. Data augmentation expanded the dataset to around 50,000 cells. Two models were compared: a baseline multi-layer perceptron (MLP) and an attention-based MIL model utilizing ResNet-50 features. The baseline MLP showed more stable performance with a mean MAE of 5.6, while the attention-based MIL had a mean MAE of 10.7 but outperformed in specific folds. The study suggests that bag-level aggregation provides a reliable baseline for droplet counting at the slide level, while attention-based MIL requires task-aligned pooling and regularization to reach its full potential in sebocyte image analysis.<br /><br />Summary: <div>
arXiv:2509.04895v1 Announce Type: new 
Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the accumulation of intracellular lipid droplets, making their quantification a key readout in sebocyte biology. Manual counting is labor-intensive and subjective, motivating automated solutions. Here, we introduce a simple attention-based multiple instance learning (MIL) framework for sebocyte image analysis. Nile Red-stained sebocyte images were annotated into 14 classes according to droplet counts, expanded via data augmentation to about 50,000 cells. Two models were benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated patch-level counts, and an attention-based MIL model leveraging ResNet-50 features with instance weighting. Experiments using five-fold cross-validation showed that the baseline MLP achieved more stable performance (mean MAE = 5.6) compared with the attention-based MIL, which was less consistent (mean MAE = 10.7) but occasionally superior in specific folds. These findings indicate that simple bag-level aggregation provides a robust baseline for slide-level droplet counting, while attention-based MIL requires task-aligned pooling and regularization to fully realize its potential in sebocyte image analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features</title>
<link>https://arxiv.org/abs/2509.04932</link>
<guid>https://arxiv.org/abs/2509.04932</guid>
<content:encoded><![CDATA[
<div> Keywords: novel view synthesis, reference images, multimodal large language model, attention mechanism, performance improvement 

Summary: 
UniView proposes a novel approach to the task of synthesizing novel views from a single image by leveraging reference images from a similar object. The model uses a retrieval and augmentation system along with a multimodal large language model to select suitable reference images. A plug-and-play adapter module with multi-level isolation layers dynamically generates reference features for target views. A decoupled triple attention mechanism is designed to align and integrate multi-branch features for preserving original input image details. Extensive experiments show that UniView significantly improves novel view synthesis performance, surpassing state-of-the-art methods on challenging datasets. <br /><br />Summary: <div>
arXiv:2509.04932v1 Announce Type: new 
Abstract: The task of synthesizing novel views from a single image is highly ill-posed due to multiple explanations for unobserved areas. Most current methods tend to generate unseen regions from ambiguity priors and interpolation near input views, which often lead to severe distortions. To address this limitation, we propose a novel model dubbed as UniView, which can leverage reference images from a similar object to provide strong prior information during view synthesis. More specifically, we construct a retrieval and augmentation system and employ a multimodal large language model (MLLM) to assist in selecting reference images that meet our requirements. Additionally, a plug-and-play adapter module with multi-level isolation layers is introduced to dynamically generate reference features for the target views. Moreover, in order to preserve the details of an original input image, we design a decoupled triple attention mechanism, which can effectively align and integrate multi-branch features into the synthesis process. Extensive experiments have demonstrated that our UniView significantly improves novel view synthesis performance and outperforms state-of-the-art methods on the challenging datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</title>
<link>https://arxiv.org/abs/2509.04957</link>
<guid>https://arxiv.org/abs/2509.04957</guid>
<content:encoded><![CDATA[
<div> semantic, temporal, video-to-audio generation, multiple foundation models, GPT-2

Summary:
- Recent advancements in Video-to-Audio (V2A) generation involve extracting semantic and temporal features from videos to condition generative models.
- Training these models from scratch is resource-intensive, leading to the utilization of foundation models (FMs) for cross-modal knowledge transfer and generalization.
- The Multiple Foundation Model Mapper (MFM-Mapper) enhances feature fusion by utilizing dual visual encoders, resulting in richer semantic and temporal information.
- By leveraging GPT-2 instead of a linear mapper, MFM-Mapper improves feature alignment, resembling autoregressive translation tasks.
- MFM-Mapper demonstrates superior training efficiency, achieving enhanced performance in semantic and temporal consistency with significantly less training resources compared to previous mapper-based approaches. <div>
arXiv:2509.04957v1 Announce Type: new 
Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework</title>
<link>https://arxiv.org/abs/2509.05000</link>
<guid>https://arxiv.org/abs/2509.05000</guid>
<content:encoded><![CDATA[
<div> perception, fusion, infrared-visible, dual-source degraded, image

Summary:<br />
- The paper introduces a new framework, GD^2Fusion, for infrared-visible image fusion in dual-source degraded scenarios.
- It combines vision-language models for degradation perception with dual-domain optimization.
- The Guided Frequency Modality-Specific Extraction module suppresses frequency-domain degradation and extracts relevant features.
- The Guided Spatial Modality-Aggregated Fusion module filters cross-modal degradation and aggregates multi-source features for enhanced fusion.
- Extensive experiments show that GD^2Fusion outperforms existing methods in dual-source degraded scenarios. 

Summary: <div>
arXiv:2509.05000v1 Announce Type: new 
Abstract: Most existing infrared-visible image fusion (IVIF) methods assume high-quality inputs, and therefore struggle to handle dual-source degraded scenarios, typically requiring manual selection and sequential application of multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion pipeline inevitably leads to error accumulation and performance degradation. To overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion), a novel framework that synergistically integrates vision-language models (VLMs) for degradation perception with dual-domain (frequency/spatial) joint optimization. Concretely, the designed Guided Frequency Modality-Specific Extraction (GFMSE) module performs frequency-domain degradation perception and suppression and discriminatively extracts fusion-relevant sub-band features. Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries out cross-modal degradation filtering and adaptive multi-source feature aggregation in the spatial domain to enhance modality complementarity and structural consistency. Extensive qualitative and quantitative experiments demonstrate that GD^2Fusion achieves superior fusion performance compared with existing algorithms and strategies in dual-source degraded scenarios. The code will be publicly released after acceptance of this paper.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study</title>
<link>https://arxiv.org/abs/2509.05004</link>
<guid>https://arxiv.org/abs/2509.05004</guid>
<content:encoded><![CDATA[
<div> Machine learning, deep learning, breast cancer classification, ultrasound images, ResNet-18 <br />
Summary: <br />
This study explores the use of machine learning and deep learning techniques for breast cancer classification using ultrasound images. The researchers evaluated various models on datasets such as BUSI, BUS-BRA, and BrEaST-Lesions USG, finding that ResNet-18 achieved the highest accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical machine learning models like SVM and KNN also showed competitive performance when coupled with deep feature extraction. Additionally, visualizations using Grad-CAM helped highlight diagnostically relevant regions in the ultrasound images, improving model transparency. These results demonstrate the potential for integrating AI-based diagnostic tools into clinical workflows for effective and interpretable breast cancer detection using ultrasound imaging. <br /> <div>
arXiv:2509.05004v1 Announce Type: new 
Abstract: Breast cancer remains a leading cause of cancer-related mortality among women worldwide. Ultrasound imaging, widely used due to its safety and cost-effectiveness, plays a key role in early detection, especially in patients with dense breast tissue. This paper presents a comprehensive study on the application of machine learning and deep learning techniques for breast cancer classification using ultrasound images. Using datasets such as BUSI, BUS-BRA, and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM, KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0, GoogLeNet). Experimental results show that ResNet-18 achieves the highest accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML models, though outperformed by CNNs, achieve competitive performance when enhanced with deep feature extraction. Grad-CAM visualizations further improve model transparency by highlighting diagnostically relevant image regions. These findings support the integration of AI-based diagnostic tools into clinical workflows and demonstrate the feasibility of deploying high-performing, interpretable systems for ultrasound-based breast cancer detection.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A biologically inspired separable learning vision model for real-time traffic object perception in Dark</title>
<link>https://arxiv.org/abs/2509.05012</link>
<guid>https://arxiv.org/abs/2509.05012</guid>
<content:encoded><![CDATA[
<div> Keywords: object perception, low-light traffic scenes, Dark-traffic dataset, Separable Learning Vision Model (SLVM), benchmark 

Summary:
Fast and accurate object perception in low-light traffic scenes is crucial but challenging due to illumination degradation. To address this, a novel illumination degradation method and the Dark-traffic dataset are introduced. A biologically inspired Separable Learning Vision Model (SLVM) is proposed, incorporating innovative features such as light-adaptive pupillary mechanism, separable learning strategy, task-specific branches, and spatial misalignment-aware fusion module. SLVM achieves superior performance over existing models, outperforming RT-DETR in detection, YOLOv12 in instance segmentation, and enhancing endpoint error metrics on Dark-traffic. On the LIS benchmark, SLVM outperforms Swin Transformer+EnlightenGAN and ConvNeXt-T+EnlightenGAN, and surpasses Mask RCNN (with light enhancement). The Dark-traffic dataset and SLVM code are available for further research and development at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2509.05012v1 Announce Type: new 
Abstract: Fast and accurate object perception in low-light traffic scenes has attracted increasing attention. However, due to severe illumination degradation and the lack of reliable visual cues, existing perception models and methods struggle to quickly adapt to and accurately predict in low-light environments. Moreover, there is the absence of available large-scale benchmark specifically focused on low-light traffic scenes. To bridge this gap, we introduce a physically grounded illumination degradation method tailored to real-world low-light settings and construct Dark-traffic, the largest densely annotated dataset to date for low-light traffic scenes, supporting object detection, instance segmentation, and optical flow estimation. We further propose the Separable Learning Vision Model (SLVM), a biologically inspired framework designed to enhance perception under adverse lighting. SLVM integrates four key components: a light-adaptive pupillary mechanism for illumination-sensitive feature extraction, a feature-level separable learning strategy for efficient representation, task-specific decoupled branches for multi-task separable learning, and a spatial misalignment-aware fusion module for precise multi-feature alignment. Extensive experiments demonstrate that SLVM achieves state-of-the-art performance with reduced computational overhead. Notably, it outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1 percentage points in instance segmentation, and reduces endpoint error (EPE) of baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end trained SLVM surpasses Swin Transformer+EnlightenGAN and ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage points. The Dark-traffic dataset and complete code is released at https://github.com/alanli1997/slvm.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition</title>
<link>https://arxiv.org/abs/2509.05019</link>
<guid>https://arxiv.org/abs/2509.05019</guid>
<content:encoded><![CDATA[
<div> MobileNet, SqueezeNet, MnasNet, ShuffleNet, transfer learning <br />
Summary:<br />
The study evaluates the integration of transfer learning with mobile-enabled convolutional neural networks for Arabic Handwritten Character Recognition. Three transfer learning strategies are compared using four lightweight models on three datasets. MobileNet performs best overall, achieving high accuracy and efficiency. Full fine-tuning is the most effective strategy, balancing accuracy and convergence speed. The IFHCDB dataset shows the highest accuracy, while the HIJJA dataset poses challenges due to variability. Future work will focus on architectural modifications, dataset analysis, data augmentation, and sensitivity analysis to enhance model robustness and generalizability. Overall, the study highlights the potential of using transfer learning and mobile networks for resource-efficient Arabic Handwritten Character Recognition. <br /> <div>
arXiv:2509.05019v1 Announce Type: new 
Abstract: The study explores the integration of transfer learning (TL) with mobile-enabled convolutional neural networks (MbNets) to enhance Arabic Handwritten Character Recognition (AHCR). Addressing challenges like extensive computational requirements and dataset scarcity, this research evaluates three TL strategies--full fine-tuning, partial fine-tuning, and training from scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD, HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently achieving superior accuracy, robustness, and efficiency, with ShuffleNet excelling in generalization, particularly under full fine-tuning. The IFHCDB dataset yielded the highest results, with 99% accuracy using MnasNet under full fine-tuning, highlighting its suitability for robust character recognition. The AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA posed significant challenges due to its variability, achieving a peak accuracy of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall performance, balancing accuracy and convergence speed, while partial fine-tuning underperformed across metrics. These findings underscore the potential of combining TL and MbNets for resource-efficient AHCR, paving the way for further optimizations and broader applications. Future work will explore architectural modifications, in-depth dataset feature analysis, data augmentation, and advanced sensitivity analysis to enhance model robustness and generalizability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUIVITON: Learned Universal Interoperable VIrtual Try-ON</title>
<link>https://arxiv.org/abs/2509.05030</link>
<guid>https://arxiv.org/abs/2509.05030</guid>
<content:encoded><![CDATA[
<div> virtual try-on, automated system, clothing draping, SMPL, geometric learning

Summary:
The article introduces LUIVITON, an automated system for virtual try-on that can drape complex, multi-layer clothing onto diverse humanoid characters in various poses. The system uses SMPL as a proxy representation and addresses the clothing-to-body draping problem through two correspondence tasks: clothing-to-SMPL and body-to-SMPL. The clothing-to-SMPL fitting is achieved through a geometric learning approach for shape correspondence prediction, while body-to-SMPL correspondence is addressed using a diffusion model-based approach with multi-view consistent appearance features. LUIVITON can handle complex geometries, non-manifold meshes, and a wide range of characters efficiently. The system allows for fast customization of clothing size and material properties post-draping. It can produce high-quality 3D clothing fittings automatically, even without 2D clothing sewing patterns. <div>
arXiv:2509.05030v1 Announce Type: new 
Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on, capable of draping complex, multi-layer clothing onto diverse and arbitrarily posed humanoid characters. To address the challenge of aligning complex garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy representation and separate the clothing-to-body draping problem into two correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence, where each has its unique challenges. While we address the clothing-to-SMPL fitting problem using a geometric learning-based approach for partial-to-complete shape correspondence prediction, we introduce a diffusion model-based approach for body-to-SMPL correspondence using multi-view consistent appearance features and a pre-trained 2D foundation model. Our method can handle complex geometries, non-manifold meshes, and generalizes effectively to a wide range of humanoid characters -- including humans, robots, cartoon subjects, creatures, and aliens, while maintaining computational efficiency for practical adoption. In addition to offering a fully automatic fitting solution, LUIVITON supports fast customization of clothing size, allowing users to adjust clothing sizes and material properties after they have been draped. We show that our system can produce high-quality 3D clothing fittings without any human labor, even when 2D clothing sewing patterns are not available.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</title>
<link>https://arxiv.org/abs/2509.05034</link>
<guid>https://arxiv.org/abs/2509.05034</guid>
<content:encoded><![CDATA[
<div> algorithm, anomaly detection, interactive image segmentation, industrial inspection, cross-modal framework <br />
<br />
Summary: <br />
Industrial product inspection often uses Anomaly Detection frameworks trained on non-defective samples. However, incorporating defective samples for training requires pixel-level annotations, limiting scalability. To address this issue, ADClick, an Interactive Image Segmentation algorithm, allows users to generate pixel-wise anomaly annotations with only a few clicks and a brief description. This precise labeling significantly improves AD model performance. Additionally, ADClick-Seg, a cross-modal framework integrating visual features and textual prompts, achieves state-of-the-art results on Multi-class anomaly detection tasks by combining pixel-level priors and language-guided cues. Overall, these approaches enhance anomaly detection accuracy and efficiency in industrial settings. <br /> 
 <div>
arXiv:2509.05034v1 Announce Type: new 
Abstract: Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction</title>
<link>https://arxiv.org/abs/2509.05071</link>
<guid>https://arxiv.org/abs/2509.05071</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, magnetic resonance imaging, motion artifacts, deep learning, generative models 

Summary: 
AI-driven methods, particularly deep learning generative models, have shown promise in detecting and correcting MRI motion artifacts to improve image quality. However, challenges such as limited generalizability, reliance on paired training data, and potential visual distortions need to be addressed. The use of standardized datasets and reporting protocols is essential to ensure the effectiveness and reliability of AI-driven solutions. Despite these challenges, AI technologies have the potential to significantly enhance MRI diagnostic accuracy, reduce healthcare costs, and improve patient care outcomes. Further advancements in adaptable deep learning techniques and the development of comprehensive public datasets are crucial for the continued progress in this field.<br /><br />Summary: <div>
arXiv:2509.05071v1 Announce Type: new 
Abstract: Background: To systematically review and perform a meta-analysis of artificial intelligence (AI)-driven methods for detecting and correcting magnetic resonance imaging (MRI) motion artifacts, assessing current developments, effectiveness, challenges, and future research directions. Methods: A comprehensive systematic review and meta-analysis were conducted, focusing on deep learning (DL) approaches, particularly generative models, for the detection and correction of MRI motion artifacts. Quantitative data were extracted regarding utilized datasets, DL architectures, and performance metrics. Results: DL, particularly generative models, show promise for reducing motion artifacts and improving image quality; however, limited generalizability, reliance on paired training data, and risk of visual distortions remain key challenges that motivate standardized datasets and reporting. Conclusions: AI-driven methods, particularly DL generative models, show significant potential for improving MRI image quality by effectively addressing motion artifacts. However, critical challenges must be addressed, including the need for comprehensive public datasets, standardized reporting protocols for artifact levels, and more advanced, adaptable DL techniques to reduce reliance on extensive paired datasets. Addressing these aspects could substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and improve patient care outcomes.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.05075</link>
<guid>https://arxiv.org/abs/2509.05075</guid>
<content:encoded><![CDATA[
<div> framework, Gaussian splatting, geometric priors, optimization, novel view synthesis

Summary:
GeoSplat is a new framework introduced in this study that utilizes both first-order and second-order geometric quantities to enhance the optimization process of Gaussian splatting. By incorporating geometric priors, such as principal curvatures, in the initialization of 3D Gaussian primitives, GeoSplat improves surface coverage compared to random initialization. The framework also introduces noise-robust estimation methods for dynamic geometric priors based on specific geometric structures like local manifolds. Extensive experiments on multiple datasets for novel view synthesis demonstrate that GeoSplat significantly enhances the performance of Gaussian splatting and surpasses previous methods. <div>
arXiv:2509.05075v1 Announce Type: new 
Abstract: A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they are also unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework: GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction</title>
<link>https://arxiv.org/abs/2509.05078</link>
<guid>https://arxiv.org/abs/2509.05078</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial Beauty Prediction, Convolutional Neural Networks, Transformers, Multi-scale representations, State-of-the-art performance

Summary: 
The article introduces the Scale-Interaction Transformer (SIT) architecture for Automated Facial Beauty Prediction (FBP), combining CNNs and Transformers to capture facial characteristics at different scales and model their interactions. The SIT model achieves a new state-of-the-art performance on the SCUT-FBP5500 dataset with a Pearson Correlation of 0.9187. The success of SIT demonstrates the importance of explicitly modeling the interplay between multi-scale visual cues for high-performance FBP. The hybrid CNN-Transformer approach shows promise for complex image regression tasks requiring a holistic understanding of context. <div>
arXiv:2509.05078v1 Announce Type: new 
Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision task due to the complex interplay of local and global facial features that influence human perception. While Convolutional Neural Networks (CNNs) excel at feature extraction, they often process information at a fixed scale, potentially overlooking the critical inter-dependencies between features at different levels of granularity. To address this limitation, we introduce the Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture that synergizes the feature extraction power of CNNs with the relational modeling capabilities of Transformers. The SIT first employs a multi-scale module with parallel convolutions to capture facial characteristics at varying receptive fields. These multi-scale representations are then framed as a sequence and processed by a Transformer encoder, which explicitly models their interactions and contextual relationships via a self-attention mechanism. We conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark dataset, where the proposed SIT model establishes a new state-of-the-art. It achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our findings demonstrate that explicitly modeling the interplay between multi-scale visual cues is crucial for high-performance FBP. The success of the SIT architecture highlights the potential of hybrid CNN-Transformer models for complex image regression tasks that demand a holistic, context-aware understanding.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</title>
<link>https://arxiv.org/abs/2509.05086</link>
<guid>https://arxiv.org/abs/2509.05086</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse mixture-of-experts, convolutional neural networks, robustness, adversarial attacks, ResNet architectures 

Summary: 
Sparse mixture-of-experts (MoE) layers are explored to enhance the robustness of convolutional neural networks (CNNs) against adversarial attacks. By replacing selected residual blocks or convolutional layers with MoE layers in deeper stages of ResNet architectures trained on CIFAR-100, improved robustness under PGD and AutoPGD attacks is achieved through adversarial training. However, the use of switch loss for balancing leads to routing collapse onto a few overused experts, concentrating adversarial training on specific paths and inadvertently making them more robust. This concentration results in some individual experts surpassing the gated MoE model in robustness, indicating the emergence of robust subpaths through specialization. This study highlights the potential of sparse MoE layers in enhancing the robustness of CNNs against adversarial attacks. Code for the study is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes. 

<br /><br />Summary: <div>
arXiv:2509.05086v1 Announce Type: new 
Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks remains challenging and often requires resource-intensive countermeasures. We explore the use of sparse mixture-of-experts (MoE) layers to improve robustness by replacing selected residual blocks or convolutional layers, thereby increasing model capacity without additional inference cost. On ResNet architectures trained on CIFAR-100, we find that inserting a single MoE layer in the deeper stages leads to consistent improvements in robustness under PGD and AutoPGD attacks when combined with adversarial training. Furthermore, we discover that when switch loss is used for balancing, it causes routing to collapse onto a small set of overused experts, thereby concentrating adversarial training on these paths and inadvertently making them more robust. As a result, some individual experts outperform the gated MoE model in robustness, suggesting that robust subpaths emerge through specialization. Our code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised Deep Transfer for Regression without Domain Alignment</title>
<link>https://arxiv.org/abs/2509.05092</link>
<guid>https://arxiv.org/abs/2509.05092</guid>
<content:encoded><![CDATA[
<div> framework, Contradistinguisher, source-free, semi-supervised, regression <br />
Summary: <br />
The article introduces CRAFT, a new framework for source-free, semi-supervised transfer learning in regression tasks. It addresses the challenge of domain adaptation in real-world applications where source data cannot be shared and labeled target data is limited. CRAFT builds upon the Contradistinguisher framework, originally designed for unsupervised DA in classification tasks, and extends it to regression tasks. The efficacy of CRAFT is demonstrated in neuroscience settings, specifically gaze prediction with EEG data and brain age prediction with MRI data, showing up to a 9% improvement in RMSE over fine-tuned models when labeled data is scarce. CRAFT outperforms four state-of-the-art source-free DA models by more than 3% and is shown to be effective in other real-world regression benchmarks. The proposed approach offers a flexible and efficient solution for semi-supervised deep transfer learning in biology and medicine. <br /> <div>
arXiv:2509.05092v1 Announce Type: new 
Abstract: Deep learning models deployed in real-world applications (e.g., medicine) face challenges because source models do not generalize well to domain-shifted target data. Many successful domain adaptation (DA) approaches require full access to source data. Yet, such requirements are unrealistic in scenarios where source data cannot be shared either because of privacy concerns or because it is too large and incurs prohibitive storage or computational costs. Moreover, resource constraints may limit the availability of labeled targets. We illustrate this challenge in a neuroscience setting where source data are unavailable, labeled target data are meager, and predictions involve continuous-valued outputs. We build upon Contradistinguisher (CUDA), an efficient framework that learns a shared model across the labeled source and unlabeled target samples, without intermediate representation alignment. Yet, CUDA was designed for unsupervised DA, with full access to source data, and for classification tasks. We develop CRAFT -- a Contradistinguisher-based Regularization Approach for Flexible Training -- for source-free (SF), semi-supervised transfer of pretrained models in regression tasks. We showcase the efficacy of CRAFT in two neuroscience settings: gaze prediction with electroencephalography (EEG) data and ``brain age'' prediction with structural MRI data. For both datasets, CRAFT yielded up to 9% improvement in root-mean-squared error (RMSE) over fine-tuned models when labeled training examples were scarce. Moreover, CRAFT leveraged unlabeled target data and outperformed four competing state-of-the-art source-free domain adaptation models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two other real-world regression benchmarks. We propose CRAFT as an efficient approach for source-free, semi-supervised deep transfer for regression that is ubiquitous in biology and medicine.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Attention-Based Approach for Image-to-3D Texture Mapping</title>
<link>https://arxiv.org/abs/2509.05131</link>
<guid>https://arxiv.org/abs/2509.05131</guid>
<content:encoded><![CDATA[
<div> Transformer-based framework, 3D texture field, single image, mesh, triplane representation

Summary:
A new transformer-based framework is proposed for generating high-quality textures in 3D content creation. Unlike existing methods, this framework predicts a 3D texture field directly from a single image and mesh, eliminating the need for UV mapping. The method integrates a triplane representation with depth-based backprojection losses for efficient training and faster texture generation, enabling high-fidelity textures to be generated in just 0.2s per shape. Extensive evaluations show that this method outperforms state-of-the-art baselines in terms of fidelity to the input image and perceptual quality, making it practical for scalable, high-quality, and controllable 3D content creation. <br /><br />Summary: <div>
arXiv:2509.05131v1 Announce Type: new 
Abstract: High-quality textures are critical for realistic 3D content creation, yet existing generative methods are slow, rely on UV maps, and often fail to remain faithful to a reference image. To address these challenges, we propose a transformer-based framework that predicts a 3D texture field directly from a single image and a mesh, eliminating the need for UV mapping and differentiable rendering, and enabling faster texture generation. Our method integrates a triplane representation with depth-based backprojection losses, enabling efficient training and faster inference. Once trained, it generates high-fidelity textures in a single forward pass, requiring only 0.2s per shape. Extensive qualitative, quantitative, and user preference evaluations demonstrate that our method outperforms state-of-the-art baselines on single-image texture reconstruction in terms of both fidelity to the input image and perceptual quality, highlighting its practicality for scalable, high-quality, and controllable 3D content creation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
<link>https://arxiv.org/abs/2509.05144</link>
<guid>https://arxiv.org/abs/2509.05144</guid>
<content:encoded><![CDATA[
<div> instance segmentation, 3D vision, semantic mask, geometric refinement, training-free

Summary:<br />
accurate 3D instance segmentation is essential in 3D vision. Existing methods struggle due to errors in the lifting process. A novel framework, SGS-3D, purifies and splits lifted masks using geometric primitives, then grows them into complete instances. SGS-3D refines without training by fusing semantic and geometric information, improving segmentation accuracy. It filters ambiguous masks using geometric co-occurrence and constructs fine-grained instances with spatial continuity and high-level features. Experimental results on various datasets show improved accuracy and robustness. The approach yields high-fidelity object instances and generalizes well across different indoor and outdoor environments. The code is available for further exploration. <br />Summary: <div>
arXiv:2509.05144v1 Announce Type: new 
Abstract: Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available in the supplementary materials.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2509.05188</link>
<guid>https://arxiv.org/abs/2509.05188</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language recognition, unsupervised learning, contrastive learning, self-supervised learning, data augmentation

Summary:
This paper discusses the challenges in sign language recognition (SLR) due to the scarcity of annotated data. It proposes a self-supervised learning framework to address these challenges. The framework includes a new self-supervised approach with free-negative pairs and a new data augmentation technique. By considering the relevance of certain parts of a sign video and addressing the issue of shared movements between different signs, the proposed approach aims to learn more meaningful representations for SLR. The framework shows significant improvements in accuracy compared to existing contrastive and self-supervised methods across various evaluation tasks, including linear evaluation, semi-supervised learning, and transferability between sign languages. This work provides a promising direction for improving sign recognition through self-supervised learning techniques. 

<br /><br />Summary: <div>
arXiv:2509.05188v1 Announce Type: new 
Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify signs in videos. Due to the scarcity of annotated data, unsupervised methods like contrastive learning have become promising in this field. They learn meaningful representations by pulling positive pairs (two augmented versions of the same instance) closer and pushing negative pairs (different from the positive pairs) apart. In SLR, in a sign video, only certain parts provide information that is truly useful for its recognition. Applying contrastive methods to SLR raises two issues: (i) contrastive learning methods treat all parts of a video in the same way, without taking into account the relevance of certain parts over others; (ii) shared movements between different signs make negative pairs highly similar, complicating sign discrimination. These issues lead to learning non-discriminative features for sign recognition and poor results in downstream tasks. In response, this paper proposes a self-supervised learning framework designed to learn meaningful representations for SLR. This framework consists of two key components designed to work together: (i) a new self-supervised approach with free-negative pairs; (ii) a new data augmentation technique. This approach shows a considerable gain in accuracy compared to several contrastive and self-supervised methods, across linear evaluation, semi-supervised learning, and transferability between sign languages.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
<link>https://arxiv.org/abs/2509.05198</link>
<guid>https://arxiv.org/abs/2509.05198</guid>
<content:encoded><![CDATA[
<div> Dataset quality, ModelNet-R, Point-SkipNet, 3D point cloud classification, neural network<br />
<br />
Summary: <br />
The paper introduces ModelNet-R, a refined version of the ModelNet40 dataset aimed at improving the quality of 3D point cloud classification datasets. ModelNet-R addresses issues such as inconsistent labeling, 2D data, and inadequate class differentiation present in ModelNet40. Additionally, the paper proposes Point-SkipNet, a lightweight graph-based neural network designed for efficient 3D point cloud classification. Point-SkipNet utilizes techniques like efficient sampling, neighborhood grouping, and skip connections to achieve high accuracy with reduced computational overhead. Experimental results demonstrate that models trained on ModelNet-R show significant performance improvements, with Point-SkipNet achieving state-of-the-art accuracy while having a lower parameter count compared to other models. This research emphasizes the importance of dataset quality in optimizing the efficiency of models for 3D point cloud classification. <div>
arXiv:2509.05198v1 Announce Type: new 
Abstract: The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Graphics Programming with Large Language Models</title>
<link>https://arxiv.org/abs/2509.05208</link>
<guid>https://arxiv.org/abs/2509.05208</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large Language Models, Symbolic Graphics Programming, Reinforcement Learning, Cross-modal grounding 
Summary: 
The study explores the capability of Large Language Models (LLMs) in generating symbolic graphics programs (SGPs) for scalable vector graphics (SVGs) based on natural-language descriptions. A benchmark, SGP-GenBench, is introduced to evaluate object fidelity, scene fidelity, and compositionality. It is found that proprietary models outperform open-source models, with performance linked to coding abilities. An reinforcement learning approach with verifiable rewards is proposed to enhance the generation of SVGs, resulting in improved quality and semantics comparable to state-of-the-art systems. Analysis of training dynamics shows finer decomposition of objects and contextual details that enhance scene coherence. The study demonstrates that symbolic graphics programming provides a precise and interpretable means for cross-modal grounding.<br /><br />Summary: <div>
arXiv:2509.05208v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</title>
<link>https://arxiv.org/abs/2509.05249</link>
<guid>https://arxiv.org/abs/2509.05249</guid>
<content:encoded><![CDATA[
<div> Framework, Compositionality, Generalization, Visual domains, Benchmark

Summary: 
COGITAO is a new data generation framework and benchmark designed to study compositionality and generalization in visual domains. Inspired by ARC-AGI's problem-setting, it creates rule-based tasks by applying transformations to objects in grid environments. The framework supports composition over 28 interoperable transformations, allowing for the creation of millions of unique task rules. State-of-the-art vision models struggle to generalize to novel combinations of familiar elements in these tasks, despite strong performance within the known domain. The framework is open-sourced, providing code and datasets for further research in this area.

<br /><br />Summary: <div>
arXiv:2509.05249v1 Announce Type: new 
Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</title>
<link>https://arxiv.org/abs/2509.05296</link>
<guid>https://arxiv.org/abs/2509.05296</guid>
<content:encoded><![CDATA[
arXiv:2509.05296v1 Announce Type: new 
Abstract: We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</title>
<link>https://arxiv.org/abs/2509.05297</link>
<guid>https://arxiv.org/abs/2509.05297</guid>
<content:encoded><![CDATA[
arXiv:2509.05297v1 Announce Type: new 
Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal hardware resources for training. FlowSeek marries the latest advances on the design space of optical flow networks with cutting-edge single-image depth foundation models and classical low-dimensional motion parametrization, implementing a compact, yet accurate architecture. FlowSeek is trained on a single consumer-grade GPU, a hardware budget about 8x lower compared to most recent methods, and still achieves superior cross-dataset generalization on Sintel Final and KITTI, with a relative improvement of 10 and 15% over the previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient Integration of New Modalities into Large Language Models</title>
<link>https://arxiv.org/abs/2509.04606</link>
<guid>https://arxiv.org/abs/2509.04606</guid>
<content:encoded><![CDATA[
arXiv:2509.04606v1 Announce Type: cross 
Abstract: Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring the Graph Structure of Images for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04677</link>
<guid>https://arxiv.org/abs/2509.04677</guid>
<content:encoded><![CDATA[
arXiv:2509.04677v1 Announce Type: cross 
Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural Network (GNN) architectures. The images are traditionally represented as a grid graph with each node representing a pixel and edges connecting neighboring pixels (vertically and horizontally). The graph signal is the values (intensities) of each pixel in the image. The graphs are commonly used as input to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the images. In this work, we improve the accuracy of downstream graph neural network tasks by finding alternative graphs to the grid graph and superpixel methods to represent the dataset images, following the approach in [5, 6]. We find row correlation, column correlation, and product graphs for each image in MNIST and Fashion-MNIST using correlations between the pixel values building on the method in [5, 6]. Experiments show that using these different graph representations and features as input into downstream GNN models improves the accuracy over using the traditional grid graph and superpixel methods in the literature.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring</title>
<link>https://arxiv.org/abs/2509.04682</link>
<guid>https://arxiv.org/abs/2509.04682</guid>
<content:encoded><![CDATA[
arXiv:2509.04682v1 Announce Type: cross 
Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal data for long-term ecological analysis, but intrinsic noise and complex signal dependencies hinder model stability and generalization. Multilayered windowing has improved target sound localization, yet variability from shifting ambient noise, diverse propagation effects, and mixed biological and anthropogenic sources demands robust architectures and rigorous evaluation. We introduce GetNetUPAM, a hierarchical nested cross-validation framework designed to quantify model stability under ecologically realistic variability. Data are partitioned into distinct site-year segments, preserving recording heterogeneity and ensuring each validation fold reflects a unique environmental subset, reducing overfitting to localized noise and sensor artifacts. Site-year blocking enforces evaluation against genuine environmental diversity, while standard cross-validation on random subsets measures generalization across UPAM's full signal distribution, a dimension absent from current benchmarks. Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution Pooling and Attention Network (ARPA-N), a neural architecture for irregular spectrogram dimensions. Adaptive pooling with spatial attention extends the receptive field, capturing global context without excessive parameters. Under GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet baselines and a log2-scale order-of-magnitude drop in variability across all metrics, enabling consistent detection across site-year folds and advancing scalable, accurate bioacoustic monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs</title>
<link>https://arxiv.org/abs/2509.04719</link>
<guid>https://arxiv.org/abs/2509.04719</guid>
<content:encoded><![CDATA[
arXiv:2509.04719v1 Announce Type: cross 
Abstract: The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
arXiv:2509.04734v1 Announce Type: cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences and similarity kernels. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) on supervised contrastive learning, we outperform the standard approach by using TV and a distance-based similarity kernel instead of KL and an angular kernel; (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded f-divergence. Our results highlight the importance of considering divergence and similarity kernel choices in representation learning optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization</title>
<link>https://arxiv.org/abs/2509.04745</link>
<guid>https://arxiv.org/abs/2509.04745</guid>
<content:encoded><![CDATA[
arXiv:2509.04745v1 Announce Type: cross 
Abstract: Sign language datasets are often not representative in terms of vocabulary, underscoring the need for models that generalize to unseen signs. Vector quantization is a promising approach for learning discrete, token-like representations, but it has not been evaluated whether the learned units capture spurious correlations that hinder out-of-vocabulary performance. This work investigates two phonological inductive biases: Parameter Disentanglement, an architectural bias, and Phonological Semi-Supervision, a regularization technique, to improve isolated sign recognition of known signs and reconstruction quality of unseen signs with a vector-quantized autoencoder. The primary finding is that the learned representations from the proposed model are more effective for one-shot reconstruction of unseen signs and more discriminative for sign identification compared to a controlled baseline. This work provides a quantitative analysis of how explicit, linguistically-motivated biases can improve the generalization of learned representations of sign language.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations</title>
<link>https://arxiv.org/abs/2509.04819</link>
<guid>https://arxiv.org/abs/2509.04819</guid>
<content:encoded><![CDATA[
arXiv:2509.04819v1 Announce Type: cross 
Abstract: Medical image synthesis has become an essential strategy for augmenting datasets and improving model generalization in data-scarce clinical settings. However, fine-grained and controllable synthesis remains difficult due to limited high-quality annotations and domain shifts across datasets. Existing methods, often designed for natural images or well-defined tumors, struggle to generalize to chest radiographs, where disease patterns are morphologically diverse and tightly intertwined with anatomical structures. To address these challenges, we propose AURAD, a controllable radiology synthesis framework that jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike prior approaches that rely on randomly sampled masks-limiting diversity, controllability, and clinical relevance-our method learns to generate masks that capture multi-pathology coexistence and anatomical-pathological consistency. It follows a progressive pipeline: pseudo masks are first generated from clinical prompts conditioned on anatomical structures, and then used to guide image synthesis. We also leverage pretrained expert medical models to filter outputs and ensure clinical plausibility. Beyond visual realism, the synthesized masks also serve as labels for downstream tasks such as detection and segmentation, bridging the gap between generative modeling and real-world clinical applications. Extensive experiments and blinded radiologist evaluations demonstrate the effectiveness and generalizability of our method across tasks and datasets. In particular, 78% of our synthesized images are classified as authentic by board-certified radiologists, and over 40% of predicted segmentation overlays are rated as clinically useful. All code, pre-trained models, and the synthesized dataset will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression</title>
<link>https://arxiv.org/abs/2509.04849</link>
<guid>https://arxiv.org/abs/2509.04849</guid>
<content:encoded><![CDATA[
arXiv:2509.04849v1 Announce Type: cross 
Abstract: This work introduces a compact and hardware efficient method for compressing color images using near term quantum devices. The approach segments the image into fixed size blocks called bixels, and computes the total intensity within each block. A global histogram with B bins is then constructed from these block intensities, and the normalized square roots of the bin counts are encoded as amplitudes into an n qubit quantum state. Amplitude embedding is performed using PennyLane and executed on real IBM Quantum hardware. The resulting state is measured to reconstruct the histogram, enabling approximate recovery of block intensities and full image reassembly. The method maintains a constant qubit requirement based solely on the number of histogram bins, independent of the resolution of the image. By adjusting B, users can control the trade off between fidelity and resource usage. Empirical results demonstrate high quality reconstructions using as few as 5 to 7 qubits, significantly outperforming conventional pixel level encodings in terms of qubit efficiency and validating the practical application of the method for current NISQ era quantum systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images</title>
<link>https://arxiv.org/abs/2509.04870</link>
<guid>https://arxiv.org/abs/2509.04870</guid>
<content:encoded><![CDATA[
arXiv:2509.04870v1 Announce Type: cross 
Abstract: Recent advances in semantic segmentation of multi-modal remote sensing images have significantly improved the accuracy of tree cover mapping, supporting applications in urban planning, forest monitoring, and ecological assessment. Integrating data from multiple modalities-such as optical imagery, light detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown superior performance over single-modality methods. However, these data are often acquired days or even months apart, during which various changes may occur, such as vegetation disturbances (e.g., logging, and wildfires) and variations in imaging quality. Such temporal misalignments introduce cross-modal uncertainty, especially in high-resolution imagery, which can severely degrade segmentation accuracy. To address this challenge, we propose MURTreeFormer, a novel multi-modal segmentation framework that mitigates and leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer treats one modality as primary and others as auxiliary, explicitly modeling patch-level uncertainty in the auxiliary modalities via a probabilistic latent representation. Uncertain patches are identified and reconstructed from the primary modality's distribution through a VAE-based resampling mechanism, producing enhanced auxiliary features for fusion. In the decoder, a gradient magnitude attention (GMA) module and a lightweight refinement head (RH) are further integrated to guide attention toward tree-like structures and to preserve fine-grained spatial details. Extensive experiments on multi-modal datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly improves segmentation performance and effectively reduces the impact of temporally induced aleatoric uncertainty.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</title>
<link>https://arxiv.org/abs/2509.04908</link>
<guid>https://arxiv.org/abs/2509.04908</guid>
<content:encoded><![CDATA[
arXiv:2509.04908v1 Announce Type: cross 
Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)</title>
<link>https://arxiv.org/abs/2509.04948</link>
<guid>https://arxiv.org/abs/2509.04948</guid>
<content:encoded><![CDATA[
arXiv:2509.04948v1 Announce Type: cross 
Abstract: Topological localization is a fundamental problem in mobile robotics, since robots must be able to determine their position in order to accomplish tasks. Visual localization and place recognition are challenging due to perceptual ambiguity, sensor noise, and illumination variations. This work addresses topological localization in an office environment using only images acquired with a perspective color camera mounted on a robot platform, without relying on temporal continuity of image sequences. We evaluate state-of-the-art visual descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions include a systematic, quantitative comparison of these features, distance measures, and classifiers. Performance was analyzed using standard evaluation metrics and visualizations, extending previous experiments. Results demonstrate the advantages of proper configurations of appearance descriptors, similarity measures, and classifiers. The quality of these configurations was further validated in the Robot Vision task of the ImageCLEF evaluation campaign, where the system identified the most likely location of novel image sequences. Future work will explore hierarchical models, ranking methods, and feature combinations to build more robust localization systems, reducing training and runtime while avoiding the curse of dimensionality. Ultimately, this aims toward integrated, real-time localization across varied illumination and longer routes.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointing-Guided Target Estimation via Transformer-Based Attention</title>
<link>https://arxiv.org/abs/2509.05031</link>
<guid>https://arxiv.org/abs/2509.05031</guid>
<content:encoded><![CDATA[
arXiv:2509.05031v1 Announce Type: cross 
Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRIM: Towards Practical In-Image Multilingual Machine Translation</title>
<link>https://arxiv.org/abs/2509.05146</link>
<guid>https://arxiv.org/abs/2509.05146</guid>
<content:encoded><![CDATA[
arXiv:2509.05146v1 Announce Type: cross 
Abstract: In-Image Machine Translation (IIMT) aims to translate images containing texts from one language to another. Current research of end-to-end IIMT mainly conducts on synthetic data, with simple background, single font, fixed text position, and bilingual translation, which can not fully reflect real world, causing a significant gap between the research and practical conditions. To facilitate research of IIMT in real-world scenarios, we explore Practical In-Image Multilingual Machine Translation (IIMMT). In order to convince the lack of publicly available data, we annotate the PRIM dataset, which contains real-world captured one-line text images with complex background, various fonts, diverse text positions, and supports multilingual translation directions. We propose an end-to-end model VisTrans to handle the challenge of practical conditions in PRIM, which processes visual text and background information in the image separately, ensuring the capability of multilingual translation while improving the visual quality. Experimental results indicate the VisTrans achieves a better translation quality and visual effect compared to other models. The code and dataset are available at: https://github.com/BITHLP/PRIM.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.05154</link>
<guid>https://arxiv.org/abs/2509.05154</guid>
<content:encoded><![CDATA[
arXiv:2509.05154v1 Announce Type: cross 
Abstract: Vision-language models and their adaptations to image segmentation tasks present enormous potential for producing highly accurate and interpretable results. However, implementations based on CLIP and BiomedCLIP are still lagging behind more sophisticated architectures such as CRIS. In this work, instead of focusing on text prompt engineering as is the norm, we attempt to narrow this gap by showing how to ensemble vision-language segmentation models (VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice score improvement of 6.3% on the BKAI polyp dataset using the ensembled BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%. Furthermore, we provide initial results on additional four radiology and non-radiology datasets. We conclude that ensembling works differently across these datasets (from outperforming to underperforming the CRIS model), indicating a topic for future investigation by the community. The code is available at https://github.com/juliadietlmeier/VLSM-Ensemble.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers</title>
<link>https://arxiv.org/abs/2509.05201</link>
<guid>https://arxiv.org/abs/2509.05201</guid>
<content:encoded><![CDATA[
arXiv:2509.05201v1 Announce Type: cross 
Abstract: This paper presents a robust model predictive control (MPC) framework that explicitly addresses the non-Gaussian noise inherent in deep learning-based perception modules used for state estimation. Recognizing that accurate uncertainty quantification of the perception module is essential for safe feedback control, our approach departs from the conventional assumption of zero-mean noise quantification of the perception error. Instead, it employs set-based state estimation with constrained zonotopes to capture biased, heavy-tailed uncertainties while maintaining bounded estimation errors. To improve computational efficiency, the robust MPC is reformulated as a linear program (LP), using a Minkowski-Lyapunov-based cost function with an added slack variable to prevent degenerate solutions. Closed-loop stability is ensured through Minkowski-Lyapunov inequalities and contractive zonotopic invariant sets. The largest stabilizing terminal set and its corresponding feedback gain are then derived via an ellipsoidal approximation of the zonotopes. The proposed framework is validated through both simulations and hardware experiments on an omnidirectional mobile robot along with a camera and a convolutional neural network-based perception module implemented within a ROS2 framework. The results demonstrate that the perception-aware MPC provides stable and accurate control performance under heavy-tailed noise conditions, significantly outperforming traditional Gaussian-noise-based designs in terms of both state estimation error bounding and overall control performance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</title>
<link>https://arxiv.org/abs/2509.05263</link>
<guid>https://arxiv.org/abs/2509.05263</guid>
<content:encoded><![CDATA[
arXiv:2509.05263v1 Announce Type: cross 
Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control</title>
<link>https://arxiv.org/abs/2509.05285</link>
<guid>https://arxiv.org/abs/2509.05285</guid>
<content:encoded><![CDATA[
arXiv:2509.05285v1 Announce Type: cross 
Abstract: Recent advances in text-driven 3D scene editing and stylization, which leverage the powerful capabilities of 2D generative models, have demonstrated promising outcomes. However, challenges remain in ensuring high-quality stylization and view consistency simultaneously. Moreover, applying style consistently to different regions or objects in the scene with semantic correspondence is a challenging task. To address these limitations, we introduce techniques that enhance the quality of 3D stylization while maintaining view consistency and providing optional region-controlled style transfer. Our method achieves stylization by re-training an initial 3D representation using stylized multi-view 2D images of the source views. Therefore, ensuring both style consistency and view consistency of stylized multi-view images is crucial. We achieve this by extending the style-aligned depth-conditioned view generation framework, replacing the fully shared attention mechanism with a single reference-based attention-sharing mechanism, which effectively aligns style across different viewpoints. Additionally, inspired by recent 3D inpainting methods, we utilize a grid of multiple depth maps as a single-image reference to further strengthen view consistency among stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced Wasserstein Distance Loss, allowing styles to be applied to distinct image regions using segmentation masks from off-the-shelf models. We demonstrate that this optional feature enhances the faithfulness of style transfer and enables the mixing of different styles across distinct regions of the scene. Experimental evaluations, both qualitative and quantitative, demonstrate that our pipeline effectively improves the results of text-driven 3D stylization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation-Centric Survey of Skeletal Action Recognition and the ANUBIS Benchmark</title>
<link>https://arxiv.org/abs/2205.02071</link>
<guid>https://arxiv.org/abs/2205.02071</guid>
<content:encoded><![CDATA[
arXiv:2205.02071v4 Announce Type: replace 
Abstract: 3D skeleton-based human action recognition has emerged as a powerful alternative to traditional RGB and depth-based approaches, offering robustness to environmental variations, computational efficiency, and enhanced privacy. Despite remarkable progress, current research remains fragmented across diverse input representations and lacks evaluation under scenarios that reflect modern real-world challenges.
  This paper presents a representation-centric survey of skeleton-based action recognition, systematically categorizing state-of-the-art methods by their input feature types: joint coordinates, bone vectors, motion flows, and extended representations, and analyzing how these choices influence spatial-temporal modeling strategies.
  Building on the insights from this review, we introduce ANUBIS, a large-scale, challenging skeleton action dataset designed to address critical gaps in existing benchmarks. ANUBIS incorporates multi-view recordings with back-view perspectives, complex multi-person interactions, fine-grained and violent actions, and contemporary social behaviors.
  We benchmark a diverse set of state-of-the-art models on ANUBIS and conduct an in-depth analysis of how different feature types affect recognition performance across 102 action categories. Our results show strong action-feature dependencies, highlight the limitations of na\"ive multi-representational fusion, and point toward the need for task-aware, semantically aligned integration strategies. This work offers both a comprehensive foundation and a practical benchmarking resource, aiming to guide the next generation of robust, generalizable skeleton-based action recognition systems for complex real-world scenarios.
  The dataset website, benchmarking framework, and download link are available at \href{https://yliu1082.github.io/ANUBIS/}{https://yliu1082.github.io/ANUBIS/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAGC:Feature Augmentation on Geodesic Curve in the Pre-Shape Space</title>
<link>https://arxiv.org/abs/2312.03325</link>
<guid>https://arxiv.org/abs/2312.03325</guid>
<content:encoded><![CDATA[
arXiv:2312.03325v4 Announce Type: replace 
Abstract: Due to the constraints on model performance imposed by the size of the training data, data augmentation has become an essential technique in deep learning. However, most existing data augmentation methods are affected by information loss and perform poorly in small-sample scenarios, which limits their application. To overcome the limitation, we propose a Feature Augmentation method on Geodesic Curve in the pre-shape space, called the FAGC. First, a pre-trained neural network model is employed to extract features from the input images. Then, the image features as a vector is projected into the pre-shape space by removing its position and scale information. In the pre-shape space, an optimal Geodesic curve is constructed to fit the feature vectors. Finally, new feature vectors are generated for model learning by interpolating along the constructed Geodesic curve. We conducted extensive experiments to demonstrate the effectiveness and versatility of the FAGC. The results demonstrate that applying the FAGC to deep learning or machine learning methods can significantly improve their performance in small-sample tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs</title>
<link>https://arxiv.org/abs/2408.11813</link>
<guid>https://arxiv.org/abs/2408.11813</guid>
<content:encoded><![CDATA[
arXiv:2408.11813v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities by integrating visual and textual inputs, yet modality alignment remains one of the most challenging aspects. Current MLLMs typically rely on simple adapter architectures and pretraining approaches to bridge vision encoders with large language models (LLM), guided by image-level supervision. We identify this paradigm often leads to suboptimal alignment between modalities, significantly constraining the LLM's ability to properly interpret and reason with visual features particularly for smaller language models. This limitation degrades overall performance-particularly for smaller language models where capacity constraints are more pronounced and adaptation capabilities are limited. To address this fundamental limitation, we propose Supervised Embedding Alignment (SEA), a token-level supervision alignment method that enables more precise visual-text alignment during pretraining. SEA introduces minimal computational overhead while preserving language capabilities and substantially improving cross-modal understanding. Our comprehensive analyses reveal critical insights into the adapter's role in multimodal integration, and extensive experiments demonstrate that SEA consistently improves performance across various model sizes, with smaller models benefiting the most (average performance gain of 7.61% for Gemma-2B). This work establishes a foundation for developing more effective alignment strategies for future multimodal systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v4 Announce Type: replace 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2411.06378</link>
<guid>https://arxiv.org/abs/2411.06378</guid>
<content:encoded><![CDATA[
arXiv:2411.06378v2 Announce Type: replace 
Abstract: In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypDAE: Hyperbolic Diffusion Autoencoders for Hierarchical Few-shot Image Generation</title>
<link>https://arxiv.org/abs/2411.17784</link>
<guid>https://arxiv.org/abs/2411.17784</guid>
<content:encoded><![CDATA[
arXiv:2411.17784v2 Announce Type: replace 
Abstract: Few-shot image generation aims to generate diverse and high-quality images for an unseen class given only a few examples in that class. A key challenge in this task is balancing category consistency and image diversity, which often compete with each other. Moreover, existing methods offer limited control over the attributes of newly generated images. In this work, we propose Hyperbolic Diffusion Autoencoders (HypDAE), a novel approach that operates in hyperbolic space to capture hierarchical relationships among images from seen categories. By leveraging pre-trained foundation models, HypDAE generates diverse new images for unseen categories with exceptional quality by varying stochastic subcodes or semantic codes. Most importantly, the hyperbolic representation introduces an additional degree of control over semantic diversity through the adjustment of radii within the hyperbolic disk. Extensive experiments and visualizations demonstrate that HypDAE significantly outperforms prior methods by achieving a better balance between preserving category-relevant features and promoting image diversity with limited data. Furthermore, HypDAE offers a highly controllable and interpretable generation process.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DirectorLLM for Human-Centric Video Generation</title>
<link>https://arxiv.org/abs/2412.14484</link>
<guid>https://arxiv.org/abs/2412.14484</guid>
<content:encoded><![CDATA[
arXiv:2412.14484v2 Announce Type: replace 
Abstract: In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spoof Trace Discovery for Deep Learning Based Explainable Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2412.17541</link>
<guid>https://arxiv.org/abs/2412.17541</guid>
<content:encoded><![CDATA[
arXiv:2412.17541v4 Announce Type: replace 
Abstract: With the rapid growth usage of face recognition in people's daily life, face anti-spoofing becomes increasingly important to avoid malicious attacks. Recent face anti-spoofing models can reach a high classification accuracy on multiple datasets but these models can only tell people "this face is fake" while lacking the explanation to answer "why it is fake". Such a system undermines trustworthiness and causes user confusion, as it denies their requests without providing any explanations. In this paper, we incorporate XAI into face anti-spoofing and propose a new problem termed X-FAS (eXplainable Face Anti-Spoofing) empowering face anti-spoofing models to provide an explanation. We propose SPTD (SPoof Trace Discovery), an X-FAS method which can discover spoof concepts and provide reliable explanations on the basis of discovered concepts. To evaluate the quality of X-FAS methods, we propose an X-FAS benchmark with annotated spoof traces by experts. We analyze SPTD explanations on face anti-spoofing dataset and compare SPTD quantitatively and qualitatively with previous XAI methods on proposed X-FAS benchmark. Experimental results demonstrate SPTD's ability to generate reliable explanations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2501.03544</link>
<guid>https://arxiv.org/abs/2501.03544</guid>
<content:encoded><![CDATA[
arXiv:2501.03544v3 Announce Type: replace 
Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Clothed Avatar Generation with Layered Representation</title>
<link>https://arxiv.org/abs/2501.04631</link>
<guid>https://arxiv.org/abs/2501.04631</guid>
<content:encoded><![CDATA[
arXiv:2501.04631v2 Announce Type: replace 
Abstract: Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. Previous methods have achieved success in generating diverse digital avatars, however, generating avatars with disentangled components (\eg, body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, the first feed-forward diffusion-based method for generating component-disentangled clothed avatars. To achieve this, we first propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation supports high-resolution and real-time rendering, as well as expressive animation including controllable gestures and facial expressions. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to address the severe occlusion problem of the innermost human body layer. Extensive experiments demonstrate the impressive performances of our method in generating disentangled clothed avatars, and we further explore its applications in component transfer. The project page is available at: https://olivia23333.github.io/LayerAvatar/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActiveGAMER: Active GAussian Mapping through Efficient Rendering</title>
<link>https://arxiv.org/abs/2501.06897</link>
<guid>https://arxiv.org/abs/2501.06897</guid>
<content:encoded><![CDATA[
arXiv:2501.06897v3 Announce Type: replace 
Abstract: We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian Splatting (3DGS) to achieve high-quality, real-time scene mapping and exploration. Unlike traditional NeRF-based methods, which are computationally demanding and restrict active mapping performance, our approach leverages the efficient rendering capabilities of 3DGS, allowing effective and efficient exploration in complex environments. The core of our system is a rendering-based information gain module that dynamically identifies the most informative viewpoints for next-best-view planning, enhancing both geometric and photometric reconstruction accuracy. ActiveGAMER also integrates a carefully balanced framework, combining coarse-to-fine exploration, post-refinement, and a global-local keyframe selection strategy to maximize reconstruction completeness and fidelity. Our system autonomously explores and reconstructs environments with state-of-the-art geometric and photometric accuracy and completeness, significantly surpassing existing approaches in both aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D highlight ActiveGAMER's effectiveness in active mapping tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Densification for Multi-Map Monocular VSLAM in Endoscopy</title>
<link>https://arxiv.org/abs/2503.14346</link>
<guid>https://arxiv.org/abs/2503.14346</guid>
<content:encoded><![CDATA[
arXiv:2503.14346v2 Announce Type: replace 
Abstract: Multi-map Sparse Monocular visual Simultaneous Localization and Mapping applied to monocular endoscopic sequences has proven efficient to robustly recover tracking after the frequent losses in endoscopy due to motion blur, temporal occlusion, tools interaction or water jets. The sparse multi-maps are adequate for robust camera localization, however they are very poor for environment representation, they are noisy, with a high percentage of inaccurately reconstructed 3D points, including significant outliers, and more importantly with an unacceptable low density for clinical applications.
  We propose a method to remove outliers and densify the maps of the state of the art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for up-to-scale depth dense predictions are aligned with the sparse CudaSIFT submaps by means of the robust to spurious LMedS. Our system mitigates the inherent scale ambiguity in monocular depth estimation while filtering outliers, leading to reliable densified 3D maps.
  We provide experimental evidence of accurate densified maps 4.15 mm RMS accuracy at affordable computing time in the C3VD phantom colon dataset. We report qualitative results on the real colonoscopy from the Endomapper dataset.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs</title>
<link>https://arxiv.org/abs/2503.20309</link>
<guid>https://arxiv.org/abs/2503.20309</guid>
<content:encoded><![CDATA[
arXiv:2503.20309v2 Announce Type: replace 
Abstract: Preference alignment has emerged as an effective strategy to enhance the performance of Multimodal Large Language Models (MLLMs) following supervised fine-tuning. While existing preference alignment methods predominantly target hallucination factors, they overlook the factors essential for multi-modal comprehension capabilities, often narrowing their improvements on hallucination mitigation. To bridge this gap, we propose Instruction-oriented Preference Alignment (IPA), a scalable framework designed to automatically construct alignment preferences grounded in instruction fulfillment efficacy. Our method involves an automated preference construction coupled with a dedicated verification process that identifies instruction-oriented factors, avoiding significant variability in response representations. Additionally, IPA incorporates a progressive preference collection pipeline, further recalling challenging samples through model self-evolution and reference-guided refinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness across multiple benchmarks, including hallucination evaluation, visual question answering, and text understanding tasks, highlighting its capability to enhance general comprehension.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RailGoerl24: G\"orlitz Rail Test Center CV Dataset 2024</title>
<link>https://arxiv.org/abs/2504.00204</link>
<guid>https://arxiv.org/abs/2504.00204</guid>
<content:encoded><![CDATA[
arXiv:2504.00204v2 Announce Type: replace 
Abstract: Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of T\"UV S\"UD Rail, in G\"orlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, available at data.fid-move.de/dataset/railgoerl24, can also be used for tasks beyond collision prediction.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&amp;E Whole Slide Images of Cutaneous Squamous Cell Carcinoma</title>
<link>https://arxiv.org/abs/2505.04672</link>
<guid>https://arxiv.org/abs/2505.04672</guid>
<content:encoded><![CDATA[
arXiv:2505.04672v2 Announce Type: replace 
Abstract: Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.884 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach</title>
<link>https://arxiv.org/abs/2505.16422</link>
<guid>https://arxiv.org/abs/2505.16422</guid>
<content:encoded><![CDATA[
arXiv:2505.16422v3 Announce Type: replace 
Abstract: The automatic control of mobile devices is essential for efficiently performing complex tasks that involve multiple sequential steps. However, these tasks pose significant challenges due to the limited environmental information available at each step, primarily through visual observations. As a result, current approaches, which typically rely on reactive policies, focus solely on immediate observations and often lead to suboptimal decision-making. To address this problem, we propose \textbf{Foresighted Planning with World Model-Driven Code Execution (FPWC)},a framework that prioritizes natural language understanding and structured reasoning to enhance the agent's global understanding of the environment by developing a task-oriented, refinable \emph{world model} at the outset of the task. Foresighted actions are subsequently generated through iterative planning within this world model, executed in the form of executable code. Extensive experiments conducted in simulated environments and on real mobile devices demonstrate that our method outperforms previous approaches, particularly achieving a 44.4\% relative improvement in task success rate compared to the state-of-the-art in the simulated environment. Code and demo are provided in the supplementary material.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model</title>
<link>https://arxiv.org/abs/2506.10605</link>
<guid>https://arxiv.org/abs/2506.10605</guid>
<content:encoded><![CDATA[
arXiv:2506.10605v3 Announce Type: replace 
Abstract: We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2506.11430</link>
<guid>https://arxiv.org/abs/2506.11430</guid>
<content:encoded><![CDATA[
arXiv:2506.11430v2 Announce Type: replace 
Abstract: We introduce Auto-Connect, a novel approach for automatic rigging that explicitly preserves skeletal connectivity through a connectivity-preserving tokenization scheme. Unlike previous methods that predict bone positions represented as two joints or first predict points before determining connectivity, our method employs special tokens to define endpoints for each joint's children and for each hierarchical layer, effectively automating connectivity relationships. This approach significantly enhances topological accuracy by integrating connectivity information directly into the prediction framework. To further guarantee high-quality topology, we implement a topology-aware reward function that quantifies topological correctness, which is then utilized in a post-training phase through reward-guided Direct Preference Optimization. Additionally, we incorporate implicit geodesic features for latent top-k bone selection, which substantially improves skinning quality. By leveraging geodesic distance information within the model's latent space, our approach intelligently determines the most influential bones for each vertex, effectively mitigating common skinning artifacts. This combination of connectivity-preserving tokenization, reward-guided fine-tuning, and geodesic-aware bone selection enables our model to consistently generate more anatomically plausible skeletal structures with superior deformation properties.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception</title>
<link>https://arxiv.org/abs/2506.17733</link>
<guid>https://arxiv.org/abs/2506.17733</guid>
<content:encoded><![CDATA[
arXiv:2506.17733v2 Announce Type: replace 
Abstract: The YOLO series models reign supreme in real-time object detection due to their superior accuracy and computational efficiency. However, both the convolutional architectures of YOLO11 and earlier versions and the area-based self-attention mechanism introduced in YOLOv12 are limited to local information aggregation and pairwise correlation modeling, lacking the capability to capture global multi-to-multi high-order correlations, which limits detection performance in complex scenarios. In this paper, we propose YOLOv13, an accurate and lightweight object detector. To address the above-mentioned challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement (HyperACE) mechanism that adaptively exploits latent high-order correlations and overcomes the limitation of previous methods that are restricted to pairwise correlation modeling based on hypergraph computation, achieving efficient global cross-location and cross-scale feature fusion and enhancement. Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD) paradigm based on HyperACE, which effectively achieves fine-grained information flow and representation synergy within the entire network by distributing correlation-enhanced features to the full pipeline. Finally, we propose to leverage depthwise separable convolutions to replace vanilla large-kernel convolutions, and design a series of blocks that significantly reduce parameters and computational complexity without sacrificing performance. We conduct extensive experiments on the widely used MS COCO benchmark, and the experimental results demonstrate that our method achieves state-of-the-art performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and models of our YOLOv13 model are available at: https://github.com/iMoonLab/yolov13.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology Images: From Giga to Mini Challenge</title>
<link>https://arxiv.org/abs/2507.04681</link>
<guid>https://arxiv.org/abs/2507.04681</guid>
<content:encoded><![CDATA[
arXiv:2507.04681v3 Announce Type: replace 
Abstract: Colorectal cancer (CRC) is the third most diagnosed cancer and the second leading cause of cancer-related death worldwide. Accurate histopathological grading of CRC is essential for prognosis and treatment planning but remains a subjective process prone to observer variability and limited by global shortages of trained pathologists. To promote automated and standardized solutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor Grading and Segmentation using the publicly available METU CCTGS dataset. The dataset comprises 103 whole-slide images with expert pixel-level annotations for five tissue classes. Participants submitted segmentation masks via Codalab, evaluated using metrics such as macro F-score and mIoU. Among 39 participating teams, six outperformed the Swin Transformer baseline (62.92 F-score). This paper presents an overview of the challenge, dataset, and the top-performing methods
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework</title>
<link>https://arxiv.org/abs/2507.05814</link>
<guid>https://arxiv.org/abs/2507.05814</guid>
<content:encoded><![CDATA[
arXiv:2507.05814v3 Announce Type: replace 
Abstract: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.
  This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields</title>
<link>https://arxiv.org/abs/2507.06269</link>
<guid>https://arxiv.org/abs/2507.06269</guid>
<content:encoded><![CDATA[
arXiv:2507.06269v3 Announce Type: replace 
Abstract: Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Linear Separability Ceiling: Aligning Representations in VLMs</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[
arXiv:2507.07574v2 Announce Type: replace 
Abstract: A challenge in advancing Visual-Language Models (VLMs) is determining whether their failures on abstract reasoning tasks, such as Bongard problems, stem from flawed perception or faulty top-down reasoning. To disentangle these factors, we introduce a diagnostic framework centered on the Linear Separability Ceiling (LSC), the performance achievable by a linear classifier on a VLM's raw visual embeddings. Applying this framework to state-of-the-art VLMs, we uncover a pervasive "alignment gap", where most models fail to generatively outperform the linear separability of their own representations. We find that the few models surpassing this ceiling do so via two mechanisms: by further refining visual representations into a more linearly separable format or by executing non-linear decision logic. We demonstrate that this bottleneck is not a fundamental limitation but a solvable alignment issue. By augmenting standard next-token prediction with a contrastive objective, our fine-tuning method activates dormant reasoning pathways, systematically improving the linear structure of representations to significantly surpass the LSC.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images</title>
<link>https://arxiv.org/abs/2507.13120</link>
<guid>https://arxiv.org/abs/2507.13120</guid>
<content:encoded><![CDATA[
arXiv:2507.13120v2 Announce Type: replace 
Abstract: Detecting tiny objects in remote sensing (RS) imagery has been a long-standing challenge due to their extremely limited spatial information, weak feature representations, and dense distributions across complex backgrounds. Despite numerous efforts devoted, mainstream detectors still underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a multi-stage feature fusion and enhancement model explicitly tailored for RS tiny object detection in various RS scenarios. RS-TinyNet comes with two novel designs: tiny object saliency modeling and feature integrity reconstruction. Guided by these principles, we design three step-wise feature enhancement modules. Among them, the multi-dimensional collaborative attention (MDCA) module employs multi-dimensional attention to enhance the saliency of tiny objects. Additionally, the auxiliary reversible branch (ARB) and a progressive fusion detection head (PFDH) module are introduced to preserve information flow and fuse multi-level features to bridge semantic gaps and retain structural detail. Comprehensive experiments on public RS dataset AI-TOD show that our RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and 6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior detection performance in diverse RS scenarios. These results demonstrate that the proposed multi-stage feature fusion strategy offers an effective and practical solution for tiny object detection in complex RS environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
<link>https://arxiv.org/abs/2507.23567</link>
<guid>https://arxiv.org/abs/2507.23567</guid>
<content:encoded><![CDATA[
arXiv:2507.23567v2 Announce Type: replace 
Abstract: Monocular 3D object detection is valuable for various applications such as robotics and AR/VR. Existing methods are confined to closed-set settings, where the training and testing sets consist of the same scenes and/or object categories. However, real-world applications often introduce new environments and novel object categories, posing a challenge to these methods. In this paper, we address monocular 3D object detection in an open-set setting and introduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD). We propose to lift the open-set 2D detection into 3D space through our designed 3D bounding box head, enabling end-to-end joint training for both 2D and 3D tasks to yield better overall performance. We condition the object queries with geometry prior and overcome the generalization for 3D estimation across diverse scenes. To further improve performance, we design the canonical image space for more efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set settings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and achieve new state-of-the-art results. Code and models are available at royyang0714.github.io/3D-MOOD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images</title>
<link>https://arxiv.org/abs/2508.10542</link>
<guid>https://arxiv.org/abs/2508.10542</guid>
<content:encoded><![CDATA[
arXiv:2508.10542v2 Announce Type: replace 
Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online 3D Gaussian Splatting Modeling with Novel View Selection</title>
<link>https://arxiv.org/abs/2508.14014</link>
<guid>https://arxiv.org/abs/2508.14014</guid>
<content:encoded><![CDATA[
arXiv:2508.14014v2 Announce Type: replace 
Abstract: This study addresses the challenge of generating online 3D Gaussian Splatting (3DGS) models from RGB-only frames. Previous studies have employed dense SLAM techniques to estimate 3D scenes from keyframes for 3DGS model construction. However, these methods are limited by their reliance solely on keyframes, which are insufficient to capture an entire scene, resulting in incomplete reconstructions. Moreover, building a generalizable model requires incorporating frames from diverse viewpoints to achieve broader scene coverage. However, online processing restricts the use of many frames or extensive training iterations. Therefore, we propose a novel method for high-quality 3DGS modeling that improves model completeness through adaptive view selection. By analyzing reconstruction quality online, our approach selects optimal non-keyframes for additional training. By integrating both keyframes and selected non-keyframes, the method refines incomplete regions from diverse viewpoints, significantly enhancing completeness. We also present a framework that incorporates an online multi-view stereo approach, ensuring consistency in 3D information throughout the 3DGS modeling process. Experimental results demonstrate that our method outperforms state-of-the-art methods, delivering exceptional performance in complex outdoor scenes.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</title>
<link>https://arxiv.org/abs/2402.12226</link>
<guid>https://arxiv.org/abs/2402.12226</guid>
<content:encoded><![CDATA[
arXiv:2402.12226v4 Announce Type: replace-cross 
Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic segmentation of Organs at Risk in Head and Neck cancer patients from CT and MRI scans</title>
<link>https://arxiv.org/abs/2405.10833</link>
<guid>https://arxiv.org/abs/2405.10833</guid>
<content:encoded><![CDATA[
arXiv:2405.10833v3 Announce Type: replace-cross 
Abstract: Purpose: To present a high-performing, robust, and flexible deep learning pipeline for automatic segmentation of 30 organs-at-risk (OARs) in head and neck (H&amp;N) cancer patients, using MRI, CT, or both. Method: We trained a segmentation pipeline on paired CT and MRI-T1 scans from 296 patients. We combined data from the H&amp;N OARs CT and MR segmentation (HaN-Seg) challenge and the Burdenko and GLIS-RT datasets from the Cancer Imaging Archive (TCIA). MRI was rigidly registered to CT, and both were stacked as input to an nnU-Net pipeline. Left and right OARs were merged into single classes during training and separated at inference time based on anatomical position. Modality Dropout was applied during the training, ensuring the model would learn from both modalities and robustly handle missing modalities during inference. The trained model was evaluated on the HaN-Seg test set and three TCIA datasets. Predictions were also compared with Limbus AI software. Dice Score (DS) and Hausdorff Distance (HD) were used as evaluation metrics. Results: The pipeline achieved state-of-the-art performance on the HaN-Seg challenge with a mean DS of 78.12% and HD of 3.42 mm. On TCIA datasets, the model maintained strong agreement with Limbus AI software (DS: 77.43% , HD: 3.27 mm), while also flagging low-quality contours. The pipeline can segment seamlessly from the CT, the MRI scan, or both. Conclusion: The proposed pipeline achieved the best DS and HD scores among all HaN-Seg challenge participants and establishes a new state-of-the-art for fully automated, multi-modal segmentation of H&amp;N OARs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLM Guided Exploration and Active Mapping using Fisher Information</title>
<link>https://arxiv.org/abs/2410.17422</link>
<guid>https://arxiv.org/abs/2410.17422</guid>
<content:encoded><![CDATA[
arXiv:2410.17422v3 Announce Type: replace-cross 
Abstract: We present an active mapping system that plans for both long-horizon exploration goals and short-term actions using a 3D Gaussian Splatting (3DGS) representation. Existing methods either do not take advantage of recent developments in multimodal Large Language Models (LLM) or do not consider challenges in localization uncertainty, which is critical in embodied agents. We propose employing multimodal LLMs for long-horizon planning in conjunction with detailed motion planning using our information-based objective. By leveraging high-quality view synthesis from our 3DGS representation, our method employs a multimodal LLM as a zero-shot planner for long-horizon exploration goals from the semantic perspective. We also introduce an uncertainty-aware path proposal and selection algorithm that balances the dual objectives of maximizing the information gain for the environment while minimizing the cost of localization errors. Experiments conducted on the Gibson and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the proposed method.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v2 Announce Type: replace-cross 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</title>
<link>https://arxiv.org/abs/2505.17114</link>
<guid>https://arxiv.org/abs/2505.17114</guid>
<content:encoded><![CDATA[
arXiv:2505.17114v3 Announce Type: replace-cross 
Abstract: Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and 8.0\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\%. Our code and dataset are available at https://github.com/BASHLab/RAVEN.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.00008</link>
<guid>https://arxiv.org/abs/2507.00008</guid>
<content:encoded><![CDATA[
arXiv:2507.00008v2 Announce Type: replace-cross 
Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
<link>https://arxiv.org/abs/2507.13802</link>
<guid>https://arxiv.org/abs/2507.13802</guid>
<content:encoded><![CDATA[
arXiv:2507.13802v2 Announce Type: replace-cross 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment</title>
<link>https://arxiv.org/abs/2508.04160</link>
<guid>https://arxiv.org/abs/2508.04160</guid>
<content:encoded><![CDATA[
arXiv:2508.04160v2 Announce Type: replace-cross 
Abstract: The underspecification of progressive levels of difficulty in measurement constructs design and assessment tests for data visualization literacy may hinder the expressivity of measurements in both test design and test reuse. To mitigate this problem, this paper proposes DRIVE-T (Discriminating and Representative Items for Validating Expressive Tests), a methodology designed to drive the construction and evaluation of assessment items. Given a data vizualization, DRIVE-T supports the identification of task-based items discriminability and representativeness for measuring levels of data visualization literacy. DRIVE-T consists of three steps: (1) tagging task-based items associated with a set of data vizualizations; (2) rating them by independent raters for their difficulty; (3) analysing raters' raw scores through a Many-Facet Rasch Measurement model. In this way, we can observe the emergence of difficulty levels of the measurement construct, derived from the discriminability and representativeness of task-based items for each data vizualization, ordered into Many-Facets construct levels. In this study, we show and apply each step of the methodology to an item bank, which models the difficulty levels of a measurement construct approximating a latent construct for data visualization literacy. This measurement construct is drawn from semiotics, i.e., based on the syntax, semantics and pragmatics knowledge that each data visualization may require to be mastered by people. The DRIVE-T methodology operationalises an inductive approach, observable in a post-design phase of the items preparation, for formative-style and practice-based measurement construct emergence. A pilot study with items selected through the application of DRIVE-T is also presented to test our approach.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network</title>
<link>https://arxiv.org/abs/2508.16897</link>
<guid>https://arxiv.org/abs/2508.16897</guid>
<content:encoded><![CDATA[
arXiv:2508.16897v2 Announce Type: replace-cross 
Abstract: Contrast-enhanced computed tomography (CT) imaging is essential for diagnosing and monitoring thoracic diseases, including aortic pathologies. However, contrast agents pose risks such as nephrotoxicity and allergic-like reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT angiography (CTA) images without contrast administration would be transformative, enhancing patient safety and accessibility while reducing healthcare costs. In this study, we propose the first bridge diffusion-based solution for synthesizing contrast-enhanced CTA images from non-contrast CT scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM), leveraging its ability to model complex mappings while maintaining consistency across slices. Unlike conventional slice-wise synthesis methods, our framework preserves full 3D anatomical integrity while operating in a high-resolution 2D fashion, allowing seamless volumetric interpretation under a low memory budget. To ensure robust spatial alignment, we implement a comprehensive preprocessing pipeline that includes resampling, registration using the Symmetric Normalization method, and a sophisticated dilated segmentation mask to extract the aorta and surrounding structures. We create two datasets from the Coltea-Lung dataset: one containing only the aorta and another including both the aorta and heart, enabling a detailed analysis of anatomical context. We compare our approach against baseline methods on both datasets, demonstrating its effectiveness in preserving vascular structures while enhancing contrast fidelity.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</title>
<link>https://arxiv.org/abs/2508.18733</link>
<guid>https://arxiv.org/abs/2508.18733</guid>
<content:encoded><![CDATA[
<div> Keywords: CAD generative modeling, parametric CAD models, vector drawing primitives, sequence-to-sequence learning, Drawing2CAD framework

Summary:
CAD generative modeling has made significant strides in creating solid models from inputs like point clouds and meshes, but transforming 2D engineering drawings into parametric CAD models remains underexplored. Drawing2CAD reframes this process as a sequence-to-sequence learning problem, utilizing a network-friendly vector primitive representation, a dual-decoder transformer architecture, and a soft target distribution loss function. The method aims to preserve geometric precision and design intent throughout the transformation process. To train and evaluate Drawing2CAD, the CAD-VGDrawing dataset was created and thorough experiments were conducted to demonstrate the effectiveness of the framework. The code and dataset are publicly available for further research and application. <div>
arXiv:2508.18733v3 Announce Type: replace 
Abstract: Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient General Feature Prediction in Masked Skeleton Modeling</title>
<link>https://arxiv.org/abs/2509.03609</link>
<guid>https://arxiv.org/abs/2509.03609</guid>
<content:encoded><![CDATA[
<div> Keywords: masked autoencoder, self-supervised action recognition, skeleton modeling, feature prediction framework, collaborative learning

Summary:
The article introduces a General Feature Prediction framework (GFP) for self-supervised skeleton-based action recognition. The framework improves upon existing methods by using high-level feature prediction instead of low-level reconstruction, leading to more efficient and meaningful representations. A target generation network dynamically generates supervision signals across spatial-temporal hierarchies, improving diversity in the features learned. The framework incorporates constrained optimization to prevent model collapse and ensure feature diversity. Experimental results on datasets demonstrate the benefits of the approach: 6.2 times faster training than standard methods and superior performance in various downstream tasks. The GFP framework achieves state-of-the-art results in action recognition tasks, showcasing its computational efficiency and representation quality. 

<br /><br />Summary: <div>
arXiv:2509.03609v1 Announce Type: new 
Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly propelled self-supervised skeleton-based action recognition. However, most existing approaches limit reconstruction targets to raw joint coordinates or their simple variants, resulting in computational redundancy and limited semantic representation. To address this, we propose a novel General Feature Prediction framework (GFP) for efficient mask skeleton modeling. Our key innovation is replacing conventional low-level reconstruction with high-level feature prediction that spans from local motion patterns to global semantic representations. Specifically, we introduce a collaborative learning framework where a lightweight target generation network dynamically produces diversified supervision signals across spatial-temporal hierarchies, avoiding reliance on pre-computed offline features. The framework incorporates constrained optimization to ensure feature diversity while preventing model collapse. Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits of our approach: Computational efficiency (with 6.2$\times$ faster training than standard masked skeleton modeling methods) and superior representation quality, achieving state-of-the-art performance in various downstream tasks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.03614</link>
<guid>https://arxiv.org/abs/2509.03614</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, artificial intelligence, pixel-level segmentation, teacher-student model, domain shift

Summary: 
Mitotic figure counting is time-consuming and prone to variability among pathologists. Artificial intelligence (AI) offers a solution by automating mitotic figure detection using pixel-level segmentation. However, AI tools can be affected by domain shift, leading to decreased performance. This study presents a teacher-student model that simultaneously detects mitotic figures and classifies atypical mitoses. The model incorporates domain generalization modules, including contrastive representation learning and domain-adversarial training, to improve robustness against domain shift. A multi-scale CNN classifier is utilized for atypical mitosis classification within a multi-task learning framework. The algorithm achieved promising results on a preliminary test set, demonstrating the efficacy of integrating segmentation-based detection and classification for robust mitosis analysis. 

<br /><br />Summary: <div>
arXiv:2509.03614v1 Announce Type: new 
Abstract: Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Attribute Bias Mitigation via Representation Learning</title>
<link>https://arxiv.org/abs/2509.03616</link>
<guid>https://arxiv.org/abs/2509.03616</guid>
<content:encoded><![CDATA[
<div> framework, Generalized Multi Bias Mitigation, biases, vision models, fairness <br />
Summary:<br />
The article introduces Generalized Multi Bias Mitigation (GMBM), a framework designed to address multiple overlapping biases in real-world images that affect the performance of vision models. GMBM consists of two stages: Adaptive Bias Integrated Learning (ABIL) to identify bias shortcuts and Gradient Suppression Fine Tuning to prune bias directions from the network. Existing bias metrics are shown to be inadequate and a new measure called Scaled Bias Amplification (SBA) is introduced. GMBM is validated on FB CMNIST, CelebA, and COCO datasets, resulting in improved group accuracy, reduced bias amplification, and low bias at test time. This makes GMBM the first practical solution for addressing multiple biases in visual recognition in an end-to-end manner. The framework is available on the project page: http://visdomlab.github.io/GMBM/ <br /> <div>
arXiv:2509.03616v1 Announce Type: new 
Abstract: Real world images frequently exhibit multiple overlapping biases, including textures, watermarks, gendered makeup, scene object pairings, etc. These biases collectively impair the performance of modern vision models, undermining both their robustness and fairness. Addressing these biases individually proves inadequate, as mitigating one bias often permits or intensifies others. We tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a lean two stage framework that needs group labels only while training and minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL) deliberately identifies the influence of known shortcuts by training encoders for each attribute and integrating them with the main backbone, compelling the classifier to explicitly recognize these biases. Then Gradient Suppression Fine Tuning prunes those very bias directions from the backbone's gradients, leaving a single compact network that ignores all the shortcuts it just learned to recognize. Moreover we find that existing bias metrics break under subgroup imbalance and train test distribution shifts, so we introduce Scaled Bias Amplification (SBA): a test time measure that disentangles model induced bias amplification from distributional differences. We validate GMBM on FB CMNIST, CelebA, and COCO, where we boost worst group accuracy, halve multi attribute bias amplification, and set a new low in SBA even as bias complexity and distribution shifts intensify, making GMBM the first practical, end to end multibias solution for visual recognition. Project page: http://visdomlab.github.io/GMBM/
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight image segmentation for echocardiography</title>
<link>https://arxiv.org/abs/2509.03631</link>
<guid>https://arxiv.org/abs/2509.03631</guid>
<content:encoded><![CDATA[
<div> segmentation, left ventricle, echocardiography, nnU-Net, data augmentation

Summary:<br />
- Accurate left ventricle segmentation in echocardiography can facilitate automatic clinical measurements.
- A study identified key components for cardiac segmentation in nnU-Net, showing simple augmentations and deep supervision drive performance.
- The study developed a lightweight U-Net with comparable performance to nnU-Net but smaller and faster.
- The lightweight U-Net achieved Dice scores of 0.93/0.85/0.89 for LV/MYO/LA on the CAMUS dataset.
- The lightweight U-Net was 16 times smaller and 4 times faster than the default nnU-Net configuration.
- Cross-dataset evaluation on an internal dataset confirmed comparable generalization. 

<br /><br />Summary: <div>
arXiv:2509.03631v1 Announce Type: new 
Abstract: Accurate segmentation of the left ventricle in echocardiography can enable fully automatic extraction of clinical measurements such as volumes and ejection fraction. While models configured by nnU-Net perform well, they are large and slow, thus limiting real-time use. We identified the most effective components of nnU-Net for cardiac segmentation through an ablation study, incrementally evaluating data augmentation schemes, architectural modifications, loss functions, and post-processing techniques. Our analysis revealed that simple affine augmentations and deep supervision drive performance, while complex augmentations and large model capacity offer diminishing returns. Based on these insights, we developed a lightweight U-Net (2M vs 33M parameters) that achieves statistically equivalent performance to nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89 for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster (1.35ms vs 5.40ms per frame) than the default nnU-Net configuration. Cross-dataset evaluation on an internal dataset (N=311) confirms comparable generalization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
<link>https://arxiv.org/abs/2509.03633</link>
<guid>https://arxiv.org/abs/2509.03633</guid>
<content:encoded><![CDATA[
<div> Keyword: Close-range laser scanning, 3D point cloud data, tree instance segmentation, unsupervised method, Python implementation
Summary: 
The article introduces a revised version of the treeX algorithm, a resource-efficient method for processing 3D point cloud data from close-range laser scanning to extract individual trees. This unsupervised method combines clustering-based stem detection with region growing for crown delineation, offering two parameter presets for ground-based and UAV-borne laser scanning. Evaluation on six public datasets shows improved accuracy and reduced runtime compared to the original treeX algorithm, with gains in instance detection F$_1$-score for ground-based data. The method achieves an F$_1$-score of 0.58 for UAV-borne laser scanning, outperforming the original algorithm. The algorithm's design makes it a suitable alternative to deep learning approaches when data characteristics align with its design, and it can also be used for generating labels for deep learning models semi-automatically. An open-source Python implementation in the pointtree package is provided for broader adoption. 
<br /><br />Summary: <div>
arXiv:2509.03633v1 Announce Type: new 
Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but requires efficient software for processing 3D point cloud data and extracting individual trees. Although recent studies have introduced deep learning methods for tree instance segmentation, these approaches require large annotated datasets and substantial computational resources. As a resource-efficient alternative, we present a revised version of the treeX algorithm, an unsupervised method that combines clustering-based stem detection with region growing for crown delineation. While the original treeX algorithm was developed for personal laser scanning (PLS) data, we provide two parameter presets, one for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one for UAV-borne laser scanning (ULS). We evaluated the method on six public datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham Woods) and compared it to six open-source methods (original treeX, treeiso, RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original treeX algorithm, our revision reduces runtime and improves accuracy, with instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data. For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original algorithm fails to segment any correct instances. For TLS and PLS data, our algorithm achieves accuracy similar to recent open-source methods, including deep learning. Given its algorithmic design, we see two main applications for our method: (1) as a resource-efficient alternative to deep learning approaches in scenarios where the data characteristics align with the method design (sufficient stem visibility and point density), and (2) for the semi-automatic generation of labels for deep learning models. To enable broader adoption, we provide an open-source Python implementation in the pointtree package.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2509.03635</link>
<guid>https://arxiv.org/abs/2509.03635</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, 3D scene understanding, Reg3D, Reconstructive Geometry Instruction Tuning, spatial reasoning

Summary: 
The paper introduces Reg3D, a framework that enhances 3D scene understanding by incorporating geometric supervision during training. Existing approaches lack the necessary geometric constraints for robust 3D spatial representations, which Reg3D addresses by focusing on reconstructing underlying geometric structures. Unlike other methods, Reg3D utilizes a dual-supervision paradigm that includes 3D information as input and learning targets. By implementing object-level and frame-level reconstruction tasks within a dual-encoder architecture, Reg3D encourages spatial reasoning capabilities and ensures geometric consistency. Experimental results on various datasets show that Reg3D outperforms existing methods, offering significant performance improvements and establishing a new training paradigm for spatially aware multimodal models. 

<br /><br />Summary: <div>
arXiv:2509.03635v1 Announce Type: new 
Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable progress in 2D visual understanding; however, extending these capabilities to 3D scene understanding remains a significant challenge. Existing approaches predominantly rely on text-only supervision, which fails to provide the geometric constraints required for learning robust 3D spatial representations. In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction Tuning framework that addresses this limitation by incorporating geometry-aware supervision directly into the training process. Our key insight is that effective 3D understanding necessitates reconstructing underlying geometric structures rather than merely describing them. Unlike existing methods that inject 3D information solely at the input level, Reg3D adopts a dual-supervision paradigm that leverages 3D geometric information both as input and as explicit learning targets. Specifically, we design complementary object-level and frame-level reconstruction tasks within a dual-encoder architecture, enforcing geometric consistency to encourage the development of spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap, ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance improvements, establishing a new training paradigm for spatially aware multimodal models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception</title>
<link>https://arxiv.org/abs/2509.03704</link>
<guid>https://arxiv.org/abs/2509.03704</guid>
<content:encoded><![CDATA[
<div> Cooperative perception, Vehicle-to-Everything (V2X) communication, multi-agent system, quantization strategy, neural network models<br />
<br />
Summary:<br />
QuantV2X is introduced as a fully quantized multi-agent system for efficient and scalable deployment of V2X cooperative perception. It implements a unified quantization strategy for neural network models and message representations, reducing computational load and transmission bandwidth while maintaining accuracy comparable to full-precision systems. The system significantly reduces system-level latency and improves mAP30 metric over baselines. It also allows for larger and more capable models to fit memory constraints, enhancing scalability. QuantV2X demonstrates the viability of a fully quantized multi-agent system for real-world deployment in resource-constrained environments. The system, available on GitHub, aims to facilitate further research in this area. <br /> <div>
arXiv:2509.03704v1 Announce Type: new 
Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication offers significant potential for enhancing vehicle perception by mitigating occlusions and expanding the field of view. However, past research has predominantly focused on improving accuracy metrics without addressing the crucial system-level considerations of efficiency, latency, and real-world deployability. Noticeably, most existing systems rely on full-precision models, which incur high computational and transmission costs, making them impractical for real-time operation in resource-constrained environments. In this paper, we introduce \textbf{QuantV2X}, the first fully quantized multi-agent system designed specifically for efficient and scalable deployment of multi-modal, multi-agent V2X cooperative perception. QuantV2X introduces a unified end-to-end quantization strategy across both neural network models and transmitted message representations that simultaneously reduces computational load and transmission bandwidth. Remarkably, despite operating under low-bit constraints, QuantV2X achieves accuracy comparable to full-precision systems. More importantly, when evaluated under deployment-oriented metrics, QuantV2X reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in mAP30 over full-precision baselines. Furthermore, QuantV2X scales more effectively, enabling larger and more capable models to fit within strict memory budgets. These results highlight the viability of a fully quantized multi-agent intermediate fusion system for real-world deployment. The system will be publicly released to promote research in this field: https://github.com/ucla-mobility/QuantV2X.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns</title>
<link>https://arxiv.org/abs/2509.03729</link>
<guid>https://arxiv.org/abs/2509.03729</guid>
<content:encoded><![CDATA[
<div> ResNet50, MobileNetV2, EfficientNetB0, Automated plant species classification, Leaf venation patterns<br />
<br />
Summary:<br />
This study compares the performance of three deep learning architectures – ResNet50, MobileNetV2, and EfficientNetB0 – for automated plant species classification based on leaf venation patterns. ResNet50 achieved high training accuracy but suffered from overfitting, while MobileNetV2 showed better generalization capabilities. EfficientNetB0 outperformed both models with a testing accuracy of 94.67% and robust precision, recall, and F1 scores. The results indicate the potential of deep learning, particularly EfficientNetB0, in developing accurate tools for automated plant taxonomy using venation traits. <div>
arXiv:2509.03729v1 Announce Type: new 
Abstract: This study evaluates the efficacy of three deep learning architectures: ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species classification based on leaf venation patterns, a critical morphological feature with high taxonomic relevance. Using the Swedish Leaf Dataset comprising images from 15 distinct species (75 images per species, totalling 1,125 images), the models were demonstrated using standard performance metrics during training and testing phases. ResNet50 achieved a training accuracy of 94.11% but exhibited overfitting, reflected by a reduced testing accuracy of 88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better generalization capabilities, attaining a testing accuracy of 93.34% and an F1 score of 93.23%, indicating its suitability for lightweight, real-time applications. EfficientNetB0 outperformed both models, achieving a testing accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%, highlighting its robustness in venation-based classification. The findings underscore the potential of deep learning, particularly EfficientNetB0, in developing scalable and accurate tools for automated plant taxonomy using venation traits.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayoutGKN: Graph Similarity Learning of Floor Plans</title>
<link>https://arxiv.org/abs/2509.03737</link>
<guid>https://arxiv.org/abs/2509.03737</guid>
<content:encoded><![CDATA[
<div> Keywords: floor plans, graph comparison, LayoutGKN, graph matching networks, efficient approach

Summary:
LayoutGKN is introduced as an efficient method for comparing floor plan graphs, essential for various applications like search, clustering, and data visualization. Unlike graph matching networks, LayoutGKN postpones cross-graph node-level interactions to the end of the joint embedding architecture. This is achieved by using a differentiable graph kernel as a distance function on the final learned node-level embeddings. Despite its streamlined approach, LayoutGKN is shown to compute similarity as effectively as graph matching networks while significantly reducing inference time. The open-source code and data for LayoutGKN are available on GitHub, allowing for further exploration and development in this area. <div>
arXiv:2509.03737v1 Announce Type: new 
Abstract: Floor plans depict building layouts and are often represented as graphs to capture the underlying spatial relationships. Comparison of these graphs is critical for applications like search, clustering, and data visualization. The most successful methods to compare graphs \ie, graph matching networks, rely on costly intermediate cross-graph node-level interactions, therefore being slow in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach that postpones the cross-graph node-level interactions to the end of the joint embedding architecture. We do so by using a differentiable graph kernel as a distance function on the final learned node-level embeddings. We show that LayoutGKN computes similarity comparably or better than graph matching networks while significantly increasing the speed. \href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are open.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singular Value Few-shot Adaptation of Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.03740</link>
<guid>https://arxiv.org/abs/2509.03740</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, CLIP-SVD, Singular Value Decomposition, domain adaptation, parameter-efficient

Summary:
CLIP-SVD is a new adaptation technique for vision-language models, specifically CLIP, that uses Singular Value Decomposition to modify the internal parameter space for domain adaptation without additional modules. This approach allows for fine-tuning only the singular values of the CLIP parameter matrices, enhancing adaptation performance using only 0.04% of the model's total parameters while preserving generalization ability. CLIP-SVD achieves state-of-the-art classification results on natural and biomedical datasets under few-shot settings. The code is available on GitHub for public use. Additionally, a natural language-based approach is used to analyze the effectiveness and dynamics of the CLIP adaptation, providing interpretability for CLIP-SVD.

Summary: <br /><br />CLIP-SVD is a novel adaptation technique for vision-language models like CLIP, using Singular Value Decomposition to modify parameter space for domain adaptation efficiently. The method achieves state-of-the-art results on various datasets and offers interpretability through natural language-based analysis. <div>
arXiv:2509.03740v1 Announce Type: new 
Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and \textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \textbf{0.04\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification</title>
<link>https://arxiv.org/abs/2509.03754</link>
<guid>https://arxiv.org/abs/2509.03754</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Search, Edge devices, Shape-Texture Attention Module, Precision Agriculture, Plant Disease Diagnosis <br />
Summary:<br />
Responding to the increasing need for global food security, leveraging precision agriculture and deep learning-based plant disease diagnosis is essential. However, deploying high-precision models on edge devices poses a challenge. Existing lightweight networks often lack the ability to capture subtle pathological features. To address this issue, a two-step solution is proposed: utilizing a training-free neural architecture search method to design an efficient network backbone for edge devices, and introducing the Shape-Texture Attention Module (STAM). STAM enhances attention by incorporating deformable convolutions and a Gabor filter bank for shape and texture awareness, respectively. The STA-Net model, incorporating STAM, achieved impressive accuracy and F1 scores on the CCMT plant disease dataset. Ablation studies confirmed the significant performance improvements provided by STAM over baseline and standard attention models. This innovative approach of integrating domain knowledge through decoupled attention holds promise for AI-based precision agriculture in edge deployments. <div>
arXiv:2509.03754v1 Announce Type: new 
Abstract: Responding to rising global food security needs, precision agriculture and deep learning-based plant disease diagnosis have become crucial. Yet, deploying high-precision models on edge devices is challenging. Most lightweight networks use attention mechanisms designed for generic object recognition, which poorly capture subtle pathological features like irregular lesion shapes and complex textures. To overcome this, we propose a twofold solution: first, using a training-free neural architecture search method (DeepMAD) to create an efficient network backbone for edge devices; second, introducing the Shape-Texture Attention Module (STAM). STAM splits attention into two branches -- one using deformable convolutions (DCNv4) for shape awareness and the other using a Gabor filter bank for texture awareness. On the public CCMT plant disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs) reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm STAM significantly improves performance over baseline and standard attention models. Integrating domain knowledge via decoupled attention thus presents a promising path for edge-deployed precision agriculture AI. The source code is available at https://github.com/RzMY/STA-Net.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2509.03786</link>
<guid>https://arxiv.org/abs/2509.03786</guid>
<content:encoded><![CDATA[
<div> benchmark, underwater camouflaged object detection, DeepCamo dataset, Semantic Localization and Enhancement Network, SLENet

Summary:
The article introduces the Underwater Camouflaged Object Detection (UCOD) task, crucial for marine ecology, and presents the DeepCamo dataset for this domain. It proposes the Semantic Localization and Enhancement Network (SLENet) framework to address challenges like optical distortions and water turbidity. SLENet incorporates the Gamma-Asymmetric Enhancement (GAE) module and a Localization Guidance Branch (LGB) to enhance feature representation and generate a location map enriched with global semantic information. This map guides the Multi-Scale Supervised Decoder (MSSD) for accurate predictions. Experiments on DeepCamo and benchmark COD datasets demonstrate SLENet's superior performance over state-of-the-art methods, indicating its high generality for the broader COD task.<br /><br />Summary: <div>
arXiv:2509.03786v1 Announce Type: new 
Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that blend seamlessly into underwater environments. This task is critically important to marine ecology. However, it remains largely underexplored and accurate identification is severely hindered by optical distortions, water turbidity, and the complex traits of marine organisms. To address these challenges, we introduce the UCOD task and present DeepCamo, a benchmark dataset designed for this domain. We also propose Semantic Localization and Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE) module and a Localization Guidance Branch (LGB) to enhance multi-scale feature representation while generating a location map enriched with global semantic information. This map guides the Multi-Scale Supervised Decoder (MSSD) to produce more accurate predictions. Experiments on our DeepCamo dataset and three benchmark COD datasets confirm SLENet's superior performance over SOTA methods, and underscore its high generality for the broader COD task.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fitting Image Diffusion Models on Video Datasets</title>
<link>https://arxiv.org/abs/2509.03794</link>
<guid>https://arxiv.org/abs/2509.03794</guid>
<content:encoded><![CDATA[
<div> Keywords: Image diffusion models, temporal inductive bias, video frames, HandCo dataset, generative diversity

Summary:
Image diffusion models are traditionally trained on static images, which limits their ability to capture temporal dynamics. A new training strategy is proposed in this work that utilizes the temporal information present in continuous video frames to enhance diffusion training. This strategy does not require any changes to the model architecture and can seamlessly integrate into existing training pipelines. The method is evaluated on the HandCo dataset, focusing on hand-object interactions with diverse finger articulations. Empirical results show that the proposed approach accelerates convergence, reduces FID on both training and validation distributions, and enhances generative diversity by capturing meaningful temporal variations. An optimization analysis reveals that the regularization employed in this method reduces gradient variance, contributing to faster convergence. Overall, this work presents a simple yet effective way to improve the training of image diffusion models using temporal information. 

<br /><br />Summary: Image diffusion models are enhanced through a new training strategy leveraging temporal information in video frames. The approach accelerates convergence, reduces FID, and increases generative diversity by capturing meaningful temporal variations. Regularization reduces gradient variance, aiding faster convergence. <div>
arXiv:2509.03794v1 Announce Type: new 
Abstract: Image diffusion models are trained on independently sampled static images. While this is the bedrock task protocol in generative modeling, capturing the temporal world through the lens of static snapshots is information-deficient by design. This limitation leads to slower convergence, limited distributional coverage, and reduced generalization. In this work, we propose a simple and effective training strategy that leverages the temporal inductive bias present in continuous video frames to improve diffusion training. Notably, the proposed method requires no architectural modification and can be seamlessly integrated into standard diffusion training pipelines. We evaluate our method on the HandCo dataset, where hand-object interactions exhibit dense temporal coherence and subtle variations in finger articulation often result in semantically distinct motions. Empirically, our method accelerates convergence by over 2$\text{x}$ faster and achieves lower FID on both training and validation distributions. It also improves generative diversity by encouraging the model to capture meaningful temporal variations. We further provide an optimization analysis showing that our regularization reduces the gradient variance, which contributes to faster convergence.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting</title>
<link>https://arxiv.org/abs/2509.03800</link>
<guid>https://arxiv.org/abs/2509.03800</guid>
<content:encoded><![CDATA[
<div> Keywords: Radiologic diagnostic errors, 3D imaging, Vision-language models, Disease detection, Radiology reports<br />
Summary:<br />
Radiologic diagnostic errors, including under-reading errors and communication failures, continue to be prevalent in clinical practice. Existing 3D vision-language models struggle to address these issues, lacking local-global understanding and struggling with report variability. In response, MedVista3D is introduced as a multi-scale semantic-enriched framework for 3D CT analysis. This framework aligns local and global image-text information to improve disease detection and interpretation. By utilizing language model rewrites and a Radiology Semantic Matching Bank, MedVista3D addresses report variability and semantic inconsistencies. The model achieves state-of-the-art performance in zero-shot disease classification, report retrieval, and medical visual question answering tasks. Additionally, it transfers effectively to organ segmentation and prognosis prediction. MedVista3D demonstrates the potential to enhance radiologic diagnosis by improving spatial reasoning, report interpretation, and disease detection accuracy. <div>
arXiv:2509.03800v1 Announce Type: new 
Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality-guided Prompt Learning for Vision-language Models via Visual Granulation</title>
<link>https://arxiv.org/abs/2509.03803</link>
<guid>https://arxiv.org/abs/2509.03803</guid>
<content:encoded><![CDATA[
<div> Keywords: Prompt learning, vision-language models, CLIP, fine-grained datasets, visual granulation<br />
<br />
Summary: 
The paper introduces CaPL, a novel method for prompt learning in vision-language models like CLIP, focusing on improving performance on fine-grained datasets. CaPL utilizes a causality-guided approach to text prompt learning through visual granulation, allowing for capturing subtle differences among classes. The method consists of two key modules: attribute disentanglement, which separates visual features into shared and class-specific attributes, and granule learning, which constructs visual granules for better recognition using causal inference. By leveraging learned visual granules, CaPL enhances the discriminative power of text prompts. Extensive experiments on 15 datasets demonstrate the superiority of CaPL over existing prompt learning methods, particularly on fine-grained datasets. <div>
arXiv:2509.03803v1 Announce Type: new 
Abstract: Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EGTM: Event-guided Efficient Turbulence Mitigation</title>
<link>https://arxiv.org/abs/2509.03808</link>
<guid>https://arxiv.org/abs/2509.03808</guid>
<content:encoded><![CDATA[
<div> Keywords: Turbulence mitigation, Event cameras, Deep learning, Sparse imaging, Real-world dataset

Summary:
The article introduces a novel approach called Event-Guided Turbulence Mitigation (EGTM) that leverages event cameras with high temporal resolution to address the inefficiencies of existing deep-learning methods in turbulence mitigation. By extracting reliable turbulence-free guidance from turbulent events, the EGTM framework significantly outperforms state-of-the-art methods in terms of model size, inference latency, and model complexity. The study also presents the first real-world event-driven TM dataset, contributing to the advancement of research in this field. The experimental results demonstrate a substantial improvement in restoration quality, with a higher PSNR and SSIM compared to existing methods on the EGTM dataset. This work showcases the efficiency and effectiveness of incorporating event cameras into turbulence mitigation tasks. <br /><br />Summary: <div>
arXiv:2509.03808v1 Announce Type: new 
Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and blurs introduced by atmospheric turbulence into frame cameras. Existing state-of-the-art deep-learning TM methods extract turbulence cues from multiple degraded frames to find the so-called "lucky'', not distorted patch, for "lucky fusion''. However, it requires high-capacity network to learn from coarse-grained turbulence dynamics between synchronous frames with limited frame-rate, thus fall short in computational and storage efficiency. Event cameras, with microsecond-level temporal resolution, have the potential to fundamentally address this bottleneck with efficient sparse and asynchronous imaging mechanism. In light of this, we (i) present the fundamental \textbf{``event-lucky insight''} to reveal the correlation between turbulence distortions and inverse spatiotemporal distribution of event streams. Then, build upon this insight, we (ii) propose a novel EGTM framework that extracts pixel-level reliable turbulence-free guidance from the explicit but noisy turbulent events for temporal lucky fusion. Moreover, we (iii) build the first turbulence data acquisition system to contribute the first real-world event-driven TM dataset. Extensive experimental results demonstrate that our approach significantly surpass the existing SOTA TM method by 710 times, 214 times and 224 times in model size, inference latency and model complexity respectively, while achieving the state-of-the-art in restoration quality (+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating the great efficiency merit of introducing event modality into TM task. Demo code and data have been uploaded in supplementary material and will be released once accepted.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection</title>
<link>https://arxiv.org/abs/2509.03872</link>
<guid>https://arxiv.org/abs/2509.03872</guid>
<content:encoded><![CDATA[
<div> adaptive collaborative sparsification, multimodal features, efficient integration, event-guided, cross-modality fusion
Summary: 
FocusMamba introduces adaptive collaborative sparsification of multimodal features by leveraging an Event-Guided Multimodal Sparsification (EGMS) strategy to discard low-information regions within each modality based on scene content changes perceived by the event camera. This approach efficiently integrates complementary features from both modalities using a Cross-Modality Focus Fusion (CMFF) module. Experimental results on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate superior performance in accuracy and efficiency compared to existing methods. The proposed method achieves a better balance between accuracy and efficiency by selectively retaining informative tokens for samples with varying complexity. The code for FocusMamba will be available at the GitHub repository. <div>
arXiv:2509.03872v1 Announce Type: new 
Abstract: Existing RGB-Event detection methods process the low-information regions of both modalities (background in images and non-event regions in event data) uniformly during feature extraction and fusion, resulting in high computational costs and suboptimal performance. To mitigate the computational redundancy during feature extraction, researchers have respectively proposed token sparsification methods for the image and event modalities. However, these methods employ a fixed number or threshold for token selection, hindering the retention of informative tokens for samples with varying complexity. To achieve a better balance between accuracy and efficiency, we propose FocusMamba, which performs adaptive collaborative sparsification of multimodal features and efficiently integrates complementary information. Specifically, an Event-Guided Multimodal Sparsification (EGMS) strategy is designed to identify and adaptively discard low-information regions within each modality by leveraging scene content changes perceived by the event camera. Based on the sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed to effectively capture and integrate complementary features from both modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that the proposed method achieves superior performance in both accuracy and efficiency compared to existing methods. The code will be available at https://github.com/Zizzzzzzz/FocusMamba.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition</title>
<link>https://arxiv.org/abs/2509.03873</link>
<guid>https://arxiv.org/abs/2509.03873</guid>
<content:encoded><![CDATA[
<div> SalientFusion; CZSFR; food recognition; Zero-Shot Learning; Compositional Zero-Shot Food Recognition <br /> 

Summary: <br />
The article introduces the task of Compositional Zero-Shot Food Recognition (CZSFR) to address the challenge of recognizing unseen food categories. It identifies three main challenges in CZSFR: the presence of redundant background information, role confusion between staple and side dishes, and semantic bias in attribute understanding. To tackle these challenges, the authors propose a context-aware method called SalientFusion, consisting of two components: SalientFormer, which removes background redundancy and resolves role confusion, and DebiasAT, which reduces semantic bias by aligning prompts with visual features. The proposed method achieves state-of-the-art results on benchmarks CZSFood-90 and CZSFood-164, as well as popular general datasets for CZSL. The code for SalientFusion is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2509.03873v1 Announce Type: new 
Abstract: Food recognition has gained significant attention, but the rapid emergence of new dishes requires methods for recognizing unseen food categories, motivating Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot Food Recognition (CZSFR), where cuisines and ingredients naturally align with attributes and objects in Compositional Zero-Shot learning (CZSL). However, CZSFR faces three challenges: (1) Redundant background information distracts models from learning meaningful food features, (2) Role confusion between staple and side dishes leads to misclassification, and (3) Semantic bias in a single attribute can lead to confusion of understanding. Therefore, we propose SalientFusion, a context-aware CZSFR method with two components: SalientFormer, which removes background redundancy and uses depth features to resolve role confusion; DebiasAT, which reduces the semantic bias by aligning prompts with visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we show that SalientFusion achieves state-of-the-art results on these benchmarks and the most popular general datasets for the general CZSL. The code is avaliable at https://github.com/Jiajun-RUC/SalientFusion.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Motion Video Generation: A Survey</title>
<link>https://arxiv.org/abs/2509.03883</link>
<guid>https://arxiv.org/abs/2509.03883</guid>
<content:encoded><![CDATA[
<div> Keywords: human motion, video generation, generative process, large language models, technological trends

Summary:
This paper presents a comprehensive survey of human motion video generation, covering over ten sub-tasks and detailing the five key phases of the generative process. It is the first survey to discuss the potential of large language models in enhancing human motion video generation. The survey reviews developments and trends across three primary modalities: vision, text, and audio, encompassing over two hundred papers. The goal of the survey is to uncover the possibilities of human motion video generation and serve as a valuable resource for advancing the applications of digital humans. The survey highlights milestone works that have driven significant technological breakthroughs in the field. A list of the models examined is available in the provided repository. 

<br /><br />Summary: 
- Comprehensive survey of human motion video generation
- Detailing five key phases of the generative process
- Discussion on the potential of large language models 
- Review of developments across vision, text, and audio modalities
- Highlighting milestone works and technological breakthroughs <div>
arXiv:2509.03883v1 Announce Type: new 
Abstract: Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction</title>
<link>https://arxiv.org/abs/2509.03887</link>
<guid>https://arxiv.org/abs/2509.03887</guid>
<content:encoded><![CDATA[
<div> Efficiency, Temporal degradation, Lack of controllability, TENS, TensFormer, Holistic pose aggregation strategy <br />
Summary: 
The paper introduces OccTENS, a generative occupancy world model that addresses challenges in generating high-fidelity long-term occupancy while maintaining computational efficiency. By reframing the model as a temporal next-scale prediction task, OccTENS decomposes spatial and temporal sequence modeling to improve efficiency and combat temporal degradation. Leveraging a TensFormer architecture, OccTENS effectively captures spatial relationships and temporal causality in occupancy sequences. The model introduces a holistic pose aggregation strategy to enhance pose controllability, offering unified sequence modeling for occupancy and ego-motion. Experiments demonstrate that OccTENS outperforms existing methods in both occupancy quality and inference speed. <br /><br /> <div>
arXiv:2509.03887v1 Announce Type: new 
Abstract: In this paper, we propose OccTENS, a generative occupancy world model that enables controllable, high-fidelity long-term occupancy generation while maintaining computational efficiency. Different from visual generation, the occupancy world model must capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models. Recent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from \textbf{inefficiency}, \textbf{temporal degradation} in long-term generation and \textbf{lack of controllability}. To holistically address these issues, we reformulate the occupancy world model as a temporal next-scale prediction (TENS) task, which decomposes the temporal sequence modeling problem into the modeling of spatial scale-by-scale generation and temporal scene-by-scene prediction. With a \textbf{TensFormer}, OccTENS can effectively manage the temporal causality and spatial relationships of occupancy sequences in a flexible and scalable way. To enhance the pose controllability, we further propose a holistic pose aggregation strategy, which features a unified sequence modeling for occupancy and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art method with both higher occupancy quality and faster inference time.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised Learning of Dense Functional Correspondences</title>
<link>https://arxiv.org/abs/2509.03893</link>
<guid>https://arxiv.org/abs/2509.03893</guid>
<content:encoded><![CDATA[
<div> Keywords: dense correspondences, shape reconstruction, robot manipulation, weakly-supervised learning, vision-language models

Summary: 
In this study, the focus is on establishing dense correspondences across images, particularly in the context of matching different categories based on object function. The concept of dense functional correspondence is introduced, utilizing a weakly-supervised learning approach that combines vision-language models and dense contrastive learning. By leveraging pseudo-labeling and multi-view images, the model can predict functional parts and spatial relationships effectively. Synthetic and real evaluation datasets were curated for benchmarking, showcasing the superiority of this approach over baseline solutions. The integration of functional and spatial knowledge leads to improved performance in tasks such as shape reconstruction and robot manipulation. <br /><br />Summary: <div>
arXiv:2509.03893v1 Announce Type: new 
Abstract: Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model</title>
<link>https://arxiv.org/abs/2509.03895</link>
<guid>https://arxiv.org/abs/2509.03895</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive vision-language models, few-shot learning, Attn-Adapter, attention mechanism, adaptability

Summary:
Attn-Adapter is an online few-shot learning framework that enhances CLIP's adaptability through a dual attention mechanism, addressing challenges in few-shot scenarios without the need for computationally intensive offline fine-tuning. The framework consists of two components: the Memory Attn-Adapter refines category embeddings using support examples, while the Local-Global Attn-Adapter enriches image embeddings with local and global features. This architecture allows dynamic adaptation from a few labeled samples without retraining the base model. Attn-Adapter surpasses current methods in cross-category and cross-dataset generalization, maintaining efficient inference and scaling across various CLIP backbone models.<br /><br />Summary: Attn-Adapter introduces a novel approach to few-shot learning in contrastive vision-language models, improving adaptability through a dual attention mechanism. By incorporating dataset-specific information and enhancing image and category embeddings, Attn-Adapter outperforms existing methods in generalization across categories and datasets, demonstrating efficient inference and scalability across different CLIP backbones. <div>
arXiv:2509.03895v1 Announce Type: new 
Abstract: Contrastive vision-language models excel in zero-shot image recognition but face challenges in few-shot scenarios due to computationally intensive offline fine-tuning using prompt learning, which risks overfitting. To overcome these limitations, we propose Attn-Adapter, a novel online few-shot learning framework that enhances CLIP's adaptability via a dual attention mechanism. Our design incorporates dataset-specific information through two components: the Memory Attn-Adapter, which refines category embeddings using support examples, and the Local-Global Attn-Adapter, which enriches image embeddings by integrating local and global features. This architecture enables dynamic adaptation from a few labeled samples without retraining the base model. Attn-Adapter outperforms state-of-the-art methods in cross-category and cross-dataset generalization, maintaining efficient inference and scaling across CLIP backbones.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation</title>
<link>https://arxiv.org/abs/2509.03897</link>
<guid>https://arxiv.org/abs/2509.03897</guid>
<content:encoded><![CDATA[
<div> evaluation metrics, image captioning, Representational Similarity, large language models, specificity<br />
<br />
Summary: 
The article introduces a new reference-free Representational Similarity (RS) metric called SPECS (Specificity-Enhanced CLIPScore) for evaluating long image captions. Traditional n-gram-based metrics are limited in capturing semantic correctness, while metrics based on large language models are expensive for iterative use. SPECS is designed to emphasize specificity, rewarding correct details and penalizing incorrect ones. The metric shows strong correlation with human judgments, matching the performance of LLM-based metrics while being more efficient. SPECS is a practical alternative for evaluating image captioning models during development, providing a valuable tool for researchers in the field. <div>
arXiv:2509.03897v1 Announce Type: new 
Abstract: As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Foundation Model for Chest Radiography</title>
<link>https://arxiv.org/abs/2509.03903</link>
<guid>https://arxiv.org/abs/2509.03903</guid>
<content:encoded><![CDATA[
<div> Keywords: ChexGen, generative vision-language model, chest radiographs, data augmentation, demographic biases <br />
Summary: <br />
The article introduces ChexGen, a generative vision-language model specifically designed for synthesis of chest radiographs using a unified framework. Pretrained on a large dataset of radiograph-report pairs, ChexGen produces accurate radiographs through expert evaluations and quantitative metrics. The model proves useful for data augmentation and supervised pretraining, enhancing performance in disease classification, detection, and segmentation tasks with minimal training data. Additionally, ChexGen aids in creating diverse patient cohorts to address demographic biases in AI models. The study highlights the potential of generative foundation models in healthcare, showcasing their ability to improve accuracy, efficiency, and fairness in medical AI systems. <br /> <div>
arXiv:2509.03903v1 Announce Type: new 
Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for developing reliable AI models in healthcare. Substantial technical advances have been made in generative foundation models for natural images. Here we develop `ChexGen', a generative vision-language foundation model that introduces a unified framework for text-, mask-, and bounding box-guided synthesis of chest radiographs. Built upon the latent diffusion transformer architecture, ChexGen was pretrained on the largest curated chest X-ray dataset to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves accurate synthesis of radiographs through expert evaluations and quantitative metrics. We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Further, our model enables the creation of diverse patient cohorts that enhance model fairness by detecting and mitigating demographic biases. Our study supports the transformative role of generative foundation models in building more accurate, data-efficient, and equitable medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMVC: An End-to-End Learned Multiview Video Coding Framework</title>
<link>https://arxiv.org/abs/2509.03922</link>
<guid>https://arxiv.org/abs/2509.03922</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiview video, end-to-end learned coding, compression efficiency, inter-view motion correlation, inter-view content correlation

Summary:
The paper introduces a novel end-to-end learned multiview video coding (LMVC) framework for efficiently compressing multiview video data. The framework leverages independent-view motion and content information to enhance dependent-view compression, improving compression efficiency. It includes a feature-based inter-view motion vector prediction method and an inter-view motion entropy model to exploit inter-view motion correlation. Additionally, a disparity-free inter-view context prediction module and an inter-view contextual entropy model are proposed to utilize inter-view content correlation. Experimental results demonstrate superior performance compared to traditional MV-HEVC standard reference software, establishing a promising foundation for future research in multiview video coding.<br /><br />Summary: <div>
arXiv:2509.03922v1 Announce Type: new 
Abstract: Multiview video is a key data source for volumetric video, enabling immersive 3D scene reconstruction but posing significant challenges in storage and transmission due to its massive data volume. Recently, deep learning-based end-to-end video coding has achieved great success, yet most focus on single-view or stereo videos, leaving general multiview scenarios underexplored. This paper proposes an end-to-end learned multiview video coding (LMVC) framework that ensures random access and backward compatibility while enhancing compression efficiency. Our key innovation lies in effectively leveraging independent-view motion and content information to enhance dependent-view compression. Specifically, to exploit the inter-view motion correlation, we propose a feature-based inter-view motion vector prediction method that conditions dependent-view motion encoding on decoded independent-view motion features, along with an inter-view motion entropy model that learns inter-view motion priors. To exploit the inter-view content correlation, we propose a disparity-free inter-view context prediction module that predicts inter-view contexts from decoded independent-view content features, combined with an inter-view contextual entropy model that captures inter-view context priors. Experimental results show that our proposed LMVC framework outperforms the reference software of the traditional MV-HEVC standard by a large margin, establishing a strong baseline for future research in this field.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes</title>
<link>https://arxiv.org/abs/2509.03938</link>
<guid>https://arxiv.org/abs/2509.03938</guid>
<content:encoded><![CDATA[
<div> Keywords: tubular structures, topological refinement, 3D reconstruction, persistent homology, anatomical modeling

Summary:
TopoSculpt is a novel framework designed for topological refinement of 3D tubular structures in medical imaging applications. It takes a holistic approach to modeling by considering the entire region in context. The introduction of the Topological Integrity Betti (TIB) constraint helps enforce Betti number priors and global integrity, leading to improved accuracy in geometry and topology. By implementing a curriculum refinement scheme with persistent homology, TopoSculpt can progressively correct errors from coarse to fine scales. Experimental results on pulmonary airway and Circle of Willis datasets show significant enhancements in topological correctness, with a substantial reduction in errors and improved detection rates for various metrics. Overall, TopoSculpt demonstrates its effectiveness in addressing critical topological errors and advancing the accurate modeling of complex 3D tubular anatomy. 

<br /><br />Summary: <div>
arXiv:2509.03938v1 Announce Type: new 
Abstract: Medical tubular anatomical structures are inherently three-dimensional conduits with lumens, enclosing walls, and complex branching topologies. Accurate reconstruction of their geometry and topology is crucial for applications such as bronchoscopic navigation and cerebral arterial connectivity assessment. Existing methods often rely on voxel-wise overlap measures, which fail to capture topological correctness and completeness. Although topology-aware losses and persistent homology constraints have shown promise, they are usually applied patch-wise and cannot guarantee global preservation or correct geometric errors at inference. To address these limitations, we propose a novel TopoSculpt, a framework for topological refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a holistic whole-region modeling strategy to capture full spatial context, (ii) first introduces a Topological Integrity Betti (TIB) constraint that jointly enforces Betti number priors and global integrity, and (iii) employs a curriculum refinement scheme with persistent homology to progressively correct errors from coarse to fine scales. Extensive experiments on challenging pulmonary airway and Circle of Willis datasets demonstrate substantial improvements in both geometry and topology. For instance, $\beta_{0}$ errors are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on the CoW dataset, with Tree length detected and branch detected rates improving by nearly 10\%. These results highlight the effectiveness of TopoSculpt in correcting critical topological errors and advancing the high-fidelity modeling of complex 3D tubular anatomy. The project homepage is available at: https://github.com/Puzzled-Hui/TopoSculpt.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture</title>
<link>https://arxiv.org/abs/2509.03950</link>
<guid>https://arxiv.org/abs/2509.03950</guid>
<content:encoded><![CDATA[
<div> segmentation, pneumothorax, deep learning, U-Net, EfficientNet-B4

Summary:
An automated deep-learning pipeline utilizing a U-Net model with an EfficientNet-B4 encoder was developed for the segmentation of pneumothorax regions on chest X-rays. The model, trained on the SIIM-ACR dataset with data augmentation and a combined loss function, achieved a high IoU of 0.7008 and Dice score of 0.8241 on the independent PTX-498 dataset. These results showcase the model's ability to accurately localize pneumothoraces, potentially aiding radiologists in detecting subtle cases that could be life-threatening if left undetected. The use of deep learning in medical imaging, particularly in the context of pneumothorax detection, holds promise for improving diagnostic accuracy and patient outcomes. <div>
arXiv:2509.03950v1 Announce Type: new 
Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be life-threatening if undetected. Chest X-rays are the first-line diagnostic tool, but small cases may be subtle. We propose an automated deep-learning pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax regions. Trained on the SIIM-ACR dataset with data augmentation and a combined binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and Dice score of 0.8241 on the independent PTX-498 dataset. These results demonstrate that the model can accurately localize pneumothoraces and support radiologists.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection</title>
<link>https://arxiv.org/abs/2509.03951</link>
<guid>https://arxiv.org/abs/2509.03951</guid>
<content:encoded><![CDATA[
<div> Keywords: Negative labels, Out-of-Distribution detection, Multimodal large language models, Near-OOD performance, ImageNet benchmark

Summary:
The paper introduces a new method called Adaptive Negative Textual Space (ANTS) for Out-of-Distribution (OOD) detection. By utilizing multimodal large language models (MLLMs), the proposed method shapes a negative textual space by generating descriptive negative sentences for OOD images, improving far-OOD detection. For near-OOD scenarios, where OOD samples resemble in-distribution (ID) classes, ANTS identifies visually similar ID classes and generates tailored negative labels to reduce false negatives and enhance detection. An adaptive weighted score is designed to balance near-OOD and far-OOD settings without prior knowledge, leading to improved adaptability in open environments. On the ImageNet benchmark, ANTS decreases the FPR95 by 4.2% without the need for training, establishing a new state-of-the-art in OOD detection. This zero-shot and highly scalable method showcases the effectiveness of leveraging MLLMs in shaping an adaptive negative textual space. 

<br /><br />Summary: <div>
arXiv:2509.03951v1 Announce Type: new 
Abstract: The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. In addition, the presence of false negative labels significantly degrades their near-OOD performance. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we identify images likely to be OOD samples as negative images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we first identify the subset of ID classes that are visually similar to negative images and then leverage the reasoning capability of MLLMs to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD) without relying on task-specific prior knowledge, making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2509.03961</link>
<guid>https://arxiv.org/abs/2509.03961</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, remote sensing, change detection, multimodal, image-text fusion

Summary:
MMChange is a new multimodal remote sensing change detection method that combines image and text modalities to improve accuracy and robustness. It introduces an Image Feature Refinement module to enhance key regions and mitigate noise, as well as a vision language model to provide semantic descriptions of images. A Textual Difference Enhancement module captures fine-grained semantic shifts, guiding the model towards meaningful changes. To integrate features from both modalities, an Image Text Feature Fusion module is designed for deep cross-modal integration. Experimental results on multiple datasets show that MMChange outperforms existing methods in terms of accuracy and robustness, demonstrating its effectiveness for multimodal remote sensing change detection. The code for MMChange is available on GitHub for further exploration. 

Summary: <br /><br />MMChange is a novel multimodal remote sensing change detection method that leverages image and text modalities. It incorporates modules for image feature refinement, vision language modeling, textual difference enhancement, and image-text feature fusion. Experimental results on various datasets demonstrate its superior performance compared to existing methods, confirming its capability in accurate and robust change detection in remote sensing applications. The source code for MMChange is publicly available on GitHub for further research and development. <div>
arXiv:2509.03961v1 Announce Type: new 
Abstract: Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2509.03973</link>
<guid>https://arxiv.org/abs/2509.03973</guid>
<content:encoded><![CDATA[
<div> Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL), WSI classification, positional encoding module, SAC block, instance correlations <br />
<br />
SAC-MIL proposes a novel approach for WSI classification by incorporating a positional encoding module and a SAC block. The positional encoding module encodes spatial relationships using instance coordinates, addressing length extrapolation issues in training and testing sequences. The SAC block utilizes an MLP-based method for full instance correlation with linear time complexity. This simple structure allows for easy deployment without requiring custom CUDA kernels, unlike Transformer-based methods. SAC-MIL achieves state-of-the-art performance on the CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be made available upon acceptance. <br />
<br />Summary: <div>
arXiv:2509.03973v1 Announce Type: new 
Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for performing WSI classification. SAC-MIL consists of a positional encoding module to encode position information and a SAC block to perform full instance correlations. The positional encoding module utilizes the instance coordinates within the slide to encode the spatial relationships instead of the instance index in the input WSI sequence. The positional encoding module can also handle the length extrapolation issue where the training and testing sequences have different lengths. The SAC block is an MLP-based method that performs full instance correlation in linear time complexity with respect to the sequence length. Due to the simple structure of MLP, it is easy to deploy since it does not require custom CUDA kernels, compared to Transformer-based methods for WSI classification. SAC-MIL has achieved state-of-the-art performance on the CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training</title>
<link>https://arxiv.org/abs/2509.03975</link>
<guid>https://arxiv.org/abs/2509.03975</guid>
<content:encoded><![CDATA[
<div> Keywords: liver vessel segmentation, magnetic resonance imaging, multi-task learning, contrast-enhanced imaging, annotated data

Summary: 
Liver vessel segmentation in MRI data is crucial for analyzing vascular changes in liver diseases. Current methods rely on contrast-enhanced images, which are not always available. A new multi-task learning framework has been proposed to segment vessels in liver MRI without contrast by leveraging auxiliary contrast-enhanced data during training. This approach reduces the need for annotated examples and enhances segmentation accuracy, even without auxiliary data during inference. The framework benefits from shared task structures and can improve segmentation accuracy, especially with limited annotations. This technique has also shown positive results in brain tumor segmentation, demonstrating its versatility across different medical imaging domains. Utilizing auxiliary informative imaging modalities during training can enhance expert annotations and improve segmentation results. 

<br /><br />Summary: <div>
arXiv:2509.03975v1 Announce Type: new 
Abstract: Liver vessel segmentation in magnetic resonance imaging data is important for the computational analysis of vascular remodelling, associated with a wide spectrum of diffuse liver diseases. Existing approaches rely on contrast enhanced imaging data, but the necessary dedicated imaging sequences are not uniformly acquired. Images without contrast enhancement are acquired more frequently, but vessel segmentation is challenging, and requires large-scale annotated data. We propose a multi-task learning framework to segment vessels in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data available only during training to reduce the need for annotated training examples. Our approach draws on paired native and contrast enhanced data with and without vessel annotations for model training. Results show that auxiliary data improves the accuracy of vessel segmentation, even if they are not available during inference. The advantage is most pronounced if only few annotations are available for training, since the feature representation benefits from the shared task structure. A validation of this approach to augment a model for brain tumor segmentation confirms its benefits across different domains. An auxiliary informative imaging modality can augment expert annotations even if it is only available during training.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promptception: How Sensitive Are Large Multimodal Models to Prompts?</title>
<link>https://arxiv.org/abs/2509.03986</link>
<guid>https://arxiv.org/abs/2509.03986</guid>
<content:encoded><![CDATA[
<div> sensitivity, prompt, LMMs, Multiple-Choice Question Answering, evaluation <br />
<br />
Prompt design for Large Multimodal Models (LMMs) in Multiple-Choice Question Answering (MCQA) is crucial yet not well understood. Minor variations in prompt phrasing can significantly impact the accuracy of models, up to 15%. This variability challenges transparent and fair evaluation as models tend to showcase their best performance with selective prompts. The Promptception framework, with 61 prompt types in various categories and supercategories, evaluates the sensitivity of 10 LMMs across 3 MCQA benchmarks. Proprietary models display higher sensitivity to prompt phrasing, while open-source models are more consistent but struggle with complex phrasing. This analysis leads to the proposal of Prompting Principles tailored for both proprietary and open-source LMMs, aiming to improve robustness and fairness in model evaluation. <br /><br />Summary: <div>
arXiv:2509.03986v1 Announce Type: new 
Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly understood. We show that even minor variations in prompt phrasing and structure can lead to accuracy deviations of up to 15% for certain prompts and models. This variability poses a challenge for transparent and fair LMM evaluation, as models often report their best-case performance using carefully selected prompts. To address this, we introduce Promptception, a systematic framework for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types, spanning 15 categories and 6 supercategories, each targeting specific aspects of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks: MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit greater sensitivity to prompt phrasing, reflecting tighter alignment with instruction semantics, while open-source models are steadier but struggle with nuanced and complex phrasing. Based on this analysis, we propose Prompting Principles tailored to proprietary and open-source LMMs, enabling more robust and fair model evaluation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation</title>
<link>https://arxiv.org/abs/2509.03999</link>
<guid>https://arxiv.org/abs/2509.03999</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, 3D perception, semantic occupancy prediction, vertical slice, channel attention<br />
<br />
Summary: <br />
Driven by the demands of autonomous driving, 3D semantic occupancy prediction has gained importance. While traditional methods focus on 2D representations, this study utilizes a 3D voxel grid to capture semantic variations along the vertical axis. The proposed SliceSemOcc framework extracts voxel features along the height axis using global and local vertical slices, enabling a fusion of spatial details and contextual information. The SEAttention3D module assigns dynamic channel attention weights to each height layer, enhancing feature emphasis at different heights. Experiment results on nuScenes-SurroundOcc and nuScenes-OpenOccupancy datasets show improved mean IoU, especially for small-object categories, validating the effectiveness of the novel framework. <div>
arXiv:2509.03999v1 Announce Type: new 
Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic occupancy prediction has become a pivotal research topic. Unlike bird's-eye-view (BEV) methods, which restrict scene representation to a 2D plane, occupancy prediction leverages a complete 3D voxel grid to model spatial structures in all dimensions, thereby capturing semantic variations along the vertical axis. However, most existing approaches overlook height-axis information when processing voxel features. And conventional SENet-style channel attention assigns uniform weight across all height layers, limiting their ability to emphasize features at different heights. To address these limitations, we propose SliceSemOcc, a novel vertical slice based multimodal framework for 3D semantic occupancy representation. Specifically, we extract voxel features along the height-axis using both global and local vertical slices. Then, a global local fusion module adaptively reconciles fine-grained spatial details with holistic contextual information. Furthermore, we propose the SEAttention3D module, which preserves height-wise resolution through average pooling and assigns dynamic channel attention weights to each height layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy datasets verify that our method significantly enhances mean IoU, achieving especially pronounced gains on most small-object categories. Detailed ablation studies further validate the effectiveness of the proposed SliceSemOcc framework.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding</title>
<link>https://arxiv.org/abs/2509.04009</link>
<guid>https://arxiv.org/abs/2509.04009</guid>
<content:encoded><![CDATA[
<div> spurious correlations, neural network, computer vision, detection, ImageNet<br />
<br />
Summary: <br />
Neural network models in computer vision can unintentionally rely on spurious correlations in data, resulting in incorrect predictions based on irrelevant signals. A novel method is proposed to detect such spurious correlations in vision transformers, with experiments on ImageNet dataset showing effectiveness. The training methodology significantly influences model reliance on spurious correlations. Certain ImageNet classes contain easily detectable spurious signals. Caution is advised in using images with spurious signals in research. A case study on invasive breast mass classification illustrates the impact of spurious signals in real-world scenarios. <div>
arXiv:2509.04009v1 Announce Type: new 
Abstract: Due to their powerful feature association capabilities, neural network-based computer vision models have the ability to detect and exploit unintended patterns within the data, potentially leading to correct predictions based on incorrect or unintended but statistically relevant signals. These clues may vary from simple color aberrations to small texts within the image. In situations where these unintended signals align with the predictive task, models can mistakenly link these features with the task and rely on them for making predictions. This phenomenon is referred to as spurious correlations, where patterns appear to be associated with the task but are actually coincidental. As a result, detection and mitigation of spurious correlations have become crucial tasks for building trustworthy, reliable, and generalizable machine learning models. In this work, we present a novel method to detect spurious correlations in vision transformers, a type of neural network architecture that gained significant popularity in recent years. Using both supervised and self-supervised trained models, we present large-scale experiments on the ImageNet dataset demonstrating the ability of the proposed method to identify spurious correlations. We also find that, even if the same architecture is used, the training methodology has a significant impact on the model's reliance on spurious correlations. Furthermore, we show that certain classes in the ImageNet dataset contain spurious signals that are easily detected by the models and discuss the underlying reasons for those spurious signals. In light of our findings, we provide an exhaustive list of the aforementioned images and call for caution in their use in future research efforts. Lastly, we present a case study investigating spurious signals in invasive breast mass classification, grounding our work in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning</title>
<link>https://arxiv.org/abs/2509.04023</link>
<guid>https://arxiv.org/abs/2509.04023</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-class Multiple-Instance Learning, Learning from Majority Label, Counting Network, Majority Proportion Enhancement Module, Environmental Monitoring

Summary: <br /><br />The paper introduces a new multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML), where the majority class of instances in a bag is assigned as the bag-level label. A Counting Network is proposed to address LML by estimating bag-level majority labels through class instance counting. Experiment analysis reveals that bags with a higher proportion of the majority class aid in learning, leading to the development of the Majority Proportion Enhancement Module (MPEM) to enhance the majority class proportion by removing minority class instances. The proposed method demonstrates superior performance on four datasets compared to conventional MIL methods, with ablation studies validating the effectiveness of each module. The code for the method is available online. <div>
arXiv:2509.04023v1 Announce Type: new 
Abstract: The paper proposes a novel multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag-level label. The goal of LML is to train a classification model that estimates the class of each instance using the majority label. This problem is valuable in a variety of applications, including pathology image segmentation, political voting prediction, customer sentiment analysis, and environmental monitoring. To solve LML, we propose a Counting Network trained to produce bag-level majority labels, estimated by counting the number of instances in each class. Furthermore, analysis experiments on the characteristics of LML revealed that bags with a high proportion of the majority class facilitate learning. Based on this result, we developed a Majority Proportion Enhancement Module (MPEM) that increases the proportion of the majority class by removing minority class instances within the bags. Experiments demonstrate the superiority of the proposed method on four datasets compared to conventional MIL methods. Moreover, ablation studies confirmed the effectiveness of each module. The code is available at \href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"</title>
<link>https://arxiv.org/abs/2509.04043</link>
<guid>https://arxiv.org/abs/2509.04043</guid>
<content:encoded><![CDATA[
<div> Phytium processors, Cambricon accelerator cards, heterogeneous computing architecture, UAV tracking, real-time video surveillance<br />
<br />
Summary: <br />
This study introduces a novel heterogeneous computing architecture using Phytium processors and Cambricon accelerator cards for real-time UAV tracking and gazing in video surveillance. The system combines Phytium FT-2000/4 processors and MLU220 accelerator cards for enhanced computing power through multi-card parallelism. It integrates a lightweight YOLOv5s detection network with a DeepSORT cascaded tracking algorithm to achieve millisecond-level response capability in dynamic scenarios. Experimental results show a single-frame processing delay of 50-100 ms with over 98.5% target recognition accuracy in 1920*1080 resolution videos. The system provides a high-precision, low-latency solution for UAV monitoring and showcases the application of domestic chips. <div>
arXiv:2509.04043v1 Announce Type: new 
Abstract: In the frontier research and application of current video surveillance technology, traditional camera systems exhibit significant limitations of response delay exceeding 200 ms in dynamic scenarios due to the insufficient deep feature extraction capability of automatic recognition algorithms and the efficiency bottleneck of computing architectures, failing to meet the real-time requirements in complex scenes. To address this issue, this study proposes a heterogeneous computing architecture based on Phytium processors and Cambricon accelerator cards, constructing a UAV tracking and gazing system with millisecond-level response capability. At the hardware level, the system adopts a collaborative computing architecture of Phytium FT-2000/4 processors and MLU220 accelerator cards, enhancing computing power through multi-card parallelism. At the software level, it innovatively integrates a lightweight YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming a closed-loop control chain of "detection-tracking-feedback". Experimental results demonstrate that the system achieves a stable single-frame comprehensive processing delay of 50-100 ms in 1920*1080 resolution video stream processing, with a multi-scale target recognition accuracy of over 98.5%, featuring both low latency and high precision. This study provides an innovative solution for UAV monitoring and the application of domestic chips.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification</title>
<link>https://arxiv.org/abs/2509.04050</link>
<guid>https://arxiv.org/abs/2509.04050</guid>
<content:encoded><![CDATA[
<div> re-identification, re-ranking, multi-view features, K-nearest Weighted Fusion, person representation
Summary:
- The study focuses on enhancing person re-identification accuracy through re-ranking using multi-view features to reduce view bias.
- The proposed re-ranking method utilizes the K-nearest Weighted Fusion approach to aggregate neighbors' features for generating multi-view features.
- Features extracted from re-identification models representing the same identity are selected in an unsupervised manner to create multi-view features, without requiring model fine-tuning or extra annotations.
- The method significantly improves Rank@1 and mAP on datasets like Market1501, MSMT17, and Occluded-DukeMTMC, with improvements of 9.8%/22.0% in Rank@1 achieved on challenging datasets.
- The approach also demonstrates substantial enhancements in computational efficiency compared to other re-ranking methods.
<br /><br />Summary: <div>
arXiv:2509.04050v1 Announce Type: new 
Abstract: In person re-identification, re-ranking is a crucial step to enhance the overall accuracy by refining the initial ranking of retrieved results. Previous studies have mainly focused on features from single-view images, which can cause view bias and issues like pose variation, viewpoint changes, and occlusions. Using multi-view features to present a person can help reduce view bias. In this work, we present an efficient re-ranking method that generates multi-view features by aggregating neighbors' features using K-nearest Weighted Fusion (KWF) method. Specifically, we hypothesize that features extracted from re-identification models are highly similar when representing the same identity. Thus, we select K neighboring features in an unsupervised manner to generate multi-view features. Additionally, this study explores the weight selection strategies during feature aggregation, allowing us to identify an effective strategy. Our re-ranking approach does not require model fine-tuning or extra annotations, making it applicable to large-scale datasets. We evaluate our method on the person re-identification datasets Market1501, MSMT17, and Occluded-DukeMTMC. The results show that our method significantly improves Rank@1 and mAP when re-ranking the top M candidates from the initial ranking results. Specifically, compared to the initial results, our re-ranking method achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets: MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach demonstrates substantial enhancements in computational efficiency compared to other re-ranking methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</title>
<link>https://arxiv.org/abs/2509.04086</link>
<guid>https://arxiv.org/abs/2509.04086</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio-Visual Video Parsing, Weakly Supervised Learning, Bi-Directional Text Fusion, Category-Aware Temporal Graph, State-of-the-Art Performance

Summary:
This paper introduces a method for Audio-Visual Video Parsing (AVVP) with weakly supervised labels. The proposed approach combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module to address the limitations of existing methods. By leveraging the strengths of both modules, the method injects semantic information into audio and visual features and propagates this information across time for improved temporal modeling. This process helps in locating cleaner semantic cues and refining pseudo-labels for better training. Experimental results on benchmark datasets show that the proposed method achieves state-of-the-art performance in identifying event categories and occurrence times in videos. The combination of BiT and CATS modules demonstrates significant advancements in AVVP tasks, showcasing the potential of integrating multiple research directions for improved results. 

<br /><br />Summary: <div>
arXiv:2509.04086v1 Announce Type: new 
Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and their occurrence times in a given video with weakly supervised labels. Existing methods typically fall into two categories: (i) designing enhanced architectures based on attention mechanism for better temporal modeling, and (ii) generating richer pseudo-labels to compensate for the absence of frame-level annotations. However, the first type methods treat noisy segment-level pseudo labels as reliable supervision and the second type methods let indiscriminate attention spread them across all frames, the initial errors are repeatedly amplified during training. To address this issue, we propose a method that combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the strengths and complementarity of the two previous research directions. We first perform semantic injection and dynamic calibration on audio and visual modality features through the BiT module, to locate and purify cleaner and richer semantic cues. Then, we leverage the CATS module for semantic propagation and connection to enable precise semantic information dissemination across time. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators on two benchmark datasets, LLP and UnAV-100.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriLiteNet: Lightweight Model for Multi-Task Visual Perception</title>
<link>https://arxiv.org/abs/2509.04092</link>
<guid>https://arxiv.org/abs/2509.04092</guid>
<content:encoded><![CDATA[
<div> Keywords: TriLiteNet, perception models, Advanced Driver Assistance Systems, real-time execution, multi-task solution

Summary: 
Efficient perception models are crucial for Advanced Driver Assistance Systems (ADAS) to ensure safety and effectiveness in real-world environments. The TriLiteNet model introduced in this study can manage multiple tasks related to panoramic driving perception in real time. It optimizes performance while keeping computational costs low. Experimental results on the BDD100k dataset show competitive performance in vehicle detection, drivable area segmentation, and lane line segmentation. The TriLiteNet_{base} achieved high recall for vehicle detection, mean Intersection over Union (mIoU) for drivable area segmentation, and accuracy for lane line segmentation with low parameters and computational cost. The tiny configuration of the model provides a multi-task solution with minimal computational demand. TriLiteNet also demonstrates low latency and reasonable power consumption on embedded devices. This model offers a practical and deployable solution for real-world autonomous driving applications. <br /><br />Summary: <div>
arXiv:2509.04092v1 Announce Type: new 
Abstract: Efficient perception models are essential for Advanced Driver Assistance Systems (ADAS), as these applications require rapid processing and response to ensure safety and effectiveness in real-world environments. To address the real-time execution needs of such perception models, this study introduces the TriLiteNet model. This model can simultaneously manage multiple tasks related to panoramic driving perception. TriLiteNet is designed to optimize performance while maintaining low computational costs. Experimental results on the BDD100k dataset demonstrate that the model achieves competitive performance across three key tasks: vehicle detection, drivable area segmentation, and lane line segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of 85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for drivable area segmentation, and an Acc of 82.3% for lane line segmentation with only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed model includes a tiny configuration with just 0.14M parameters, which provides a multi-task solution with minimal computational demand. Evaluated for latency and power consumption on embedded devices, TriLiteNet in both configurations shows low latency and reasonable power during inference. By balancing performance, computational efficiency, and scalability, TriLiteNet offers a practical and deployable solution for real-world autonomous driving applications. Code is available at https://github.com/chequanghuy/TriLiteNet.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset</title>
<link>https://arxiv.org/abs/2509.04117</link>
<guid>https://arxiv.org/abs/2509.04117</guid>
<content:encoded><![CDATA[
<div> Keywords: Event cameras, Dynamic Vision Sensors, DVS-PedX, pedestrian detection, spiking neural networks<br />
Summary:<br />
Event cameras, such as Dynamic Vision Sensors (DVS), offer advantages like low latency and high dynamic range by reporting micro-timed brightness changes instead of full frames. The DVS-PedX dataset is designed for pedestrian detection and crossing-intention analysis in various weather conditions. It includes synthetic event streams from the CARLA simulator and real-world JAAD dash-cam videos converted to event streams. Each sequence in the dataset contains RGB frames, DVS "event frames," and frame-level labels for crossing or not crossing. Raw event files and video files are also provided for flexible processing. Baseline spiking neural networks using SpikingJelly demonstrate the dataset's usability but highlight a gap between simulation and reality, suggesting the need for domain adaptation and multimodal fusion. Overall, DVS-PedX aims to advance research in event-based pedestrian safety, intention prediction, and neuromorphic perception. <br /> <div>
arXiv:2509.04117v1 Announce Type: new 
Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness changes instead of full frames, offering low latency, high dynamic range, and motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a neuromorphic dataset designed for pedestrian detection and crossing-intention analysis in normal and adverse weather conditions across two complementary sources: (1) synthetic event streams generated in the CARLA simulator for controlled "approach-cross" scenes under varied weather and lighting; and (2) real-world JAAD dash-cam videos converted to event streams using the v2e tool, preserving natural behaviors and backgrounds. Each sequence includes paired RGB frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0 event files and AVI DVS video files and metadata for flexible re-processing. Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset usability and reveal a sim-to-real gap, motivating domain adaptation and multimodal fusion. DVS-PedX aims to accelerate research in event-based pedestrian safety, intention prediction, and neuromorphic perception.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering</title>
<link>https://arxiv.org/abs/2509.04123</link>
<guid>https://arxiv.org/abs/2509.04123</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-story visualization, character consistency, dialogue rendering, iterative process, identity-consistent self-attention 

Summary:
TaleDiffusion is a new framework for generating multi-character stories with a focus on maintaining character consistency and accurate dialogue rendering. It uses a pre-trained LLM to generate per-frame descriptions, character details, and dialogues, followed by a technique to control character interactions and minimize artifacts. An identity-consistent self-attention mechanism ensures consistency across frames, and region-aware cross-attention helps with precise object placement. Dialogues are rendered as bubbles and assigned to characters using CLIPSeg. Experimental results show that TaleDiffusion performs better than existing methods in terms of consistency, noise reduction, and dialogue rendering. This framework addresses the challenges of text-to-story visualization by emphasizing character interactions, accurate dialogue assignment, and maintaining a cohesive storytelling experience. It shows significant improvements in generating coherent and visually appealing multi-character narratives. 

<br /><br />Summary: <div>
arXiv:2509.04123v1 Announce Type: new 
Abstract: Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation</title>
<link>https://arxiv.org/abs/2509.04126</link>
<guid>https://arxiv.org/abs/2509.04126</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-image diffusion models, Multi-Expert Planning and Generation Framework, large language models, spatial-semantic expert modules, image quality and style diversity

Summary: 
The article introduces the Multi-Expert Planning and Generation Framework (MEPG) to enhance text-to-image diffusion models. It combines position- and style-aware large language models with spatial-semantic expert modules. The framework consists of a Position-Style-Aware (PSA) module for precise decomposition of input prompts and a Multi-Expert Diffusion (MED) module for cross-region generation through expert routing. During generation, specialized models are activated based on spatial partitioning. The architecture allows for easy integration and replacement of expert models and supports real-time spatial layout editing and style selection. Experimental results demonstrate that MEPG outperforms baseline models in both image quality and style diversity. <div>
arXiv:2509.04126v1 Announce Type: new 
Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality
  and style diversity.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Simple Baselines for In-The-Wild Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.04150</link>
<guid>https://arxiv.org/abs/2509.04150</guid>
<content:encoded><![CDATA[
<div> deepfake detectors, benchmark, Deepfake-Eval-2024, performance, commercial, code
Summary:<br /><br />Researchers focus on evaluating deepfake detectors using the "in-the-wild" benchmark, Deepfake-Eval-2024. They revisit a baseline approach, achieving 81% accuracy by tuning hyperparameters, surpassing previous results by 18% and competing with commercial detectors. The study explores tradeoffs in accuracy, computational costs, and interpretability, considering practical deployment in real-world settings. The code for the improved deepfake detector is available at https://github.com/Deepfake-Detection-KKO/deepfake-detection. <div>
arXiv:2509.04150v1 Announce Type: new 
Abstract: The widespread adoption of synthetic media demands accessible deepfake detectors and realistic benchmarks. While most existing research evaluates deepfake detectors on highly controlled datasets, we focus on the recently released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on Deepfake-Eval-2024 showed that three finetuned open-source models achieve accuracies between 61% and 69%, significantly lagging behind the leading commercial deepfake detector with 82% accuracy. Our work revisits one of these baseline approaches, originally introduced by Ojha et al., which adapts standard pretrained vision backbones to produce generalizable deepfake detectors. We demonstrate that with better-tuned hyperparameters, this simple approach actually yields much higher performance -- 81% accuracy on Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this baseline approach by 18% and competing with commercial deepfake detectors. We discuss tradeoffs in accuracy, computational costs, and interpretability, focusing on how practical these deepfake detectors might be when deployed in real-world settings. Our code can be found at https://github.com/Deepfake-Detection-KKO/deepfake-detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components</title>
<link>https://arxiv.org/abs/2509.04156</link>
<guid>https://arxiv.org/abs/2509.04156</guid>
<content:encoded><![CDATA[
<div> Keywords: UAVs, defect detection, wind power plants, YOLO-based deep learning models, multispectral imagery

Summary: 
Unmanned aerial vehicles (UAVs) are being used to monitor wind power plants, including critical components like blades and towers, using advanced sensors. To improve defect detection accuracy, a research study developed an ensemble of YOLO-based deep learning models that integrated visible and thermal channels. The proposed ensemble approach combined a general-purpose YOLOv8 model with a specialized thermal model and utilized a bounding box fusion algorithm to merge their predictions. Experiments demonstrated that this approach outperformed a standalone YOLOv8 model, achieving a mean Average Precision of 0.93 and an F1-score of 0.90. By combining multiple YOLO architectures with fused multispectral data, the detection of visual and thermal defects in wind power plants was significantly improved. 

<br /><br />Summary: <div>
arXiv:2509.04156v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up new opportunities for monitoring wind power plants, including blades, towers, and other critical components. However, reliable defect detection requires high-resolution data and efficient methods to process multispectral imagery. In this research, we aim to enhance defect detection accuracy through the development of an ensemble of YOLO-based deep learning models that integrate both visible and thermal channels. We propose an ensemble approach that integrates a general-purpose YOLOv8 model with a specialized thermal model, using a sophisticated bounding box fusion algorithm to combine their predictions. Our experiments show this approach achieves a mean Average Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that combining multiple YOLO architectures with fused multispectral data provides a more reliable solution, improving the detection of both visual and thermal defects.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</title>
<link>https://arxiv.org/abs/2509.04180</link>
<guid>https://arxiv.org/abs/2509.04180</guid>
<content:encoded><![CDATA[
<div> AI models, annotation, image labeling, automation, VisioFirm. 
<br /> 
<br />Summary: 
VisioFirm is a new web application that simplifies image labeling by using AI-assisted automation. It integrates state-of-the-art foundation models to reduce the manual input required for annotation. By combining CLIP and pre-trained detectors like Ultralytics models, VisioFirm can generate initial annotations with low-confidence thresholding for common and custom labels. Users can interactively refine these annotations using tools supporting various formats such as bounding boxes, oriented bounding boxes, and polygons. The tool also offers on-the-fly segmentation powered by Segment Anything, making it efficient for browser-side processing. VisioFirm supports multiple export formats and can operate offline after model caching, improving accessibility. Through benchmarks on diverse datasets, VisioFirm has shown up to a 90% reduction in manual effort while maintaining high annotation accuracy through clustering and redundant detection suppression techniques. Access VisioFirm from <a href="https://github.com/OschAI/VisioFirm">here</a>. <div>
arXiv:2509.04180v1 Announce Type: new 
Abstract: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval</title>
<link>https://arxiv.org/abs/2509.04193</link>
<guid>https://arxiv.org/abs/2509.04193</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised cross-domain image retrieval, feature disentanglement, object features, domain gap, text-to-image generative model  
Summary:  
- Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images without annotations across diverse domains.  
- Existing UCIR methods struggle with the domain gap, where object features are entangled with domain-specific styles.  
- DUDE is a novel UCIR method that utilizes feature disentanglement to separate object features from domain-specific styles for better retrieval performance.  
- DUDE leverages a text-to-image generative model to achieve feature disentanglement.  
- The method aligns mutual neighbors within and across domains in a progressive manner to ensure reliable alignment of the disentangled features.  
<br /><br />Summary: <div>
arXiv:2509.04193v1 Announce Type: new 
Abstract: Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of the same category across diverse domains without relying on annotations. Existing UCIR methods, which align cross-domain features for the entire image, often struggle with the domain gap, as the object features critical for retrieval are frequently entangled with domain-specific styles. To address this challenge, we propose DUDE, a novel UCIR method building upon feature disentanglement. In brief, DUDE leverages a text-to-image generative model to disentangle object features from domain-specific styles, thus facilitating semantical image retrieval. To further achieve reliable alignment of the disentangled object features, DUDE aligns mutual neighbors from within domains to across domains in a progressive manner. Extensive experiments demonstrate that DUDE achieves state-of-the-art performance across three benchmark datasets over 13 domains. The code will be released.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding</title>
<link>https://arxiv.org/abs/2509.04243</link>
<guid>https://arxiv.org/abs/2509.04243</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, LASER, active perception capabilities, coordinate prediction, IoU-based region quality evaluation

Summary:<br />
Vision Language Models (VLMs) have shown remarkable progress in integrating visual perception and linguistic reasoning. The LASER framework proposed in this work aims to enhance VLMs' ability to reason over appropriate image regions, particularly in complex visual interactions. By combining Monte Carlo quality estimation with IoU-based region quality evaluation, LASER guides VLMs to focus on instruction-relevant key regions and adaptively allocate reasoning steps based on task complexity. Experimental results on ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrate consistent performance improvements. When fine-tuned on the GTA1-7B dataset, LASER achieves a new state-of-the-art score of 55.7 on the ScreenSpot-Pro benchmark among 7B-scale models. This approach effectively enables VLMs to enhance their multi-step perception capabilities, leading to more precise coordinate predictions and improved downstream task performance. 

<br /><br />Summary: <div>
arXiv:2509.04243v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) have recently achieved significant progress in bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model introduced a zoom-in search strategy that effectively elicits active perception capabilities in VLMs, improving downstream task performance. However, enabling VLMs to reason effectively over appropriate image regions remains a core challenge in GUI grounding, particularly under high-resolution inputs and complex multi-element visual interactions. In this work, we propose LASER, a self-evolving framework that progressively endows VLMs with multi-step perception capabilities, enabling precise coordinate prediction. Specifically, our approach integrate Monte Carlo quality estimation with Intersection-over-Union (IoU)-based region quality evaluation to jointly encourage both accuracy and diversity in constructing high-quality preference data. This combination explicitly guides the model to focus on instruction-relevant key regions while adaptively allocating reasoning steps based on task complexity. Comprehensive experiments on the ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new state-of-the-art (SoTA) among 7B-scale models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Morphological Profile Neural Networks for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.04268</link>
<guid>https://arxiv.org/abs/2509.04268</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic segmentation, Remote sensing imagery, Grayscale morphology, Differential morphological profile, Deep Neural Networks

Summary:
This study explores enhancing semantic segmentation of overhead remote sensing imagery by incorporating the differential morphological profile (DMP) into modern segmentation networks. The DMP, a multi-scale shape extraction method based on grayscale morphology, provides critical shape information to Deep Neural Networks for better performance in overhead imagery analysis. The researchers extend prior work by integrating DMP features into three state-of-the-art semantic segmentation architectures. They evaluate different DMP differentials and structuring element shapes to enhance shape information for the model. Results show that hybrid DMP models consistently outperform direct-input variants and can surpass non-DMP models on metrics like mIoU, F1, and Recall. This approach addresses challenges specific to remote sensing imagery such as extreme scale variation and foreground-background imbalance, making it a promising technique for applications in mapping, urban planning, and disaster response.

<br /><br />Summary: <div>
arXiv:2509.04268v1 Announce Type: new 
Abstract: Semantic segmentation of overhead remote sensing imagery enables applications in mapping, urban planning, and disaster response. State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. We explore the incorporation of the differential morphological profile (DMP), a multi-scale shape extraction method based on grayscale morphology, into modern segmentation networks. Prior studies have shown that the DMP can provide critical shape information to Deep Neural Networks to enable superior detection and classification performance in overhead imagery. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. We utilize both direct input, which adapts the input stem of feature extraction architectures to accept DMP channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP differentials and structuring element shapes to more effectively provide shape information to the model. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models</title>
<link>https://arxiv.org/abs/2509.04269</link>
<guid>https://arxiv.org/abs/2509.04269</guid>
<content:encoded><![CDATA[
<div> Keywords: tau pathology, positron emission tomography, Alzheimer's disease, plasma biomarkers, 3D diffusion model

Summary: 
This study introduces a novel approach for synthesizing 3D tau PET images using a text-guided 3D diffusion model. By incorporating information from plasma measurements and structural MRI, the proposed framework can generate realistic tau PET images that provide valuable insights into Alzheimer's disease progression. The use of textual prompts from plasma p-tau217 measurements and anatomical constraints from MRI data allows for the accurate representation of tau pathology across different disease stages. The synthesized images offer a cost-effective and non-invasive alternative for visualizing tau pathology, making it a valuable tool for diagnosing and monitoring AD. Additionally, the framework can be used for tau PET data augmentation and simulating disease progression under varying plasma biomarker levels and cognitive conditions. Overall, this approach enhances our understanding of tau pathology in Alzheimer's disease and has the potential to improve diagnostic and monitoring capabilities. 

<br /><br />Summary: <div>
arXiv:2509.04269v1 Announce Type: new 
Abstract: Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.04273</link>
<guid>https://arxiv.org/abs/2509.04273</guid>
<content:encoded><![CDATA[
<div> framework, spatial regularization, volume priors, semi-supervised, medical image segmentation <br />
Summary: 
- A novel semi-supervised medical image segmentation framework is proposed. 
- The framework integrates spatial regularization methods and volume priors for improved segmentation accuracy. 
- An explicit volume prior and Threshold Dynamics spatial regularization are integrated into the segmentation network. 
- Regression network estimates target region volumes for unlabeled images, enforcing class ratio consistency. 
- A dataset-scale Wasserstein distance loss ensures volume distribution similarity between labeled and unlabeled datasets. 
- Experimental results on various datasets demonstrate the effectiveness of the proposed method. <br /> <div>
arXiv:2509.04273v1 Announce Type: new 
Abstract: Despite signi cant progress in semi-supervised medical image segmentation, most existing segmentation networks overlook e ective methodological guidance for feature extraction and important prior information from
  datasets. In this paper, we develop a semi-supervised medical image segmentation framework that e ectively integrates spatial regularization methods and volume priors. Speci cally, our approach integrates a strong explicit volume prior at the image scale and Threshold Dynamics spatial regularization, both derived from variational models, into the backbone segmentation network. The target region volumes for each unlabeled image are estimated by a regression network, which e ectively regularizes the backbone segmentation network through an image-scale Wasserstein distance constraint, ensuring that the class ratios in the segmentation results for each unlabeled image match those predicted by the regression network. Additionally, we design a dataset-scale Wasserstein distance loss function based on a weak implicit volume prior, which enforces that the volume distribution predicted for the unlabeled dataset is similar to that of labeled dataset. Experimental results on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset show the superiority of the proposed method.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAOLI: Pose-free Articulated Object Learning from Sparse-view Images</title>
<link>https://arxiv.org/abs/2509.04276</link>
<guid>https://arxiv.org/abs/2509.04276</guid>
<content:encoded><![CDATA[
arXiv:2509.04276v1 Announce Type: new 
Abstract: We present a novel self-supervised framework for learning articulated object representations from sparse-view, unposed images. Unlike prior methods that require dense multi-view observations and ground-truth camera poses, our approach operates with as few as four views per articulation and no camera supervision. To address the inherent challenges, we first reconstruct each articulation independently using recent advances in sparse-view 3D reconstruction, then learn a deformation field that establishes dense correspondences across poses. A progressive disentanglement strategy further separates static from moving parts, enabling robust separation of camera and object motion. Finally, we jointly optimize geometry, appearance, and kinematics with a self-supervised loss that enforces cross-view and cross-pose consistency. Experiments on the standard benchmark and real-world examples demonstrate that our method produces accurate and detailed articulated object representations under significantly weaker input assumptions than existing approaches.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noisy Label Refinement with Semantically Reliable Synthetic Images</title>
<link>https://arxiv.org/abs/2509.04298</link>
<guid>https://arxiv.org/abs/2509.04298</guid>
<content:encoded><![CDATA[
arXiv:2509.04298v1 Announce Type: new 
Abstract: Semantic noise in image classification datasets, where visually similar categories are frequently mislabeled, poses a significant challenge to conventional supervised learning approaches. In this paper, we explore the potential of using synthetic images generated by advanced text-to-image models to address this issue. Although these high-quality synthetic images come with reliable labels, their direct application in training is limited by domain gaps and diversity constraints. Unlike conventional approaches, we propose a novel method that leverages synthetic images as reliable reference points to identify and correct mislabeled samples in noisy datasets. Extensive experiments across multiple benchmark datasets show that our approach significantly improves classification accuracy under various noise conditions, especially in challenging scenarios with semantic label noise. Additionally, since our method is orthogonal to existing noise-robust learning techniques, when combined with state-of-the-art noise-robust training methods, it achieves superior performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100 under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise conditions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Odd-One-Out Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.04326</link>
<guid>https://arxiv.org/abs/2509.04326</guid>
<content:encoded><![CDATA[
arXiv:2509.04326v1 Announce Type: new 
Abstract: The recently introduced odd-one-out anomaly detection task involves identifying the odd-looking instances within a multi-object scene. This problem presents several challenges for modern deep learning models, demanding spatial reasoning across multiple views and relational reasoning to understand context and generalize across varying object categories and layouts. We argue that these challenges must be addressed with efficiency in mind. To this end, we propose a DINO-based model that reduces the number of parameters by one third and shortens training time by a factor of three compared to the current state-of-the-art, while maintaining competitive performance. Our experimental evaluation also introduces a Multimodal Large Language Model baseline, providing insights into its current limitations in structured visual reasoning tasks. The project page can be found at https://silviochito.github.io/EfficientOddOneOut/
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization</title>
<link>https://arxiv.org/abs/2509.04334</link>
<guid>https://arxiv.org/abs/2509.04334</guid>
<content:encoded><![CDATA[
arXiv:2509.04334v1 Announce Type: new 
Abstract: Image geolocalization aims to predict the geographic location of images captured anywhere on Earth, but its global nature presents significant challenges. Current evaluation methodologies suffer from two major limitations. First, data leakage: advanced approaches often rely on large vision-language models (LVLMs) to predict image locations, yet these models are frequently pretrained on the test datasets, compromising the accuracy of evaluating a model's actual geolocalization capability. Second, existing metrics primarily rely on exact geographic coordinates to assess predictions, which not only neglects the reasoning process but also raises privacy concerns when user-level location data is required. To address these issues, we propose GeoArena, a first open platform for evaluating LVLMs on worldwide image geolocalization tasks, offering true in-the-wild and human-centered benchmarking. GeoArena enables users to upload in-the-wild images for a more diverse evaluation corpus, and it leverages pairwise human judgments to determine which model output better aligns with human expectations. Our platform has been deployed online for two months, during which we collected over thousands voting records. Based on this data, we conduct a detailed analysis and establish a leaderboard of different LVLMs on the image geolocalization task.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Editor to Dense Geometry Estimator</title>
<link>https://arxiv.org/abs/2509.04338</link>
<guid>https://arxiv.org/abs/2509.04338</guid>
<content:encoded><![CDATA[
arXiv:2509.04338v1 Announce Type: new 
Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\times$ data. The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2509.04344</link>
<guid>https://arxiv.org/abs/2509.04344</guid>
<content:encoded><![CDATA[
arXiv:2509.04344v1 Announce Type: new 
Abstract: Dynamic facial expression recognition (DFER) faces significant challenges due to long-tailed category distributions and complexity of spatio-temporal feature modeling. While existing deep learning-based methods have improved DFER performance, they often fail to address these issues, resulting in severe model induction bias. To overcome these limitations, we propose a novel multi-instance learning framework called MICACL, which integrates spatio-temporal dependency modeling and long-tailed contrastive learning optimization. Specifically, we design the Graph-Enhanced Instance Interaction Module (GEIIM) to capture intricate spatio-temporal between adjacent instances relationships through adaptive adjacency matrices and multiscale convolutions. To enhance instance-level feature aggregation, we develop the Weighted Instance Aggregation Network (WIAN), which dynamically assigns weights based on instance importance. Furthermore, we introduce a Multiscale Category-aware Contrastive Learning (MCCL) strategy to balance training between major and minor categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and FERV39k) demonstrate that MICACL achieves state-of-the-art performance with superior robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage</title>
<link>https://arxiv.org/abs/2509.04370</link>
<guid>https://arxiv.org/abs/2509.04370</guid>
<content:encoded><![CDATA[
arXiv:2509.04370v1 Announce Type: new 
Abstract: First responders widely adopt body-worn cameras to document incident scenes and support post-event analysis. However, reviewing lengthy video footage is impractical in time-critical situations. Effective situational awareness demands a concise visual summary that can be quickly interpreted. This work presents a computer vision pipeline that transforms body-camera footage into informative panoramic images summarizing the incident scene. Our method leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate camera trajectories and reconstruct the spatial layout of the environment. Key viewpoints are identified by clustering camera poses along the trajectory, and representative frames from each cluster are selected. These frames are fused into spatially coherent panoramic images using multi-frame stitching techniques. The resulting summaries enable rapid understanding of complex environments and facilitate efficient decision-making and incident review.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search</title>
<link>https://arxiv.org/abs/2509.04376</link>
<guid>https://arxiv.org/abs/2509.04376</guid>
<content:encoded><![CDATA[
arXiv:2509.04376v1 Announce Type: new 
Abstract: With growing public safety demands, text-based person anomaly search has emerged as a critical task, aiming to retrieve individuals with abnormal behaviors via natural language descriptions. Unlike conventional person search, this task presents two unique challenges: (1) fine-grained cross-modal alignment between textual anomalies and visual behaviors, and (2) anomaly recognition under sparse real-world samples. While Large Multi-modal Models (LMMs) excel in multi-modal understanding, their potential for fine-grained anomaly retrieval remains underexplored, hindered by: (1) a domain gap between generative knowledge and discriminative retrieval, and (2) the absence of efficient adaptation strategies for deployment. In this work, we propose AnomalyLMM, the first framework that harnesses LMMs for text-based person anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline integrating LMMs to bridge generative world knowledge with retrieval-centric anomaly detection; (2) A training-free adaptation cookbook featuring masked cross-modal prompting, behavioral saliency prediction, and knowledge-aware re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study to explore LMMs for this task, we conduct a rigorous evaluation on the PAB dataset, the only publicly available benchmark for text-based person anomaly search, with its curated real-world anomalies covering diverse scenarios (e.g., falling, collision, and being hit). Experiments show the effectiveness of the proposed method, surpassing the competitive baseline by +0.96% Recall@1 accuracy. Notably, our method reveals interpretable alignment between textual anomalies and visual behaviors, validated via qualitative analysis. Our code and models will be released for future research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aesthetic Image Captioning with Saliency Enhanced MLLMs</title>
<link>https://arxiv.org/abs/2509.04378</link>
<guid>https://arxiv.org/abs/2509.04378</guid>
<content:encoded><![CDATA[
arXiv:2509.04378v1 Announce Type: new 
Abstract: Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</title>
<link>https://arxiv.org/abs/2509.04379</link>
<guid>https://arxiv.org/abs/2509.04379</guid>
<content:encoded><![CDATA[
arXiv:2509.04379v1 Announce Type: new 
Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning neural representations for X-ray ptychography reconstruction with unknown probes</title>
<link>https://arxiv.org/abs/2509.04402</link>
<guid>https://arxiv.org/abs/2509.04402</guid>
<content:encoded><![CDATA[
arXiv:2509.04402v1 Announce Type: new 
Abstract: X-ray ptychography provides exceptional nanoscale resolution and is widely applied in materials science, biology, and nanotechnology. However, its full potential is constrained by the critical challenge of accurately reconstructing images when the illuminating probe is unknown. Conventional iterative methods and deep learning approaches are often suboptimal, particularly under the low-signal conditions inherent to low-dose and high-speed experiments. These limitations compromise reconstruction fidelity and restrict the broader adoption of the technique. In this work, we introduce the Ptychographic Implicit Neural Representation (PtyINR), a self-supervised framework that simultaneously addresses the object and probe recovery problem. By parameterizing both as continuous neural representations, PtyINR performs end-to-end reconstruction directly from raw diffraction patterns without requiring any pre-characterization of the probe. Extensive evaluations demonstrate that PtyINR achieves superior reconstruction quality on both simulated and experimental data, with remarkable robustness under challenging low-signal conditions. Furthermore, PtyINR offers a generalizable, physics-informed framework for addressing probe-dependent inverse problems, making it applicable to a wide range of computational microscopy problems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios</title>
<link>https://arxiv.org/abs/2509.04403</link>
<guid>https://arxiv.org/abs/2509.04403</guid>
<content:encoded><![CDATA[
arXiv:2509.04403v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are rapidly evolving, presenting increasingly complex safety challenges. However, current dataset construction methods, which are risk-oriented, fail to cover the growing complexity of real-world multimodal safety scenarios (RMS). And due to the lack of a unified evaluation metric, their overall effectiveness remains unproven. This paper introduces a novel image-oriented self-adaptive dataset construction method for RMS, which starts with images and end constructing paired text and guidance responses. Using the image-oriented method, we automatically generate an RMS dataset comprising 35k image-text pairs with guidance responses. Additionally, we introduce a standardized safety dataset evaluation metric: fine-tuning a safety judge model and evaluating its capabilities on other safety datasets.Extensive experiments on various tasks demonstrate the effectiveness of the proposed image-oriented pipeline. The results confirm the scalability and effectiveness of the image-oriented approach, offering a new perspective for the construction of real-world multimodal safety datasets.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</title>
<link>https://arxiv.org/abs/2509.04406</link>
<guid>https://arxiv.org/abs/2509.04406</guid>
<content:encoded><![CDATA[
arXiv:2509.04406v1 Announce Type: new 
Abstract: Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Durian: Dual Reference-guided Portrait Animation with Attribute Transfer</title>
<link>https://arxiv.org/abs/2509.04434</link>
<guid>https://arxiv.org/abs/2509.04434</guid>
<content:encoded><![CDATA[
arXiv:2509.04434v1 Announce Type: new 
Abstract: We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform</title>
<link>https://arxiv.org/abs/2509.04437</link>
<guid>https://arxiv.org/abs/2509.04437</guid>
<content:encoded><![CDATA[
arXiv:2509.04437v1 Announce Type: new 
Abstract: Collimation in X-ray imaging restricts exposure to the region-of-interest (ROI) and minimizes the radiation dose applied to the patient. The detection of collimator shadows is an essential image-based preprocessing step in digital radiography posing a challenge when edges get obscured by scattered X-ray radiation. Regardless, the prior knowledge that collimation forms polygonal-shaped shadows is evident. For this reason, we introduce a deep learning-based segmentation that is inherently constrained to its geometry. We achieve this by incorporating a differentiable Hough transform-based network to detect the collimation borders and enhance its capability to extract the information about the ROI center. During inference, we combine the information of both tasks to enable the generation of refined, line-constrained segmentation masks. We demonstrate robust reconstruction of collimated regions achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real Xray images. While this application involves at most four shadow borders, our method is not fundamentally limited by a specific number of edges.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Telephone Game: Evaluating Semantic Drift in Unified Models</title>
<link>https://arxiv.org/abs/2509.04438</link>
<guid>https://arxiv.org/abs/2509.04438</guid>
<content:encoded><![CDATA[
arXiv:2509.04438v1 Announce Type: new 
Abstract: Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T, as consistency between understanding and generation is critical for downstream use. Existing evaluations consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These single-pass metrics do not reveal whether a model that understands a concept can also render it, nor whether meaning is preserved when cycling between image and text modalities. To address this, we introduce the Unified Consistency Framework for Unified Models (UCF-UM), a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic loss; (ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO, which is widely used in training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and evaluate on seven recent models. UCF-UM reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantics over many alternations, whereas others like Vila-u drift quickly despite strong single-pass scores. Our results highlight cyclic consistency as a necessary complement to standard I2T and T2I evaluations, and provide practical metrics to consistently assess unified model's cross-modal stability and strength of their shared representations. Code: https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</title>
<link>https://arxiv.org/abs/2509.04444</link>
<guid>https://arxiv.org/abs/2509.04444</guid>
<content:encoded><![CDATA[
arXiv:2509.04444v1 Announce Type: new 
Abstract: Driven by the demand for spatial intelligence and holistic scene perception, omnidirectional images (ODIs), which provide a complete 360\textdegree{} field of view, are receiving growing attention across diverse applications such as virtual reality, autonomous driving, and embodied robotics. Despite their unique characteristics, ODIs exhibit remarkable differences from perspective images in geometric projection, spatial distribution, and boundary continuity, making it challenging for direct domain adaption from perspective methods. This survey reviews recent panoramic vision techniques with a particular emphasis on the perspective-to-panorama adaptation. We first revisit the panoramic imaging pipeline and projection methods to build the prior knowledge required for analyzing the structural disparities. Then, we summarize three challenges of domain adaptation: severe geometric distortions near the poles, non-uniform sampling in Equirectangular Projection (ERP), and periodic boundary continuity. Building on this, we cover 20+ representative tasks drawn from more than 300 research papers in two dimensions. On one hand, we present a cross-method analysis of representative strategies for addressing panoramic specific challenges across different tasks. On the other hand, we conduct a cross-task comparison and classify panoramic vision into four major categories: visual quality enhancement and assessment, visual understanding, multimodal understanding, and visual generation. In addition, we discuss open challenges and future directions in data, models, and applications that will drive the advancement of panoramic vision research. We hope that our work can provide new insight and forward looking perspectives to advance the development of panoramic vision technologies. Our project page is https://insta360-research-team.github.io/Survey-of-Panorama
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2509.04446</link>
<guid>https://arxiv.org/abs/2509.04446</guid>
<content:encoded><![CDATA[
arXiv:2509.04446v1 Announce Type: new 
Abstract: Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2509.04448</link>
<guid>https://arxiv.org/abs/2509.04448</guid>
<content:encoded><![CDATA[
arXiv:2509.04448v1 Announce Type: new 
Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview</title>
<link>https://arxiv.org/abs/2509.04450</link>
<guid>https://arxiv.org/abs/2509.04450</guid>
<content:encoded><![CDATA[
arXiv:2509.04450v1 Announce Type: new 
Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Fine Structure in Protoplanetary Disks with Physics Constrained Neural Fields</title>
<link>https://arxiv.org/abs/2509.03623</link>
<guid>https://arxiv.org/abs/2509.03623</guid>
<content:encoded><![CDATA[
arXiv:2509.03623v1 Announce Type: cross 
Abstract: Protoplanetary disks are the birthplaces of planets, and resolving their three-dimensional structure is key to understanding disk evolution. The unprecedented resolution of ALMA demands modeling approaches that capture features beyond the reach of traditional methods. We introduce a computational framework that integrates physics-constrained neural fields with differentiable rendering and present RadJAX, a GPU-accelerated, fully differentiable line radiative transfer solver achieving up to 10,000x speedups over conventional ray tracers, enabling previously intractable, high-dimensional neural reconstructions. Applied to ALMA CO observations of HD 163296, this framework recovers the vertical morphology of the CO-rich layer, revealing a pronounced narrowing and flattening of the emission surface beyond 400 au - a feature missed by existing approaches. Our work establish a new paradigm for extracting complex disk structure and advancing our understanding of protoplanetary evolution.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights from Gradient Dynamics: Gradient Autoscaled Normalization</title>
<link>https://arxiv.org/abs/2509.03677</link>
<guid>https://arxiv.org/abs/2509.03677</guid>
<content:encoded><![CDATA[
arXiv:2509.03677v1 Announce Type: cross 
Abstract: Gradient dynamics play a central role in determining the stability and generalization of deep neural networks. In this work, we provide an empirical analysis of how variance and standard deviation of gradients evolve during training, showing consistent changes across layers and at the global scale in convolutional networks. Motivated by these observations, we propose a hyperparameter-free gradient normalization method that aligns gradient scaling with their natural evolution. This approach prevents unintended amplification, stabilizes optimization, and preserves convergence guarantees. Experiments on the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN demonstrate that our method maintains or improves test accuracy even under strong generalization. Beyond practical performance, our study highlights the importance of directly tracking gradient dynamics, aiming to bridge the gap between theoretical expectations and empirical behaviors, and to provide insights for future optimization research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxDiT: Lighting Estimation with Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2509.03680</link>
<guid>https://arxiv.org/abs/2509.03680</guid>
<content:encoded><![CDATA[
arXiv:2509.03680v1 Announce Type: cross 
Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping on a Budget: Optimizing Spatial Data Collection for ML</title>
<link>https://arxiv.org/abs/2509.03749</link>
<guid>https://arxiv.org/abs/2509.03749</guid>
<content:encoded><![CDATA[
arXiv:2509.03749v1 Announce Type: cross 
Abstract: In applications across agriculture, ecology, and human development, machine learning with satellite imagery (SatML) is limited by the sparsity of labeled training data. While satellite data cover the globe, labeled training datasets for SatML are often small, spatially clustered, and collected for other purposes (e.g., administrative surveys or field measurements). Despite the pervasiveness of this issue in practice, past SatML research has largely focused on new model architectures and training algorithms to handle scarce training data, rather than modeling data conditions directly. This leaves scientists and policymakers who wish to use SatML for large-scale monitoring uncertain about whether and how to collect additional data to maximize performance. Here, we present the first problem formulation for the optimization of spatial training data in the presence of heterogeneous data collection costs and realistic budget constraints, as well as novel methods for addressing this problem. In experiments simulating different problem settings across three continents and four tasks, our strategies reveal substantial gains from sample optimization. Further experiments delineate settings for which optimized sampling is particularly effective. The problem formulation and methods we introduce are designed to generalize across application domains for SatML; we put special emphasis on a specific problem setting where our coauthors can immediately use our findings to augment clustered agricultural surveys for SatML monitoring in Togo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction</title>
<link>https://arxiv.org/abs/2509.03775</link>
<guid>https://arxiv.org/abs/2509.03775</guid>
<content:encoded><![CDATA[
arXiv:2509.03775v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai</title>
<link>https://arxiv.org/abs/2509.03830</link>
<guid>https://arxiv.org/abs/2509.03830</guid>
<content:encoded><![CDATA[
arXiv:2509.03830v1 Announce Type: cross 
Abstract: Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Augmented Quantization-Aware Knowledge Distillation</title>
<link>https://arxiv.org/abs/2509.03850</link>
<guid>https://arxiv.org/abs/2509.03850</guid>
<content:encoded><![CDATA[
arXiv:2509.03850v1 Announce Type: cross 
Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. Existing KD and QAT works focus on improving the accuracy of quantized models from the network output perspective by designing better KD loss functions or optimizing QAT's forward and backward propagation. However, limited attention has been given to understanding the impact of input transformations, such as data augmentation (DA). The relationship between quantization-aware KD and DA remains unexplored. In this paper, we address the question: how to select a good DA in quantization-aware KD, especially for the models with low precisions? We propose a novel metric which evaluates DAs according to their capacity to maximize the Contextual Mutual Information--the information not directly related to an image's label--while also ensuring the predictions for each class are close to the ground truth labels on average. The proposed method automatically ranks and selects DAs, requiring minimal training overhead, and it is compatible with any KD or QAT algorithm. Extensive evaluations demonstrate that selecting DA strategies using our metric significantly improves state-of-the-art QAT and KD works across various model architectures and datasets.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.03891</link>
<guid>https://arxiv.org/abs/2509.03891</guid>
<content:encoded><![CDATA[
arXiv:2509.03891v1 Announce Type: cross 
Abstract: Smartphones have become indispensable in people's daily lives, permeating nearly every aspect of modern society. With the continuous advancement of large language models (LLMs), numerous LLM-based mobile agents have emerged. These agents are capable of accurately parsing diverse user queries and automatically assisting users in completing complex or repetitive operations. However, current agents 1) heavily rely on the comprehension ability of LLMs, which can lead to errors caused by misoperations or omitted steps during tasks, 2) lack interaction with the external environment, often terminating tasks when an app cannot fulfill user queries, and 3) lack memory capabilities, requiring each instruction to reconstruct the interface and being unable to learn from and correct previous mistakes. To alleviate the above issues, we propose MobileRAG, a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG), which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly and accurately identify user queries and accomplish complex and long-sequence mobile tasks. Additionally, to more comprehensively assess the performance of MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark characterized by numerous complex, real-world mobile tasks that require external knowledge assistance. Extensive experimental results on MobileRAG-Eval demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving 10.3\% improvement over state-of-the-art methods with fewer operational steps. Our code is publicly available at: https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media</title>
<link>https://arxiv.org/abs/2509.04047</link>
<guid>https://arxiv.org/abs/2509.04047</guid>
<content:encoded><![CDATA[
arXiv:2509.04047v1 Announce Type: cross 
Abstract: Estimating scattering parameters of heterogeneous media from images is a severely under-constrained and challenging problem. Most of the existing approaches model BSSRDF either through an analysis-by-synthesis approach, approximating complex path integrals, or using differentiable volume rendering techniques to account for heterogeneity. However, only a few studies have applied learning-based methods to estimate subsurface scattering parameters, but they assume homogeneous media. Interestingly, no specific distribution is known to us that can explicitly model the heterogeneous scattering parameters in the real world. Notably, procedural noise models such as Perlin and Fractal Perlin noise have been effective in representing intricate heterogeneities of natural, organic, and inorganic surfaces. Leveraging this, we first create HeteroSynth, a synthetic dataset comprising photorealistic images of heterogeneous media whose scattering parameters are modeled using Fractal Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a learning-based feed-forward framework to estimate these Perlin-distributed heterogeneous scattering parameters from sparse multi-view image observations. Instead of directly predicting the 3D scattering parameter volume, TensoIS uses learnable low-rank tensor components to represent the scattering volume. We evaluate TensoIS on unseen heterogeneous variations over shapes from the HeteroSynth test set, smoke and cloud geometries obtained from open-source realistic volumetric simulations, and some real-world samples to establish its effectiveness for inverse scattering. Overall, this study is an attempt to explore Perlin noise distribution, given the lack of any such well-defined distribution in literature, to potentially model real-world heterogeneous scattering in a feed-forward manner.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMooGPT: Stylized Motion Generation using Large Language Models</title>
<link>https://arxiv.org/abs/2509.04058</link>
<guid>https://arxiv.org/abs/2509.04058</guid>
<content:encoded><![CDATA[
arXiv:2509.04058v1 Announce Type: cross 
Abstract: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity</title>
<link>https://arxiv.org/abs/2509.04107</link>
<guid>https://arxiv.org/abs/2509.04107</guid>
<content:encoded><![CDATA[
arXiv:2509.04107v1 Announce Type: cross 
Abstract: Federated Learning (FL) provides decentralised model training, which effectively tackles problems such as distributed data and privacy preservation. However, the generalisation of global models frequently faces challenges from data heterogeneity among clients. This challenge becomes even more pronounced when datasets are limited in size and class imbalance. To address data heterogeneity, we propose a novel method, \textit{FedQuad}, that explicitly optimises smaller intra-class variance and larger inter-class variance across clients, thereby decreasing the negative impact of model aggregation on the global model over client representations. Our approach minimises the distance between similar pairs while maximising the distance between negative pairs, effectively disentangling client data in the shared feature space. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets under various data distributions and with many clients, demonstrating superior performance compared to existing approaches. Furthermore, we provide a detailed analysis of metric learning-based strategies within both supervised and federated learning paradigms, highlighting their efficacy in addressing representational learning challenges in federated settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion</title>
<link>https://arxiv.org/abs/2509.04145</link>
<guid>https://arxiv.org/abs/2509.04145</guid>
<content:encoded><![CDATA[
arXiv:2509.04145v1 Announce Type: cross 
Abstract: Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</title>
<link>https://arxiv.org/abs/2509.04324</link>
<guid>https://arxiv.org/abs/2509.04324</guid>
<content:encoded><![CDATA[
arXiv:2509.04324v1 Announce Type: cross 
Abstract: Grasping assistance is essential for restoring autonomy in individuals with motor impairments, particularly in unstructured environments where object categories and user intentions are diverse and unpredictable. We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. To enhance generalization in open environments, OVGrasp incorporates a vision-language foundation model with an open-vocabulary mechanism, allowing zero-shot detection of previously unseen objects without retraining. A multimodal decision-maker further fuses spatial and linguistic cues to infer user intent, such as grasp or release, in multi-object scenarios. We deploy the complete framework on a custom egocentric-view wearable exoskeleton and conduct systematic evaluations on 15 objects across three grasp types. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking</title>
<link>https://arxiv.org/abs/2509.04351</link>
<guid>https://arxiv.org/abs/2509.04351</guid>
<content:encoded><![CDATA[
arXiv:2509.04351v1 Announce Type: cross 
Abstract: The dominant paradigm in image retrieval systems today is to search large databases using global image features, and re-rank those initial results with local image feature matching techniques. This design, dubbed global-to-local, stems from the computational cost of local matching approaches, which can only be afforded for a small number of retrieved images. However, emerging efficient local feature search approaches have opened up new possibilities, in particular enabling detailed retrieval at large scale, to find partial matches which are often missed by global feature search. In parallel, global feature-based re-ranking has shown promising results with high computational efficiency. In this work, we leverage these building blocks to introduce a local-to-global retrieval paradigm, where efficient local feature search meets effective global feature re-ranking. Critically, we propose a re-ranking method where global features are computed on-the-fly, based on the local feature retrieval similarities. Such re-ranking-only global features leverage multidimensional scaling techniques to create embeddings which respect the local similarities obtained during search, enabling a significant re-ranking boost. Experimentally, we demonstrate solid retrieval performance, setting new state-of-the-art results on the Revisited Oxford and Paris datasets.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transition Models: Rethinking the Generative Learning Objective</title>
<link>https://arxiv.org/abs/2509.04394</link>
<guid>https://arxiv.org/abs/2509.04394</guid>
<content:encoded><![CDATA[
arXiv:2509.04394v1 Announce Type: cross 
Abstract: A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation</title>
<link>https://arxiv.org/abs/2509.04441</link>
<guid>https://arxiv.org/abs/2509.04441</guid>
<content:encoded><![CDATA[
arXiv:2509.04441v1 Announce Type: cross 
Abstract: We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization</title>
<link>https://arxiv.org/abs/2309.16494</link>
<guid>https://arxiv.org/abs/2309.16494</guid>
<content:encoded><![CDATA[
arXiv:2309.16494v3 Announce Type: replace 
Abstract: Recently, deep learning-based methods have dominated image dehazing domain. A multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and the cross non-local block (CNLB) is presented in this paper to further enhance the performance. We start with extracting richer features for dehazing. Specifically, a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$), is designed for extracting multi-scale features. Following MSFE, an attention sub-block is employed to make the model adaptively focus on important channels/regions. These two sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in a representation space specially designed for dehazing. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Straighter Flow Matching via a Diffusion-Based Coupling Prior</title>
<link>https://arxiv.org/abs/2311.16507</link>
<guid>https://arxiv.org/abs/2311.16507</guid>
<content:encoded><![CDATA[
arXiv:2311.16507v2 Announce Type: replace 
Abstract: Flow matching as a paradigm of generative model achieves notable success across various domains. However, existing methods use either multi-round training or knowledge within minibatches, posing challenges in finding a favorable coupling strategy for straightening trajectories to few-step generation. To address this issue, we propose a novel approach, Straighter trajectories of Flow Matching (StraightFM). It straightens trajectories with the coupling strategy from the entire distribution level. More specifically, during training, StraightFM creates couplings of images and noise via one diffusion model as a coupling prior to straighten trajectories for few-step generation. Our coupling strategy can also integrate with the existing coupling direction from real data to noise, improving image quality in few-step generation. Experimental results on pixel space and latent space show that StraightFM yields attractive samples within 5 steps. Moreover, our unconditional StraightFM is seamlessly compatible with training-free multimodal conditional generation, maintaining high-quality image generation in few steps.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Transfer to Calvin and Hobbes comics using Stable Diffusion</title>
<link>https://arxiv.org/abs/2312.03993</link>
<guid>https://arxiv.org/abs/2312.03993</guid>
<content:encoded><![CDATA[
arXiv:2312.03993v2 Announce Type: replace 
Abstract: This project report summarizes our journey to perform stable diffusion fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to convert any given input image into the comic style of Calvin and Hobbes, essentially performing style transfer. We train stable-diffusion-v1.5 using Low Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The diffusion itself is handled by a Variational Autoencoder (VAE), which is a U-net. Our results were visually appealing for the amount of training time and the quality of input data that went into training.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replication Study and Benchmarking of Real-Time Object Detection Models</title>
<link>https://arxiv.org/abs/2405.06911</link>
<guid>https://arxiv.org/abs/2405.06911</guid>
<content:encoded><![CDATA[
arXiv:2405.06911v2 Announce Type: replace 
Abstract: This work examines the reproducibility and benchmarking of state-of-the-art real-time object detection models. As object detection models are often used in real-world contexts, such as robotics, where inference time is paramount, simply measuring models' accuracy is not enough to compare them. We thus compare a large variety of object detection models' accuracy and inference speed on multiple graphics cards. In addition to this large benchmarking attempt, we also reproduce the following models from scratch using PyTorch on the MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we propose a unified training and evaluation pipeline, based on MMDetection's features, to better compare models. Our implementation of DETR and ViTDet could not achieve accuracy or speed performances comparable to what is declared in the original papers. On the other hand, reproduced RTMDet and YOLOv7 could match such performances. Studied papers are also found to be generally lacking for reproducibility purposes. As for MMDetection pretrained models, speed performances are severely reduced with limited computing resources (larger, more accurate models even more so). Moreover, results exhibit a strong trade-off between accuracy and speed, prevailed by anchor-free models - notably RTMDet or YOLOx models. The code used is this paper and all the experiments is available in the repository at https://github.com/willGuimont/segdet_mlcr2024.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution</title>
<link>https://arxiv.org/abs/2405.11491</link>
<guid>https://arxiv.org/abs/2405.11491</guid>
<content:encoded><![CDATA[
arXiv:2405.11491v2 Announce Type: replace 
Abstract: Synthetic image attribution addresses the problem of tracing back the origin of images produced by generative models. Extensive efforts have been made to explore unique representations of generative models and use them to attribute a synthetic image to the model that produced it. Most of the methods classify the models or the architectures among those in a closed set without considering the possibility that the system is fed with samples produced by unknown architectures. With the continuous progress of AI technology, new generative architectures continuously appear, thus driving the attention of researchers towards the development of tools capable of working in open-set scenarios. In this paper, we propose a framework for open set attribution of synthetic images, named BOSC (Backdoor-based Open Set Classification), that relies on the concept of backdoor attacks to design a classifier with rejection option. BOSC works by purposely injecting class-specific triggers inside a portion of the images in the training set to induce the network to establish a matching between class features and trigger features. The behavior of the trained model with respect to triggered samples is then exploited at test time to perform sample rejection using an ad-hoc score. Experiments show that the proposed method has good performance, always surpassing the state-of-the-art. Robustness against image processing is also very good. Although we designed our method for the task of synthetic image attribution, the proposed framework is a general one and can be used for other image forensic applications.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid Registration</title>
<link>https://arxiv.org/abs/2405.20188</link>
<guid>https://arxiv.org/abs/2405.20188</guid>
<content:encoded><![CDATA[
arXiv:2405.20188v2 Announce Type: replace 
Abstract: Existing optimization-based methods for non-rigid registration typically minimize an alignment error metric based on the point-to-point or point-to-plane distance between corresponding point pairs on the source surface and target surface. However, these metrics can result in slow convergence or a loss of detail. In this paper, we propose SPARE, a novel formulation that utilizes a symmetrized point-to-plane distance for robust non-rigid registration. The symmetrized point-to-plane distance relies on both the positions and normals of the corresponding points, resulting in a more accurate approximation of the underlying geometry and can achieve higher accuracy than existing methods. To solve this optimization problem efficiently, we introduce an as-rigid-as-possible regulation term to estimate the deformed normals and propose an alternating minimization solver using a majorization-minimization strategy. Moreover, for effective initialization of the solver, we incorporate a deformation graph-based coarse alignment that improves registration quality and efficiency. Extensive experiments show that the proposed method greatly improves the accuracy of non-rigid registration problems and maintains relatively high solution efficiency. The code is publicly available at https://github.com/yaoyx689/spare.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FADE: A Dataset for Detecting Falling Objects around Buildings in Video</title>
<link>https://arxiv.org/abs/2408.05750</link>
<guid>https://arxiv.org/abs/2408.05750</guid>
<content:encoded><![CDATA[
arXiv:2408.05750v2 Announce Type: replace 
Abstract: Falling objects from buildings can cause severe injuries to pedestrians due to the great impact force they exert. Although surveillance cameras are installed around some buildings, it is challenging for humans to capture such events in surveillance videos due to the small size and fast motion of falling objects, as well as the complex background. Therefore, it is necessary to develop methods to automatically detect falling objects around buildings in surveillance videos. To facilitate the investigation of falling object detection, we propose a large, diverse video dataset called FADE (FAlling Object DEtection around Buildings) for the first time. FADE contains 1,881 videos from 18 scenes, featuring 8 falling object categories, 4 weather conditions, and 4 video resolutions. Additionally, we develop a new object detection method called FADE-Net, which effectively leverages motion information and produces small-sized but high-quality proposals for detecting falling objects around buildings. Importantly, our method is extensively evaluated and analyzed by comparing it with the previous approaches used for generic object detection, video object detection, and moving object detection on the FADE dataset. Experimental results show that the proposed FADE-Net significantly outperforms other methods, providing an effective baseline for future research. The dataset and code are publicly available at https://fadedataset.github.io/FADE.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance</title>
<link>https://arxiv.org/abs/2409.06002</link>
<guid>https://arxiv.org/abs/2409.06002</guid>
<content:encoded><![CDATA[
arXiv:2409.06002v5 Announce Type: replace 
Abstract: Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Blending to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a class balancing algorithm to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures for On-Device Image Generation</title>
<link>https://arxiv.org/abs/2411.06119</link>
<guid>https://arxiv.org/abs/2411.06119</guid>
<content:encoded><![CDATA[
arXiv:2411.06119v2 Announce Type: replace 
Abstract: Vision Transformers and U-Net architectures have been widely adopted in the implementation of Diffusion Models. However, each architecture presents specific challenges while realizing them on-device. Vision Transformers require positional embedding to maintain correspondence between the tokens processed by the transformer, although they offer the advantage of using fixed-size, reusable repetitive blocks following tokenization. The U-Net architecture lacks these attributes, as it utilizes variable-sized intermediate blocks for down-convolution and up-convolution in the noise estimation backbone for the diffusion process. To address these issues, we propose an architecture that utilizes a fixed-size, reusable transformer block as a core structure, making it more suitable for hardware implementation. Our architecture is characterized by low complexity, token-free design, absence of positional embeddings, uniformity, and scalability, making it highly suitable for deployment on mobile and resource-constrained devices. The proposed model exhibit competitive and consistent performance across both unconditional and conditional image generation tasks. The model achieved a state-of-the-art FID score of 1.6 on unconditional image generation with the CelebA.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUNBa: Machine Unlearning via Nash Bargaining</title>
<link>https://arxiv.org/abs/2411.15537</link>
<guid>https://arxiv.org/abs/2411.15537</guid>
<content:encoded><![CDATA[
arXiv:2411.15537v4 Announce Type: replace 
Abstract: Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions. To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions. To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point. Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective. We evaluate our algorithm's effectiveness on a diverse set of tasks across image classification and image generation. Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving. Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OFTSR: One-Step Flow for Image Super-Resolution with Tunable Fidelity-Realism Trade-offs</title>
<link>https://arxiv.org/abs/2412.09465</link>
<guid>https://arxiv.org/abs/2412.09465</guid>
<content:encoded><![CDATA[
arXiv:2412.09465v2 Announce Type: replace 
Abstract: Recent advances in diffusion and flow-based generative models have demonstrated remarkable success in image restoration tasks, achieving superior perceptual quality compared to traditional deep learning approaches. However, these methods either require numerous sampling steps to generate high-quality images, resulting in significant computational overhead, or rely on common model distillation, which usually imposes a fixed fidelity-realism trade-off and thus lacks flexibility. In this paper, we introduce OFTSR, a novel flow-based framework for one-step image super-resolution that can produce outputs with tunable levels of fidelity and realism. Our approach first trains a conditional flow-based super-resolution model to serve as a teacher model. We then distill this teacher model by applying a specialized constraint. Specifically, we force the predictions from our one-step student model for same input to lie on the same sampling ODE trajectory of the teacher model. This alignment ensures that the student model's single-step predictions from initial states match the teacher's predictions from a closer intermediate state. Through extensive experiments on datasets including FFHQ (256$\times$256), DIV2K, and ImageNet (256$\times$256), we demonstrate that OFTSR achieves state-of-the-art performance for one-step image super-resolution, while having the ability to flexibly tune the fidelity-realism trade-off. Codes: \href{https://github.com/yuanzhi-zhu/OFTSR}{https://github.com/yuanzhi-zhu/OFTSR}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending LVLMs Against Vision Attacks through Partial-Perception Supervision</title>
<link>https://arxiv.org/abs/2412.12722</link>
<guid>https://arxiv.org/abs/2412.12722</guid>
<content:encoded><![CDATA[
arXiv:2412.12722v2 Announce Type: replace 
Abstract: Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision). In this approach, the model is prompted using the responses generated by a model that perceives only a partial image. With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Our findings show that the weak model can supervise the strong model: when faced with an attacked input, the strong model becomes less confident and adjusts its response based on the weak model's partial understanding, effectively defending against the attack. With clean input, it confidently maintains its original response. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images with Depth and Normal Supervision</title>
<link>https://arxiv.org/abs/2502.08352</link>
<guid>https://arxiv.org/abs/2502.08352</guid>
<content:encoded><![CDATA[
arXiv:2502.08352v2 Announce Type: replace 
Abstract: With advancements in satellite imaging technology, acquiring high-resolution multi-view satellite imagery has become increasingly accessible, enabling rapid and location-independent ground model reconstruction. However, traditional stereo matching methods struggle to capture fine details, and while neural radiance fields (NeRFs) achieve high-quality reconstructions, their training time is prohibitively long. Moreover, challenges such as low visibility of building facades, illumination and style differences between pixels, and weakly textured regions in satellite imagery further make it hard to reconstruct reasonable terrain geometry and detailed building facades. To address these issues, we propose Sat-DN, a novel framework leveraging a progressively trained multi-resolution hash grid reconstruction architecture with explicit depth guidance and surface normal consistency constraints to enhance reconstruction quality. The multi-resolution hash grid accelerates training, while the progressive strategy incrementally increases the learning frequency, using coarse low-frequency geometry to guide the reconstruction of fine high-frequency details. The depth and normal constraints ensure a clear building outline and correct planar distribution. Extensive experiments on the DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving state-of-the-art results in both qualitative and quantitative evaluations. The code is available at https://github.com/costune/SatDN.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Embedding Sampling Method for Diverse Captioning</title>
<link>https://arxiv.org/abs/2502.10118</link>
<guid>https://arxiv.org/abs/2502.10118</guid>
<content:encoded><![CDATA[
arXiv:2502.10118v2 Announce Type: replace 
Abstract: Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, comparably smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset, respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection</title>
<link>https://arxiv.org/abs/2502.14891</link>
<guid>https://arxiv.org/abs/2502.14891</guid>
<content:encoded><![CDATA[
arXiv:2502.14891v3 Announce Type: replace 
Abstract: Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to explore the use of diffusion models to address the noise problem between multi-agent systems. In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's sampling. This process denoises coarse feature maps and progressively refines the fused features. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose and delay information of agents is with high-level noise. The code is released at https://github.com/HuangZhe885/CoDiff
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</title>
<link>https://arxiv.org/abs/2502.20323</link>
<guid>https://arxiv.org/abs/2502.20323</guid>
<content:encoded><![CDATA[
arXiv:2502.20323v5 Announce Type: replace 
Abstract: Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast rigid alignment of heterogeneous images in sliced Wasserstein distance</title>
<link>https://arxiv.org/abs/2503.13756</link>
<guid>https://arxiv.org/abs/2503.13756</guid>
<content:encoded><![CDATA[
arXiv:2503.13756v2 Announce Type: replace 
Abstract: Many applications of computer vision rely on the alignment of similar but non-identical images. We present a fast algorithm for aligning heterogeneous images based on optimal transport. Our approach combines the speed of fast Fourier methods with the robustness of sliced probability metrics and allows us to efficiently compute the alignment between two $L \times L$ images using the sliced 2-Wasserstein distance in $O(L^2 \log L)$ operations. We show that our method is robust to translations, rotations and deformations in the images.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification</title>
<link>https://arxiv.org/abs/2503.20652</link>
<guid>https://arxiv.org/abs/2503.20652</guid>
<content:encoded><![CDATA[
arXiv:2503.20652v5 Announce Type: replace 
Abstract: The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload. Multi-label classification of Three-Dimensional (3D) CT scans is a challenging task due to the volumetric nature of the data and the variety of anomalies to be detected. Existing deep learning methods based on Convolutional Neural Networks (CNNs) struggle to capture long-range dependencies effectively, while Vision Transformers require extensive pre-training, posing challenges for practical use. Additionally, these existing methods do not explicitly model the radiologist's navigational behavior while scrolling through CT scan slices, which requires both global context understanding and local detail awareness. In this study, we present CT-Scroll, a novel global-local attention model specifically designed to emulate the scrolling behavior of radiologists during the analysis of 3D CT scans. Our approach is evaluated on two public datasets, demonstrating its efficacy through comprehensive experiments and an ablation study that highlights the contribution of each model component.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model</title>
<link>https://arxiv.org/abs/2503.23746</link>
<guid>https://arxiv.org/abs/2503.23746</guid>
<content:encoded><![CDATA[
arXiv:2503.23746v2 Announce Type: replace 
Abstract: Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation</title>
<link>https://arxiv.org/abs/2504.05774</link>
<guid>https://arxiv.org/abs/2504.05774</guid>
<content:encoded><![CDATA[
arXiv:2504.05774v2 Announce Type: replace 
Abstract: Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs' attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.13392</link>
<guid>https://arxiv.org/abs/2504.13392</guid>
<content:encoded><![CDATA[
arXiv:2504.13392v2 Announce Type: replace 
Abstract: State-of-the-art visual generative AI tools hold immense potential to assist users in the early ideation stages of creative tasks -- offering the ability to generate (rather than search for) novel and unprecedented (instead of existing) images of considerable quality that also adhere to boundless combinations of user specifications. However, many large-scale text-to-image systems are designed for broad applicability, yielding conventional output that may limit creative exploration. They also employ interaction methods that may be difficult for beginners. Given that creative end users often operate in diverse, context-specific ways that are often unpredictable, more variation and personalization are necessary. We introduce POET, a real-time interactive tool that (1) automatically discovers dimensions of homogeneity in text-to-image generative models, (2) expands these dimensions to diversify the output space of generated images, and (3) learns from user feedback to personalize expansions. An evaluation with 28 users spanning four creative task domains demonstrated POET's ability to generate results with higher perceived diversity and help users reach satisfaction in fewer prompts during creative tasks, thereby prompting them to deliberate and reflect more on a wider range of possible produced results during the co-creative process. Focusing on visual creativity, POET offers a first glimpse of how interaction techniques of future text-to-image generation tools may support and align with more pluralistic values and the needs of end users during the ideation stages of their work.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking</title>
<link>https://arxiv.org/abs/2505.02980</link>
<guid>https://arxiv.org/abs/2505.02980</guid>
<content:encoded><![CDATA[
arXiv:2505.02980v2 Announce Type: replace 
Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. Among the various Spatial Transcriptomics techniques available, Visium has emerged as the most widely adopted. However, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. Additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. To address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. Yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. To bridge this gap, we introduce SpaRED, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. We further propose SpaCKLE, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE substantially improves the results across all the gene expression prediction models. Altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on Spatial Transcriptomics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Res-MoCoDiff: Residual-guided diffusion models for motion artifact correction in brain MRI</title>
<link>https://arxiv.org/abs/2505.03498</link>
<guid>https://arxiv.org/abs/2505.03498</guid>
<content:encoded><![CDATA[
arXiv:2505.03498v2 Announce Type: replace 
Abstract: Objective. Motion artifacts in brain MRI, mainly from rigid head motion, degrade image quality and hinder downstream applications. Conventional methods to mitigate these artifacts, including repeated acquisitions or motion tracking, impose workflow burdens. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model specifically designed for MRI motion artifact correction.Approach.Res-MoCoDiff exploits a novel residual error shifting mechanism during the forward diffusion process to incorporate information from motion-corrupted images. This mechanism allows the model to simulate the evolution of noise with a probability distribution closely matching that of the corrupted data, enabling a reverse diffusion process that requires only four steps. The model employs a U-net backbone, with attention layers replaced by Swin Transformer blocks, to enhance robustness across resolutions. Furthermore, the training process integrates a combined l1+l2 loss function, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on both an in-silico dataset generated using a realistic motion simulation framework and an in-vivo MR-ART dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and a diffusion model with a vision transformer backbone, using quantitative metrics such as PSNR, SSIM, and NMSE.Main results. The proposed method demonstrated superior performance in removing motion artifacts across minor, moderate, and heavy distortion levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks</title>
<link>https://arxiv.org/abs/2505.03522</link>
<guid>https://arxiv.org/abs/2505.03522</guid>
<content:encoded><![CDATA[
arXiv:2505.03522v2 Announce Type: replace 
Abstract: Deep learning has substantially advanced the field of Single Image Super-Resolution (SISR). However, existing research has predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of "Universality" and its associated definitions, which extend the traditional notion of "Generalization" to encompass the ease of transferability of modules. We then propose the Universality Assessment Equation (UAE), a metric that quantifies how readily a given module can be transplanted across models and reveals the combined influence of multiple existing metrics on transferability. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules: the Cycle Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, and other low-level tasks, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-art methods, achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity. Similar optimization approaches could be applied to a broader range of basic modules, offering a new paradigm for the design of plug-and-play modules.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods, Datasets, and Future Directions</title>
<link>https://arxiv.org/abs/2505.07611</link>
<guid>https://arxiv.org/abs/2505.07611</guid>
<content:encoded><![CDATA[
arXiv:2505.07611v2 Announce Type: replace 
Abstract: Traffic accident prediction and detection are critical for enhancing road safety, and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning. This paper reviews 147 recent studies, focusing on the application of supervised, unsupervised, and hybrid deep learning models for accident prediction, alongside the use of real-world and synthetic datasets. Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatio-temporal feature-based prediction, scene understanding, and multi modal data fusion. While these methods demonstrate significant potential, challenges such as data scarcity, limited generalization to complex scenarios, and real-time performance constraints remain prevalent. This review highlights opportunities for future research, including the integration of multi modal data fusion, self-supervised learning, and Transformer-based architectures to enhance prediction accuracy and scalability. By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems, contributing to road safety and traffic management.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification</title>
<link>https://arxiv.org/abs/2505.10823</link>
<guid>https://arxiv.org/abs/2505.10823</guid>
<content:encoded><![CDATA[
arXiv:2505.10823v2 Announce Type: replace 
Abstract: Foundation models provide robust embeddings for diverse tasks, including medical imaging. We evaluate embeddings from seven general and medical-specific foundation models (e.g., DenseNet121, BiomedCLIP, MedImageInsight, Rad-DINO, CXR-Foundation) for training lightweight adapters in multi-class radiography classification. Using a dataset of 8,842 radiographs across seven classes, we trained adapters with algorithms like K-Nearest Neighbors, logistic regression, SVM, random forest, and MLP. The combination of MedImageInsight embeddings with an SVM or MLP adapter achieved the highest mean area under the curve (mAUC) of 93.1%. This performance was statistically superior to other models, including MedSigLIP with an MLP (91.0%), Rad-DINO with an SVM (90.7%), and CXR-Foundation with logistic regression (88.6%). In contrast, models like BiomedCLIP (82.8%) and Med-Flamingo (78.5%) showed lower performance. Crucially, these lightweight adapters are computationally efficient, training in minutes and performing inference in seconds on a CPU, making them practical for clinical use. A fairness analysis of the top-performing MedImageInsight adapter revealed minimal performance disparities across patient gender (within 1.8%) and age groups (std. dev < 1.4%), with no significant statistical differences. These findings confirm that embeddings from specialized foundation models, particularly MedImageInsight, can power accurate, efficient, and equitable diagnostic tools using simple, lightweight adapters.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.20789</link>
<guid>https://arxiv.org/abs/2505.20789</guid>
<content:encoded><![CDATA[
arXiv:2505.20789v3 Announce Type: replace 
Abstract: Inverse problems (IPs) involve reconstructing signals from noisy observations. Recently, diffusion models (DMs) have emerged as a powerful framework for solving IPs, achieving remarkable reconstruction performance. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approaches under appropriate conditions and validate their superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</title>
<link>https://arxiv.org/abs/2505.23525</link>
<guid>https://arxiv.org/abs/2505.23525</guid>
<content:encoded><![CDATA[
arXiv:2505.23525v2 Announce Type: replace 
Abstract: Generating highly dynamic and photorealistic portrait animations driven by audio and skeletal motion remains challenging due to the need for precise lip synchronization, natural facial expressions, and high-fidelity body motion dynamics. We propose a human-preference-aligned diffusion framework that addresses these challenges through two key innovations. First, we introduce direct preference optimization tailored for human-centric animation, leveraging a curated dataset of human preferences to align generated outputs with perceptual metrics for portrait motion-video alignment and naturalness of expression. Second, the proposed temporal motion modulation resolves spatiotemporal resolution mismatches by reshaping motion conditions into dimensionally aligned latent features through temporal channel redistribution and proportional feature expansion, preserving the fidelity of high-frequency motion details in diffusion-based synthesis. The proposed mechanism is complementary to existing UNet and DiT-based portrait diffusion approaches, and experiments demonstrate obvious improvements in lip-audio synchronization, expression vividness, body motion coherence over baseline methods, alongside notable gains in human preference metrics. Our model and source code can be found at: https://github.com/xyz123xyz456/hallo4.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Based Autonomous MM-Wave Reflector Using ArUco-Driven Angle-of-Arrival Estimation</title>
<link>https://arxiv.org/abs/2506.05195</link>
<guid>https://arxiv.org/abs/2506.05195</guid>
<content:encoded><![CDATA[
arXiv:2506.05195v2 Announce Type: replace 
Abstract: Reliable millimeter-wave (mmWave) communication in non-line-of-sight (NLoS) conditions remains a major challenge for both military and civilian operations, especially in urban or infrastructure-limited environments. This paper presents a vision-aided autonomous reflector system designed to enhance mmWave link performance by dynamically steering signal reflections using a motorized metallic plate. The proposed system leverages a monocular camera to detect ArUco markers on allied transmitter and receiver nodes, estimate their angles of arrival, and align the reflector in real time for optimal signal redirection. This approach enables selective beam coverage by serving only authenticated targets with visible markers and reduces the risk of unintended signal exposure. The designed prototype, built on a Raspberry Pi 4 and low-power hardware, operates autonomously without reliance on external infrastructure or GPS. Experimental results at 60\,GHz demonstrate a 23\,dB average gain in received signal strength and an 0.89 probability of maintaining signal reception above a target threshold of -65 dB in an indoor environment, far exceeding the static and no-reflector baselines. These results demonstrate the system's potential for resilient and adaptive mmWave connectivity in complex and dynamic environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Controllable Real Image Denoising with Camera Parameters</title>
<link>https://arxiv.org/abs/2507.01587</link>
<guid>https://arxiv.org/abs/2507.01587</guid>
<content:encoded><![CDATA[
arXiv:2507.01587v2 Announce Type: replace 
Abstract: Recent deep learning-based image denoising methods have shown impressive performance; however, many lack the flexibility to adjust the denoising strength based on the noise levels, camera settings, and user preferences. In this paper, we introduce a new controllable denoising framework that adaptively removes noise from images by utilizing information from camera parameters. Specifically, we focus on ISO, shutter speed, and F-number, which are closely related to noise levels. We convert these selected parameters into a vector to control and enhance the performance of the denoising network. Experimental results show that our method seamlessly adds controllability to standard denoising neural networks and improves their performance. Code is available at https://github.com/OBAKSA/CPADNet.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecological Legacies of Pre-Columbian Settlements Evident in Palm Clusters of Neotropical Mountain Forests</title>
<link>https://arxiv.org/abs/2507.06949</link>
<guid>https://arxiv.org/abs/2507.06949</guid>
<content:encoded><![CDATA[
arXiv:2507.06949v2 Announce Type: replace 
Abstract: Ancient populations markedly transformed Neotropical forests, yet the spatial extent of their ecological influence remains underexplored at high resolution. Here we present a deep learning and remote sensing based approach to estimate areas of pre-Columbian forest modification based on modern vegetation. We apply this method to high-resolution satellite imagery from the Sierra Nevada de Santa Marta, Colombia, as a demonstration of a scalable approach, to evaluate palm tree distributions in relation to archaeological infrastructure. Palms were significantly more abundant near archaeological sites with large infrastructure investment. The extent of the largest palm cluster indicates that ancient human-managed areas linked to major infrastructure sites may be up to two orders of magnitude bigger than indicated by current archaeological evidence alone. Our findings suggest that pre-Columbian populations influenced vegetation, fostering conditions conducive to palm proliferation, leaving a lasting ecological footprint. This may have lowered the logistical costs of establishing infrastructure-heavy settlements in less accessible locations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic-aware fine-grained classification of pediatric wrist fractures</title>
<link>https://arxiv.org/abs/2507.12964</link>
<guid>https://arxiv.org/abs/2507.12964</guid>
<content:encoded><![CDATA[
arXiv:2507.12964v5 Announce Type: replace 
Abstract: Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. This study addresses the problem using a multifaceted approach: framing it as a fine-grained recognition task, fusing patient metadata with X-rays, and leveraging weights from a separate fine-grained dataset rather than from a coarse-grained dataset like ImageNet. Unlike prior work, this is the first application of metadata integration for wrist pathology recognition. Our results show that combining fine-grained transformer approach, fine-grained pre-training, and metadata integration improves diagnostic accuracy by 2% on small custom curated dataset and over 10% on a larger fracture dataset.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP</title>
<link>https://arxiv.org/abs/2507.14904</link>
<guid>https://arxiv.org/abs/2507.14904</guid>
<content:encoded><![CDATA[
arXiv:2507.14904v2 Announce Type: replace 
Abstract: 3D visual grounding allows an embodied agent to understand visual information in real-world 3D environments based on human instructions, which is crucial for embodied intelligence. Existing 3D visual grounding methods typically rely on separate encoders for different modalities (e.g., RGB images, text, and 3D point clouds), resulting in large and complex models that are inefficient to train. While some approaches use pre-trained 2D multi-modal models like CLIP for 3D tasks, they still struggle with aligning point cloud data to 2D encoders. As a result, these methods continue to depend on 3D encoders for feature extraction, further increasing model complexity and training inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal network to process all three modalities (RGB images, text, and point clouds), significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal model with adapter-based fine-tuning, this framework effectively adapts to the tri-modal setting, improving both adaptability and performance across modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module is designed to fuse geometric multi-scale features from point clouds and images. We then integrate textual features for final modality fusion and introduce a multi-modal decoder to facilitate deep cross-modal understanding. Together, our method achieves unified feature extraction and fusion across the three modalities, enabling an end-to-end 3D visual grounding model. Compared to the baseline, our method reduces the number of trainable parameters by approximately 58\%, while achieving a 6.52\% improvement in the 3D detection task and a 6.25\% improvement in the 3D visual grounding task.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Video Generation for High-Efficiency Video Compression</title>
<link>https://arxiv.org/abs/2507.15269</link>
<guid>https://arxiv.org/abs/2507.15269</guid>
<content:encoded><![CDATA[
arXiv:2507.15269v2 Announce Type: replace 
Abstract: Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</title>
<link>https://arxiv.org/abs/2507.22627</link>
<guid>https://arxiv.org/abs/2507.22627</guid>
<content:encoded><![CDATA[
arXiv:2507.22627v2 Announce Type: replace 
Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations and Models in Modern Computer Vision: Key Building Blocks in Landmark Architectures</title>
<link>https://arxiv.org/abs/2507.23357</link>
<guid>https://arxiv.org/abs/2507.23357</guid>
<content:encoded><![CDATA[
arXiv:2507.23357v2 Announce Type: replace 
Abstract: This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer architecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recognition. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TexVerse: A Universe of 3D Objects with High-Resolution Textures</title>
<link>https://arxiv.org/abs/2508.10868</link>
<guid>https://arxiv.org/abs/2508.10868</guid>
<content:encoded><![CDATA[
arXiv:2508.10868v2 Announce Type: replace 
Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model</title>
<link>https://arxiv.org/abs/2508.13238</link>
<guid>https://arxiv.org/abs/2508.13238</guid>
<content:encoded><![CDATA[
arXiv:2508.13238v2 Announce Type: replace 
Abstract: Recent advances in large vision-language models (LVLMs) have enabled a new paradigm of end-to-end document image parsing, excelling in Optical Character Recognition (OCR) tasks such as text, table, and formula recognition. However, generative LVLMs, similarly to large language models (LLMs), are prone to hallucinations--generating words that do not exist in input images. Furthermore, LVLMs are designed for general purposes and tend to be less effective on OCR tasks compared to expert models that are trained on domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a reasoning-enhanced framework designed to address these limitations through training reasoning-and-tool interleaved VLMs. Given a recognition instruction, our DianJin-OCR-R1 model first recognizes the content in the input image by its own OCR capabilities, and then calls other tools (i.e., other expert models) to obtain their results as references, finally "looks again" the image and rethinks about the reasoning process to provide the final recognized content. Since architectures of expert models are tailored for specific OCR tasks, which makes them less prone to hallucinations, their results can help VLMs mitigate hallucinations. We evaluate our model on ReST and OmniDocBench, and experimental results show that our DianJin-OCR-R1 models consistently outperform their non-reasoning counterparts and expert OCR models, which proves the effectiveness of our method. Additionally, the results indicate that enhancing expert models, which are typically small and easy to iterate, enable performance improvements for VLMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</title>
<link>https://arxiv.org/abs/2508.17832</link>
<guid>https://arxiv.org/abs/2508.17832</guid>
<content:encoded><![CDATA[
arXiv:2508.17832v2 Announce Type: replace 
Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior design, embodied intelligence, and scene understanding. While existing methods have made progress in coarse-scale furniture arrangement, they struggle to capture fine-grained object placements, limiting the realism and utility of generated environments. This gap hinders immersive virtual experiences and detailed scene comprehension for embodied AI applications. To address these issues, we propose Hierarchical Layout Generation (HLG), a novel method for fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine hierarchical approach, refining scene layouts from large-scale furniture placement to intricate object arrangements. Specifically, our fine-grained layout alignment module constructs a hierarchical layout through vertical and horizontal decoupling, effectively decomposing complex 3D indoor scenes into multiple levels of granularity. Additionally, our trainable layout optimization network addresses placement issues, such as incorrect positioning, orientation errors, and object intersections, ensuring structurally coherent and physically plausible scene generation. We demonstrate the effectiveness of our approach through extensive experiments, showing superior performance in generating realistic indoor scenes compared to existing methods. This work advances the field of scene generation and opens new possibilities for applications requiring detailed 3D environments. We will release our code upon publication to encourage future research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis from 3D OCT Imaging</title>
<link>https://arxiv.org/abs/2403.05702</link>
<guid>https://arxiv.org/abs/2403.05702</guid>
<content:encoded><![CDATA[
arXiv:2403.05702v2 Announce Type: replace-cross 
Abstract: Glaucoma, a leading cause of irreversible blindness, necessitates early detection for accurate and timely intervention to prevent irreversible vision loss. In this study, we present a novel deep learning framework that leverages the diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for automated glaucoma detection. In this framework, we integrate a pre-trained Vision Transformer on retinal data for rich slice-wise feature extraction and a bidirectional Gated Recurrent Unit for capturing inter-slice spatial dependencies. This dual-component approach enables comprehensive analysis of local nuances and global structural integrity, crucial for accurate glaucoma diagnosis. Experimental results on a large dataset demonstrate the superior performance of the proposed method over state-of-the-art ones, achieving an F1-score of 93.01%, Matthews Correlation Coefficient (MCC) of 69.33%, and AUC of 94.20%. The framework's ability to leverage the valuable information in 3D OCT data holds significant potential for enhancing clinical decision support systems and improving patient outcomes in glaucoma management.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-based Manipulation from Single Human Video with Open-World Object Graphs</title>
<link>https://arxiv.org/abs/2405.20321</link>
<guid>https://arxiv.org/abs/2405.20321</guid>
<content:encoded><![CDATA[
arXiv:2405.20321v2 Announce Type: replace-cross 
Abstract: This work presents an object-centric approach to learning vision-based manipulation skills from human videos. We investigate the problem of robot manipulation via imitation in the open-world setting, where a robot learns to manipulate novel objects from a single video demonstration. We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB or RGB-D video and deriving a policy that conditions on the extracted plan. Our method enables the robot to learn from videos captured by daily mobile devices and to generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances. We systematically evaluate our method on both short-horizon and long-horizon tasks, using RGB-D and RGB-only demonstration videos. Across varied tasks and demonstration types (RGB-D / RGB), we observe an average success rate of 74.4%, demonstrating the efficacy of ORION in learning from a single human video in the open world. Additional materials can be found on our project website: https://ut-austin-rpl.github.io/ORION-release.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents</title>
<link>https://arxiv.org/abs/2406.13923</link>
<guid>https://arxiv.org/abs/2406.13923</guid>
<content:encoded><![CDATA[
arXiv:2406.13923v2 Announce Type: replace-cross 
Abstract: Recent advancements in large multimodal models (LMMs) have leveraged extensive multimodal datasets to enhance capabilities in complex knowledge-driven tasks. However, persistent challenges in perceptual and reasoning errors limit their efficacy, particularly in interpreting intricate visual data and deducing multimodal relationships. To address these issues, we introduce PIN (Paired and INterleaved multimodal documents), a novel data format designed to foster a deeper integration of visual and textual knowledge. The PIN format uniquely combines semantically rich Markdown files, which preserve fine-grained textual structures, with holistic overall images that capture the complete document layout. Following this format, we construct and release two large-scale, open-source datasets: PIN-200M (~200 million documents) and PIN-14M (~14 million), compiled from diverse web and scientific sources in both English and Chinese. To maximize usability, we provide detailed statistical analyses and equip the datasets with quality signals, enabling researchers to easily filter and select data for specific tasks. Our work provides the community with a versatile data format and substantial resources, offering a foundation for new research in pre-training strategies and the development of more powerful knowledge-intensive LMMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPETIII: The Tracer Frontier. What Frontier?</title>
<link>https://arxiv.org/abs/2410.02807</link>
<guid>https://arxiv.org/abs/2410.02807</guid>
<content:encoded><![CDATA[
arXiv:2410.02807v2 Announce Type: replace-cross 
Abstract: For the last three years, the AutoPET competition gathered the medical imaging community around a hot topic: lesion segmentation on Positron Emitting Tomography (PET) scans. Each year a different aspect of the problem is presented; in 2024 the multiplicity of existing and used tracers was at the core of the challenge. Specifically, this year's edition aims to develop a fully automatic algorithm capable of performing lesion segmentation on a PET/CT scan, without knowing the tracer, which can either be a FDG or PSMA-based tracer. In this paper we describe how we used the nnUNetv2 framework to train two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion segmentation as well as a MIP-CNN to choose which set of models to use for segmentation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Distillation as Pushforward Optimal Quantization</title>
<link>https://arxiv.org/abs/2501.07681</link>
<guid>https://arxiv.org/abs/2501.07681</guid>
<content:encoded><![CDATA[
arXiv:2501.07681v2 Announce Type: replace-cross 
Abstract: Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose Dataset Distillation by Optimal Quantization, based on clustering in a latent space. Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better performance and inter-model generalization on the ImageNet-1K dataset with trivial additional computation, and SOTA performance in higher image-per-class settings. Using the distilled noise initializations in a stronger diffusion transformer model, we obtain SOTA distillation performance on ImageNet-1K and its subsets, outperforming diffusion guidance methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?</title>
<link>https://arxiv.org/abs/2502.06289</link>
<guid>https://arxiv.org/abs/2502.06289</guid>
<content:encoded><![CDATA[
arXiv:2502.06289v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Supervised and Unsupervised Segmentation and Classification of Materials Microstructure Images</title>
<link>https://arxiv.org/abs/2502.07107</link>
<guid>https://arxiv.org/abs/2502.07107</guid>
<content:encoded><![CDATA[
arXiv:2502.07107v2 Announce Type: replace-cross 
Abstract: Microstructure of materials is often characterized through image analysis to understand processing-structure-properties linkages. We propose a largely automated framework that integrates unsupervised and supervised learning methods to classify micrographs according to microstructure phase/class and, for multiphase microstructures, segments them into different homogeneous regions. With the advance of manufacturing and imaging techniques, the ultra-high resolution of imaging that reveals the complexity of microstructures and the rapidly increasing quantity of images (i.e., micrographs) enables and necessitates a more powerful and automated framework to extract materials characteristics and knowledge. The framework we propose can be used to gradually build a database of microstructure classes relevant to a particular process or group of materials, which can help in analyzing and discovering/identifying new materials. The framework has three steps: (1) segmentation of multiphase micrographs through a recently developed score-based method so that different microstructure homogeneous regions can be identified in an unsupervised manner; (2) {identification and classification of} homogeneous regions of micrographs through an uncertainty-aware supervised classification network trained using the segmented micrographs from Step $1$ with their identified labels verified via the built-in uncertainty quantification and minimal human inspection; (3) supervised segmentation (more powerful than the segmentation in Step $1$) of multiphase microstructures through a segmentation network trained with micrographs and the results from Steps $1$-$2$ using a form of data augmentation. This framework can iteratively characterize/segment new homogeneous or multiphase materials while expanding the database to enhance performance. The framework is demonstrated on various sets of materials and texture images.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis</title>
<link>https://arxiv.org/abs/2504.09885</link>
<guid>https://arxiv.org/abs/2504.09885</guid>
<content:encoded><![CDATA[
arXiv:2504.09885v2 Announce Type: replace-cross 
Abstract: Automating the synthesis of coordinated bimanual piano performances poses significant challenges, particularly in capturing the intricate choreography between the hands while preserving their distinct kinematic signatures. In this paper, we propose a dual-stream neural framework designed to generate synchronized hand gestures for piano playing from audio input, addressing the critical challenge of modeling both hand independence and coordination. Our framework introduces two key innovations: (i) a decoupled diffusion-based generation framework that independently models each hand's motion via dual-noise initialization, sampling distinct latent noise for each while leveraging a shared positional condition, and (ii) a Hand-Coordinated Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise to highlight asymmetric hand-specific features, while adaptively enhancing inter-hand coordination during denoising. Comprehensive evaluations demonstrate that our framework outperforms existing state-of-the-art methods across multiple metrics. Our project is available at https://monkek123king.github.io/S2C_page/.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryo-em images are intrinsically low dimensional</title>
<link>https://arxiv.org/abs/2504.11249</link>
<guid>https://arxiv.org/abs/2504.11249</guid>
<content:encoded><![CDATA[
arXiv:2504.11249v3 Announce Type: replace-cross 
Abstract: Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pulling Back the Curtain on ReLU Networks</title>
<link>https://arxiv.org/abs/2507.22832</link>
<guid>https://arxiv.org/abs/2507.22832</guid>
<content:encoded><![CDATA[
arXiv:2507.22832v3 Announce Type: replace-cross 
Abstract: Since any ReLU network is piecewise affine, its hidden units can be characterized by their pullbacks through the active subnetwork, i.e., by their gradients (up to bias terms). However, gradients of deeper neurons are notoriously misaligned, which obscures the network's internal representations. We posit that models do align gradients with data, yet this is concealed by the intrinsic noise of the ReLU hard gating. We validate this intuition by applying soft gating in the backward pass only, reducing the local impact of weakly excited neurons. The resulting modified gradients, which we call "excitation pullbacks", exhibit striking perceptual alignment on a number of ImageNet-pretrained architectures, while the rudimentary pixel-space gradient ascent quickly produces easily interpretable input- and target-specific features. Inspired by these findings, we formulate the "path stability" hypothesis, claiming that the binary activation patterns largely stabilize during training and get encoded in the pre-activation distribution of the final model. When true, excitation pullbacks become aligned with the gradients of a kernel machine that mainly determines the network's decision. This provides a theoretical justification for the apparent faithfulness of the feature attributions based on these pullbacks, potentially even leading to mechanistic interpretability of deeper models. Incidentally, we give a possible explanation for the effectiveness of Batch Normalization and Deep Features, together with a novel perspective on the network's internal memory and generalization properties. We release the code and an interactive app for easier exploration of the excitation pullbacks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation</title>
<link>https://arxiv.org/abs/2508.18826</link>
<guid>https://arxiv.org/abs/2508.18826</guid>
<content:encoded><![CDATA[
arXiv:2508.18826v2 Announce Type: replace-cross 
Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias in real-world scenarios, posing significant challenges in ethically sensitive domains such as healthcare. Such bias can negatively affect model fairness, model generalization abilities and further risks amplifying social discrimination. There is a need to remove biases from trained models. Existing debiasing approaches often necessitate access to original training data and need extensive model retraining; they also typically exhibit trade-offs between model fairness and discriminative performance. To address these challenges, we propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that efficiently improves fairness while preserving discriminative performance with much less debiasing costs. Notably, SWiFT requires only a small external dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to first find the relative, and yet distinct, contributions of model parameters to both bias and predictive performance. Then, a two-step fine-tuning process updates each parameter with different gradient flows defined by its contribution. Extensive experiments with three bias sensitive attributes (gender, skin tone, and age) across four dermatological and two chest X-ray datasets demonstrate that SWiFT can consistently reduce model bias while achieving competitive or even superior diagnostic accuracy under common fairness and accuracy metrics, compared to the state-of-the-art. Specifically, we demonstrate improved model generalization ability as evidenced by superior performance on several out-of-distribution (OOD) datasets.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model</title>
<link>https://arxiv.org/abs/2509.02659</link>
<guid>https://arxiv.org/abs/2509.02659</guid>
<content:encoded><![CDATA[
<div> Vision Language Models, end-to-end autonomous driving, deep neural networks, driving tasks, camera-only solution <br />
<br />
Summary: 
The article explores the use of Vision Language Models (VLM) in end-to-end autonomous driving tasks. While many current approaches rely on modular deep neural networks, this study investigates the potential benefits of incorporating powerful large language models (LLM) in driving systems. By combining end-to-end architectural design with knowledgeable VLMs, the researchers achieve impressive performance on driving tasks using only a single camera. Their method emerges as the top camera-only solution on the leaderboard, showcasing the efficacy of a vision-based driving approach for end-to-end driving tasks. This research highlights the importance of leveraging VLMs in autonomous driving systems and demonstrates the promising prospects for enhancing driving performance through innovative technological integration. <div>
arXiv:2509.02659v1 Announce Type: new 
Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many works focus on using modular deep neural networks to construct the end-to-end archi-tecture. However, whether using powerful large language models (LLM), especially multi-modality Vision Language Models (VLM) could benefit the end-to-end driving tasks remain a question. In our work, we demonstrate that combining end-to-end architectural design and knowledgeable VLMs yield impressive performance on the driving tasks. It is worth noting that our method only uses a single camera and is the best camera-only solution across the leaderboard, demonstrating the effectiveness of vision-based driving approach and the potential for end-to-end driving tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</title>
<link>https://arxiv.org/abs/2509.02807</link>
<guid>https://arxiv.org/abs/2509.02807</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, video MLLMs, visual grounding, motion-centric benchmark, spatiotemporal grounding<br />
Summary:<br />
The study investigates the incorporation of motion in pixel-level visual grounding using multi-modal large language models (MLLMs) in videos. Existing benchmarks are limited in capturing motion, leading to the introduction of a motion-centric benchmark, MoCentric-Bench. Four probing techniques were developed to assess video MLLMs' ability to recognize true motion and motion order. Strong single-image baselines were established, challenging models to enhance spatiotemporal grounding in videos. The study provides insights into motion-language interaction in visual grounding tasks and proposes simple motion-centric adaptation techniques for improved performance. The findings emphasize the importance of dense spatiotemporal grounding and pixel-level understanding in videos. The code and datasets will be publicly available for further research and development at the provided GitHub repository. <div>
arXiv:2509.02807v1 Announce Type: new 
Abstract: Multi-modal large language models (MLLMs) have shown impressive generalization across tasks using images and text modalities. While their extension to video has enabled tasks such as video question answering and video captioning, their pixel-level visual grounding abilities are less studied. In this work, we raise the pertinent question of whether motion is used in pixel-level visual grounding and whether video MLLMs can segment objects based on natural language expressions describing their motion patterns. We identify the shortcomings in the current benchmarks, where we show that a single frame can often suffice for capturing the motion referring expression without any temporal reasoning. To address this, we introduce four motion-centric probing techniques, particularly designed for the visual grounding task, to study video MLLMs' ability to identify true motion from a fake one and their ability to grasp the motion order. Consequently, we provide a motion-centric benchmark, MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging the interaction between motion and language rather than being dominated by static appearance cues emphasized in existing visual grounding datasets. We further establish strong single-image baselines that are on par with or outperform prior methods. Finally, we explore simple motion-centric adaptation techniques that provide state-of-the-art performance on our MoCentric-Bench. Our motion-centric benchmark, evaluation and findings challenge future models to improve dense spatiotemporal grounding and pixel-level understanding within videos. Code and datasets will be made publicly available at https://github.com/MSiam/PixFoundation-2.0.git.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach</title>
<link>https://arxiv.org/abs/2509.02851</link>
<guid>https://arxiv.org/abs/2509.02851</guid>
<content:encoded><![CDATA[
<div> Keywords: Colon cancer, deep learning, histopathological images, transformer modules, capsule networks

Summary: 
This research introduces a hybrid multi-scale deep learning architecture for colon cancer classification using the LC25000 dataset. The proposed HG-TNet model combines capsule networks, graph attention mechanisms, transformer modules, and residual learning to enhance classification accuracy in early-stage detection. The model leverages a transformer branch for global contextual information and a CNN branch for fine-grained details, allowing for a robust diagnostic representation. By incorporating self-supervised rotation prediction, the model surpasses standard architectures in performance metrics. The use of capsule networks preserves spatial orders and improves understanding of how individual elements form whole structures. The results demonstrate superior performance not only in accuracy and loss function but also in overall algorithmic performance. <div>
arXiv:2509.02851v1 Announce Type: new 
Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant types of cancer worldwide. Early-stage detection of colon cancer is highly crucial to prevent its deterioration. This research presents a hybrid multi-scale deep learning architecture that synergizes capsule networks, graph attention mechanisms, transformer modules, and residual learning to advance colon cancer classification on the Lung and Colon Cancer Histopathological Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the HG-TNet model that introduces a hybrid architecture that joins strength points in transformers and convolutional neural networks to capture multi-scale features in histopathological images. Mainly, a transformer branch extracts global contextual bonds by partitioning the image into patches by convolution-based patch embedding and then processing these patches through a transformer encoder. Analogously, a dedicated CNN branch captures fine-grained, local details through successive Incorporation these diverse features, combined with a self-supervised rotation prediction objective, produce a robust diagnostic representation that surpasses standard architectures in performance. Results show better performance not only in accuracy or loss function but also in these algorithms by utilizing capsule networks to preserve spatial orders and realize how each element individually combines and forms whole structures.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis</title>
<link>https://arxiv.org/abs/2509.02898</link>
<guid>https://arxiv.org/abs/2509.02898</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, active video acquisition, aortic stenosis, echocardiography, point-of-care ultrasound <br />
Summary: <br />
Researchers have developed a reinforcement learning-driven active video acquisition framework to improve the diagnosis of aortic stenosis (AS). AS, a dangerous condition caused by a narrowed aortic valve, often faces diagnostic limitations due to restricted access to echocardiography in rural and underserved areas. The proposed framework dynamically selects the most informative echo videos for each patient, optimizing accuracy and efficiency by continuously evaluating the need for additional imaging. Tested on data from 2,572 patients, the method achieved an 80.6% classification accuracy while using only 47% of the echo videos compared to a full acquisition. This innovative approach showcases the potential of active feature acquisition to enhance AS diagnosis, enabling more efficient, scalable, and personalized echocardiographic assessments. The source code for the framework is available on GitHub for further exploration and development. <br /> <div>
arXiv:2509.02898v1 Announce Type: new 
Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of the aortic valve, leading to impaired blood flow. Despite its high prevalence, access to echocardiography (echo), the gold-standard diagnostic tool, is often limited due to resource constraints, particularly in rural and underserved areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative but is restricted by operator expertise and the challenge of selecting the most relevant imaging views. To address this, we propose a reinforcement learning (RL)-driven active video acquisition framework that dynamically selects each patient's most informative echo videos. Unlike traditional methods that rely on a fixed set of videos, our approach continuously evaluates whether additional imaging is needed, optimizing both accuracy and efficiency. Tested on data from 2,572 patients, our method achieves 80.6% classification accuracy while using only 47% of the echo videos compared to a full acquisition. These results demonstrate the potential of active feature acquisition to enhance AS diagnosis, making echocardiographic assessments more efficient, scalable, and personalized. Our source code is available at: https://github.com/Armin-Saadat/PRECISE-AS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiGuard: A Streamlined Open-Source Framework for Rapid &amp; Interactive Lidar Research</title>
<link>https://arxiv.org/abs/2509.02902</link>
<guid>https://arxiv.org/abs/2509.02902</guid>
<content:encoded><![CDATA[
<div> Keywords: lidar, autonomous mobility, Intelligent Transportation Systems, LiGuard, open-source software

Summary:
LiGuard is an open-source software framework designed to simplify the development of lidar-based projects for autonomous mobility and Intelligent Transportation Systems. It provides built-in support for data input/output, pre/post processing, and common algorithms, allowing researchers to quickly develop code for their projects. Users can easily customize algorithms, adjust parameters, and visualize results for tasks such as classification, detection, segmentation, and tracking. LiGuard creates structured code files in directories, enabling easy sharing of projects and components for reuse by other researchers. The effectiveness of LiGuard is demonstrated through case studies, showcasing its potential to streamline research efforts and encourage collaboration in the field of lidar-based research. 

<br /><br />Summary: LiGuard, an open-source software framework, simplifies lidar-based project development by offering built-in support for data I/O, processing, and algorithms. Users can customize algorithms, visualize results, and easily share projects, fostering collaboration among researchers. <div>
arXiv:2509.02902v1 Announce Type: new 
Abstract: There is a growing interest in the development of lidar-based autonomous mobility and Intelligent Transportation Systems (ITS). To operate and research on lidar data, researchers often develop code specific to application niche. This approach leads to duplication of efforts across studies that, in many cases, share multiple methodological steps such as data input/output (I/O), pre/post processing, and common algorithms in multi-stage solutions. Moreover, slight changes in data, algorithms, and/or research focus may force major revisions in the code. To address these challenges, we present LiGuard, an open-source software framework that allows researchers to: 1) rapidly develop code for their lidar-based projects by providing built-in support for data I/O, pre/post processing, and commonly used algorithms, 2) interactively add/remove/reorder custom algorithms and adjust their parameters, and 3) visualize results for classification, detection, segmentation, and tracking tasks. Moreover, because it creates all the code files in structured directories, it allows easy sharing of entire projects or even the individual components to be reused by other researchers. The effectiveness of LiGuard is demonstrated via case studies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2509.02903</link>
<guid>https://arxiv.org/abs/2509.02903</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR-based perception, intelligent transportation systems, deep neural networks, simulation learning, synthetic datasets

Summary:
This paper discusses the challenges of creating large-scale labeled datasets for LiDAR-based perception systems in intelligent transportation systems (ITS). It introduces a methodology for generating high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs). The workflow outlined includes steps for replicating real-world environments, such as static geometry modeling, road infrastructure replication, and dynamic traffic scenario generation. By leveraging open-source resources like satellite imagery and OpenStreetMap data, robust synthetic environments can be constructed. These environments enable scalable, cost-effective, and diverse dataset generation, supporting Sim2Real learning. The effectiveness of this approach is dependent on the fidelity of the source simulation to the real-world, including environment structure, actor dynamics, and sensor emulations.<br /><br />Summary: <div>
arXiv:2509.02903v1 Announce Type: new 
Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks such as object detection, tracking, and semantic and instance segmentation, is predominantly solved by deep neural network models which often require large-scale labeled datasets during training to achieve generalization. However, creating these datasets is costly. time consuming and require human labor before the datasets are ready for training models. This hinders scalability of the LiDAR-based perception systems in ITS. Sim2Real learning offers scalable alternative, however, its effectiveness is dependent on the fidelity of the source simulation(s) to real-world, in terms of environment structure, actor dynamics, and sensor emulations. In response, this paper introduces a rigorous and reproducible methodology for creating large-scale, high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs). The proposed workflow outlines the steps, tools, and best practices for digitally replicating real-world environments, encompassing static geometry modeling, road infrastructure replication, and dynamic traffic scenario generation. Leveraging open-source and readily available resources such as satellite imagery and OpenStreetMap data, alongside specific sensor configurations, this paper provides practical, detailed guidance for constructing robust synthetic environments. These environments subsequently facilitate scalable, cost-effective, and diverse dataset generation, forming a reliable foundation for robust Sim2Real learning.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception</title>
<link>https://arxiv.org/abs/2509.02904</link>
<guid>https://arxiv.org/abs/2509.02904</guid>
<content:encoded><![CDATA[
<div> digital twins, LiDAR perception, Sim2Real transfer, domain adaptation, Intelligent Transportation Systems 

Summary:
The paper introduces a high-fidelity digital twin (HiFi DT) framework for improving LiDAR-based perception in Intelligent Transportation Systems (ITS) by addressing the Sim2Real gap. The framework incorporates real-world background geometry, road topology, and sensor specifications to generate in-domain synthetic data. Training an object detector on HiFi DT-generated data leads to a 4.8% performance improvement compared to models trained on real data. Various metrics like Chamfer Distance, Maximum Mean Discrepancy, Earth Mover's Distance, and Fréchet Distance are used to quantify the alignment between synthetic and real data, showing a substantial reduction in domain shift. These results highlight the significance of digital twins in enabling reliable simulation-based LiDAR perception for real-world ITS applications. 

<br /><br />Summary: <div>
arXiv:2509.02904v1 Announce Type: new 
Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for developing LiDAR-based perception (e.g., object detection, tracking, segmentation) in Intelligent Transportation Systems (ITS). However, perception models trained in simulation often under perform on real-world data due to distributional shifts. To address this Sim2Real gap, this paper proposes a high-fidelity digital twin (HiFi DT) framework that incorporates real-world background geometry, lane-level road topology, and sensor-specific specifications and placement. We formalize the domain adaptation challenge underlying Sim2Real learning and present a systematic method for constructing simulation environments that yield in-domain synthetic data. An off-the-shelf 3D object detector is trained on HiFi DT-generated synthetic data and evaluated on real data. Our experiments show that the DT-trained model outperforms the equivalent model trained on real data by 4.8%. To understand this gain, we quantify distributional alignment between synthetic and real data using multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy (MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both raw-input and latent-feature levels. Results demonstrate that HiFi DTs substantially reduce domain shift and improve generalization across diverse evaluation scenarios. These findings underscore the significant role of digital twins in enabling reliable, simulation-based LiDAR perception for real-world ITS applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach</title>
<link>https://arxiv.org/abs/2509.02918</link>
<guid>https://arxiv.org/abs/2509.02918</guid>
<content:encoded><![CDATA[
<div> KG-DG, neuro-symbolic framework, diabetic retinopathy, domain generalization, medical imaging<br />
Summary:<br />
- KG-DG is proposed for DR classification, using vision transformers and symbolic reasoning.<br />
- Clinical lesion ontologies and retinal vessel segmentation are integrated via confidence-weighted strategy.<br />
- KL divergence minimization aligns high-level clinical semantics for generalization.<br />
- Significantly improves accuracy in cross-domain and baseline ViT models.<br />
- Symbolic components act as effective regularizers enhancing interpretability and performance.<br /> <div>
arXiv:2509.02918v1 Announce Type: new 
Abstract: Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images</title>
<link>https://arxiv.org/abs/2509.02928</link>
<guid>https://arxiv.org/abs/2509.02928</guid>
<content:encoded><![CDATA[
<div> RetinaNet, DDR-Net, aerial imaging, deep learning, small object detection<br />
<br />
Summary:<br />
- DDR-Net is a deep-learning model designed to improve the detection of small objects in aerial imaging applications.
- DDR-Net uses data-driven techniques to optimize feature maps and anchor estimations for more accurate detection.
- The model introduces a novel sampling technique to enhance performance with limited training data.
- DDR-Net's enhanced detection capabilities support various critical applications such as wildlife monitoring and public safety improvements.
- The model reduces data collection and training costs while maintaining efficiency and accuracy.
- Empirical assessments show that DDR-Net outperforms other models in detecting small objects in aerial imagery datasets.
- These innovations will have wide-ranging impacts on sectors like agriculture, security, and archaeology. <br />  <div>
arXiv:2509.02928v1 Announce Type: new 
Abstract: In the realm of aerial imaging, the ability to detect small objects is pivotal for a myriad of applications, encompassing environmental surveillance, urban design, and crisis management. Leveraging RetinaNet, this work unveils DDR-Net: a data-driven, deep-learning model devised to enhance the detection of diminutive objects. DDR-Net introduces novel, data-driven techniques to autonomously ascertain optimal feature maps and anchor estimations, cultivating a tailored and proficient training process while maintaining precision. Additionally, this paper presents an innovative sampling technique to bolster model efficacy under limited data training constraints. The model's enhanced detection capabilities support critical applications including wildlife and habitat monitoring, traffic flow optimization, and public safety improvements through accurate identification of small objects like vehicles and pedestrians. DDR-Net significantly reduces the cost and time required for data collection and training, offering efficient performance even with limited data. Empirical assessments over assorted aerial avian imagery datasets demonstrate that DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These innovations advance current aerial image analysis technologies and promise wide-ranging impacts across multiple sectors including agriculture, security, and archaeology.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images</title>
<link>https://arxiv.org/abs/2509.02952</link>
<guid>https://arxiv.org/abs/2509.02952</guid>
<content:encoded><![CDATA[
<div> Keywords: whole-slide histopathological images, registration, artificial intelligence, rigid framework, open-source

Summary:
STAR (Serial Tissue Alignment for Rigid registration) is a new, fast, and robust framework for aligning serial whole-slide histopathological images (WSIs). It integrates stain-conditioned preprocessing and a hierarchical coarse-to-fine correlation strategy to achieve reliable rigid registration across various tissue types and staining protocols. The framework includes adaptive kernel scaling and built-in quality control for improved alignment performance. STAR was evaluated on different datasets and showed stable alignments within minutes per slide, demonstrating robustness to cross-stain variability. The tool is open-source and lightweight, providing a reproducible baseline that can lower the barrier for clinical adoption and facilitate large-scale paired data preparation for AI workflows in computational pathology. Case studies on H&amp;E-IHC alignment, multi-IHC panels construction, and typical failure modes were presented, illustrating the utility and limitations of STAR. 

<br /><br />Summary: <div>
arXiv:2509.02952v1 Announce Type: new 
Abstract: Registration of serial whole-slide histopathological images (WSIs) is critical for enabling direct comparison across diverse stains and for preparing paired datasets in artificial intelligence (AI) workflows such as virtual staining and biomarker prediction. While existing methods often rely on complex deformable or deep learning approaches that are computationally intensive and difficult to reproduce, lightweight rigid frameworks-sufficient for many consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial Tissue Alignment for Rigid registration), a fast and robust open-source framework for multi-WSI alignment. STAR integrates stain-conditioned preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive kernel scaling, and built-in quality control, achieving reliable rigid registration across heterogeneous tissue types and staining protocols, including hematoxylin-eosin (H&amp;E), special histochemical stains (e.g., PAS, PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67). Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs and scanning conditions, STAR consistently produced stable alignments within minutes per slide, demonstrating robustness to cross-stain variability and partial tissue overlap. Beyond benchmarks, we present case studies on H&amp;E-IHC alignment, construction of multi-IHC panels, and typical failure modes, underscoring both utility and limitations. Released as an open and lightweight tool, STAR provides a reproducible baseline that lowers the barrier for clinical adoption and enables large-scale paired data preparation for next-generation computational pathology.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability</title>
<link>https://arxiv.org/abs/2509.02962</link>
<guid>https://arxiv.org/abs/2509.02962</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal industrial surface defect detection, Modality-missing, Cross-modal prompt learning, Symmetric contrastive learning, Triple-modal contrastive pre-training

Summary:
Cross-modal prompt learning is proposed to address modality-missing in multimodal industrial surface defect detection. It includes cross-modal consistency prompts, modality-specific prompts, and missing-aware prompts to handle information consistency and vacancy. Symmetric contrastive learning uses text modality as a bridge for fusion, employing paired antithetical text prompts and triple-modal contrastive pre-training for multimodal learning. Experimental results show superior performance over existing methods, achieving high I-AUROC and P-AUROC with a total missing rate of 0.7 for RGB and 3D modalities. The proposed method outperforms competitors under different missing types and rates. The source code is available for further exploration and implementation. 

<br /><br />Summary: Cross-modal prompt learning addresses modality-missing in multimodal defect detection, enabling information consistency and adaptation. Symmetric contrastive learning utilizes text as a fusion bridge, enhancing multimodal learning performance and outperforming existing methods. <div>
arXiv:2509.02962v1 Announce Type: new 
Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and locate defect in industrial products by fusing RGB and 3D modalities. This article focuses on modality-missing problems caused by uncertain sensors availability in MISDD. In this context, the fusion of multiple modalities encounters several troubles, including learning mode transformation and information vacancy. To this end, we first propose cross-modal prompt learning, which includes: i) the cross-modal consistency prompt serves the establishment of information consistency of dual visual modalities; ii) the modality-specific prompt is inserted to adapt different input patterns; iii) the missing-aware prompt is attached to compensate for the information vacancy caused by dynamic modalities-missing. In addition, we propose symmetric contrastive learning, which utilizes text modality as a bridge for fusion of dual vision modalities. Specifically, a paired antithetical text prompt is designed to generate binary text semantics, and triple-modal contrastive pre-training is offered to accomplish multimodal learning. Experiment results show that our proposed method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7 for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58% respectively), and outperforms existing approaches to varying degrees under different missing types and rates. The source code will be available at https://github.com/SvyJ/MISDD-MM.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeAttNet: Towards Barb-Aware Filament Segmentation</title>
<link>https://arxiv.org/abs/2509.02964</link>
<guid>https://arxiv.org/abs/2509.02964</guid>
<content:encoded><![CDATA[
<div> Keywords: solar filaments, segmentation, EdgeAttNet, U-Net, MAGFILO dataset

Summary:
EdgeAttNet is a novel segmentation architecture designed to accurately segment solar filaments in H-alpha observations, capturing fine-scale structures such as barbs more effectively. It integrates a learnable edge map into the model to guide the self-attention mechanism at the network's bottleneck, enhancing spatial sensitivity and segmentation accuracy while reducing trainable parameters. Compared to U-Net and other U-Net-based transformer baselines, EdgeAttNet outperforms in segmentation accuracy and barb recognition on the MAGFILO dataset. It also provides faster inference performance, making it suitable for practical deployment. This approach addresses the limitations of existing methods in capturing long-range dependencies and spatial detail, crucial for determining filament chirality and understanding the behavior of Coronal Mass Ejections (CMEs). EdgeAttNet demonstrates the potential of incorporating structural priors into attention computations for improved segmentation performance in solar observations. 

<br /><br />Summary: <div>
arXiv:2509.02964v1 Announce Type: new 
Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical for determining filament chirality, a key factor in the behavior of Coronal Mass Ejections (CMEs). However, existing methods often fail to capture fine-scale filament structures, particularly barbs, due to a limited ability to model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone by introducing a novel, learnable edge map derived directly from the input image. This edge map is incorporated into the model by linearly transforming the attention Key and Query matrices with the edge information, thereby guiding the self-attention mechanism at the network's bottleneck to more effectively capture filament boundaries and barbs. By explicitly integrating this structural prior into the attention computations, EdgeAttNet enhances spatial sensitivity and segmentation accuracy while reducing the number of trainable parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based transformer baselines on the MAGFILO dataset. It achieves higher segmentation accuracy and significantly better recognition of filament barbs, with faster inference performance suitable for practical deployment.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.02966</link>
<guid>https://arxiv.org/abs/2509.02966</guid>
<content:encoded><![CDATA[
<div> knowledge-enhanced VLM, trajectory prediction, autonomous driving, scene dynamics, self-supervised learning <br />
Summary:<br />
Accurate short-horizon trajectory prediction is crucial for autonomous driving safety. The KEPT framework combines a TFSF video encoder with a retrieval stack for scene-aligned exemplars, enhancing model reasoning with domain knowledge. By embedding retrieved priors into CoT prompts and incrementally aligning language with spatial cues, motion feasibility, and temporal planning, KEPT achieves state-of-the-art performance on nuScenes dataset. Three fine-tuning stages contribute to improved accuracy and safety, with Top-2 retrieved exemplars offering the best trade-off. The k-means-clustered HNSW index supports efficient retrieval, enabling practical deployment of interpretable and trustworthy autonomous driving models.<br /> <div>
arXiv:2509.02966v1 Announce Type: new 
Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable autonomous driving, yet existing vision-language models (VLMs) often fail to effectively ground their reasoning in scene dynamics and domain knowledge. To address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM framework that predicts ego trajectories directly from consecutive front-view driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video encoder, trained via self-supervised learning with hard-negative mining, with a scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars. Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit planning constraints, while a triple-stage fine-tuning schedule incrementally aligns the language head to metric spatial cues, physically feasible motion, and temporally conditioned front-view planning. Evaluated on nuScenes dataset, KEPT achieves state-of-the-art performance across open-loop protocols: under NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision rate. Ablation studies show that all three fine-tuning stages contribute complementary benefits, and that using Top-2 retrieved exemplars yields the best accuracy-safety trade-off. The k-means-clustered HNSW index delivers sub-millisecond retrieval latency, supporting practical deployment. These results indicate that retrieval-augmented, CoT-guided VLMs offer a promising, data-efficient pathway toward interpretable and trustworthy autonomous driving.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results</title>
<link>https://arxiv.org/abs/2509.02969</link>
<guid>https://arxiv.org/abs/2509.02969</guid>
<content:encoded><![CDATA[
<div> Challenge, VQualA 2025, Engagement Prediction, Short Videos, User-Generated Content <br />
Summary: 
The paper provides an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, focusing on the popularity of user-generated content (UGC) short videos on social media platforms. A new short-form UGC dataset with engagement metrics was used to promote robust modeling strategies that capture factors influencing user engagement. Participants explored multi-modal features such as visual content, audio, and metadata to predict engagement. The challenge attracted 97 participants and received 15 valid test submissions, making significant progress in short-form UGC video engagement prediction. <div>
arXiv:2509.02969v1 Announce Type: new 
Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System</title>
<link>https://arxiv.org/abs/2509.02973</link>
<guid>https://arxiv.org/abs/2509.02973</guid>
<content:encoded><![CDATA[
<div> Keywords: instance segmentation, data augmentation, large language models, diffusion models, dual-agent system

Summary: 
InstaDA proposes a novel Dual-Agent system for augmenting instance segmentation datasets. The Text-Agent (T-Agent) enhances data diversity through collaboration between large language models and diffusion models, utilizing a Prompt Rethink mechanism to refine prompts iteratively. The Image-Agent (I-Agent) enriches data distribution by generating new instances conditioned on training images. Both agents operate as independent and automated workflows for practicality and efficiency. Experimental results on the LVIS 1.0 validation set show significant improvements in box and mask average precision compared to baseline and outperforming the leading model, DiverGen. InstaDA achieves a notable performance improvement on common and frequent categories, demonstrating its effectiveness in enhancing instance segmentation datasets.<br /><br />Summary: <div>
arXiv:2509.02973v1 Announce Type: new 
Abstract: Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.02993</link>
<guid>https://arxiv.org/abs/2509.02993</guid>
<content:encoded><![CDATA[
<div> Few-Shot Medical Image Segmentation, Prototype-based methods, Self-guided Prototype Enhancement Network, Multi-level Prototype Generation, Query-guided Local Prototype Enhancement

Summary:
The study introduces the Self-guided Prototype Enhancement Network (SPENet) to improve Few-Shot Medical Image Segmentation. The Multi-level Prototype Generation (MPG) module generates global and local prototypes to measure similarities between support and query images. The Query-guided Local Prototype Enhancement (QLPE) module refines support prototypes based on guidance from the query image, addressing discrepancies. SPENet outperforms existing methods on public medical datasets, demonstrating superior performance in segmenting novel classes with limited labeled images. <div>
arXiv:2509.02993v1 Announce Type: new 
Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intra-class variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies between the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing state-of-the-art methods, achieving superior performance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2509.03002</link>
<guid>https://arxiv.org/abs/2509.03002</guid>
<content:encoded><![CDATA[
<div> Keywords: small object segmentation, remote sensing imagery, instance segmentation, SOPSeg, bounding boxes

Summary: 
SOPSeg is a prompt-based framework designed specifically for small object segmentation in remote sensing imagery. It addresses the lack of datasets for small object instance segmentation by introducing a region-adaptive magnification strategy to preserve fine details and a customized decoder for accurate boundary delineation. A novel prompting mechanism tailored to oriented bounding boxes is also implemented. Compared to existing methods, SOPSeg outperforms in small object segmentation and aids in efficient dataset construction for remote sensing tasks. The proposed framework addresses the limitations of current methods, such as the loss of fine spatial details in small object segmentation. Additionally, a comprehensive small object instance segmentation dataset based on SODA-A is constructed to support future research. Both the model and dataset will be released to facilitate advancements in small object segmentation for remote sensing applications. 

<br /><br />Summary: <div>
arXiv:2509.03002v1 Announce Type: new 
Abstract: Extracting small objects from remote sensing imagery plays a vital role in various applications, including urban planning, environmental monitoring, and disaster management. While current research primarily focuses on small object detection, instance segmentation for small objects remains underexplored, with no dedicated datasets available. This gap stems from the technical challenges and high costs of pixel-level annotation for small objects. While the Segment Anything Model (SAM) demonstrates impressive zero-shot generalization, its performance on small-object segmentation deteriorates significantly, largely due to the coarse 1/16 feature resolution that causes severe loss of fine spatial details. To this end, we propose SOPSeg, a prompt-based framework specifically designed for small object segmentation in remote sensing imagery. It incorporates a region-adaptive magnification strategy to preserve fine-grained details, and employs a customized decoder that integrates edge prediction and progressive refinement for accurate boundary delineation. Moreover, we introduce a novel prompting mechanism tailored to the oriented bounding boxes widely adopted in remote sensing applications. SOPSeg outperforms existing methods in small object segmentation and facilitates efficient dataset construction for remote sensing tasks. We further construct a comprehensive small object instance segmentation dataset based on SODA-A, and will release both the model and dataset to support future research.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</title>
<link>https://arxiv.org/abs/2509.03006</link>
<guid>https://arxiv.org/abs/2509.03006</guid>
<content:encoded><![CDATA[
<div> post-processing watermarking, ensemble attack network, CNN, Transformer, robustness

Summary: 
The study focuses on enhancing the robustness of post-processing watermarking by incorporating an ensemble attack network during training. Different versions of attack networks using CNN and Transformer in spatial and frequency domains are constructed to analyze their impact on the watermarking model's robustness. Results show that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain achieves the highest robustness. Evaluation on the WAVES benchmark indicates that the ensemble attack network improves the robustness of baseline watermarking methods, with a significant enhancement in performance against the Regeneration Attack scenario, improving StegaStamp by 18.743%. The code for the proposed method is available at https://github.com/aiiu-lab/DeepRobustWatermark. <div>
arXiv:2509.03006v1 Announce Type: new 
Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations</title>
<link>https://arxiv.org/abs/2509.03011</link>
<guid>https://arxiv.org/abs/2509.03011</guid>
<content:encoded><![CDATA[
<div> Keywords: lesion-aware image captioning, ulcerative colitis, ResNet embeddings, Grad-CAM heatmaps, T5 decoder

Summary: 
A new lesion-aware image captioning framework for ulcerative colitis (UC) is presented in this paper. The model incorporates ResNet embeddings, Grad-CAM heatmaps, and CBAM-enhanced attention with a T5 decoder to improve caption quality and classification accuracy of Mayo Endoscopic Subscore (MES). Clinical metadata including MES score, vascular pattern, bleeding, erythema, friability, and ulceration are utilized as prompts for generating descriptive captions aligned with clinical practice. The system not only enhances the interpretability of the descriptions but also provides MES classification and lesion tags. By outperforming baselines, this approach facilitates reliable endoscopic reporting in UC cases. <br /><br />Summary: <div>
arXiv:2509.03011v1 Announce Type: new 
Abstract: We present a lesion-aware image captioning framework for ulcerative colitis (UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3, vascular pattern, bleeding, erythema, friability, ulceration) is injected as natural-language prompts to guide caption generation. The system produces structured, interpretable descriptions aligned with clinical practice and provides MES classification and lesion tags. Compared with baselines, our approach improves caption quality and MES classification accuracy, supporting reliable endoscopic reporting.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens</title>
<link>https://arxiv.org/abs/2509.03025</link>
<guid>https://arxiv.org/abs/2509.03025</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, visual grounding, Feed-Forward Network, Visual Absence-aware neurons, detection module

Summary:
Large Vision-Language Models (LVLMs) often incorrectly perceive text inputs without visual evidence as part of the image, resulting in inaccurate responses. This study investigates LVLMs' ability to determine if textual concepts are grounded in the image and identifies Visual Absence-aware (VA) neurons in the Feed-Forward Network that signal visual absence. A detection module is developed based on VA neuron activation patterns to classify visually grounded tokens. By refining outputs and reinterpreting input tokens, the model effectively reduces false presumptions of visual presence in text input. Extensive experiments demonstrate the method's effectiveness across various LVLMs. The study provides insights into improving the contextual relevancy and accuracy of LVLM responses by considering visual grounding in text interpretation and generation. 

<br /><br />Summary: Large Vision-Language Models often misinterpret text inputs without visual evidence, leading to erroneous responses. VA neurons in the Feed-Forward Network signal visual absence, enabling the development of a detection module that classifies visually grounded tokens. By refining outputs based on detection results, the model mitigates false presumptions of visual presence in text input, enhancing the accuracy and contextual relevancy of LVLM responses. This approach shows promise for improving the performance of LVLMs in various applications. <div>
arXiv:2509.03025v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification</title>
<link>https://arxiv.org/abs/2509.03032</link>
<guid>https://arxiv.org/abs/2509.03032</guid>
<content:encoded><![CDATA[
<div> semantic cues, person re-identification, background information, foreground information, feature extraction <br />
Summary:
This paper introduces a novel approach to person re-identification that addresses the challenges of locating the target accurately and extracting fine-grained features. By incorporating both foreground and background information in a dual-branch cross-modal feature extraction pipeline, the proposed framework leverages semantic alignment and adversarial learning strategies to enhance the discriminative power of the network. By aligning visual and textual features with similar semantics and penalizing similarity between foreground and background features, the model is able to suppress noisy background regions and focus on identity-relevant foreground cues. Experimental results on various benchmark datasets show that the proposed method outperforms current state-of-the-art approaches in terms of accuracy and generality. <br /> <br />Summary: <div>
arXiv:2509.03032v1 Announce Type: new 
Abstract: Person re-identification faces two core challenges: precisely locating the foreground target while suppressing background noise and extracting fine-grained features from the target region. Numerous visual-only approaches address these issues by partitioning an image and applying attention modules, yet they rely on costly manual annotations and struggle with complex occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic cues to guide visual understanding. However, they focus solely on foreground information, but overlook the potential value of background cues. Inspired by human perception, we argue that background semantics are as important as the foreground semantics in ReID, as humans tend to eliminate background distractions while focusing on target appearance. Therefore, this paper proposes an end-to-end framework that jointly models foreground and background information within a dual-branch cross-modal feature extraction pipeline. To help the network distinguish between the two domains, we propose an intra-semantic alignment and inter-semantic adversarial learning strategy. Specifically, we align visual and textual features that share the same semantics across domains, while simultaneously penalizing similarity between foreground and background features to enhance the network's discriminative power. This strategy drives the model to actively suppress noisy background regions and enhance attention toward identity-relevant foreground cues. Comprehensive experiments on two holistic and two occluded ReID benchmarks demonstrate the effectiveness and generality of the proposed method, with results that match or surpass those of current state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model</title>
<link>https://arxiv.org/abs/2509.03041</link>
<guid>https://arxiv.org/abs/2509.03041</guid>
<content:encoded><![CDATA[
<div> skin-lesion segmentation, Convolutional Neural Networks, Vision Transformers, lightweight, dermoscopic<br />
Summary:<br />
- Accurate skin-lesion segmentation is challenging for computer-aided diagnosis of skin cancer. 
- Convolutional Neural Networks struggle with long-range dependencies.
- Vision Transformers capture global context but are not suitable for small-sample medical datasets.
- MedLiteNet is a lightweight CNN Transformer hybrid designed for dermoscopic segmentation.
- MedLiteNet uses Mobile Inverted Bottleneck blocks, cross-scale token-mixing, and boundary-aware self-attention for precise lesion contouring. <br /> <div>
arXiv:2509.03041v1 Announce Type: new 
Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks</title>
<link>https://arxiv.org/abs/2509.03044</link>
<guid>https://arxiv.org/abs/2509.03044</guid>
<content:encoded><![CDATA[
<div> Keywords: Conditional diffusion models, Multi-task scenarios, Ill-posed tasks, Dynamic conditions, Image processing

Summary: 
Conditional diffusion models have shown significant progress in image processing tasks. However, in multi-task scenarios, exploiting the correlation between tasks and dealing with ill-posed tasks with limited training data present challenges. To address these issues, a dynamic conditional double diffusion bridge training paradigm is proposed. This paradigm separates the diffusion and condition generation processes, allowing the diffusion model to operate without supervised data in ill-posed tasks. Dynamic conditions are introduced to adjust statistical characteristics gradually, embedding time-related information and facilitating network learning in multi-task scenarios. Analysis of the network's learning objectives and attention weights demonstrates the effectiveness of dynamic conditions. By applying this paradigm to dehazing and visible-infrared fusion tasks, superior performance is achieved on public datasets. The code for the proposed framework is publicly available. 

Summary: <div>
arXiv:2509.03044v1 Announce Type: new 
Abstract: Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isolated Bangla Handwritten Character Classification using Transfer Learning</title>
<link>https://arxiv.org/abs/2509.03061</link>
<guid>https://arxiv.org/abs/2509.03061</guid>
<content:encoded><![CDATA[
<div> transfer learning, Bangla characters, handwritten recognition, deep neural network, classification

Summary:
The study focuses on utilizing transfer learning techniques to classify Bangla characters, including basic, distinct, and compound forms. Deep Neural Network models such as 3DCNN, ResNet, and MobileNet are employed to classify handwritten Bangla characters using the Bangla Lekha Isolated dataset. The model achieves high accuracy rates of 99.82% on training data and 99.46% on test data. By comparing with state-of-the-art benchmarks, the proposed model outperforms in accurately classifying Bangla handwritten characters. The research contributes to the advancement of Bangla language processing and recognition technologies. 
<br /><br />Summary: <div>
arXiv:2509.03061v1 Announce Type: new 
Abstract: Bangla language consists of fifty distinct characters and many compound characters. Several notable studies have been performed to recognize Bangla characters, both handwritten and optical. Our approach uses transfer learning to classify the basic, distinct, as well as compound Bangla handwritten characters while avoiding the vanishing gradient problem. Deep Neural Network techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural Network (ResNet), and MobileNet are applied to generate an end-to-end classification of all possible standard formations of handwritten characters in the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105 Bangla character image samples categorized into 84 distinct classes, is used for this classification model. The model achieved 99.82% accuracy on training data and 99.46% accuracy on test data. Comparisons with various state-of-the-art benchmarks of Bangla handwritten character classification show that the proposed model achieves better accuracy in classifying the data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Cursive Complex Character Recognition using GAN External Classifier</title>
<link>https://arxiv.org/abs/2509.03062</link>
<guid>https://arxiv.org/abs/2509.03062</guid>
<content:encoded><![CDATA[
<div> Classifier, Generative Adversarial Network, Handwritten characters, Cursive, Complex<br />
<br />
Summary: <br />
Handwritten characters pose challenges for classification due to their cursive and complex nature. In this study, an external classifier in conjunction with a Generative Adversarial Network (GAN) was proposed to address this issue. The GAN generated fake handwritten character images, which were then used to augment the training data. By adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network, the proposed model, ADA-GAN, exhibited robustness in classifying both cursive and complex characters. While the accuracy of convolutional neural networks decreased with increasing character complexity, the ADA-GAN model remained effective in handling the challenging classification tasks. <div>
arXiv:2509.03062v1 Announce Type: new 
Abstract: Handwritten characters can be trickier to classify due to their complex and cursive nature compared to simple and non-cursive characters. We present an external classifier along with a Generative Adversarial Network that can classify highly cursive and complex characters. The generator network produces fake handwritten character images, which are then used to augment the training data after adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network. The results show that the accuracy of convolutional neural networks decreases as character complexity increases, but our proposed model, ADA-GAN, remains more robust and effective for both cursive and complex characters.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis</title>
<link>https://arxiv.org/abs/2509.03095</link>
<guid>https://arxiv.org/abs/2509.03095</guid>
<content:encoded><![CDATA[
<div> Generative Model, Feature Transfer, Aneurysm Analysis, Neural Networks, 3D Data<br />
Summary:<br />
This study addresses the challenges in detecting and modeling intracranial aneurysms by proposing a cross-domain feature-transfer approach. The approach utilizes latent geometric embeddings learned by the TRELLIS generative model from non-medical 3D datasets to enhance neural networks for aneurysm analysis. By incorporating TRELLIS surface features in place of traditional descriptors, the study improves the accuracy of tasks such as aneurysm classification, vessel segmentation, and blood-flow prediction. Experimental results demonstrate significant gains in accuracy, F1-score, and segmentation quality over existing methods, as well as a 15% reduction in simulation error. The study highlights the potential of transferring 3D representations from general-purpose generative models to specialized medical tasks, offering promising avenues for improving the detection and analysis of intracranial aneurysms. <br /> <div>
arXiv:2509.03095v1 Announce Type: new 
Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to detect, delineate and model due to limited annotated 3D data. We propose a cross-domain feature-transfer approach that leverages the latent geometric embeddings learned by TRELLIS, a generative model trained on large-scale non-medical 3D datasets, to augment neural networks for aneurysm analysis. By replacing conventional point normals or mesh descriptors with TRELLIS surface features, we systematically enhance three downstream tasks: (i) classifying aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving blood-flow fields using a graph neural network on the AnXplore dataset. Our experiments show that the inclusion of these features yields strong gains in accuracy, F1-score and segmentation quality over state-of-the-art baselines, and reduces simulation error by 15\%. These results illustrate the broader potential of transferring 3D representations from general-purpose generative models to specialized medical tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods</title>
<link>https://arxiv.org/abs/2509.03108</link>
<guid>https://arxiv.org/abs/2509.03108</guid>
<content:encoded><![CDATA[
<div> Spoofing attacks, Face recognition systems, Backdoor poisoning attack, Deep learning, Anti-spoofing detection<br />
Summary:<br />
Face recognition systems are vulnerable to spoofing attacks using user face photos. To prevent this, detecting whether an input image is live or spoofed is crucial. Most spoofing attack detection methods rely on deep learning but require a large amount of training data, making them susceptible to backdoor poisoning attacks. In this study, a novel backdoor poisoning attack method is proposed to show the threat it poses to face anti-spoofing detection. This method allows certain spoofing attacks to evade detection by embedding features from a spoofed face image into a live face image without any visible changes. Experiments on public datasets confirm the effectiveness of this method, highlighting the need for improved security measures in face recognition systems. <br /><br /> <div>
arXiv:2509.03108v1 Announce Type: new 
Abstract: Face recognition systems are robust against environmental changes and noise, and thus may be vulnerable to illegal authentication attempts using user face photos, such as spoofing attacks. To prevent such spoofing attacks, it is crucial to discriminate whether the input image is a live user image or a spoofed image prior to the face recognition process. Most existing spoofing attack detection methods utilize deep learning, which necessitates a substantial amount of training data. Consequently, if malicious data is injected into a portion of the training dataset, a specific spoofing attack may be erroneously classified as live, leading to false positives.In this paper, we propose a novel backdoor poisoning attack method to demonstrate the latent threat of backdoor poisoning within face anti-spoofing detection. The proposed method enables certain spoofing attacks to bypass detection by embedding features extracted from the spoofing attack's face image into a live face image without inducing any perceptible visual alterations.Through experiments conducted on public datasets, we demonstrate that the proposed method constitutes a realistic threat to existing spoofing attack detection systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information transmission: Inferring change area from change moment in time series remote sensing images</title>
<link>https://arxiv.org/abs/2509.03112</link>
<guid>https://arxiv.org/abs/2509.03112</guid>
<content:encoded><![CDATA[
<div> Keywords: time series change detection, deep learning, spatial change detection, Class Activation Mapping, CAIM-Net<br />
Summary: <br />
Time series change detection in ecosystem dynamics using remote sensing images is important for identifying changes in both area and moment. The proposed CAIM-Net model integrates change area and moment detection, ensuring consistency. The model consists of three main components: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. The model uses a lightweight encoder and boundary enhancement convolution to rapidly extract and amplify difference features. Coarse change moments are determined through spatiotemporal correlation analysis, followed by fine change moment extraction using a multiscale temporal Class Activation Mapping module. The inferred change moments are then used to identify change areas, leveraging the relationship between time series analysis and spatial change detection. <div>
arXiv:2509.03112v1 Announce Type: new 
Abstract: Time series change detection is a critical task for exploring ecosystem dynamics using time series remote sensing images, because it can simultaneously indicate where and when change occur. While deep learning has shown excellent performance in this domain, it continues to approach change area detection and change moment identification as distinct tasks. Given that change area can be inferred from change moment, we propose a time series change detection network, named CAIM-Net (Change Area Inference from Moment Network), to ensure consistency between change area and change moment results. CAIM-Net infers change area from change moment based on the intrinsic relationship between time series analysis and spatial change detection. The CAIM-Net comprises three key steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. In the Difference Extraction and Enhancement, a lightweight encoder with batch dimension stacking is designed to rapidly extract difference features. Subsequently, boundary enhancement convolution is applied to amplify these difference features. In the Coarse Change Moment Extraction, the enhanced difference features from the first step are used to spatiotemporal correlation analysis, and then two distinct methods are employed to determine coarse change moments. In the Fine Change Moment Extraction and Change Area Inference, a multiscale temporal Class Activation Mapping (CAM) module first increases the weight of the change-occurring moment from coarse change moments. Then the weighted change moment is used to infer change area based on the fact that pixels with the change moment must have undergone a change.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</title>
<link>https://arxiv.org/abs/2509.03113</link>
<guid>https://arxiv.org/abs/2509.03113</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucinations, multimodal large language model, text-visual bias, co-occurrence bias, contrastive decoding framework

Summary: 
This paper addresses the issue of hallucinations in multimodal large language models, identifying the root causes as the text-visual bias and co-occurrence bias. Traditional mitigation methods have been limited in their effectiveness due to a lack of understanding of the varying bias levels across different instances. The proposed solution involves estimating the influence of different token types (visual, prompt, previous outputs) using a gradient-based self-reflection method. This estimation allows for the detection of object-related visual tokens and their integration into an influence-aware contrastive decoding framework. The method effectively mitigates both biases without the need for additional resources such as fine-tuning or extra models. Experimental results demonstrate significant improvements in reducing hallucinations, achieving up to a 92% accuracy increase on the LLaVA-QA90 dataset. 

<br /><br />Summary: <div>
arXiv:2509.03113v1 Announce Type: new 
Abstract: Hallucinations in multimodal large language model are caused by the text-visual bias and the co-occurrence bias. The former reflects an over-reliance on text information in the decision-making process, while the latter arises from the statistical object-pairing patterns abstracted from the training data. Existing mitigation methods heuristically address these biases without understanding the fluctuating bias level across the instances. We first propose estimating the influence of respective token types (visual, prompt, and previous outputs) using a gradient-based self-reflection method. The estimated token influence further enables the detection of object-related visual tokens and their integration into an influence-aware contrastive decoding framework to mitigate both types of biases simultaneously. Our method operates without the need for additional resources, such as costly fine-tuning, extra models, or data statistics. Extensive experiments show it effectively reduces hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge</title>
<link>https://arxiv.org/abs/2509.03114</link>
<guid>https://arxiv.org/abs/2509.03114</guid>
<content:encoded><![CDATA[
<div> Keywords: hand-object interaction, pose estimation, deformable hand surface, GravityDB, semantic information

Summary:
The article introduces a new approach called Gravity-Field Based Diffusion Bridge (GravityDB) for simulating hand-object interactions. Existing methods often struggle with interpenetration or gaps in contact regions due to the complex geometry of hands and objects. The GravityDB method models interaction as an attraction-driven process, ensuring physically plausible interactions without interpenetration. It captures realistic hand deformations during interaction, improving the accuracy of pose estimation. Additionally, semantic information from textual descriptions is used to guide the construction of the gravitational field, leading to more meaningful interaction regions. Extensive experiments on various datasets demonstrate the effectiveness of GravityDB in generating stable grasping and realistic hand-object interactions. <div>
arXiv:2509.03114v1 Announce Type: new 
Abstract: Existing reconstruction or hand-object pose estimation methods are capable of producing coarse interaction states. However, due to the complex and diverse geometry of both human hands and objects, these approaches often suffer from interpenetration or leave noticeable gaps in regions that are supposed to be in contact. Moreover, the surface of a real human hand undergoes non-negligible deformations during interaction, which are difficult to capture and represent with previous methods. To tackle these challenges, we formulate hand-object interaction as an attraction-driven process and propose a Gravity-Field Based Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand surface and rigid objects. Our approach effectively resolves the aforementioned issues by generating physically plausible interactions that are free of interpenetration, ensure stable grasping, and capture realistic hand deformations. Furthermore, we incorporate semantic information from textual descriptions to guide the construction of the gravitational field, enabling more semantically meaningful interaction regions. Extensive qualitative and quantitative experiments on multiple datasets demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation</title>
<link>https://arxiv.org/abs/2509.03141</link>
<guid>https://arxiv.org/abs/2509.03141</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, brain progression, 3D modeling, diffusion model, temporal awareness

Summary:<br />
Generating realistic MRI images for accurate prediction of brain structural changes over time is crucial for clinicians. Current methods have limitations like not capturing the relationship between structural changes and time intervals for age-imbalanced datasets, only relying on scan interpolation, and using 2D slice-based architectures. To address these limitations, a 3D Temporally-Aware Diffusion Model (TADM-3D) is proposed. It incorporates a Brain-Age Estimator (BAE) to guide the diffusion model for generating MRI volumes reflecting expected age differences. The model is trained bidirectionally to predict both forward and backward scans to improve temporal accuracy. Tested on the OASIS-3 dataset with validation on the NACC dataset, TADM-3D shows promising results in predicting brain progression accurately. The code for the model will be available upon acceptance. <br /><br />Summary: <div>
arXiv:2509.03141v1 Announce Type: new 
Abstract: Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving instance continuity and length in segmentation through connectivity-aware loss computation</title>
<link>https://arxiv.org/abs/2509.03154</link>
<guid>https://arxiv.org/abs/2509.03154</guid>
<content:encoded><![CDATA[
<div> novel loss functions, Negative Centerline Loss, Simplified Topology Loss, Convolutional Neural Networks, biological applications, structural priors <br />
Summary:<br />
The article introduces two novel loss functions, Negative Centerline Loss and Simplified Topology Loss, aimed at preserving connectivity of output instances in biomedical segmentation tasks. These loss functions, integrated with Convolutional Neural Networks, enhance the continuity and length preservation of elongated structures in segmentation masks. The study discusses the importance of experiment design characteristics like downscaling and spacing correction in achieving continuous segmentation masks. Evaluation on a 3D light-sheet fluorescence microscopy dataset of axon initial segments showcases the effectiveness of the proposed methods in reducing segmentation discontinuities, especially in regions with missing input signal. The findings highlight the significant improvement in instance length calculation for biological applications through the incorporation of structural priors in loss design. <br /> <div>
arXiv:2509.03154v1 Announce Type: new 
Abstract: In many biomedical segmentation tasks, the preservation of elongated structure continuity and length is more important than voxel-wise accuracy. We propose two novel loss functions, Negative Centerline Loss and Simplified Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help preserve connectivity of output instances. Moreover, we discuss characteristics of experiment design, such as downscaling and spacing correction, that help obtain continuous segmentation masks. We evaluate our approach on a 3D light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a task prone to discontinuity due to signal dropout. Compared to standard CNNs and existing topology-aware losses, our methods reduce the number of segmentation discontinuities per instance, particularly in regions with missing input signal, resulting in improved instance length calculation in downstream applications. Our findings demonstrate that structural priors embedded in the loss design can significantly enhance the reliability of segmentation for biological applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Count2Density: Crowd Density Estimation without Location-level Annotations</title>
<link>https://arxiv.org/abs/2509.03170</link>
<guid>https://arxiv.org/abs/2509.03170</guid>
<content:encoded><![CDATA[
<div> Keywords: crowd density estimation, count-level annotations, deep networks, pseudo-density maps, spatial awareness <br />
<br />
Summary: Count2Density introduces a novel pipeline for crowd density estimation that reduces the need for fine-grained location-level annotations by utilizing count-level annotations. The approach leverages a Historical Map Bank to generate pseudo-density maps based on past predictions, reducing confirmation bias. The model incorporates an unsupervised saliency estimator and iterative updates to improve spatial priors. By sampling locations within crowd areas, the pipeline successfully retrieves spatial information from count-level annotations. Additionally, a self-supervised contrastive spatial regularizer enhances spatial awareness in the model. Experimental results demonstrate superior performance compared to cross-domain adaptation methods and recent state-of-the-art approaches in semi-supervised settings, showcasing the effectiveness of Count2Density in accurately estimating crowd density and enabling precise subregion counting.<br /> <div>
arXiv:2509.03170v1 Announce Type: new 
Abstract: Crowd density estimation is a well-known computer vision task aimed at estimating the density distribution of people in an image. The main challenge in this domain is the reliance on fine-grained location-level annotations, (i.e. points placed on top of each individual) to train deep networks. Collecting such detailed annotations is both tedious, time-consuming, and poses a significant barrier to scalability for real-world applications. To alleviate this burden, we present Count2Density: a novel pipeline designed to predict meaningful density maps containing quantitative spatial information using only count-level annotations (i.e., total number of people) during training. To achieve this, Count2Density generates pseudo-density maps leveraging past predictions stored in a Historical Map Bank, thereby reducing confirmation bias. This bank is initialised using an unsupervised saliency estimator to provide an initial spatial prior and is iteratively updated with an EMA of predicted density maps. These pseudo-density maps are obtained by sampling locations from estimated crowd areas using a hypergeometric distribution, with the number of samplings determined by the count-level annotations. To further enhance the spatial awareness of the model, we add a self-supervised contrastive spatial regulariser to encourage similar feature representations within crowded regions while maximising dissimilarity with background regions. Experimental results demonstrate that our approach significantly outperforms cross-domain adaptation methods and achieves better results than recent state-of-the-art approaches in semi-supervised settings across several datasets. Additional analyses validate the effectiveness of each individual component of our pipeline, confirming the ability of Count2Density to effectively retrieve spatial information from count-level annotations and enabling accurate subregion counting.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain</title>
<link>https://arxiv.org/abs/2509.03179</link>
<guid>https://arxiv.org/abs/2509.03179</guid>
<content:encoded><![CDATA[
<div> Dataset, Poisoning attacks, Object detection, Military, Detection methods
Summary:
The research focuses on the threat of poisoning attacks on military object detection systems. Limited research exists on this topic despite its severity. A custom dataset called MilCivVeh was created to investigate this threat. The study implemented a modified version of the BadDet attack to assess the vulnerability of military object detectors. The impact of the poisoning attack was found to require a substantial portion of the data to be poisoned for a positive attack success rate. Detection methods were tested, including specialized poisoning detection and anomaly detection methods, but were found lacking. A new patch detection method called AutoDetect, based on autoencoders, was introduced and showed promising results in separating clean from poisoned samples. Availability of large, representative datasets in the military domain is deemed crucial for evaluating risks and opportunities in patch detection.<br /><br />Summary: <div>
arXiv:2509.03179v1 Announce Type: new 
Abstract: Poisoning attacks pose an increasing threat to the security and robustness of Artificial Intelligence systems in the military domain. The widespread use of open-source datasets and pretrained models exacerbates this risk. Despite the severity of this threat, there is limited research on the application and detection of poisoning attacks on object detection systems. This is especially problematic in the military domain, where attacks can have grave consequences. In this work, we both investigate the effect of poisoning attacks on military object detectors in practice, and the best approach to detect these attacks. To support this research, we create a small, custom dataset featuring military vehicles: MilCivVeh. We explore the vulnerability of military object detectors for poisoning attacks by implementing a modified version of the BadDet attack: a patch-based poisoning attack. We then assess its impact, finding that while a positive attack success rate is achievable, it requires a substantial portion of the data to be poisoned -- raising questions about its practical applicability. To address the detection challenge, we test both specialized poisoning detection methods and anomaly detection methods from the visual industrial inspection domain. Since our research shows that both classes of methods are lacking, we introduce our own patch detection method: AutoDetect, a simple, fast, and lightweight autoencoder-based method. Our method shows promising results in separating clean from poisoned samples using the reconstruction error of image slices, outperforming existing methods, while being less time- and memory-intensive. We urge that the availability of large, representative datasets in the military domain is a prerequisite to further evaluate risks of poisoning attacks and opportunities patch detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising</title>
<link>https://arxiv.org/abs/2509.03185</link>
<guid>https://arxiv.org/abs/2509.03185</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, low-dose computed tomography, denoising, image quality, COVID-19

Summary:
PPORLD-EDNetLDCT is introduced as a reinforcement learning-based approach for improving image quality in low-dose CT scans. Using a dynamic RL-based method with PPO algorithm, the model optimizes denoising policies in real time based on image feedback. Experimental results show superior performance to traditional and DL-based denoising methods, achieving high peak signal-to-noise ratio, structural similarity index measure, and low root mean squared error. Additionally, validation in the COVID-19 LDCT dataset demonstrates improved classification accuracy with the processed images, showcasing the effectiveness of RL-based denoising. This innovative approach offers a promising solution for enhancing the safety and accuracy of low-dose CT imaging.<br /><br />Summary: <div>
arXiv:2509.03185v1 Announce Type: new 
Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation exposure, but it often leads to increased noise and reduced image quality. Traditional denoising methods, such as iterative optimization or supervised learning, often fail to preserve image quality. To address these challenges, we introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in which an advanced posterior policy optimization (PPO) algorithm is used to optimize denoising policies in real time, based on image quality feedback, trained via a custom gym environment. The experimental results on the low dose CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT model outperforms traditional denoising techniques and other DL-based methods, achieving a peak signal-to-noise ratio of 41.87, a structural similarity index measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of 41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality of denoising using a classification task in the COVID-19 LDCT dataset, where the images processed by our method improved the classification accuracy to 94\%, achieving 4\% higher accuracy compared to denoising without RL-based denoising. This method offers a promising solution for safer and more accurate LDCT imaging.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</title>
<link>https://arxiv.org/abs/2509.03212</link>
<guid>https://arxiv.org/abs/2509.03212</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multimodal Sentiment Perception, AI-based virtual companion, Emotional cues, Human-Computer Interaction

Summary: 
This article discusses the limitations of Large Language Models (LLMs) in processing emotional cues from non-verbal signals, hindering immersive interactions in Human-Computer Interaction (HCI). To address this issue, the authors propose an emotion-aware virtual companion called \ours, which integrates Multimodal Sentiment Perception Network (MSPN) using cross-modal fusion transformer and supervised contrastive learning. \ours captures emotional cues from various modalities to enable emotionally aligned interactions. Additionally, the system includes an emotion-aware prompt engineering strategy for generating empathetic responses, a Text-to-Speech (TTS) system, and an animated avatar module for expressive interactions. The framework presented in this work can be applied in companion robotics, social care, mental health, and human-centered AI. 

<br /><br />Summary: 
- Discussion on limitations of Large Language Models (LLMs) in interpreting emotional cues from non-verbal signals 
- Proposal of emotion-aware virtual companion \ours integrating Multimodal Sentiment Perception Network (MSPN) 
- Use of cross-modal fusion transformer and supervised contrastive learning for capturing emotional cues 
- Integration of emotion-aware prompt engineering strategy, Text-to-Speech (TTS) system, and animated avatar module for expressive interactions 
- Application potential in companion robotics, social care, mental health, and human-centered AI <div>
arXiv:2509.03212v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</title>
<link>https://arxiv.org/abs/2509.03214</link>
<guid>https://arxiv.org/abs/2509.03214</guid>
<content:encoded><![CDATA[
<div> Keywords: fMRI, brain disorder diagnosis, multimodal feature fusion, text generation, deep learning<br />
Summary:<br />
The research introduces RTGMFF, a framework that combines automatic ROI-level text generation with multimodal feature fusion for brain disorder diagnosis using fMRI data. The framework includes three components: ROI-driven fMRI text generation, a Hybrid frequency-spatial encoder, and an Adaptive semantic alignment module. The ROI-driven fMRI text generation condenses subject-specific information into reproducible text tokens. The Hybrid encoder captures frequency-domain structure and spatial dependencies using a cross-scale Transformer encoder. The Adaptive alignment module embeds ROI token sequences and visual features in a shared space to narrow the modality gap. Experimentation on ADHD-200 and ABIDE datasets demonstrates that RTGMFF outperforms existing methods in diagnostic accuracy, achieving improved sensitivity, specificity, and area under the ROC curve. The code for RTGMFF is available on GitHub. <div>
arXiv:2509.03214v1 Announce Type: new 
Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at https://github.com/BeistMedAI/RTGMFF.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking</title>
<link>https://arxiv.org/abs/2509.03221</link>
<guid>https://arxiv.org/abs/2509.03221</guid>
<content:encoded><![CDATA[
<div> Segmentation, tracking, organoids, deep learning, LGBP-OrgaNet

Summary:
The paper introduces LGBP-OrgaNet, a deep learning-based system for automated organoid segmentation and tracking. It addresses the challenge of accurately assessing the developmental status of organoids without compromising their structure. The model combines information from CNN and Transformer modules, utilizing the Learnable Gaussian Band Pass Fusion module for feature fusion. It introduces the Bidirectional Cross Fusion Block in the decoder to fuse multi-scale features, followed by progressive concatenation and upsampling for decoding. The system, SROrga, demonstrates high segmentation accuracy and robustness on organoid datasets, providing a valuable tool for organoid research. Overall, the proposed approach offers a non-destructive method for organoid analysis, enhancing the understanding of organoid structure and function in various applications such as tumor treatment and drug screening. 

<br /><br />Summary: <div>
arXiv:2509.03221v1 Announce Type: new 
Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR</title>
<link>https://arxiv.org/abs/2509.03262</link>
<guid>https://arxiv.org/abs/2509.03262</guid>
<content:encoded><![CDATA[
<div> framework, 3D parametric curve, point clouds, geometry-aware matching, LiDAR

Summary:
PI3DETR is a new end-to-end framework for predicting 3D parametric curve instances directly from raw point clouds. It avoids the use of intermediate representations and multi-stage processing seen in previous approaches. The model extends 3DETR by introducing a geometry-aware matching strategy and specialized loss functions, enabling unified detection of different types of parameterized curves in a single forward pass. Optional post-processing steps refine predictions without adding complexity. This design enhances robustness to noise and varying sampling densities, crucial in real-world LiDAR and 3D sensing applications. PI3DETR achieves state-of-the-art performance on the ABC dataset and generalizes well to real sensor data. It provides a simple yet effective solution for 3D edge and curve estimation. <div>
arXiv:2509.03262v1 Announce Type: new 
Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D parametric curve instances from raw point clouds, avoiding the intermediate representations and multi-stage processing common in prior work. Extending 3DETR, our model introduces a geometry-aware matching strategy and specialized loss functions that enable unified detection of differently parameterized curve types, including cubic B\'ezier curves, line segments, circles, and arcs, in a single forward pass. Optional post-processing steps further refine predictions without adding complexity. This streamlined design improves robustness to noise and varying sampling densities, addressing critical challenges in real world LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC dataset and generalizes effectively to real sensor data, offering a simple yet powerful solution for 3D edge and curve estimation.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model</title>
<link>https://arxiv.org/abs/2509.03267</link>
<guid>https://arxiv.org/abs/2509.03267</guid>
<content:encoded><![CDATA[
<div> diffusion model, SynBT, breast tumor, segmentation, MRI <br />
Summary: 
The paper introduces a new 3D medical diffusion model, SynBT, for generating high-quality breast tumors in contrast-enhanced MRI images. This model addresses the limitations of existing tumor synthesis methods when dealing with large spatial volumes like breast tumors in MRI with a large FOV. The method includes a patch-to-volume autoencoder to compress high-resolution MRIs into a compact latent space and a mask-conditioned diffusion model to synthesize realistic breast tumors within selected regions of breast tissue. The proposed SynBT model showed an improved performance of 2-3% Dice Score in tumor segmentation tasks on a large public dataset. This suggests that the high-quality tumor synthesis method can enhance common segmentation models and aid in accurate tumor segmentation in MRI images. <br /> <div>
arXiv:2509.03267v1 Announce Type: new 
Abstract: Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.03277</link>
<guid>https://arxiv.org/abs/2509.03277</guid>
<content:encoded><![CDATA[
<div> PointAD+, 3D anomalies, hierarchical representation learning, anomaly semantics, ZS anomaly detection

Summary:
PointAD+ proposes a unified framework for detecting and segmenting 3D anomalies by incorporating both point- and pixel-level information. It introduces PointAD to represent anomalies through rendering pixel representations and PointAD+ to include spatial abnormality through explicit 3D representation. The framework incorporates geometry information and utilizes hierarchical representation learning with rendering and geometry prompts for anomaly detection. A cross-hierarchy contrastive alignment enhances interaction between rendering and geometry layers, leading to better anomaly learning. PointAD+ demonstrates superior performance in zero-shot 3D anomaly detection on unseen objects with diverse class semantics, providing a comprehensive understanding of abnormality. The framework can also integrate RGB information for improved detection performance. Overall, PointAD+ offers a holistic approach to identifying and understanding 3D anomalies across a wide range of objects. 

<br /><br />Summary: <div>
arXiv:2509.03277v1 Announce Type: new 
Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization capabilities to identify 3D anomalies across unseen objects of highly diverse class semantics. To this end, we propose a unified framework to comprehensively detect and segment 3D anomalies by leveraging both point- and pixel-level information. We first design PointAD, which leverages point-pixel correspondence to represent 3D anomalies through their associated rendering pixel representations. This approach is referred to as implicit 3D representation, as it focuses solely on rendering pixel anomalies but neglects the inherent spatial relationships within point clouds. Then, we propose PointAD+ to further broaden the interpretation of 3D anomalies by introducing explicit 3D representation, emphasizing spatial abnormality to uncover abnormal spatial relationships. Hence, we propose G-aggregation to involve geometry information to enable the aggregated point representations spatially aware. To simultaneously capture rendering and spatial abnormality, PointAD+ proposes hierarchical representation learning, incorporating implicit and explicit anomaly semantics into hierarchical text prompts: rendering prompts for the rendering layer and geometry prompts for the geometry layer. A cross-hierarchy contrastive alignment is further introduced to promote the interaction between the rendering and geometry layers, facilitating mutual anomaly learning. Finally, PointAD+ integrates anomaly semantics from both layers to capture the generalized anomaly semantics. During the test, PointAD+ can integrate RGB information in a plug-and-play manner and further improve its detection performance. Extensive experiments demonstrate the superiority of PointAD+ in ZS 3D anomaly detection across unseen objects with highly diverse class semantics, achieving a holistic understanding of abnormality.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Lightweight MLLMs with Reasoning via Long CoT SFT</title>
<link>https://arxiv.org/abs/2509.03321</link>
<guid>https://arxiv.org/abs/2509.03321</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Multimodal Language Models, Chain-of-Thought Data, Supervised Fine-Tuning

Summary:
This paper explores the impact of incorporating long Chain-of-Thought (CoT) data in enhancing the reasoning abilities of lightweight multimodal language models (MLLMs). The study reveals that employing Supervised Fine-Tuning (SFT) with long CoT data leads to significant improvements in MLLM reasoning. Additionally, it is noted that MLLMs can further enhance their performance through a subsequent Reinforcement Learning (RL) stage post-SFT. The research emphasizes the importance of an initial SFT phase with long CoT data as a crucial step in developing the reasoning capabilities of lightweight MLLMs. <div>
arXiv:2509.03321v1 Announce Type: new 
Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the reasoning of large-scale language models (LLMs), its efficacy for lightweight multimodal language models (MLLMs) with fewer than seven billion parameters remains underexplored. This paper investigates the role of long Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT data significantly improves MLLM reasoning. Furthermore, we observe that after this initial SFT phase, MLLMs can achieve additional performance gains through a subsequent RL stage. We conclude that a SFT stage with long CoT data is a critical prerequisite for developing the reasoning capabilities of lightweight MLLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions</title>
<link>https://arxiv.org/abs/2509.03323</link>
<guid>https://arxiv.org/abs/2509.03323</guid>
<content:encoded><![CDATA[
<div> Keywords: astrocyte, histological images, CNN Transformer detector, ALDH1L1, GFAP stained

Summary:
The study introduces a novel hybrid CNN Transformer detector designed for automated detection of astrocytes in histological images. Astrocytes play a crucial role in neurological disorders, and their altered morphology can provide insights into various conditions. Traditional methods struggle with the complex branching and variability of astrocytes in stained images, making accurate detection challenging. The proposed model overcomes these limitations by combining local feature extraction with global contextual reasoning. It utilizes a heatmap guided query mechanism to detect small and faint astrocytes and a lightweight Transformer module to improve discrimination in dense clusters. Evaluation on ALDH1L1 and GFAP stained datasets shows superior performance compared to existing methods such as Faster R-CNN, YOLOv11, and DETR. The model achieves higher sensitivity with fewer false positives, demonstrating its potential for robust astrocyte detection and paving the way for advanced computational pathology tools.<br /><br />Summary: The study presents a hybrid CNN Transformer detector for more accurate and efficient detection of astrocytes in histological images, addressing challenges posed by their intricate morphology and stained image variability. <div>
arXiv:2509.03323v1 Announce Type: new 
Abstract: Astrocytes are critical glial cells whose altered morphology and density are hallmarks of many neurological disorders. However, their intricate branching and stain dependent variability make automated detection of histological images a highly challenging task. To address these challenges, we propose a hybrid CNN Transformer detector that combines local feature extraction with global contextual reasoning. A heatmap guided query mechanism generates spatially grounded anchors for small and faint astrocytes, while a lightweight Transformer module improves discrimination in dense clusters. Evaluated on ALDH1L1 and GFAP stained astrocyte datasets, the model consistently outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with fewer false positives, as confirmed by FROC analysis. These results highlight the potential of hybrid CNN Transformer architectures for robust astrocyte detection and provide a foundation for advanced computational pathology tools.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</title>
<link>https://arxiv.org/abs/2509.03324</link>
<guid>https://arxiv.org/abs/2509.03324</guid>
<content:encoded><![CDATA[
<div> Point clouds, infrastructure monitoring, semantic segmentation, brick-level segmentation, defect detection<br />
Summary:<br />
The paper introduces InfraDiffusion, a zero-shot framework for enhancing brick-level segmentation of masonry structures in point clouds, which are commonly used for infrastructure monitoring. In low-light environments like masonry tunnels, high-resolution images are impractical, making point clouds a robust alternative. However, these point clouds are unstructured, sparse, and noisy, hampering fine-grained segmentation. InfraDiffusion addresses this challenge by converting point clouds into depth maps using virtual cameras and applying the Denoising Diffusion Null-space Model (DDNM) to improve visual clarity and geometric consistency. By incorporating the Segment Anything Model (SAM), InfraDiffusion significantly enhances brick-level segmentation in masonry bridge and tunnel datasets, showcasing its potential for automated inspection of masonry assets. The code and data are publicly available for further research and application. <br /><br /> <div>
arXiv:2509.03324v1 Announce Type: new 
Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing</title>
<link>https://arxiv.org/abs/2509.03376</link>
<guid>https://arxiv.org/abs/2509.03376</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral unmixing, deep learning, transformer, graph neural network, remote sensing

Summary:
This letter introduces a novel transformer-guided content-adaptive graph unmixing framework (T-CAGU) for hyperspectral unmixing in remote sensing images. T-CAGU utilizes a transformer to capture global dependencies and a content-adaptive graph neural network to enhance local relationships, addressing the challenges of preserving both long-range interactions and boundary details. By integrating multiple propagation orders and incorporating a graph residual mechanism, T-CAGU dynamically learns the graph structure and stabilizes training against noise. Experimental results demonstrate T-CAGU's superiority over existing methods, showcasing its effectiveness in accurately decomposing mixed pixels into endmembers and their abundances. The code is available for further exploration and utilization. 

<br /><br />Summary: <div>
arXiv:2509.03376v1 Announce Type: new 
Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote sensing images into a set of endmembers and their corresponding abundances. Despite significant progress in this field using deep learning, most methods fail to simultaneously characterize global dependencies and local consistency, making it difficult to preserve both long-range interactions and boundary details. This letter proposes a novel transformer-guided content-adaptive graph unmixing framework (T-CAGU), which overcomes these challenges by employing a transformer to capture global dependencies and introducing a content-adaptive graph neural network to enhance local relationships. Unlike previous work, T-CAGU integrates multiple propagation orders to dynamically learn the graph structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a graph residual mechanism to preserve global information and stabilize training. Experimental results demonstrate its superiority over the state-of-the-art methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</title>
<link>https://arxiv.org/abs/2509.03379</link>
<guid>https://arxiv.org/abs/2509.03379</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, ViTs, image classification, computational costs, token dropping <br />
Summary: <br />
The article introduces TinyDrop, a training-free token dropping framework designed to reduce inference costs in large ViTs while maintaining accuracy. This framework is guided by a lightweight vision model that estimates token importance during inference, allowing for selective dropping of low-importance tokens to decrease FLOPs. TinyDrop does not require architectural modifications and is compatible with various ViT architectures. Evaluations on image classification benchmarks show a reduction in FLOPs by up to 80% with minimal accuracy loss, demonstrating the framework's generalizability and practicality for efficient ViT-based classification. <div>
arXiv:2509.03379v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation</title>
<link>https://arxiv.org/abs/2509.03385</link>
<guid>https://arxiv.org/abs/2509.03385</guid>
<content:encoded><![CDATA[
<div> Keywords: concept customization, evaluation metrics, human preference, benchmark dataset, Multimodal Large Language Model

Summary:
The article introduces a new evaluation method called Decomposed GPT Score (D-GPTScore) for assessing concept customization, which involves evaluating fidelity to generative prompts and concept images. The method breaks down evaluation criteria into finer aspects and incorporates aspect-wise assessments using a Multimodal Large Language Model (MLLM). Additionally, the authors have created a benchmark dataset called Human Preference-Aligned Concept Customization Benchmark (CC-AlignBench), which includes single- and multi-concept tasks for evaluating customization difficulty. The proposed method outperforms existing approaches on this benchmark, showing higher correlation with human preferences. This work sets a new standard for evaluating concept customization and emphasizes the challenges for future research. 

<br /><br />Summary: <div>
arXiv:2509.03385v1 Announce Type: new 
Abstract: Evaluating concept customization is challenging, as it requires a comprehensive assessment of fidelity to generative prompts and concept images. Moreover, evaluating multiple concepts is considerably more difficult than evaluating a single concept, as it demands detailed assessment not only for each individual concept but also for the interactions among concepts. While humans can intuitively assess generated images, existing metrics often provide either overly narrow or overly generalized evaluations, resulting in misalignment with human preference. To address this, we propose Decomposed GPT Score (D-GPTScore), a novel human-aligned evaluation method that decomposes evaluation criteria into finer aspects and incorporates aspect-wise assessments using Multimodal Large Language Model (MLLM). Additionally, we release Human Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark dataset containing both single- and multi-concept tasks, enabling stage-wise evaluation across a wide difficulty range -- from individual actions to multi-person interactions. Our method significantly outperforms existing approaches on this benchmark, exhibiting higher correlation with human preferences. This work establishes a new standard for evaluating concept customization and highlights key challenges for future research. The benchmark and associated materials are available at https://github.com/ReinaIshikawa/D-GPTScore.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping</title>
<link>https://arxiv.org/abs/2509.03408</link>
<guid>https://arxiv.org/abs/2509.03408</guid>
<content:encoded><![CDATA[
<div> Keywords: healthcare applications, multimodal integration, breast cancer subtyping, copy number variation, histopathology images

Summary:
This research focuses on enhancing breast cancer molecular subtyping through a scalable and adaptable multimodal framework. By integrating diverse data sources such as copy number variation (CNV), clinical records, and histopathology images, the framework aims to improve personalized treatment and patient prognosis. The proposed dual-based representation for whole slide images (WSIs) combines traditional image-based and graph-based models, resulting in enhanced performance. A new multimodal fusion strategy is introduced to optimize performance in various multimodal conditions. The results show that the integration of the dual-based WSI representation with CNV and clinical health records, along with the fusion strategy, outperforms existing methods in breast cancer subtyping. The framework's flexibility allows for easy scaling and adaptation to other cancer types without the need for re-training existing modalities.<br /><br />Summary: This research presents a scalable multimodal framework for enhancing breast cancer subtyping by integrating CNV, clinical records, and histopathology images. The novel dual-based WSI representation and multimodal fusion strategy significantly improve performance, outperforming state-of-the-art methods. <div>
arXiv:2509.03408v1 Announce Type: new 
Abstract: Healthcare applications are inherently multimodal, benefiting greatly from the integration of diverse data sources. However, the modalities available in clinical settings can vary across different locations and patients. A key area that stands to gain from multimodal integration is breast cancer molecular subtyping, an important clinical task that can facilitate personalized treatment and improve patient prognosis. In this work, we propose a scalable and loosely-coupled multimodal framework that seamlessly integrates data from various modalities, including copy number variation (CNV), clinical records, and histopathology images, to enhance breast cancer subtyping. While our primary focus is on breast cancer, our framework is designed to easily accommodate additional modalities, offering the flexibility to scale up or down with minimal overhead without requiring re-training of existing modalities, making it applicable to other types of cancers as well. We introduce a dual-based representation for whole slide images (WSIs), combining traditional image-based and graph-based WSI representations. This novel dual approach results in significant performance improvements. Moreover, we present a new multimodal fusion strategy, demonstrating its ability to enhance performance across a range of multimodal conditions. Our comprehensive results show that integrating our dual-based WSI representation with CNV and clinical health records, along with our pipeline and fusion strategy, outperforms state-of-the-art methods in breast cancer subtyping.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Scaling State-Space Models for Dense Video Captioning</title>
<link>https://arxiv.org/abs/2509.03426</link>
<guid>https://arxiv.org/abs/2509.03426</guid>
<content:encoded><![CDATA[
<div> Keywords: dense video captioning, State-Space Models, long sequences, online processing, computational efficiency

Summary:
State-Space Models with Transfer State (SMTS) is proposed to tackle the challenges of dense video captioning, allowing for the processing of even longer video sequences while minimizing computational complexity. This model combines the benefits of SSMs with transfer states to sustain long contexts effectively. By enabling online processing of videos, SMTS eliminates the need to wait for the full video to be processed, making it suitable for generating captions on-the-fly. The approach scales efficiently with video lengths and achieves a 7x reduction in FLOPs compared to traditional methods. SMTS offers a promising solution for dense video captioning tasks by enhancing the scalability and computational efficiency of state-space models. 

<br /><br />Summary: State-Space Models with Transfer State addresses challenges in dense video captioning by effectively handling long video sequences, enabling online processing, and reducing computational complexity. <div>
arXiv:2509.03426v1 Announce Type: new 
Abstract: Dense video captioning is a challenging video understanding task which aims to simultaneously segment the video into a sequence of meaningful consecutive events and to generate detailed captions to accurately describe each event. Existing methods often encounter difficulties when working with the long videos associated with dense video captioning, due to the computational complexity and memory limitations. Furthermore, traditional approaches require the entire video as input, in order to produce an answer, which precludes online processing of the video. We address these challenges by time-scaling State-Space Models (SSMs) to even longer sequences than before. Our approach, State-Space Models with Transfer State, combines both the long-sequence and recurrent properties of SSMs and addresses the main limitation of SSMs which are otherwise not able to sustain their state for very long contexts, effectively scaling SSMs further in time. The proposed model is particularly suitable for generating captions on-the-fly, in an online or streaming manner, without having to wait for the full video to be processed, which is more beneficial in practice. When applied to dense video captioning, our approach scales well with video lengths and uses 7x fewer FLOPs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Visual Neural Representations by Multimodal with Dynamic Balancing</title>
<link>https://arxiv.org/abs/2509.03433</link>
<guid>https://arxiv.org/abs/2509.03433</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, image data, text data, multimodal fusion, semantic correspondence <br />
<br />
Summary: 
The proposed framework integrates EEG, image, and text data to decode visual neural representations from low signal-to-noise ratio EEG signals. By incorporating text modality, semantic correspondence between EEG signals and visual content is enhanced. An adapter module is introduced to stabilize high-dimensional representation and align cross-modal features. The Modal Consistency Dynamic Balance (MCDB) strategy adjusts contribution weights of each modality to address feature imbalance. A stochastic perturbation regularization (SPR) term improves generalization ability by adding dynamic Gaussian noise in the optimization process. Evaluation on the ThingsEEG dataset demonstrates superior performance in Top-1 and Top-5 accuracy metrics, outperforming previous state-of-the-art methods by 2.0% and 4.7% respectively. <br /><br /> <div>
arXiv:2509.03433v1 Announce Type: new 
Abstract: In this work, we propose an innovative framework that integrates EEG, image, and text data, aiming to decode visual neural representations from low signal-to-noise ratio EEG signals. Specifically, we introduce text modality to enhance the semantic correspondence between EEG signals and visual content. With the explicit semantic labels provided by text, image and EEG features of the same category can be more closely aligned with the corresponding text representations in a shared multimodal space. To fully utilize pre-trained visual and textual representations, we propose an adapter module that alleviates the instability of high-dimensional representation while facilitating the alignment and fusion of cross-modal features. Additionally, to alleviate the imbalance in multimodal feature contributions introduced by the textual representations, we propose a Modal Consistency Dynamic Balance (MCDB) strategy that dynamically adjusts the contribution weights of each modality. We further propose a stochastic perturbation regularization (SPR) term to enhance the generalization ability of semantic perturbation-based models by introducing dynamic Gaussian noise in the modality optimization process. The evaluation results on the ThingsEEG dataset show that our method surpasses previous state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by 2.0\% and 4.7\% respectively.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Training of Image Generator and Detector for Road Defect Detection</title>
<link>https://arxiv.org/abs/2509.03465</link>
<guid>https://arxiv.org/abs/2509.03465</guid>
<content:encoded><![CDATA[
arXiv:2509.03465v1 Announce Type: new 
Abstract: Road defect detection is important for road authorities to reduce the vehicle damage caused by road defects. Considering the practical scenarios where the defect detectors are typically deployed on edge devices with limited memory and computational resource, we aim at performing road defect detection without using ensemble-based methods or test-time augmentation (TTA). To this end, we propose to Jointly Train the image Generator and Detector for road defect detection (dubbed as JTGD). We design the dual discriminators for the generative model to enforce both the synthesized defect patches and overall images to look plausible. The synthesized image quality is improved by our proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in JTGD is trained jointly with the detector to encourage the generative model to synthesize harder examples for the detector. Since harder synthesized images of better quality caused by the aforesaid design are used in the data augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road defect detection benchmark across various countries under the condition of no ensemble and TTA. JTGD only uses less than 20% of the number of parameters compared with the competing baseline, which makes it more suitable for deployment on edge devices in practice.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA</title>
<link>https://arxiv.org/abs/2509.03494</link>
<guid>https://arxiv.org/abs/2509.03494</guid>
<content:encoded><![CDATA[
arXiv:2509.03494v1 Announce Type: new 
Abstract: In this paper, we propose a novel parameter-efficient adaptation method for No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base model), while keeping the underlying model fully frozen. During inference, these visual prompts are combined with images via addition and processed by mPLUG-Owl2 with the textual query "Rate the technical quality of the image." Evaluations across distortion types (synthetic, realistic, AI-generated) on KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on KADID-10k. To our knowledge, this is the first work to leverage pixel-space visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level vision tasks. The source code is publicly available at https: // github. com/ yahya-ben/ mplug2-vp-for-nriqa .
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</title>
<link>https://arxiv.org/abs/2509.03498</link>
<guid>https://arxiv.org/abs/2509.03498</guid>
<content:encoded><![CDATA[
arXiv:2509.03498v1 Announce Type: new 
Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video</title>
<link>https://arxiv.org/abs/2509.03499</link>
<guid>https://arxiv.org/abs/2509.03499</guid>
<content:encoded><![CDATA[
arXiv:2509.03499v1 Announce Type: new 
Abstract: Benchmarking multi-object tracking and object detection model performance is an essential step in machine learning model development, as it allows researchers to evaluate model detection and tracker performance on human-generated 'test' data, facilitating consistent comparisons between models and trackers and aiding performance optimization. In this study, a novel benchmark video dataset was developed and used to assess the performance of several Monterey Bay Aquarium Research Institute object detection models and a FathomNet single-class object detection model together with several trackers. The dataset consists of four video sequences representing midwater and benthic deep-sea habitats. Performance was evaluated using Higher Order Tracking Accuracy, a metric that balances detection, localization, and association accuracy. To the best of our knowledge, this is the first publicly available benchmark for multi-object tracking in deep-sea video footage. We provide the benchmark data, a clearly documented workflow for generating additional benchmark videos, as well as example Python notebooks for computing metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</title>
<link>https://arxiv.org/abs/2509.03501</link>
<guid>https://arxiv.org/abs/2509.03501</guid>
<content:encoded><![CDATA[
arXiv:2509.03501v1 Announce Type: new 
Abstract: Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting</title>
<link>https://arxiv.org/abs/2509.03510</link>
<guid>https://arxiv.org/abs/2509.03510</guid>
<content:encoded><![CDATA[
arXiv:2509.03510v1 Announce Type: new 
Abstract: This paper introduces a comprehensive database for research and investigation on the effects of inheritance on handwriting. A database has been created that can be used to answer questions such as: Is there a genetic component to handwriting? Is handwriting inherited? Do family relationships affect handwriting? Varieties of samples of handwritten components such as: digits, letters, shapes and free paragraphs of 210 families including (grandparents, parents, uncles, aunts, siblings, cousins, nephews and nieces) have been collected using specially designed forms, and family relationships of all writers are captured. To the best of our knowledge, no such database is presently available. Based on comparisons and investigation of features of handwritings of family members, similarities among their features and writing styles are detected. Our database is freely available to the pattern recognition community and hope it will pave the way for investigations on the effects of inheritance and family relationships on handwritings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?</title>
<link>https://arxiv.org/abs/2509.03516</link>
<guid>https://arxiv.org/abs/2509.03516</guid>
<content:encoded><![CDATA[
arXiv:2509.03516v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title>
<link>https://arxiv.org/abs/2508.13073</link>
<guid>https://arxiv.org/abs/2508.13073</guid>
<content:encoded><![CDATA[
arXiv:2508.13073v2 Announce Type: cross 
Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of Quantum Convolutional Neural Networks for MRI-Based Brain Tumor Detection and Classification</title>
<link>https://arxiv.org/abs/2509.02582</link>
<guid>https://arxiv.org/abs/2509.02582</guid>
<content:encoded><![CDATA[
arXiv:2509.02582v1 Announce Type: cross 
Abstract: This study explores the application of Quantum Convolutional Neural Networks (QCNNs) for brain tumor classification using MRI images, leveraging quantum computing for enhanced computational efficiency. A dataset of 3,264 MRI images, including glioma, meningioma, pituitary tumors, and non-tumor cases, was utilized. The data was split into 80% training and 20% testing, with an oversampling technique applied to address class imbalance. The QCNN model consists of quantum convolution layers, flatten layers, and dense layers, with a filter size of 2, depth of 4, and 4 qubits, trained over 10 epochs. Two models were developed: a binary classification model distinguishing tumor presence and a multiclass classification model categorizing tumor types. The binary model achieved 88% accuracy, improving to 89% after data balancing, while the multiclass model achieved 52% accuracy, increasing to 62% after oversampling. Despite strong binary classification performance, the multiclass model faced challenges due to dataset complexity and quantum circuit limitations. These findings suggest that QCNNs hold promise for medical imaging applications, particularly in binary classification. However, further refinements, including optimized quantum circuit architectures and hybrid classical-quantum approaches, are necessary to enhance multiclass classification accuracy and improve QCNN applicability in clinical settings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.02585</link>
<guid>https://arxiv.org/abs/2509.02585</guid>
<content:encoded><![CDATA[
arXiv:2509.02585v1 Announce Type: cross 
Abstract: This report details our submission to the Mitotic Domain Generalization (MIDOG) 2025 challenge, which addresses the critical task of mitotic figure detection in histopathology for cancer prognostication. Following the "Bitter Lesson"\cite{sutton2019bitterlesson} principle that emphasizes data scale over algorithmic novelty, we have publicly released two new datasets to bolster training data for both conventional \cite{Shen2024framework} and atypical mitoses \cite{shen_2025_16780587}. Besides, we implement up-to-date training methodologies for both track and reach a Track-1 F1-Score of 0.8407 on our test set, as well as a Track-2 balanced accuracy of 0.9107 for atypical mitotic cell classification.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping</title>
<link>https://arxiv.org/abs/2509.02586</link>
<guid>https://arxiv.org/abs/2509.02586</guid>
<content:encoded><![CDATA[
arXiv:2509.02586v1 Announce Type: cross 
Abstract: Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Hard Mining: a data-centric approach for Mitosis Detection</title>
<link>https://arxiv.org/abs/2509.02588</link>
<guid>https://arxiv.org/abs/2509.02588</guid>
<content:encoded><![CDATA[
arXiv:2509.02588v1 Announce Type: cross 
Abstract: With a continuously growing availability of annotated datasets of mitotic figures in histology images, finding the best way to optimally use with this unprecedented amount of data to optimally train deep learning models has become a new challenge. Here, we build upon previously proposed approaches with a focus on efficient sampling of training data inspired by boosting techniques and present our candidate solutions for the two tracks of the MIDOG 2025 challenge.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2509.02589</link>
<guid>https://arxiv.org/abs/2509.02589</guid>
<content:encoded><![CDATA[
arXiv:2509.02589v1 Announce Type: cross 
Abstract: We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical Mitosis Classification</title>
<link>https://arxiv.org/abs/2509.02591</link>
<guid>https://arxiv.org/abs/2509.02591</guid>
<content:encoded><![CDATA[
arXiv:2509.02591v1 Announce Type: cross 
Abstract: Mitotic figures are classified into typical and atypical variants, with atypical counts correlating strongly with tumor aggressiveness. Accurate differentiation is therefore essential for patient prognostication and resource allocation, yet remains challenging even for expert pathologists. Here, we leveraged Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and applied parameter-efficient fine-tuning via low-rank adaptation. During training, we employ a fisheye transform to emphasize mitoses and Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled multiple PFMs to integrate complementary morphological insights, achieving a high balanced accuracy on the Preliminary Evaluation Phase dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</title>
<link>https://arxiv.org/abs/2509.02593</link>
<guid>https://arxiv.org/abs/2509.02593</guid>
<content:encoded><![CDATA[
arXiv:2509.02593v1 Announce Type: cross 
Abstract: Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figures detection approach based on the YOLOv12 object detection architecture, achieving a $F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge, without relying on external data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvNeXt with Histopathology-Specific Augmentations for Mitotic Figure Classification</title>
<link>https://arxiv.org/abs/2509.02595</link>
<guid>https://arxiv.org/abs/2509.02595</guid>
<content:encoded><![CDATA[
arXiv:2509.02595v1 Announce Type: cross 
Abstract: Accurate mitotic figure classification is crucial in computational pathology, as mitotic activity informs cancer grading and patient prognosis. Distinguishing atypical mitotic figures (AMFs), which indicate higher tumor aggressiveness, from normal mitotic figures (NMFs) remains challenging due to subtle morphological differences and high intra-class variability. This task is further complicated by domain shifts, including variations in organ, tissue type, and scanner, as well as limited annotations and severe class imbalance. To address these challenges in Track 2 of the MIDOG 2025 Challenge, we propose a solution based on the lightweight ConvNeXt architecture, trained on all available datasets (AMi-Br, AtNorM-Br, AtNorM-MD, and OMG-Octo) to maximize domain coverage. Robustness is enhanced through a histopathology-specific augmentation pipeline, including elastic and stain-specific transformations, and balanced sampling to mitigate class imbalance. A grouped 5-fold cross-validation strategy ensures reliable evaluation. On the preliminary leaderboard, our model achieved a balanced accuracy of 0.8961, ranking among the top entries. These results highlight that broad domain exposure combined with targeted augmentation strategies is key to building accurate and generalizable mitotic figure classifiers.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solutions for Mitotic Figure Detection and Atypical Classification in MIDOG 2025</title>
<link>https://arxiv.org/abs/2509.02597</link>
<guid>https://arxiv.org/abs/2509.02597</guid>
<content:encoded><![CDATA[
arXiv:2509.02597v1 Announce Type: cross 
Abstract: Deep learning has driven significant advances in mitotic figure analysis within computational pathology. In this paper, we present our approach to the Mitosis Domain Generalization (MIDOG) 2025 Challenge, which consists of two distinct tasks, i.e., mitotic figure detection and atypical mitosis classification. For the mitotic figure detection task, we propose a two-stage detection-classification framework that first localizes candidate mitotic figures and subsequently refines the predictions using a dedicated classification module. For the atypical mitosis classification task, we employ an ensemble strategy that integrates predictions from multiple state-of-the-art deep learning architectures to improve robustness and accuracy. Extensive experiments demonstrate the effectiveness of our proposed methods across both tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RF-DETR for Robust Mitotic Figure Detection: A MIDOG 2025 Track 1 Approach</title>
<link>https://arxiv.org/abs/2509.02599</link>
<guid>https://arxiv.org/abs/2509.02599</guid>
<content:encoded><![CDATA[
arXiv:2509.02599v1 Announce Type: cross 
Abstract: Mitotic figure detection in histopathology images remains challenging due to significant domain shifts across different scanners, staining protocols, and tissue types. This paper presents our approach for the MIDOG 2025 challenge Track 1, focusing on robust mitotic figure detection across diverse histological contexts. While we initially planned a two-stage approach combining high-recall detection with subsequent classification refinement, time constraints led us to focus on optimizing a single-stage detection pipeline. We employed RF-DETR (Roboflow Detection Transformer) with hard negative mining, trained on MIDOG++ dataset. On the preliminary test set, our method achieved an F1 score of 0.789 with a recall of 0.839 and precision of 0.746, demonstrating effective generalization across unseen domains. The proposed solution offers insights into the importance of training data balance and hard negative mining for addressing domain shift challenges in mitotic figure detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team Westwood Solution for MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.02600</link>
<guid>https://arxiv.org/abs/2509.02600</guid>
<content:encoded><![CDATA[
arXiv:2509.02600v1 Announce Type: cross 
Abstract: This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies</title>
<link>https://arxiv.org/abs/2509.02601</link>
<guid>https://arxiv.org/abs/2509.02601</guid>
<content:encoded><![CDATA[
arXiv:2509.02601v1 Announce Type: cross 
Abstract: We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs). The approach leverages pathology-specific foundation model H-optimus-0, selected based on recent cross-domain generalization benchmarks and our empirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp augmentation. Implementation includes soft labels based on multi-expert consensus, hard negative mining, and adaptive focal loss, metric learning and domain adaptation. The method demonstrates both the promise and challenges of applying foundation models to this complex classification task, achieving reasonable performance in the preliminary evaluation phase.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition</title>
<link>https://arxiv.org/abs/2509.02612</link>
<guid>https://arxiv.org/abs/2509.02612</guid>
<content:encoded><![CDATA[
arXiv:2509.02612v1 Announce Type: cross 
Abstract: The MIDOG 2025 challenge extends prior work on mitotic figure detection by introducing a new Track 2 on atypical mitosis classification. This task aims to distinguish normal from atypical mitotic figures in histopathology images, a clinically relevant but highly imbalanced and cross-domain problem. We investigated two complementary backbones: (i) ConvNeXt-Small, pretrained on ImageNet, and (ii) a histopathology-specific ViT from Lunit trained via self-supervision. To address the strong prevalence imbalance (9408 normal vs. 1741 atypical), we synthesized additional atypical examples to approximate class balance and compared models trained with real-only vs. real+synthetic data. Using five-fold cross-validation, both backbones reached strong performance (mean AUROC approximately 95 percent), with ConvNeXt achieving slightly higher peaks while Lunit exhibited greater fold-to-fold stability. Synthetic balancing, however, did not lead to consistent improvements. On the organizers' preliminary hidden test set, explicitly designed as an out-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4 percent), whereas Lunit remained competitive on balanced accuracy. These findings suggest that both ImageNet and domain-pretrained backbones are viable for atypical mitosis classification, with domain-pretraining conferring robustness and ImageNet pretraining reaching higher peaks, while naive synthetic balancing has limited benefit. Full hidden test set results will be reported upon challenge completion.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection</title>
<link>https://arxiv.org/abs/2509.02630</link>
<guid>https://arxiv.org/abs/2509.02630</guid>
<content:encoded><![CDATA[
arXiv:2509.02630v1 Announce Type: cross 
Abstract: Mitotic figure detection remains a challenging task in computational pathology due to domain variability and morphological complexity. This paper describes our participation in the MIDOG 2025 challenge, focusing on robust mitotic figure detection across diverse tissue domains. We developed a two-stage pipeline combining Faster R-CNN for candidate detection with an ensemble of three classifiers (DenseNet-121, EfficientNet-v2, InceptionResNet-v2) for false positive reduction. Our best submission achieved F1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN trained solely on MIDOG++ dataset. While our high recall demonstrates effective mitotic figure detection, the critically low precision (12.67%) reveals fundamental challenges in distinguishing true mitoses from morphologically similar imposters across diverse domains. Analysis of six submission variants showed that subsequent optimization attempts were counterproductive, highlighting the omplexity of domain generalization in histopathology. This work provides valuable insights into the practical challenges of developing robust mitotic figure detection algorithms and emphasizes the importance of effective false positive suppression strategies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection</title>
<link>https://arxiv.org/abs/2509.02637</link>
<guid>https://arxiv.org/abs/2509.02637</guid>
<content:encoded><![CDATA[
arXiv:2509.02637v1 Announce Type: cross 
Abstract: Mitotic figure detection is a crucial task in computational pathology, as mitotic activity serves as a strong prognostic marker for tumor aggressiveness. However, domain variability that arises from differences in scanners, tissue types, and staining protocols poses a major challenge to the robustness of automated detection methods. In this study, we introduce SDF-YOLO (Single Detect Focused YOLO), a lightweight yet domain-robust detection framework designed specifically for small, rare targets such as mitotic figures. The model builds on YOLOv11 with task-specific modifications, including a single detection head aligned with mitotic figure scale, coordinate attention to enhance positional sensitivity, and improved cross-channel feature mixing. Experiments were conducted on three datasets that span human and canine tumors: MIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary carcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025 challenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a precision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of 5.793, demonstrating both competitive accuracy and computational efficiency. These results indicate that SDF-YOLO provides a reliable and efficient framework for robust mitotic figure detection across diverse domains.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge</title>
<link>https://arxiv.org/abs/2509.02640</link>
<guid>https://arxiv.org/abs/2509.02640</guid>
<content:encoded><![CDATA[
arXiv:2509.02640v1 Announce Type: cross 
Abstract: Atypical mitotic figures (AMFs) are clinically relevant indicators of abnormal cell division, yet their reliable detection remains challenging due to morphological ambiguity and scanner variability. In this work, we investigated three variants of adapting the pathology foundation model UNI2-h for the MIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that visual prompt tuning (VPT) substantially improved generalization, and that further integrating test-time augmentation (TTA) with Vahadane and Macenko stain normalization provided the best robustness. Our final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary leaderboard, ranking within the top 10 teams. These results demonstrate that prompt-based adaptation combined with stain-normalization TTA offers an effective strategy for atypical mitosis classification under diverse imaging conditions.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women</title>
<link>https://arxiv.org/abs/2509.02710</link>
<guid>https://arxiv.org/abs/2509.02710</guid>
<content:encoded><![CDATA[
arXiv:2509.02710v1 Announce Type: cross 
Abstract: Accurate breast MRI lesion detection is critical for early cancer diagnosis, especially in high-risk populations. We present a classification pipeline that adapts a pretrained foundation model, the Medical Slice Transformer (MST), for breast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI). Leveraging DINOv2-based self-supervised pretraining, MST generates robust per-slice feature embeddings, which are then used to train a Kolmogorov--Arnold Network (KAN) classifier. The KAN provides a flexible and interpretable alternative to conventional convolutional networks by enabling localized nonlinear transformations via adaptive B-spline activations. This enhances the model's ability to differentiate benign from malignant lesions in imbalanced and heterogeneous clinical datasets. Experimental results demonstrate that the MST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80 \pm 0.02 while preserving interpretability through attention-based heatmaps. Our findings highlight the effectiveness of combining foundation model embeddings with advanced classification strategies for building robust and generalizable breast MRI analysis tools.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly</title>
<link>https://arxiv.org/abs/2509.02949</link>
<guid>https://arxiv.org/abs/2509.02949</guid>
<content:encoded><![CDATA[
arXiv:2509.02949v1 Announce Type: cross 
Abstract: Assistants on assembly tasks have a large potential to benefit humans from everyday tasks to industrial settings. However, no testbeds support application-oriented system evaluation in a practical setting, especially in assembly. To foster the development, we propose a new multimodal QA dataset on assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs that require the multimodal understanding of human-activity recordings and their instruction manuals in an online-style manner. In the development, we adopt a semi-automated QA annotation approach, where LLMs generate candidates and humans verify them, as a cost-effective method, and further improve it by integrating fine-grained action labels to diversify question types. Furthermore, we create instruction task graphs for the target tasks of assembling toy vehicles. These newly created task graphs are used in our benchmarking experiment, as well as to facilitate the human verification process in the QA annotation. Utilizing our dataset, we benchmark models, including competitive proprietary multimodal models. Our results suggest great room for improvement for the current models. We believe our new evaluation dataset can contribute to the further development of procedural-activity assistants.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble YOLO Framework for Multi-Domain Mitotic Figure Detection in Histopathology Images</title>
<link>https://arxiv.org/abs/2509.02957</link>
<guid>https://arxiv.org/abs/2509.02957</guid>
<content:encoded><![CDATA[
arXiv:2509.02957v1 Announce Type: cross 
Abstract: Accurate detection of mitotic figures in whole slide histopathological images remains a challenging task due to their scarcity, morphological heterogeneity, and the variability introduced by tissue preparation and staining protocols. The MIDOG competition series provides standardized benchmarks for evaluating detection approaches across diverse domains, thus motivating the development of generalizable deep learning models. In this work, we investigate the performance of two modern one-stage detectors, YOLOv5 and YOLOv8, trained on MIDOG++, CMC, and CCMCT datasets. To enhance robustness, training incorporated stain-invariant color perturbations and texture preserving augmentations. In internal validation, YOLOv5 achieved superior precision, while YOLOv8 provided improved recall, reflecting architectural trade-offs between anchor-based and anchor-free detection. To capitalize on these complementary strengths, we employed an ensemble of the two models, which improved sensitivity without a major reduction in precision. These findings highlight the effectiveness of ensemble strategies built upon contemporary object detectors to advance automated mitosis detection in digital pathology.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</title>
<link>https://arxiv.org/abs/2509.02983</link>
<guid>https://arxiv.org/abs/2509.02983</guid>
<content:encoded><![CDATA[
arXiv:2509.02983v1 Announce Type: cross 
Abstract: Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression</title>
<link>https://arxiv.org/abs/2509.03012</link>
<guid>https://arxiv.org/abs/2509.03012</guid>
<content:encoded><![CDATA[
arXiv:2509.03012v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are increasingly being used in autonomous systems. However, DNNs do not generalize well to domain shift. Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous systems deployed to the real world. Recent work on test-time training proposes methods that adapt to a new test distribution on the fly by optimizing the DNN model for each test input using self-supervision. However, these techniques result in a sharp increase in inference time as multiple forward and backward passes are required for a single test sample (for test-time training) before finally making the prediction based on the fine-tuned features. This is undesirable for real-world robotics applications where these models may be deployed to resource constraint hardware with strong latency requirements. In this work, we propose a new framework (called UT$^3$) that leverages test-time training for improved performance in the presence of continuous domain shift while also decreasing the inference time, making it suitable for real-world applications. Our method proposes an uncertainty-aware self-supervision task for efficient test-time training that leverages the quantified uncertainty to selectively apply the training leading to sharp improvements in the inference time while performing comparably to standard test-time training protocol. Our proposed protocol offers a continuous setting to identify the selected keyframes, allowing the end-user to control how often to apply test-time training. We demonstrate the efficacy of our method on a dense regression task - monocular depth estimation.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Self-knowledge Distillation: A hierarchical supervised learning for coronary artery segmentation</title>
<link>https://arxiv.org/abs/2509.03173</link>
<guid>https://arxiv.org/abs/2509.03173</guid>
<content:encoded><![CDATA[
arXiv:2509.03173v1 Announce Type: cross 
Abstract: Coronary artery disease is a leading cause of mortality, underscoring the critical importance of precise diagnosis through X-ray angiography. Manual coronary artery segmentation from these images is time-consuming and inefficient, prompting the development of automated models. However, existing methods, whether rule-based or deep learning models, struggle with issues like poor performance and limited generalizability. Moreover, current knowledge distillation methods applied in this field have not fully exploited the hierarchical knowledge of the model, leading to certain information waste and insufficient enhancement of the model's performance capabilities for segmentation tasks. To address these issues, this paper introduces Deep Self-knowledge Distillation, a novel approach for coronary artery segmentation that leverages hierarchical outputs for supervision. By combining Deep Distribution Loss and Pixel-wise Self-knowledge Distillation Loss, our method enhances the student model's segmentation performance through a hierarchical learning strategy, effectively transferring knowledge from the teacher model. Our method combines a loosely constrained probabilistic distribution vector with tightly constrained pixel-wise supervision, providing dual regularization for the segmentation model while also enhancing its generalization and robustness. Extensive experiments on XCAD and DCA1 datasets demonstrate that our approach outperforms the dice coefficient, accuracy, sensitivity and IoU compared to other models in comparative evaluations.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images</title>
<link>https://arxiv.org/abs/2509.03188</link>
<guid>https://arxiv.org/abs/2509.03188</guid>
<content:encoded><![CDATA[
arXiv:2509.03188v1 Announce Type: cross 
Abstract: Segmentation of small and irregularly shaped abdominal organs, such as the adrenal glands in CT imaging, remains a persistent challenge due to severe class imbalance, poor spatial context, and limited annotated data. In this work, we propose a unified framework that combines variational reconstruction, supervised segmentation, and adversarial patch-based feedback to address these limitations in a principled and scalable manner. Our architecture is built upon a VAE-UNet backbone that jointly reconstructs input patches and generates voxel-level segmentation masks, allowing the model to learn disentangled representations of anatomical structure and appearance. We introduce a patch-based training pipeline that selectively injects synthetic patches generated from the learned latent space, and systematically study the effects of varying synthetic-to-real patch ratios during training. To further enhance output fidelity, the framework incorporates perceptual reconstruction loss using VGG features, as well as a PatchGAN-style discriminator for adversarial supervision over spatial realism. Comprehensive experiments on the BTCV dataset demonstrate that our approach improves segmentation accuracy, particularly in boundary-sensitive regions, while maintaining strong reconstruction quality. Our findings highlight the effectiveness of hybrid generative-discriminative training regimes for small-organ segmentation and provide new insights into balancing realism, diversity, and anatomical consistency in data-scarce scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Active Training for Deep LiDAR Odometry</title>
<link>https://arxiv.org/abs/2509.03211</link>
<guid>https://arxiv.org/abs/2509.03211</guid>
<content:encoded><![CDATA[
arXiv:2509.03211v1 Announce Type: cross 
Abstract: Robust and efficient deep LiDAR odometry models are crucial for accurate localization and 3D reconstruction, but typically require extensive and diverse training data to adapt to diverse environments, leading to inefficiencies. To tackle this, we introduce an active training framework designed to selectively extract training data from diverse environments, thereby reducing the training load and enhancing model generalization. Our framework is based on two key strategies: Initial Training Set Selection (ITSS) and Active Incremental Selection (AIS). ITSS begins by breaking down motion sequences from general weather into nodes and edges for detailed trajectory analysis, prioritizing diverse sequences to form a rich initial training dataset for training the base model. For complex sequences that are difficult to analyze, especially under challenging snowy weather conditions, AIS uses scene reconstruction and prediction inconsistency to iteratively select training samples, refining the model to handle a wide range of real-world scenarios. Experiments across datasets and weather conditions validate our approach's effectiveness. Notably, our method matches the performance of full-dataset training with just 52\% of the sequence volume, demonstrating the training efficiency and robustness of our active training paradigm. By optimizing the training process, our approach sets the stage for more agile and reliable LiDAR odometry systems, capable of navigating diverse environmental conditions with greater precision.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics</title>
<link>https://arxiv.org/abs/2509.03421</link>
<guid>https://arxiv.org/abs/2509.03421</guid>
<content:encoded><![CDATA[
arXiv:2509.03421v1 Announce Type: cross 
Abstract: Medical foundation models, pre-trained with large-scale clinical data, demonstrate strong performance in diverse clinically relevant applications. RETFound, trained on nearly one million retinal images, exemplifies this approach in applications with retinal images. However, the emergence of increasingly powerful and multifold larger generalist foundation models such as DINOv2 and DINOv3 raises the question of whether domain-specific pre-training remains essential, and if so, what gap persists. To investigate this, we systematically evaluated the adaptability of DINOv2 and DINOv3 in retinal image applications, compared to two specialist RETFound models, RETFound-MAE and RETFound-DINOv2. We assessed performance on ocular disease detection and systemic disease prediction using two adaptation strategies: fine-tuning and linear probing. Data efficiency and adaptation efficiency were further analysed to characterise trade-offs between predictive performance and computational cost. Our results show that although scaling generalist models yields strong adaptability across diverse tasks, RETFound-DINOv2 consistently outperforms these generalist foundation models in ocular-disease detection and oculomics tasks, demonstrating stronger generalisability and data efficiency. These findings suggest that specialist retinal foundation models remain the most effective choice for clinical applications, while the narrowing gap with generalist foundation models suggests that continued data and model scaling can deliver domain-relevant gains and position them as strong foundations for future medical foundation models.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting</title>
<link>https://arxiv.org/abs/2509.03430</link>
<guid>https://arxiv.org/abs/2509.03430</guid>
<content:encoded><![CDATA[
arXiv:2509.03430v1 Announce Type: cross 
Abstract: The ability to detect touch events on uninstrumented, everyday surfaces has been a long-standing goal for mixed reality systems. Prior work has shown that virtual interfaces bound to physical surfaces offer performance and ergonomic benefits over tapping at interfaces floating in the air. A wide variety of approaches have been previously developed, to which we contribute a new headset-integrated technique called \systemname. We use a combination of a computer-triggered camera and one or more infrared emitters to create structured shadows, from which we can accurately estimate hover distance (mean error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our technique works across a range of conditions, including surface material, interaction orientation, and environmental lighting.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data</title>
<link>https://arxiv.org/abs/2509.03451</link>
<guid>https://arxiv.org/abs/2509.03451</guid>
<content:encoded><![CDATA[
arXiv:2509.03451v1 Announce Type: cross 
Abstract: The ability to track a user's arm pose could be valuable in a wide range of applications, including fitness, rehabilitation, augmented reality input, life logging, and context-aware assistants. Unfortunately, this capability is not readily available to consumers. Systems either require cameras, which carry privacy issues, or utilize multiple worn IMUs or markers. In this work, we describe how an off-the-shelf smartphone and smartwatch can work together to accurately estimate arm pose. Moving beyond prior work, we take advantage of more recent ultra-wideband (UWB) functionality on these devices to capture absolute distance between the two devices. This measurement is the perfect complement to inertial data, which is relative and suffers from drift. We quantify the performance of our software-only approach using off-the-shelf devices, showing it can estimate the wrist and elbow joints with a \hl{median positional error of 11.0~cm}, without the user having to provide training data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sam-llm: interpretable lane change trajectoryprediction via parametric finetuning</title>
<link>https://arxiv.org/abs/2509.03462</link>
<guid>https://arxiv.org/abs/2509.03462</guid>
<content:encoded><![CDATA[
arXiv:2509.03462v1 Announce Type: cross 
Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the gap between the contextual reasoning of Large Language Models (LLMs) and the physical precision of kinematic lane change models for autonomous driving. The system is designed for interpretable lane change trajectory prediction by finetuning an LLM to output the core physical parameters of a trajectory model instead of raw coordinates. For lane-keeping scenarios, the model predicts discrete coordinates, but for lane change maneuvers, it generates the parameters for an enhanced Sinusoidal Acceleration Model (SAM), including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach yields a complete, continuous, and physically plausible trajectory model that is inherently interpretable and computationally efficient, achieving an 80% reduction in output size compared to coordinate-based methods. The SAM-LLM achieves a state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating performance equivalent to traditional LLM predictors while offering significant advantages in explainability and resource efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning</title>
<link>https://arxiv.org/abs/2509.03477</link>
<guid>https://arxiv.org/abs/2509.03477</guid>
<content:encoded><![CDATA[
arXiv:2509.03477v1 Announce Type: cross 
Abstract: Addressing missing modalities and limited labeled data is crucial for advancing robust multimodal learning. We propose Robult, a scalable framework designed to mitigate these challenges by preserving modality-specific information and leveraging redundancy through a novel information-theoretic approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled (PU) contrastive loss that maximizes task-relevant feature alignment while effectively utilizing limited labeled data in semi-supervised settings, and (2) a latent reconstruction loss that ensures unique modality-specific information is retained. These strategies, embedded within a modular design, enhance performance across various downstream tasks and ensure resilience to incomplete modalities during inference. Experimental results across diverse datasets validate that Robult achieves superior performance over existing approaches in both semi-supervised learning and missing modality contexts. Furthermore, its lightweight design promotes scalability and seamless integration with existing architectures, making it suitable for real-world multimodal applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repurposing SAM for User-Defined Semantics Aware Segmentation</title>
<link>https://arxiv.org/abs/2312.02420</link>
<guid>https://arxiv.org/abs/2312.02420</guid>
<content:encoded><![CDATA[
arXiv:2312.02420v3 Announce Type: replace 
Abstract: The Segment Anything Model (SAM) excels at generating precise object masks from input prompts but lacks semantic awareness, failing to associate its generated masks with specific object categories. To address this limitation, we propose U-SAM, a novel framework that imbibes semantic awareness into SAM, enabling it to generate targeted masks for user-specified object categories. Given only object class names as input from the user, U-SAM provides pixel-level semantic annotations for images without requiring any labeled/unlabeled samples from the test data distribution. Our approach leverages synthetically generated or web crawled images to accumulate semantic information about the desired object classes. We then learn a mapping function between SAM's mask embeddings and object class labels, effectively enhancing SAM with granularity-specific semantic recognition capabilities. As a result, users can obtain meaningful and targeted segmentation masks for specific objects they request, rather than generic and unlabeled masks. We evaluate U-SAM on PASCAL VOC 2012 and MSCOCO-80, achieving significant mIoU improvements of +17.95% and +5.20%, respectively, over state-of-the-art methods. By transforming SAM into a semantically aware segmentation model, U-SAM offers a practical and flexible solution for pixel-level annotation across diverse and unseen domains in a resource-constrained environment.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Machine and Human Visual Representations across Abstraction Levels</title>
<link>https://arxiv.org/abs/2409.06509</link>
<guid>https://arxiv.org/abs/2409.06509</guid>
<content:encoded><![CDATA[
arXiv:2409.06509v4 Announce Type: replace 
Abstract: Deep neural networks have achieved success across a wide range of applications, including as models of human behavior and neural representations in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-aligned behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-aligned structure from its representations to refine the representations of pretrained state-of-the-art vision foundation models via finetuning. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognitive judgments and more practically useful, thus paving the way toward more robust, interpretable, and human-aligned artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Domain Gap for Flight-Ready Spaceborne Vision</title>
<link>https://arxiv.org/abs/2409.11661</link>
<guid>https://arxiv.org/abs/2409.11661</guid>
<content:encoded><![CDATA[
arXiv:2409.11661v2 Announce Type: replace 
Abstract: This work presents Spacecraft Pose Network v3 (SPNv3), a Neural Network (NN) for monocular pose estimation of a known, non-cooperative target spacecraft. SPNv3 is designed and trained to be computationally efficient while providing robustness to spaceborne images that have not been observed during offline training and validation on the ground. These characteristics are essential to deploying NNs on space-grade edge devices. They are achieved through careful NN design choices, and an extensive trade-off analysis reveals features such as data augmentation, transfer learning and vision transformer architecture as a few of those that contribute to simultaneously maximizing robustness and minimizing computational overhead. Experiments demonstrate that the final SPNv3 can achieve state-of-the-art pose accuracy on hardware-in-the-loop images from a robotic testbed while having trained exclusively on computer-generated synthetic images, effectively bridging the domain gap between synthetic and real imagery. At the same time, SPNv3 runs well above the update frequency of modern satellite navigation filters when tested on a representative graphical processing unit system with flight heritage. Overall, SPNv3 is an efficient, flight-ready NN model readily applicable to close-range rendezvous and proximity operations with target resident space objects.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Consistency Representation Learning for Lifelong Person Re-Identification</title>
<link>https://arxiv.org/abs/2409.19954</link>
<guid>https://arxiv.org/abs/2409.19954</guid>
<content:encoded><![CDATA[
arXiv:2409.19954v4 Announce Type: replace 
Abstract: Lifelong person re-identification (LReID) exhibits a contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps emphasize domain consistency. Achieving a trade-off between maximizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise representations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute-wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute-oriented anti-forgetting (AF) strategy that explores attribute-wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR achieves superior performance compared to state-of-the-art LReID methods. Our code is available at https://github.com/LiuShiBen/DCR.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refinement of Monocular Depth Maps via Multi-View Differentiable Rendering</title>
<link>https://arxiv.org/abs/2410.03861</link>
<guid>https://arxiv.org/abs/2410.03861</guid>
<content:encoded><![CDATA[
arXiv:2410.03861v2 Announce Type: replace 
Abstract: Accurate depth estimation is at the core of many applications in computer graphics, vision, and robotics. Current state-of-the-art monocular depth estimators, trained on extensive datasets, generalize well but lack 3D consistency needed for many applications. In this paper, we combine the strength of those generalizing monocular depth estimation techniques with multi-view data by framing this as an analysis-by-synthesis optimization problem to lift and refine such relative depth maps to accurate error-free depth maps. After an initial global scale estimation through structure-from-motion point clouds, we further refine the depth map through optimization enforcing multi-view consistency via photometric and geometric losses with differentiable rendering of the meshed depth map. In a two-stage optimization, scaling is further refined first, and afterwards artifacts and errors in the depth map are corrected via nearby-view photometric supervision. Our evaluation shows that our method is able to generate detailed, high-quality, view consistent, accurate depth maps, also in challenging indoor scenarios, and outperforms state-of-the-art multi-view depth reconstruction approaches on such datasets.
  Project page and source code can be found at https://lorafib.github.io/ref_depth/.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Neural Association Network for Self-supervised Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2411.11514</link>
<guid>https://arxiv.org/abs/2411.11514</guid>
<content:encoded><![CDATA[
arXiv:2411.11514v2 Announce Type: replace 
Abstract: This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GalaxAlign: Mimicking Citizen Scientists' Multimodal Guidance for Galaxy Morphology Analysis</title>
<link>https://arxiv.org/abs/2411.19475</link>
<guid>https://arxiv.org/abs/2411.19475</guid>
<content:encoded><![CDATA[
arXiv:2411.19475v2 Announce Type: replace 
Abstract: Galaxy morphology analysis involves studying galaxies based on their shapes and structures. For such studies, fundamental tasks include identifying and classifying galaxies in astronomical images, as well as retrieving visually or structurally similar galaxies through similarity search. Existing methods either directly train domain-specific foundation models on large, annotated datasets or fine-tune vision foundation models on a smaller set of images. The former is effective but costly, while the latter is more resource-efficient but often yields lower accuracy. To address these challenges, we introduce GalaxAlign, a multimodal approach inspired by how citizen scientists identify galaxies in astronomical images by following textual descriptions and matching schematic symbols. Specifically, GalaxAlign employs a tri-modal alignment framework to align three types of data during fine-tuning: (1) schematic symbols representing galaxy shapes and structures, (2) textual labels for these symbols, and (3) galaxy images. By incorporating multimodal instructions, GalaxAlign eliminates the need for expensive pretraining and enhances the effectiveness of fine-tuning. Experiments on galaxy classification and similarity search demonstrate that our method effectively fine-tunes general pre-trained models for astronomical tasks by incorporating domain-specific multi-modal knowledge. Code is available at https://github.com/RapidsAtHKUST/GalaxAlign.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting</title>
<link>https://arxiv.org/abs/2412.00177</link>
<guid>https://arxiv.org/abs/2412.00177</guid>
<content:encoded><![CDATA[
arXiv:2412.00177v3 Announce Type: replace 
Abstract: We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.
  Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Universal Synthetic Video Detector: From Face or Background Manipulations to Fully AI-Generated Content</title>
<link>https://arxiv.org/abs/2412.12278</link>
<guid>https://arxiv.org/abs/2412.12278</guid>
<content:encoded><![CDATA[
arXiv:2412.12278v2 Announce Type: replace 
Abstract: Existing DeepFake detection techniques primarily focus on facial manipulations, such as face-swapping or lip-syncing. However, advancements in text-to-video (T2V) and image-to-video (I2V) generative models now allow fully AI-generated synthetic content and seamless background alterations, challenging face-centric detection methods and demanding more versatile approaches.
  To address this, we introduce the \underline{U}niversal \underline{N}etwork for \underline{I}dentifying \underline{T}ampered and synth\underline{E}tic videos (\texttt{UNITE}) model, which, unlike traditional detectors, captures full-frame manipulations. \texttt{UNITE} extends detection capabilities to scenarios without faces, non-human subjects, and complex background modifications. It leverages a transformer-based architecture that processes domain-agnostic features extracted from videos via the SigLIP-So400M foundation model. Given limited datasets encompassing both facial/background alterations and T2V/I2V content, we integrate task-irrelevant data alongside standard DeepFake datasets in training. We further mitigate the model's tendency to over-focus on faces by incorporating an attention-diversity (AD) loss, which promotes diverse spatial attention across video frames. Combining AD loss with cross-entropy improves detection performance across varied contexts. Comparative evaluations demonstrate that \texttt{UNITE} outperforms state-of-the-art detectors on datasets (in cross-data settings) featuring face/background manipulations and fully synthetic T2V/I2V videos, showcasing its adaptability and generalizable detection capabilities.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Hand Gesture Recognition from Visual Input</title>
<link>https://arxiv.org/abs/2501.11992</link>
<guid>https://arxiv.org/abs/2501.11992</guid>
<content:encoded><![CDATA[
arXiv:2501.11992v3 Announce Type: replace 
Abstract: Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViDDAR: Vision Language Model-Based Task-Detrimental Content Detection for Augmented Reality</title>
<link>https://arxiv.org/abs/2501.12553</link>
<guid>https://arxiv.org/abs/2501.12553</guid>
<content:encoded><![CDATA[
arXiv:2501.12553v2 Announce Type: replace 
Abstract: In Augmented Reality (AR), virtual content enhances user experience by providing additional information. However, improperly positioned or designed virtual content can be detrimental to task performance, as it can impair users' ability to accurately interpret real-world information. In this paper we examine two types of task-detrimental virtual content: obstruction attacks, in which virtual content prevents users from seeing real-world objects, and information manipulation attacks, in which virtual content interferes with users' ability to accurately interpret real-world information. We provide a mathematical framework to characterize these attacks and create a custom open-source dataset for attack evaluation. To address these attacks, we introduce ViDDAR (Vision language model-based Task-Detrimental content Detector for Augmented Reality), a comprehensive full-reference system that leverages Vision Language Models (VLMs) and advanced deep learning techniques to monitor and evaluate virtual content in AR environments, employing a user-edge-cloud architecture to balance performance with low latency. To the best of our knowledge, ViDDAR is the first system to employ VLMs for detecting task-detrimental content in AR settings. Our evaluation results demonstrate that ViDDAR effectively understands complex scenes and detects task-detrimental content, achieving up to 92.15% obstruction detection accuracy with a detection latency of 533 ms, and an 82.46% information manipulation content detection accuracy with a latency of 9.62 s.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Next-Day Wildfire Predictability of MODIS and VIIRS Satellite Data</title>
<link>https://arxiv.org/abs/2503.08580</link>
<guid>https://arxiv.org/abs/2503.08580</guid>
<content:encoded><![CDATA[
arXiv:2503.08580v4 Announce Type: replace 
Abstract: Multiple studies have performed next-day fire prediction using satellite imagery. Two main satellites are used to detect wildfires: MODIS and VIIRS. Both satellites provide fire mask products, called MOD14 and VNP14, respectively. Studies have used one or the other, but there has been no comparison between them to determine which might be more suitable for next-day fire prediction. In this paper, we first evaluate how well VIIRS and MODIS data can be used to forecast wildfire spread one day ahead. We find that the model using VIIRS as input and VNP14 as target achieves the best results. Interestingly, the model using MODIS as input and VNP14 as target performs significantly better than using VNP14 as input and MOD14 as target. Next, we discuss why MOD14 might be harder to use for predicting next-day fires. We find that the MOD14 fire mask is highly stochastic and does not correlate with reasonable fire spread patterns. This is detrimental for machine learning tasks, as the model learns irrational patterns. Therefore, we conclude that MOD14 is unsuitable for next-day fire prediction and that VNP14 is a much better option. However, using MODIS input and VNP14 as target, we achieve a significant improvement in predictability. This indicates that an improved fire detection model is possible for MODIS. The full code and dataset is available online: https://github.com/justuskarlsson/wildfire-mod14-vnp14
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization</title>
<link>https://arxiv.org/abs/2503.12615</link>
<guid>https://arxiv.org/abs/2503.12615</guid>
<content:encoded><![CDATA[
arXiv:2503.12615v2 Announce Type: replace 
Abstract: Text-to-image latent diffusion models (LDMs) have recently emerged as powerful generative models with great potential for solving inverse problems in imaging. However, leveraging such models in a Plug & Play (PnP), zero-shot manner remains challenging because it requires identifying a suitable text prompt for the unknown image of interest. Also, existing text-to-image PnP approaches are highly computationally expensive. We herein address these challenges by proposing a novel PnP inference paradigm specifically designed for embedding generative models within stochastic inverse solvers, with special attention to Latent Consistency Models (LCMs), which distill LDMs into fast generators. We leverage our framework to propose LAtent consisTency INverse sOlver (LATINO), the first zero-shot PnP framework to solve inverse problems with priors encoded by LCMs. Our conditioning mechanism avoids automatic differentiation and reaches SOTA quality in as little as 8 neural function evaluations. As a result, LATINO delivers remarkably accurate solutions and is significantly more memory and computationally efficient than previous approaches. We then embed LATINO within an empirical Bayesian framework that automatically calibrates the text prompt from the observed measurements by marginal maximum likelihood estimation. Extensive experiments show that prompt self-calibration greatly improves estimation, allowing LATINO with PRompt Optimization to define new SOTAs in image reconstruction quality and computational efficiency. The code is available at https://latino-pro.github.io
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deeply Supervised Flow-Based Generative Models</title>
<link>https://arxiv.org/abs/2503.14494</link>
<guid>https://arxiv.org/abs/2503.14494</guid>
<content:encoded><![CDATA[
arXiv:2503.14494v2 Announce Type: replace 
Abstract: Flow based generative models have charted an impressive path across multiple visual generation tasks by adhering to a simple principle: learning velocity representations of a linear interpolant. However, we observe that training velocity solely from the final layer output underutilizes the rich inter layer representations, potentially impeding model convergence. To address this limitation, we introduce DeepFlow, a novel framework that enhances velocity representation through inter layer communication. DeepFlow partitions transformer layers into balanced branches with deep supervision and inserts a lightweight Velocity Refiner with Acceleration (VeRA) block between adjacent branches, which aligns the intermediate velocity features within transformer blocks. Powered by the improved deep supervision via the internal velocity alignment, DeepFlow converges 8 times faster on ImageNet with equivalent performance and further reduces FID by 2.6 while halving training time compared to previous flow based models without a classifier free guidance. DeepFlow also outperforms baselines in text to image generation tasks, as evidenced by evaluations on MSCOCO and zero shot GenEval.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TruthLens: Visual Grounding for Universal DeepFake Reasoning</title>
<link>https://arxiv.org/abs/2503.15867</link>
<guid>https://arxiv.org/abs/2503.15867</guid>
<content:encoded><![CDATA[
arXiv:2503.15867v3 Announce Type: replace 
Abstract: Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, while existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel, unified, and highly generalizable framework that goes beyond traditional binary classification, providing detailed, textual reasoning for its predictions. Distinct from conventional methods, TruthLens performs MLLM grounding.
  TruthLens uses a task-driven representation integration strategy that unites global semantic context from a multimodal large language model (MLLM) with region-specific forensic cues through explicit cross-modal adaptation of a vision-only model. This enables nuanced, region-grounded reasoning for both face-manipulated and fully synthetic content, and supports fine-grained queries such as "Does the eyes/nose/mouth look real or fake?"- capabilities beyond pretrained MLLMs alone. Extensive experiments across diverse datasets demonstrate that TruthLens sets a new benchmark in both forensic interpretability and detection accuracy, generalizing to seen and unseen manipulations alike. By unifying high-level scene understanding with fine-grained region grounding, TruthLens delivers transparent DeepFake forensics, bridging a critical gap in the literature.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAEA: A Geolocation Aware Conversational Assistant</title>
<link>https://arxiv.org/abs/2503.16423</link>
<guid>https://arxiv.org/abs/2503.16423</guid>
<content:encoded><![CDATA[
arXiv:2503.16423v3 Announce Type: replace 
Abstract: Image geolocalization, in which an AI model traditionally predicts the precise GPS coordinates of an image, is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge beyond the GPS coordinates; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with the tremendous progress of large multimodal models (LMMs) -- proprietary and open-source -- researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, such as geolocalization, LMMs struggle. In this work, we propose solving this problem by introducing a conversational model, GAEA, that provides information regarding the location of an image as the user requires. No large-scale dataset enabling the training of such a model exists. Thus, we propose GAEA-1.4M, a comprehensive dataset comprising over 800k images and approximately 1.4M question-answer pairs, constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark, GAEA-Bench, comprising 3.5k image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision, by 18.2% and the best proprietary model, GPT-4o, by 7.2%. Our dataset, model and codes are available.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the representation of stack operators by mathematical morphology</title>
<link>https://arxiv.org/abs/2504.09766</link>
<guid>https://arxiv.org/abs/2504.09766</guid>
<content:encoded><![CDATA[
arXiv:2504.09766v2 Announce Type: replace 
Abstract: This paper introduces the class of grey-scale image stack operators as those that (a) map binary-images into binary-images and (b) commute on average with cross-sectioning. Equivalently, stack operators are 1-Lipchitz extensions of set operators which can be represented by applying a characteristic set operator to the cross-sections of the image and adding. In particular, they are a generalisation of stack filters, for which the characteristic set operators are increasing. Our main result is that stack operators inherit lattice properties of the characteristic set operators. We focus on the case of translation-invariant and locally defined stack operators and show the main result by deducing the characteristic function, kernel, and basis representation of stack operators. The results of this paper have implications on the design of image operators, since imply that to solve some grey-scale image processing problems it is enough to design an operator for performing the desired transformation on binary images, and then considering its extension given by a stack operator. We leave many topics for future research regarding the machine learning of stack operators and the characterisation of the image processing problems that can be solved by them.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada</title>
<link>https://arxiv.org/abs/2504.13231</link>
<guid>https://arxiv.org/abs/2504.13231</guid>
<content:encoded><![CDATA[
arXiv:2504.13231v3 Announce Type: replace 
Abstract: Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. In this work, we focus on multimodal wildfire social media data, which, although existing in current datasets, is currently underrepresented in Canadian contexts. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. We evaluate zero-shot vision-language models on this dataset and compare their results with those of custom-trained and baseline classifiers. We show that while baseline methods and zero-shot prompting offer quick deployment, custom-trained models outperform them when labelled data is available. Our best-performing custom model reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We also demonstrate how this model can be used to uncover trends during wildfires, through the collection and analysis of a large unlabeled dataset. Our dataset facilitates future research in wildfire response, and our findings highlight the importance of tailored datasets and task-specific training. Importantly, such datasets should be localized, as disaster response requirements vary across regions and contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucination in Large Vision-Language Models through Aligning Attention Distribution to Information Flow</title>
<link>https://arxiv.org/abs/2505.14257</link>
<guid>https://arxiv.org/abs/2505.14257</guid>
<content:encoded><![CDATA[
arXiv:2505.14257v2 Announce Type: replace 
Abstract: Due to the unidirectional masking mechanism, Decoder-Only models propagate information from left to right. LVLMs (Large Vision-Language Models) follow the same architecture, with visual information gradually integrated into semantic representations during forward propagation. Through systematic analysis, we observe that the majority of the visual information is absorbed into the semantic representations. However, the model's attention distribution does not exhibit sufficient emphasis on semantic representations. This misalignment between the attention distribution and the actual information flow undermines the model's visual understanding ability and contributes to hallucinations. To address this issue, we enhance the model's visual understanding by leveraging the core information embedded in semantic representations. Specifically, we identify attention heads that focus on core semantic representations based on their attention distributions. Then, through a two-stage optimization paradigm, we propagate the advantages of these attention heads across the entire model, aligning the attention distribution with the actual information flow. We evaluate our method on three image captioning benchmarks using five different LVLMs, demonstrating its effectiveness in significantly reducing hallucinations. Further experiments reveal a trade-off between reduced hallucinations and richer details. Notably, our method allows for manual adjustment of the model's conservativeness, enabling flexible control to meet diverse real-world requirements.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets</title>
<link>https://arxiv.org/abs/2505.23980</link>
<guid>https://arxiv.org/abs/2505.23980</guid>
<content:encoded><![CDATA[
arXiv:2505.23980v2 Announce Type: replace 
Abstract: Understanding Greenland's subglacial topography is critical for projecting the future mass loss of the ice sheet and its contribution to global sea-level rise. However, the complex and sparse nature of observational data, particularly information about the bed topography under the ice sheet, significantly increases the uncertainty in model projections. Bed topography is traditionally measured by airborne ice-penetrating radar that measures the ice thickness directly underneath the aircraft, leaving data gap of tens of kilometers in between flight lines. This study introduces a deep learning framework, which we call as DeepTopoNet, that integrates radar-derived ice thickness observations and BedMachine Greenland data through a novel dynamic loss-balancing mechanism. Among all efforts to reconstruct bed topography, BedMachine has emerged as one of the most widely used datasets, combining mass conservation principles and ice thickness measurements to generate high-resolution bed elevation estimates. The proposed loss function adaptively adjusts the weighting between radar and BedMachine data, ensuring robustness in areas with limited radar coverage while leveraging the high spatial resolution of BedMachine predictions i.e. bed estimates. Our approach incorporates gradient-based and trend surface features to enhance model performance and utilizes a CNN architecture designed for subgrid-scale predictions. By systematically testing on the Upernavik Isstr{\o}m) region, the model achieves high accuracy, outperforming baseline methods in reconstructing subglacial terrain. This work demonstrates the potential of deep learning in bridging observational gaps, providing a scalable and efficient solution to inferring subglacial topography.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2506.13265</link>
<guid>https://arxiv.org/abs/2506.13265</guid>
<content:encoded><![CDATA[
arXiv:2506.13265v3 Announce Type: replace 
Abstract: Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection</title>
<link>https://arxiv.org/abs/2506.18368</link>
<guid>https://arxiv.org/abs/2506.18368</guid>
<content:encoded><![CDATA[
arXiv:2506.18368v3 Announce Type: replace 
Abstract: Detecting anomalous human behaviour is an important visual task in safety-critical applications such as healthcare monitoring, workplace safety, or public surveillance. In these contexts, abnormalities are often reflected with unusual human poses. Thus, we propose SeeKer, a method for detecting anomalies in sequences of human skeletons. Our method formulates the skeleton sequence density through autoregressive factorization at the keypoint level. The corresponding conditional distributions represent probable keypoint locations given prior skeletal motion. We formulate the joint distribution of the considered skeleton as causal prediction of conditional Gaussians across its constituent keypoints. A skeleton is flagged as anomalous if its keypoint locations surprise our model (i.e. receive a low density). In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals, where the weights account for the confidence of the underlying keypoint detector. Despite its conceptual simplicity, SeeKer surpasses all previous methods on the UBnormal and MSAD-HR datasets while delivering competitive performance on the ShanghaiTech dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23903</link>
<guid>https://arxiv.org/abs/2506.23903</guid>
<content:encoded><![CDATA[
arXiv:2506.23903v2 Announce Type: replace 
Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</title>
<link>https://arxiv.org/abs/2507.00790</link>
<guid>https://arxiv.org/abs/2507.00790</guid>
<content:encoded><![CDATA[
arXiv:2507.00790v3 Announce Type: replace 
Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at https://github.com/AMAP-ML/LD-RPS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Stability for Image Restoration via Gradient Management</title>
<link>https://arxiv.org/abs/2507.06656</link>
<guid>https://arxiv.org/abs/2507.06656</guid>
<content:encoded><![CDATA[
arXiv:2507.06656v2 Announce Type: replace 
Abstract: Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at https://github.com/74587887/SPGD.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding</title>
<link>https://arxiv.org/abs/2508.01197</link>
<guid>https://arxiv.org/abs/2508.01197</guid>
<content:encoded><![CDATA[
arXiv:2508.01197v2 Announce Type: replace 
Abstract: Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</title>
<link>https://arxiv.org/abs/2508.04416</link>
<guid>https://arxiv.org/abs/2508.04416</guid>
<content:encoded><![CDATA[
arXiv:2508.04416v2 Announce Type: replace 
Abstract: The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. Code is available at https://zhang9302002.github.io/thinkingwithvideos-page/.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal</title>
<link>https://arxiv.org/abs/2508.12750</link>
<guid>https://arxiv.org/abs/2508.12750</guid>
<content:encoded><![CDATA[
arXiv:2508.12750v2 Announce Type: replace 
Abstract: Shadow removal aims to restore images that are partially degraded by shadows, where the degradation is spatially localized and non-uniform. Unlike general restoration tasks that assume global degradation, shadow removal can leverage abundant information from non-shadow regions for guidance. However, the transformation required to correct shadowed areas often differs significantly from that of well-lit regions, making it challenging to apply uniform correction strategies. This necessitates the effective integration of non-local contextual cues and adaptive modeling of region-specific transformations. To this end, we propose a novel Mamba-based network featuring dual-scale fusion and dual-path scanning to selectively propagate contextual information based on transformation similarity across regions. Specifically, the proposed Dual-Scale Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing original features with low-resolution features, effectively reducing boundary artifacts. The Dual-Path Mamba Group (DPMG) captures global features via horizontal scanning and incorporates a mask-aware adaptive scanning strategy, which improves structural continuity and fine-grained region modeling. Experimental results demonstrate that our method significantly outperforms existing state-of-the-art approaches on shadow removal benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report</title>
<link>https://arxiv.org/abs/2508.13401</link>
<guid>https://arxiv.org/abs/2508.13401</guid>
<content:encoded><![CDATA[
arXiv:2508.13401v2 Announce Type: replace 
Abstract: This report presents an overview of the AIM 2025 RipSeg Challenge, a competition designed to advance techniques for automatic rip current segmentation in still images. Rip currents are dangerous, fast-moving flows that pose a major risk to beach safety worldwide, making accurate visual detection an important and underexplored research task. The challenge builds on RipVIS, the largest available rip current dataset, and focuses on single-class instance segmentation, where precise delineation is critical to fully capture the extent of rip currents. The dataset spans diverse locations, rip current types, and camera orientations, providing a realistic and challenging benchmark.
  In total, $75$ participants registered for this first edition, resulting in $5$ valid test submissions. Teams were evaluated on a composite score combining $F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and application-relevant rankings. The top-performing methods leveraged deep learning architectures, domain adaptation techniques, pretrained models, and domain generalization strategies to improve performance under diverse conditions.
  This report outlines the dataset details, competition framework, evaluation metrics, and final results, providing insights into the current state of rip current segmentation. We conclude with a discussion of key challenges, lessons learned from the submissions, and future directions for expanding RipSeg.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents</title>
<link>https://arxiv.org/abs/2208.13266</link>
<guid>https://arxiv.org/abs/2208.13266</guid>
<content:encoded><![CDATA[
arXiv:2208.13266v4 Announce Type: replace-cross 
Abstract: Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2411.05085</link>
<guid>https://arxiv.org/abs/2411.05085</guid>
<content:encoded><![CDATA[
arXiv:2411.05085v2 Announce Type: replace-cross 
Abstract: Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models. In this work, we present a dataset called PadChest-GR (Grounded-Reporting) derived from PadChest aimed at training GRRG models for CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with grounded reports (3,099 abnormal and 1,456 normal), each containing complete lists of sentences describing individual present (positive) and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7,037 positive and 3,422 negative finding sentences. Every positive finding sentence is associated with up to two independent sets of bounding boxes labelled by different readers and has categorical labels for finding type, locations, and progression. To the best of our knowledge, PadChest-GR is the first manually curated dataset designed to train GRRG models for understanding and interpreting radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, it provides a valuable resource for developing and evaluating GRRG models from CXR images. PadChest-GR can be downloaded under request from https://bimcv.cipf.es/bimcv-projects/padchest-gr/
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Feature Mapping GAP: Integrating Real HDRTV Priors for Superior SDRTV-to-HDRTV Conversion</title>
<link>https://arxiv.org/abs/2411.10775</link>
<guid>https://arxiv.org/abs/2411.10775</guid>
<content:encoded><![CDATA[
arXiv:2411.10775v2 Announce Type: replace-cross 
Abstract: The rise of HDR-WCG display devices has highlighted the need to convert SDRTV to HDRTV, as most video sources are still in SDR. Existing methods primarily focus on designing neural networks to learn a single-style mapping from SDRTV to HDRTV. However, the limited information in SDRTV and the diversity of styles in real-world conversions render this process an ill-posed problem, thereby constraining the performance and generalization of these methods. Inspired by generative approaches, we propose a novel method for SDRTV to HDRTV conversion guided by real HDRTV priors. Despite the limited information in SDRTV, introducing real HDRTV as reference priors significantly constrains the solution space of the originally high-dimensional ill-posed problem. This shift transforms the task from solving an unreferenced prediction problem to making a referenced selection, thereby markedly enhancing the accuracy and reliability of the conversion process. Specifically, our approach comprises two stages: the first stage employs a Vector Quantized Generative Adversarial Network to capture HDRTV priors, while the second stage matches these priors to the input SDRTV content to recover realistic HDRTV outputs. We evaluate our method on public datasets, demonstrating its effectiveness with significant improvements in both objective and subjective metrics across real and synthetic datasets.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-TransFormers for Continual Learning</title>
<link>https://arxiv.org/abs/2411.16073</link>
<guid>https://arxiv.org/abs/2411.16073</guid>
<content:encoded><![CDATA[
arXiv:2411.16073v2 Announce Type: replace-cross 
Abstract: Inspired by the Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network for each task. During sequential training in CL, a well-initialized Soft-TF mask optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks, while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on the Vision Transformer (ViT) and the Language Transformer (Bert) demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across Vision and Language Class Incremental Learning (CIL) scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Similarity Guided License Plate Super Resolution</title>
<link>https://arxiv.org/abs/2501.01483</link>
<guid>https://arxiv.org/abs/2501.01483</guid>
<content:encoded><![CDATA[
arXiv:2501.01483v3 Announce Type: replace-cross 
Abstract: Super-resolution (SR) techniques play a pivotal role in enhancing the quality of low-resolution images, particularly for applications such as security and surveillance, where accurate license plate recognition is crucial. This study proposes a novel framework that combines pixel-based loss with embedding similarity learning to address the unique challenges of license plate super-resolution (LPSR). The introduced pixel and embedding consistency loss (PECL) integrates a Siamese network and applies contrastive loss to force embedding similarities to improve perceptual and structural fidelity. By effectively balancing pixel-wise accuracy with embedding-level consistency, the framework achieves superior alignment of fine-grained features between high-resolution (HR) and super-resolved (SR) license plates. Extensive experiments on the CCPD and PKU dataset validate the efficacy of the proposed framework, demonstrating consistent improvements over state-of-the-art methods in terms of PSNR, SSIM, LPIPS, and optical character recognition (OCR) accuracy. These results highlight the potential of embedding similarity learning to advance both perceptual quality and task-specific performance in extreme super-resolution scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-preserving contrastive learning for spatial time series</title>
<link>https://arxiv.org/abs/2502.06380</link>
<guid>https://arxiv.org/abs/2502.06380</guid>
<content:encoded><![CDATA[
arXiv:2502.06380v4 Announce Type: replace-cross 
Abstract: The effectiveness of neural network models largely relies on learning meaningful latent patterns from data, where self-supervised learning of informative representations can enhance model performance and generalisability. However, self-supervised representation learning for spatially characterised time series, which are ubiquitous in transportation domain, poses unique challenges due to the necessity of maintaining fine-grained spatio-temporal similarities in the latent space. In this study, we introduce two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance the contrastive learning objective and the need for structure preservation, we propose a dynamic weighting mechanism that adaptively manages this trade-off and stabilises training. We validate the proposed method through extensive experiments, including multivariate time series classification to demonstrate its general applicability, as well as macroscopic and microscopic traffic prediction to highlight its particular usefulness in encoding traffic interactions. Across all tasks, our method preserves the similarity structures more effectively and improves state-of-the-art task performances. This method can be integrated with an arbitrary neural network model and is particularly beneficial for time series data with spatial or geographical features. Furthermore, our findings suggest that well-preserved similarity structures in the latent space indicate more informative and useful representations. This provides insights to design more effective neural networks for data-driven transportation research. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture or Semantics? Vision-Language Models Get Lost in Font Recognition</title>
<link>https://arxiv.org/abs/2503.23768</link>
<guid>https://arxiv.org/abs/2503.23768</guid>
<content:encoded><![CDATA[
arXiv:2503.23768v3 Announce Type: replace-cross 
Abstract: Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AstroClearNet: Deep image prior for multi-frame astronomical image restoration</title>
<link>https://arxiv.org/abs/2504.06463</link>
<guid>https://arxiv.org/abs/2504.06463</guid>
<content:encoded><![CDATA[
arXiv:2504.06463v2 Announce Type: replace-cross 
Abstract: Recovering high-fidelity images of the night sky from blurred observations is a fundamental problem in astronomy, where traditional methods typically fall short. In ground-based astronomy, combining multiple exposures to enhance signal-to-noise ratios is further complicated by variations in the point-spread function caused by atmospheric turbulence. In this work, we present a self-supervised multi-frame method, based on deep image priors, for denoising, deblurring, and coadding ground-based exposures. Central to our approach is a carefully designed convolutional neural network that integrates information across multiple observations and enforces physically motivated constraints. We demonstrate the method's potential by processing Hyper Suprime-Cam exposures, yielding promising preliminary results with sharper restored images.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond</title>
<link>https://arxiv.org/abs/2504.13037</link>
<guid>https://arxiv.org/abs/2504.13037</guid>
<content:encoded><![CDATA[
arXiv:2504.13037v4 Announce Type: replace-cross 
Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual's disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy</title>
<link>https://arxiv.org/abs/2504.18829</link>
<guid>https://arxiv.org/abs/2504.18829</guid>
<content:encoded><![CDATA[
arXiv:2504.18829v2 Announce Type: replace-cross 
Abstract: Generalizable dexterous grasping with suitable grasp types is a fundamental skill for intelligent robots. Developing such skills requires a large-scale and high-quality dataset that covers numerous grasp types (i.e., at least those categorized by the GRASP taxonomy), but collecting such data is extremely challenging. Existing automatic grasp synthesis methods are often limited to specific grasp types or object categories, hindering scalability. This work proposes an efficient pipeline capable of synthesizing contact-rich, penetration-free, and physically plausible grasps for any grasp type, object, and articulated hand. Starting from a single human-annotated template for each hand and grasp type, our pipeline tackles the complicated synthesis problem with two stages: optimize the object to fit the hand template first, and then locally refine the hand to fit the object in simulation. To validate the synthesized grasps, we introduce a contact-aware control strategy that allows the hand to apply the appropriate force at each contact point to the object. Those validated grasps can also be used as new grasp templates to facilitate future synthesis. Experiments show that our method significantly outperforms previous type-unaware grasp synthesis baselines in simulation. Using our algorithm, we construct a dataset containing 10.7k objects and 9.5M grasps, covering 31 grasp types in the GRASP taxonomy. Finally, we train a type-conditional generative model that successfully performs the desired grasp type from single-view object point clouds, achieving an 82.3% success rate in real-world experiments. Project page: https://pku-epic.github.io/Dexonomy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation</title>
<link>https://arxiv.org/abs/2505.02476</link>
<guid>https://arxiv.org/abs/2505.02476</guid>
<content:encoded><![CDATA[
arXiv:2505.02476v2 Announce Type: replace-cross 
Abstract: The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</title>
<link>https://arxiv.org/abs/2505.20353</link>
<guid>https://arxiv.org/abs/2505.20353</guid>
<content:encoded><![CDATA[
arXiv:2505.20353v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Per-Garment Virtual Try-On with Temporal Consistency for Loose-Fitting Garments</title>
<link>https://arxiv.org/abs/2506.12348</link>
<guid>https://arxiv.org/abs/2506.12348</guid>
<content:encoded><![CDATA[
arXiv:2506.12348v2 Announce Type: replace-cross 
Abstract: Per-garment virtual try-on methods collect garment-specific datasets and train networks tailored to each garment to achieve superior results. However, these approaches often struggle with loose-fitting garments due to two key limitations: (1) They rely on human body semantic maps to align garments with the body, but these maps become unreliable when body contours are obscured by loose-fitting garments, resulting in degraded outcomes; (2) They train garment synthesis networks on a per-frame basis without utilizing temporal information, leading to noticeable jittering artifacts. To address the first limitation, we propose a two-stage approach for robust semantic map estimation. First, we extract a garment-invariant representation from the raw input image. This representation is then passed through an auxiliary network to estimate the semantic map. This enhances the robustness of semantic map estimation under loose-fitting garments during garment-specific dataset generation. To address the second limitation, we introduce a recurrent garment synthesis framework that incorporates temporal dependencies to improve frame-to-frame coherence while maintaining real-time performance. We conducted qualitative and quantitative evaluations to demonstrate that our method outperforms existing approaches in both image quality and temporal coherence. Ablation studies further validate the effectiveness of the garment-invariant representation and the recurrent synthesis framework.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Medical Image Binding via Shared Text Embeddings</title>
<link>https://arxiv.org/abs/2506.18072</link>
<guid>https://arxiv.org/abs/2506.18072</guid>
<content:encoded><![CDATA[
arXiv:2506.18072v2 Announce Type: replace-cross 
Abstract: Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bind's effectiveness in achieving cross-image-modal alignment for medical analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Data Protection in the (Generative) Artificial Intelligence Era</title>
<link>https://arxiv.org/abs/2507.03034</link>
<guid>https://arxiv.org/abs/2507.03034</guid>
<content:encoded><![CDATA[
arXiv:2507.03034v4 Announce Type: replace-cross 
Abstract: The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid-Reg: Detector-Free Gridized Feature Learning and Matching for Large-Scale SAR-Optical Image Registration</title>
<link>https://arxiv.org/abs/2507.04233</link>
<guid>https://arxiv.org/abs/2507.04233</guid>
<content:encoded><![CDATA[
arXiv:2507.04233v2 Announce Type: replace-cross 
Abstract: It is highly challenging to register large-scale, heterogeneous SAR and optical images, particularly across platforms, due to significant geometric, radiometric, and temporal differences, which most existing methods struggle to address. To overcome these challenges, we propose Grid-Reg, a grid-based multimodal registration framework comprising a domain-robust descriptor extraction network, Hybrid Siamese Correlation Metric Learning Network (HSCMLNet), and a grid-based solver (Grid-Solver) for transformation parameter estimation. In heterogeneous imagery with large modality gaps and geometric differences, obtaining accurate correspondences is inherently difficult. To robustly measure similarity between gridded patches, HSCMLNet integrates a hybrid Siamese module with a correlation metric learning module (CMLModule) based on equiangular unit basis vectors (EUBVs), together with a manifold consistency loss to promote modality-invariant, discriminative feature learning. The Grid-Solver estimates transformation parameters by minimizing a global grid matching loss through a progressive dual-loop search strategy to reliably find patch correspondences across entire images. Furthermore, we curate a challenging benchmark dataset for SAR-to-optical registration using real-world UAV MiniSAR data and Google Earth optical imagery. Extensive experiments demonstrate that our proposed approach achieves superior performance over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</title>
<link>https://arxiv.org/abs/2507.20800</link>
<guid>https://arxiv.org/abs/2507.20800</guid>
<content:encoded><![CDATA[
arXiv:2507.20800v3 Announce Type: replace-cross 
Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNet's efficacy. Quantitative analysis revealed significant reductions (p < 0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the system's adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring GUI Elements through Vision Language Models: Towards Action Space Generation</title>
<link>https://arxiv.org/abs/2508.16271</link>
<guid>https://arxiv.org/abs/2508.16271</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, graphical user interface, data augmentation, IoU-based coordinate sampling, maximum likelihood estimation

Summary: 
Multimodal large language models (MLLMs) are being increasingly utilized in the field of graphical user interface (GUI) element structuring to process user instructions based on screen contents. However, the challenge lies in their ability to precisely generate UI element coordinates due to the lack of semantic representation for numerical UI coordinates in language spaces. To overcome this limitation, an IoU-Augmented Maximum Likelihood (IAML) training paradigm is introduced. This approach involves a novel pipeline for IoU-based coordinate sampling to augment training data, focusing on proximity to ground truth coordinates. By fine-tuning MLLMs under the IAML paradigm, exposure bias problems inherent in traditional maximum likelihood estimation are mitigated. Extensive experiments demonstrate the superior performance of the IAML training approach compared to traditional paradigms. 

<br /><br />Summary: <div>
arXiv:2508.16271v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction. In this paper we focus on the application of MLLMs in the field of graphical user interface (GUI) elements structuring, where they assist in processing user instructions based on screen contents. Despite the promise of MLLMs, their performance in precisely generating UI element coordinates, a critical aspect of GUI understanding, is hindered by the nature of next-token prediction training. This challenge arises from the semantic void surrounding numerical UI coordinates in language representation spaces, necessitating a substantial and diverse dataset to bolster visual module capabilities. To address these limitations, we introduce an IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our approach involves a novel pipeline for IoU-based coordinate sampling to augment the training data, which considers the proximity to ground truth coordinates. This data augmentation strategy is then employed to fine-tune MLLMs under the IAML paradigm, which is designed to mitigate the exposure bias problem inherent in traditional maximum likelihood estimation. Through extensive experiments, we demonstrate the superior performance of our IAML training approach over traditional training paradigms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2508.17009</link>
<guid>https://arxiv.org/abs/2508.17009</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly Supervised Semantic Segmentation, Contrastive Prompt Clustering, Large Language Models, Inter-class relationships, Fine-grained discrimination

Summary:
Contrastive Prompt Clustering (CPC) is introduced as a novel Weakly Supervised Semantic Segmentation (WSSS) framework that leverages Large Language Models (LLMs) to derive category clusters and enforce intra-class consistency and inter-class separation through a class-aware patch-level contrastive loss. CPC aims to address the limitations of existing WSSS methods by emphasizing the shared semantics among related categories while maintaining fine-grained discrimination. The hierarchical design of CPC utilizes clusters as coarse-grained semantic priors to reduce confusion among visually similar categories. Experimental results on PASCAL VOC 2012 and MS COCO 2014 datasets show that CPC outperforms state-of-the-art methods in WSSS, highlighting the effectiveness of leveraging inter-class relationships and fine-grained boundaries in semantic segmentation tasks.<br /><br />Summary: <div>
arXiv:2508.17009v2 Announce Type: replace 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17243</link>
<guid>https://arxiv.org/abs/2508.17243</guid>
<content:encoded><![CDATA[
<div> Keywords: LVLMs, vision-language models, visual token pruning, CoViPAL, Plug-and-Play Pruning Module

Summary: 
CoViPAL is a novel approach for efficient and effective pruning of redundant visual tokens in Large Vision-Language Models (LVLMs). By utilizing a lightweight Plug-and-Play Pruning Module (PPM), CoViPAL predicts and removes unnecessary vision tokens before they are processed by the model, resulting in improved computational efficiency without sacrificing accuracy. The method operates independently of LVLM architecture, making it adaptable to various models. CoViPAL surpasses both training-free and training-based pruning methods in terms of performance under equal token budgets. By leveraging contextual signals, CoViPAL demonstrates the ability to identify and eliminate redundant visual information even in shallow layers, where traditional methods struggle due to lack of contextual information. Overall, CoViPAL offers a scalable and efficient solution for enhancing inference efficiency in LVLMs by reducing computational costs and memory overhead associated with processing visual inputs. <br /><br />Summary: <div>
arXiv:2508.17243v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.17394</link>
<guid>https://arxiv.org/abs/2508.17394</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical decision-making, multimodal retriever, LVLM, medical diagnosis, visual information retrieval

Summary:
In this paper, the authors propose a model that combines a multimodal retriever with a LVLM for medical diagnosis, optimizing them jointly to enhance diagnostic accuracy. Unlike standard approaches, this model propagates error signals down to the retriever, resulting in competitive results with medically-pretrained models for clinical multi-label classification and visual question answering tasks. The analysis shows that different top retrieved images can lead to different predictions for the same target, presenting a challenge for all models. The joint retrieval optimization significantly improves these challenging cases over standard methods, although there is still a performance gap from the oracle diagnosis. Future methods could potentially improve the model further, as there is room for enhancement in closing the performance gap. The code for the model will be made publicly available. 

<br /><br />Summary: <div>
arXiv:2508.17394v2 Announce Type: replace 
Abstract: Clinical decision-making often involves interpreting images (e.g., radiology) for making diagnoses. Retrieving relevant visual information from medical literature and hospital records could enhance diagnostic accuracy. In this paper, we develop a model in which a multimodal retriever is jointly optimized with an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal is not propagated down to the retriever. We show that using only general-purpose backbones, with only lightweight fine-tuning, our model is able to achieve competitive results with medically-pretrained models across clinical multi-label classification and visual question answering tasks. In a novel analysis, we additionally find that in many cases different top retrieved images each lead to different predictions for a given target, and that these cases are empirically challenging for all models, even for non-retrieval models. Our joint retrieval optimization significantly improves these challenging cases over standard RAG. However, oracle analysis reveals that while the correct diagnosis is frequently achievable using one of the top retrieved images, in practice there is a large performance gap from the oracle, and rerankers using frontier LVLMs do not close this gap -- leaving ample room for improvement by future methods. Code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Domain Gaps for Indoor 3D Object Detection</title>
<link>https://arxiv.org/abs/2508.17439</link>
<guid>https://arxiv.org/abs/2508.17439</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor 3D object detection, domain adaptation, point cloud datasets, benchmark, adaptation scenarios 

Summary: 
This paper addresses the challenge of adapting indoor 3D object detectors across different datasets, introducing a benchmark with various datasets including newly proposed large-scale datasets. The study explores different adaptation scenarios such as synthetic-to-real adaptation and analyzes the impact of various domain gaps on the performance of 3D object detectors. The experiments conducted highlight the need for improved generalization abilities of detectors across domains, considering factors such as point cloud quality, bounding box layout, and instance features. Several approaches are introduced to enhance adaptation performances, providing baselines for domain adaptive indoor 3D object detection. The project homepage containing more details can be accessed at https://jeremyzhao1998.github.io/DAVoteNet-release/. 

<br /><br />Summary: <div>
arXiv:2508.17439v2 Announce Type: replace 
Abstract: As a fundamental task for indoor scene understanding, 3D object detection has been extensively studied, and the accuracy on indoor point cloud data has been substantially improved. However, existing researches have been conducted on limited datasets, where the training and testing sets share the same distribution. In this paper, we consider the task of adapting indoor 3D object detectors from one dataset to another, presenting a comprehensive benchmark with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator. Since indoor point cloud datasets are collected and constructed in different ways, the object detectors are likely to overfit to specific factors within each dataset, such as point cloud quality, bounding box layout and instance features. We conduct experiments across datasets on different adaptation scenarios including synthetic-to-real adaptation, point cloud quality adaptation, layout adaptation and instance feature adaptation, analyzing the impact of different domain gaps on 3D object detectors. We also introduce several approaches to improve adaptation performances, providing baselines for domain adaptive indoor 3D object detection, hoping that future works may propose detectors with stronger generalization ability across domains. Our project homepage can be found in https://jeremyzhao1998.github.io/DAVoteNet-release/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventTracer: Fast Path Tracing-based Event Stream Rendering</title>
<link>https://arxiv.org/abs/2508.18071</link>
<guid>https://arxiv.org/abs/2508.18071</guid>
<content:encoded><![CDATA[
<div> Keywords: event-based vision, simulation, path tracing, spiking network, event streams

Summary: 
EventTracer is introduced as a path tracing-based rendering pipeline for simulating high-fidelity event sequences from complex 3D scenes efficiently and in a physics-aware manner. The pipeline utilizes low sample-per-pixel path tracing to speed up the rendering process and a lightweight event spiking network to denoise RGB videos into realistic event sequences. The network is equipped with a bipolar leaky integrate-and-fired spiking unit and trained with a bidirectional earth mover distance loss to capture physical properties of event streams. EventTracer runs at a speed of approximately 4 minutes per second of 720p video and accurately models spatiotemporal information. It outperforms existing event simulators in capturing scene details and similarity to real-world event data, making it a promising tool for generating large-scale event-RGB datasets cost-effectively. This advancement bridges the gap between simulation and reality in event-based vision research, benefiting fields such as robotics, autonomous driving, and VR/AR applications.<br /><br />Summary: <div>
arXiv:2508.18071v2 Announce Type: replace 
Abstract: Simulating event streams from 3D scenes has become a common practice in event-based vision research, as it meets the demand for large-scale, high temporal frequency data without setting up expensive hardware devices or undertaking extensive data collections. Yet existing methods in this direction typically work with noiseless RGB frames that are costly to render, and therefore they can only achieve a temporal resolution equivalent to 100-300 FPS, far lower than that of real-world event data. In this work, we propose EventTracer, a path tracing-based rendering pipeline that simulates high-fidelity event sequences from complex 3D scenes in an efficient and physics-aware manner. Specifically, we speed up the rendering process via low sample-per-pixel (SPP) path tracing, and train a lightweight event spiking network to denoise the resulting RGB videos into realistic event sequences. To capture the physical properties of event streams, the network is equipped with a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at a speed of about 4 minutes per second of 720p video, and it inherits the merit of accurate spatiotemporal modeling from its path tracing backbone. We show in two downstream tasks that EventTracer captures better scene details and demonstrates a greater similarity to real-world event data than other event simulators, which establishes it as a promising tool for creating large-scale event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based vision, and boosting various application scenarios such as robotics, autonomous driving, and VRAR.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary</title>
<link>https://arxiv.org/abs/2509.00033</link>
<guid>https://arxiv.org/abs/2509.00033</guid>
<content:encoded><![CDATA[
<div> Keywords: YOLOv8, LSTM model, ASR, LLM, computer vision

Summary: 
This research study combines a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence, and an ASR (whisper-based) to extract data for a LLM (TinyLLaMa) to predict recipes and generate step-by-step cooking guides. The author collected data to create a robust system tailored for complex environments, showcasing the versatility of computer vision in activities like cooking. The integration of different models enhances performance in challenging situations, demonstrating the practical application of computer vision in everyday tasks. This work expands the scope of computer vision technology to address essential aspects of daily life.<br /><br /> <div>
arXiv:2509.00033v1 Announce Type: new 
Abstract: This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMMKD: Adaptive Multimodal Multi-teacher Distillation for Lightweight Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.00039</link>
<guid>https://arxiv.org/abs/2509.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: visual language pretraining, image-text retrieval, knowledge distillation, multi-modal feature fusion, adaptive optimization

Summary: 
AMMKD proposes a novel framework for lightweight yet effective image-text retrieval models by integrating multi-modal feature fusion, multi-teacher distillation, and adaptive optimization. The method starts with a feature fusion network that extracts and merges discriminative features from both image and text modalities. It utilizes a multi-teacher knowledge distillation approach to pre-train two CLIP teacher models and leverages KL scatter for probability distribution matching. Text features are pre-computed and stored as class vectors to enhance efficiency. An adaptive dynamic weighting scheme is implemented to treat multi-teacher distillation as a multi-objective optimization problem, dynamically adjusting the influence of each teacher to guide the student towards optimal learning directions. Extensive experiments on benchmark datasets demonstrate that AMMKD achieves superior performance while reducing model complexity, showcasing its effectiveness and flexibility.<br /><br />Summary: <div>
arXiv:2509.00039v1 Announce Type: new 
Abstract: The success of large-scale visual language pretraining (VLP) models has driven widespread adoption of image-text retrieval tasks. However, their deployment on mobile devices remains limited due to large model sizes and computational complexity. We propose Adaptive Multi-Modal Multi-Teacher Knowledge Distillation (AMMKD), a novel framework that integrates multi-modal feature fusion, multi-teacher distillation, and adaptive optimization to deliver lightweight yet effective retrieval models. Specifically, our method begins with a feature fusion network that extracts and merges discriminative features from both the image and text modalities. To reduce model parameters and further improve performance, we design a multi-teacher knowledge distillation framework to pre-train two CLIP teacher models. We decouple modalities by pre-computing and storing text features as class vectors via the teacher text encoder to enhance efficiency. To better align teacher and student outputs, we apply KL scatter for probability distribution matching. Finally, we design an adaptive dynamic weighting scheme that treats multi-teacher distillation as a multi-objective optimization problem. By leveraging gradient space diversity, we dynamically adjust the influence of each teacher, reducing conflicts and guiding the student toward more optimal learning directions. Extensive experiments on three benchmark datasets demonstrate that AMMKD achieves superior performance while significantly reducing model complexity, validating its effectiveness and flexibility.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization</title>
<link>https://arxiv.org/abs/2509.00042</link>
<guid>https://arxiv.org/abs/2509.00042</guid>
<content:encoded><![CDATA[
<div> depth estimation, anomaly detection, learnable curiosity scoring, autonomous exploration, planetary surfaces

Summary:
ARTPS (Autonomous Rover Target Prioritization System) is a novel hybrid AI system designed for autonomous exploration of planetary surfaces. The system combines monocular depth estimation using Vision Transformers, multi-component anomaly detection, and a weighted curiosity score. It has achieved state-of-the-art performance on Mars rover datasets with high AUROC, AUPRC, and F1-Score. Through ablation studies, the system has demonstrated significant improvements in target prioritization accuracy. The hybrid fusion approach reduces false positives by 23% while maintaining a high detection sensitivity across various terrain types. Comprehensive analysis of component contributions showcases the effectiveness of integrating depth estimation, anomaly detection, and learnable curiosity scoring in enhancing the overall performance of autonomous exploration systems. <div>
arXiv:2509.00042v1 Announce Type: new 
Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel hybrid AI system that combines depth estimation, anomaly detection, and learnable curiosity scoring for autonomous exploration of planetary surfaces. Our approach integrates monocular depth estimation using Vision Transformers with multi-component anomaly detection and a weighted curiosity score that balances known value, anomaly signals, depth variance, and surface roughness. The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of 0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant improvements in target prioritization accuracy through ablation studies and provide comprehensive analysis of component contributions. The hybrid fusion approach reduces false positives by 23% while maintaining high detection sensitivity across diverse terrain types.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance is not All You Need: Sustainability Considerations for Algorithms</title>
<link>https://arxiv.org/abs/2509.00045</link>
<guid>https://arxiv.org/abs/2509.00045</guid>
<content:encoded><![CDATA[
<div> Keywords: carbon emissions, deep learning, energy efficiency, sustainability evaluation, algorithm performance

Summary: 
This work addresses the significant carbon emissions associated with deep learning model training by proposing a novel two-dimensional sustainability evaluation system. The system introduces two quantitative indicators, the sustainable harmonic mean (FMS) and the area under the sustainability curve (ASC), to balance algorithm performance and energy consumption. By integrating energy efficiency ratio and accuracy, the FMS reveals algorithm performance under unit energy consumption, while the ASC constructs a performance-power consumption curve to characterize energy efficiency throughout the cycle. The study tested the framework on various multimodal tasks and benchmarks, such as image classification and pose estimation, demonstrating its universality and effectiveness. The provided sustainability evaluation framework code enables industry practitioners to establish algorithm energy efficiency standards, promoting the practical implementation of green AI research. <br /><br />Summary: <div>
arXiv:2509.00045v1 Announce Type: new 
Abstract: This work focuses on the high carbon emissions generated by deep learning model training, specifically addressing the core challenge of balancing algorithm performance and energy consumption. It proposes an innovative two-dimensional sustainability evaluation system. Different from the traditional single performance-oriented evaluation paradigm, this study pioneered two quantitative indicators that integrate energy efficiency ratio and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy consumption and performance parameters through the harmonic mean to reveal the algorithm performance under unit energy consumption; the area under the sustainability curve (ASC) constructs a performance-power consumption curve to characterize the energy efficiency characteristics of the algorithm throughout the cycle. To verify the universality of the indicator system, the study constructed benchmarks in various multimodal tasks, including image classification, segmentation, pose estimation, and batch and online learning. Experiments demonstrate that the system can provide a quantitative basis for evaluating cross-task algorithms and promote the transition of green AI research from theory to practice. Our sustainability evaluation framework code can be found here, providing methodological support for the industry to establish algorithm energy efficiency standards.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition</title>
<link>https://arxiv.org/abs/2509.00056</link>
<guid>https://arxiv.org/abs/2509.00056</guid>
<content:encoded><![CDATA[
<div> novel input modalities, Micro-expression Spatio-Temporal Image, Gradient Attention block, MEGANet, state-of-the-art results <br />
Summary: 
This study introduces the Micro-expression Spatio-Temporal Image (MESTI) as a new input modality for micro-expression recognition (MER), enhancing the capture of subtle facial movements. The proposed Micro-expression Gradient Attention Network (MEGANet) incorporates a Gradient Attention block to extract fine-grained motion features from micro-expressions. Experimental results demonstrate the effectiveness of MESTI, showing performance improvements across different CNN architectures. By integrating MESTI and MEGANet, the network achieves state-of-the-art results on CASMEII and SAMM datasets, setting a new benchmark for MER. The study highlights the potential of MESTI as a superior input modality and MEGANet as an advanced recognition network, offering promising advancements in MER technology for various applications. <br /><br /> <div>
arXiv:2509.00056v1 Announce Type: new 
Abstract: Micro-expression recognition (MER) is a challenging task due to the subtle and fleeting nature of micro-expressions. Traditional input modalities, such as Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture these brief facial movements, resulting in suboptimal performance. In this study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel dynamic input modality that transforms a video sequence into a single image while preserving the essential characteristics of micro-movements. Additionally, we present the Micro-expression Gradient Attention Network (MEGANet), which incorporates a novel Gradient Attention block to enhance the extraction of fine-grained motion features from micro-expressions. By combining MESTI and MEGANet, we aim to establish a more effective approach to MER. Extensive experiments were conducted to evaluate the effectiveness of MESTI, comparing it with existing input modalities across three CNN architectures (VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing the input of previously published MER networks with MESTI leads to consistent performance improvements. The performance of MEGANet, both with MESTI and Dynamic Image, is also evaluated, showing that our proposed network achieves state-of-the-art results on the CASMEII and SAMM datasets. The combination of MEGANet and MESTI achieves the highest accuracy reported to date, setting a new benchmark for micro-expression recognition. These findings underscore the potential of MESTI as a superior input modality and MEGANet as an advanced recognition network, paving the way for more effective MER systems in a variety of applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion</title>
<link>https://arxiv.org/abs/2509.00062</link>
<guid>https://arxiv.org/abs/2509.00062</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse, multi-category, 3D voxel structures, Scaffold Diffusion, generative model

Summary: 
Scaffold Diffusion is a generative model designed for sparse multi-category 3D voxel structures, addressing the challenges posed by memory scaling and class imbalance. By utilizing a discrete diffusion language model that treats voxels as tokens, this model can generate realistic and coherent 3D structures. The study demonstrates the extension of discrete diffusion language models to spatial domains and evaluates the model's performance on Minecraft house structures from the 3D-Craft dataset. Unlike previous baselines, Scaffold Diffusion can produce high-quality structures even with over 98% sparsity in the training data. Through an interactive viewer, readers can visualize the generation process and generated samples, showcasing the model's effectiveness in 3D voxel generative modeling. The results indicate that discrete diffusion presents a promising framework for addressing challenges in generating realistic sparse multi-category 3D voxel structures.<br /><br />Summary: <div>
arXiv:2509.00062v1 Announce Type: new 
Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult due to the cubic memory scaling of voxel structures and moreover the significant class imbalance caused by sparsity. We introduce Scaffold Diffusion, a generative model designed for sparse multi-category 3D voxel structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete diffusion language model to generate 3D voxel structures. We show that discrete diffusion language models can be extended beyond inherently sequential domains such as text to generate spatially coherent 3D structures. We evaluate on Minecraft house structures from the 3D-Craft dataset and demonstrate that, unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion produces realistic and coherent structures even when trained on data with over 98% sparsity. We provide an interactive viewer where readers can visualize generated samples and the generation process. Our results highlight discrete diffusion as a promising framework for 3D sparse voxel generative modeling.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Stage Global and Local Feature Framework for Image Dehazing</title>
<link>https://arxiv.org/abs/2509.00108</link>
<guid>https://arxiv.org/abs/2509.00108</guid>
<content:encoded><![CDATA[
<div> keywords: image dehazing, high-resolution imagery, global features generator, local features enhancer, Uformer architecture

Summary:
The article introduces a novel framework, the Streamlined Global and Local Features Combinator (SGLC), to enhance image dehazing for high-resolution imagery. The framework consists of the Global Features Generator (GFG) and the Local Features Enhancer (LFE), which work together to combine global contextual information with local details effectively. By incorporating SGLC into the Uformer architecture, the performance of dehazing models on high-resolution datasets significantly improves in terms of peak signal-to-noise ratio (PSNR). This approach allows for the refinement of preliminary dehazed outputs by capturing both global appearance and local structure, enhancing visual fidelity in large-scale environments. The SGLC design is model-agnostic, making it versatile for integration with various dehazing networks, providing a comprehensive solution for addressing atmospheric haze in high-resolution imagery. 

<br /><br />Summary: <div>
arXiv:2509.00108v1 Announce Type: new 
Abstract: Addressing the challenge of removing atmospheric fog or haze from digital images, known as image dehazing, has recently gained significant traction in the computer vision community. Although contemporary dehazing models have demonstrated promising performance, few have thoroughly investigated high-resolution imagery. In such scenarios, practitioners often resort to downsampling the input image or processing it in smaller patches, which leads to a notable performance degradation. This drop is primarily linked to the difficulty of effectively combining global contextual information with localized, fine-grained details as the spatial resolution grows. In this chapter, we propose a novel framework, termed the Streamlined Global and Local Features Combinator (SGLC), to bridge this gap and enable robust dehazing for high-resolution inputs. Our approach is composed of two principal components: the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The GFG produces an initial dehazed output by focusing on broad contextual understanding of the scene. Subsequently, the LFE refines this preliminary output by enhancing localized details and pixel-level features, thereby capturing the interplay between global appearance and local structure. To evaluate the effectiveness of SGLC, we integrated it with the Uformer architecture, a state-of-the-art dehazing model. Experimental results on high-resolution datasets reveal a considerable improvement in peak signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in addressing haze in large-scale imagery. Moreover, the SGLC design is model-agnostic, allowing any dehazing network to be augmented with the proposed global-and-local feature fusion mechanism. Through this strategy, practitioners can harness both scene-level cues and granular details, significantly improving visual fidelity in high-resolution environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised large-scale kidney abnormality detection in drug safety assessment studies</title>
<link>https://arxiv.org/abs/2509.00131</link>
<guid>https://arxiv.org/abs/2509.00131</guid>
<content:encoded><![CDATA[
<div> kidney abnormality detection, self-supervised model, drug safety assessment, UNI foundation model, toxicologic pathology <br />
<br />
Summary: 
This study introduces a large-scale self-supervised model for detecting kidney abnormalities in drug safety assessment studies. Traditional methods are time-consuming and costly, necessitating the examination of numerous whole-slide images to identify potential toxic effects. The study utilizes features from the UNI foundation model but initially finds that a k-nearest neighbor classifier on these features performs at chance level. However, a self-supervised approach applied to the same features yields better results, achieving an area under the receiver operating characteristic curve of 0.62 and a negative predictive value of 89%. This innovative model shows promise in ruling out normal slides, potentially reducing costs and time in drug development processes. Further development and refinement could enhance the model's performance and practical utility in preclinical drug development. <br /><br /> Summary: <div>
arXiv:2509.00131v1 Announce Type: new 
Abstract: Kidney abnormality detection is required for all preclinical drug development. It involves a time-consuming and costly examination of hundreds to thousands of whole-slide images per drug safety study, most of which are normal, to detect any subtle changes indicating toxic effects. In this study, we present the first large-scale self-supervised abnormality detection model for kidney toxicologic pathology, spanning drug safety assessment studies from 158 compounds. We explore the complexity of kidney abnormality detection on this scale using features extracted from the UNI foundation model (FM) and show that a simple k-nearest neighbor classifier on these features performs at chance, demonstrating that the FM-generated features alone are insufficient for detecting abnormalities. We then demonstrate that a self-supervised method applied to the same features can achieve better-than-chance performance, with an area under the receiver operating characteristic curve of 0.62 and a negative predictive value of 89%. With further development, such a model can be used to rule out normal slides in drug safety assessment studies, reducing the costs and time associated with drug development.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments</title>
<link>https://arxiv.org/abs/2509.00176</link>
<guid>https://arxiv.org/abs/2509.00176</guid>
<content:encoded><![CDATA[
<div> waste classification, Vision Large Language Models, complex environments, deformed shaped objects, dataset<br />
Summary:<br />
Recent advancements in Vision Large Language Models (VLLMs) have enabled them to tackle a variety of visual tasks, but their performance in complex environments with deformed objects remains unexplored. A new dataset focused on waste classification in real-world scenarios with challenging conditions has been introduced in this work. The dataset aims to evaluate the robustness and accuracy of VLLMs through a comprehensive analysis. The findings emphasize the need for further improvements in VLLM's ability to operate effectively in complex environments. The dataset and experimental code will be publicly available for further research and development. <div>
arXiv:2509.00176v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for Vision Large Language Models (VLLMs) capable of performing a wide range of visual understanding tasks. While LLMs have demonstrated impressive performance on standard natural images, their capabilities have not been thoroughly explored in cluttered datasets where there is complex environment having deformed shaped objects. In this work, we introduce a novel dataset specifically designed for waste classification in real-world scenarios, characterized by complex environments and deformed shaped objects. Along with this dataset, we present an in-depth evaluation approach to rigorously assess the robustness and accuracy of VLLMs. The introduced dataset and comprehensive analysis provide valuable insights into the performance of VLLMs under challenging conditions. Our findings highlight the critical need for further advancements in VLLM's robustness to perform better in complex environments. The dataset and code for our experiments will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders</title>
<link>https://arxiv.org/abs/2509.00177</link>
<guid>https://arxiv.org/abs/2509.00177</guid>
<content:encoded><![CDATA[
<div> Keyword: text-to-image retrieval, vision-and-language models, generative diffusion model, aggregation network, evaluation

Summary:
The work focuses on improving text-to-image retrieval performance by bridging the modality gap between text and images. The proposed two-step approach involves transforming text queries into visual queries using a generative diffusion model and estimating image-to-image similarity with a vision model. An aggregation network is introduced to combine multiple generated images and fuse similarity scores across both query modalities. Leveraging advancements in vision encoders, VLMs, and text-to-image generation models, the approach consistently outperforms retrieval methods relying solely on text queries. Extensive evaluations demonstrate the effectiveness of the proposed approach. Source code for the implementation is available on GitHub at https://github.com/faixan-khan/cletir.<br /><br />Summary: <div>
arXiv:2509.00177v1 Announce Type: new 
Abstract: This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: https://github.com/faixan-khan/cletir
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety</title>
<link>https://arxiv.org/abs/2509.00192</link>
<guid>https://arxiv.org/abs/2509.00192</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, biometric leakage, PRISM benchmark, Safe-LLaVA dataset, privacy-preserving training

Summary:<br /><br />
Multimodal Large Language Models (MLLMs) have shown impressive capabilities in vision-language tasks. However, they often unintentionally reveal sensitive biometric attributes like race and gender, posing privacy concerns. To address this issue, the PRISM benchmark evaluates MLLMs on refusing biometric queries and detecting implicit biometric leakage while maintaining accuracy. An audit of LLaVA datasets found extensive biometric leakage, leading to the creation of the Safe-LLaVA dataset, which removes biometric information for privacy preservation. Evaluations on PRISM showed biometric leaks across MLLMs, underscoring privacy violations. Fine-tuning a model on Safe-LLaVA significantly reduced biometric leakage. Safe-LLaVA and PRISM establish a new standard for privacy-centric MLLM development and evaluation. The datasets and codes are publicly available, promoting transparent and ethical MLLM research. <div>
arXiv:2509.00192v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks. However, these models often infer and reveal sensitive biometric attributes - such as race, gender, age, body weight, and eye color - even when such information is not explicitly requested. This raises critical concerns, particularly in real-world applications and socially-sensitive domains. Despite increasing awareness, no publicly available dataset or benchmark exists to comprehensively evaluate or mitigate biometric leakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware Evaluation of Responses in Sensitive Modalities), a new benchmark designed to assess MLLMs on two fronts: (1) refuse biometric-related queries and (2) implicit biometric leakage in general responses while maintaining semantic faithfulness. Further, we conduct a detailed audit of the widely used LLaVA datasets and uncover extensive biometric leakage across pretraining and instruction data. To address this, we present Safe-LLaVA dataset, the first privacy-preserving MLLM training dataset constructed by systematically removing explicit and implicit biometric information from LLaVA dataset. Our evaluations on PRISM reveal biometric leakages across MLLMs for different attributes, highlighting the detailed privacy-violations. We also fine-tune a model on Safe-LLaVA dataset and show that it substantially reduces the biometric leakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned development and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark are publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA, and the source code is available at https://github.com/Kimyounggun99/Safe-LLaVA.git.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</title>
<link>https://arxiv.org/abs/2509.00210</link>
<guid>https://arxiv.org/abs/2509.00210</guid>
<content:encoded><![CDATA[
<div> Keywords: embodied intelligence, vision-language models, spatio-temporal reasoning, cross-modal alignment, episodic experiences

Summary:
The article introduces VEME, a new method aiming to enhance deep learning models' reasoning capabilities in dynamic environments. By incorporating a cross-modal alignment framework, VEME improves the generalization of vision-language models by integrating fine-grained spatio-temporal cues. Additionally, VEME utilizes a dynamic cognitive map to facilitate task-relevant memory retrieval and an instruction-based navigation system leveraging embodied priors for efficient exploration. Through the embedding of geometry-aware spatio-temporal episodic experiences, VEME significantly enhances reasoning and planning in unknown environments. Experimental results on benchmark datasets show a 1%-3% improvement in accuracy and exploration efficiency compared to conventional approaches.<br /><br />Summary: <div>
arXiv:2509.00210v1 Announce Type: new 
Abstract: Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data</title>
<link>https://arxiv.org/abs/2509.00213</link>
<guid>https://arxiv.org/abs/2509.00213</guid>
<content:encoded><![CDATA[
<div> deep learning, breast ultrasound images, Phyllodes tumors, multimodal framework, diagnostic accuracy

Summary:
- Phyllodes tumors (PTs) are rare breast lesions often misclassified as benign fibroadenomas.
- A new multimodal deep learning framework integrates breast ultrasound images and clinical data for improved diagnostic accuracy.
- A dual-branch neural network was developed to extract and fuse features from ultrasound images and patient metadata.
- Class-aware sampling and subject-stratified cross-validation were applied to prevent biases in the data analysis.
- The multimodal method outperformed unimodal baselines in differentiating benign from borderline/malignant PTs, showing promise for non-invasive diagnostic tools in breast tumor management. 

<br /><br />Summary: <div>
arXiv:2509.00213v1 Announce Type: new 
Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline/malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery</title>
<link>https://arxiv.org/abs/2509.00226</link>
<guid>https://arxiv.org/abs/2509.00226</guid>
<content:encoded><![CDATA[
<div> Keywords: gravitational lensing, dark matter, LSST, ViT models, transfer learning

Summary: 
Gravitational lensing is a valuable tool for studying dark matter and determining cosmological parameters. The Legacy Survey of Space and Time (LSST) is expected to discover a large number of gravitational lenses in the coming years, requiring automated classification methods. GraViT, a PyTorch pipeline, utilizes pretraining of Vision Transformer (ViT) models and MLP-Mixer for gravitational lens detection. Transfer learning significantly improves classification performance, with factors like data quality, model architecture, and training strategies impacting results. Ten architectures were fine-tuned using datasets from HOLISMOKES VI and SuGOHI X, and compared against convolutional baselines. The study replicates previous experiments on neural networks and provides insights into the detectability of strong gravitational lenses. Discussions include complexity and inference-time analysis of the models. <div>
arXiv:2509.00226v1 Announce Type: new 
Abstract: Gravitational lensing offers a powerful probe into the properties of dark matter and is crucial to infer cosmological parameters. The Legacy Survey of Space and Time (LSST) is predicted to find O(10^5) gravitational lenses over the next decade, demanding automated classifiers. In this work, we introduce GraViT, a PyTorch pipeline for gravitational lens detection that leverages extensive pretraining of state-of-the-art Vision Transformer (ViT) models and MLP-Mixer. We assess the impact of transfer learning on classification performance by examining data quality (source and sample size), model architecture (selection and fine-tuning), training strategies (augmentation, normalization, and optimization), and ensemble predictions. This study reproduces the experiments in a previous systematic comparison of neural networks and provides insights into the detectability of strong gravitational lenses on that common test sample. We fine-tune ten architectures using datasets from HOLISMOKES VI and SuGOHI X, and benchmark them against convolutional baselines, discussing complexity and inference-time analysis.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Accuracy Fast Hough Transform with Linear-Log-Cubed Computational Complexity for Arbitrary-Shaped Images</title>
<link>https://arxiv.org/abs/2509.00231</link>
<guid>https://arxiv.org/abs/2509.00231</guid>
<content:encoded><![CDATA[
<div> Hough transform, computational complexity, accuracy, fast algorithm, superpixel<br />
Summary:<br />
The paper introduces the FHT2SP algorithm, a fast and highly accurate Hough transform algorithm. It combines Brady's superpixel concept with the FHT2DT algorithm, allowing for efficient computation with near-optimal complexity of O(wh ln^3 w) and constant-bounded approximation error. The algorithm extends the superpixel concept to arbitrary shapes, enabling accurate line approximation independent of image size. The choice of superpixel size can control the approximation error, making it a versatile and efficient solution for various image analysis applications. Experimental and theoretical analyses confirm the algorithm's efficiency and accuracy, showcasing its potential for improving Hough transform computations in different domains. <br /><br />Summary: <div>
arXiv:2509.00231v1 Announce Type: new 
Abstract: The Hough transform (HT) is a fundamental tool across various domains, from classical image analysis to neural networks and tomography. Two key aspects of the algorithms for computing the HT are their computational complexity and accuracy - the latter often defined as the error of approximation of continuous lines by discrete ones within the image region. The fast HT (FHT) algorithms with optimal linearithmic complexity - such as the Brady-Yong algorithm for power-of-two-sized images - are well established. Generalizations like $FHT2DT$ extend this efficiency to arbitrary image sizes, but with reduced accuracy that worsens with scale. Conversely, accurate HT algorithms achieve constant-bounded error but require near-cubic computational cost. This paper introduces $FHT2SP$ algorithm - a fast and highly accurate HT algorithm. It builds on our development of Brady's superpixel concept, extending it to arbitrary shapes beyond the original power-of-two square constraint, and integrates it into the $FHT2DT$ algorithm. With an appropriate choice of the superpixel's size, for an image of shape $w \times h$, the $FHT2SP$ algorithm achieves near-optimal computational complexity $\mathcal{O}(wh \ln^3 w)$, while keeping the approximation error bounded by a constant independent of image size, and controllable via a meta-parameter. We provide theoretical and experimental analyses of the algorithm's complexity and accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Industrial Contour Detection: A Language-Guided Vision System</title>
<link>https://arxiv.org/abs/2509.00284</link>
<guid>https://arxiv.org/abs/2509.00284</guid>
<content:encoded><![CDATA[
<div> Keywords: industrial computer vision, language-guided generative vision system, remnant contour detection, conditional GAN, vision-language modeling.

Summary:
Industrial computer vision systems face challenges with noise and material variability, hindering classical edge detection. A language-guided generative vision system for remnant contour detection in manufacturing is proposed. The system comprises data acquisition, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling. Standardized prompts are utilized in a human-in-the-loop process for image-text guided synthesis. The system improves contour fidelity, enhances edge continuity and geometric alignment, reducing manual tracing. Benchmarking various vision-language models, including GPT-image-1 and Gemini 2.0 Flash, GPT-image-1 outperformed in structural accuracy and perceptual quality under standardized conditions. This study showcases the potential of vision-language modeling for advancing industrial computer vision beyond traditional pipelines.<br /><br />Summary: <div>
arXiv:2509.00284v1 Announce Type: new 
Abstract: Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Aware Information Maximization for Transductive Few-Shot CLIP</title>
<link>https://arxiv.org/abs/2509.00305</link>
<guid>https://arxiv.org/abs/2509.00305</guid>
<content:encoded><![CDATA[
<div> Keywords: transductive learning, few-shot learning, vision-language models, parameter-efficient fine-tuning, information-theoretic concepts

Summary:<br /><br />
Transductive few-shot learning in vision-language models is still an emerging field, with limited existing methods. This study introduces a novel approach called Language-aware Information MaximizatiOn (LIMO) that incorporates information-theoretic concepts and parameter-efficient fine-tuning (PEFT). The LIMO method includes three key components: mutual information between vision inputs and textual class descriptions, KL divergence to penalize deviation from text-driven predictions, and a standard cross-entropy loss. By challenging traditional fine-tuning practices and exploring PEFT strategies, the study demonstrates significant performance improvements over recent transductive few-shot CLIP methods and inductive approaches. The findings highlight the potential of adapting a subset of model parameters in transductive few-shot learning scenarios. The comprehensive evaluations and publicly available code contribute to advancing the field of vision-language models in few-shot learning scenarios. <div>
arXiv:2509.00305v1 Announce Type: new 
Abstract: Transductive few-shot learning has triggered an abundant literature focusing on vision-only models, but is still at a nascent stage within the recent context of foundational vision-language models (VLMs). Only a few recent methods addressed the problem, pointing to the potential of tranduction in VLMs and to the need for VLM-tailored methods. Building on this momentum, we leverage information-theoretic concepts and recent progress in parameter-efficient fine-tuning (PEFT), developing a highly competitive transductive few-shot CLIP method. Specifically, we introduce a novel Language-aware Information MaximizatiOn (LIMO) loss integrating three complementary terms: (i) the mutual information between the vision inputs and the textual class descriptions; (ii) a Kullback-Leibler (KL) divergence penalizing deviation of the network's probabilistic outputs from the text-driven zero-shot predictions; and (iii) a standard cross-entropy loss based on the labeled shots. Furthermore, we challenge the commonly followed fine-tuning practices in the context of transductive few-shot learning, and explore PEFT strategies, completely overlooked in this context. Surprisingly, we observe substantial boosts in performances, which points to the potential of adapting a subset of the model's parameters in the transductive few-shot setting. We report comprehensive evaluations, which show that LIMO outperforms the very recent transductive few-shot CLIP methods by a large margin and yields significant gains over the best-performing inductive methods. Our code is publicly available at:\[ \href{https://github.com/ghassenbaklouti/LIMO}{\text{here}} \]
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</title>
<link>https://arxiv.org/abs/2509.00311</link>
<guid>https://arxiv.org/abs/2509.00311</guid>
<content:encoded><![CDATA[
<div> Keywords: domain generalization, computational histopathology, nuclear morphology, supervised contrastive learning, resilient representations

Summary:
Domain generalization in computational histopathology is challenging due to variations in whole slide images (WSIs) across institutions. Pathologists rely on domain-invariant morphological cues for diagnosis, such as nuclear atypia and spatial organization. In this study, MorphGen, a method integrating images, augmentations, and nuclear segmentation masks, is proposed for learning resilient cancer representations. By aligning latent representations of images and masks, MorphGen prioritizes diagnostic features over staining artifacts. Incorporating stochastic weight averaging (SWA) enhances out-of-distribution robustness. Attention map analyses reveal that MorphGen focuses on nuclear morphology and spatial organization for classification. The method demonstrates resilience to image corruptions and adversarial attacks, addressing vulnerabilities in current deep learning systems for digital pathology. Overall, MorphGen enhances domain generalization in computational histopathology by emphasizing nuclear morphology and spatial organization for robust cancer representation. 

<br /><br />Summary: <div>
arXiv:2509.00311v1 Announce Type: new 
Abstract: Domain generalization in computational histopathology is hindered by heterogeneity in whole slide images (WSIs), caused by variations in tissue preparation, staining, and imaging conditions across institutions. Unlike machine learning systems, pathologists rely on domain-invariant morphological cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia, chromatin texture, spatial disorganization), structural atypia (abnormal architecture and gland formation), and overall morphological atypia that remain diagnostic across diverse settings. Motivated by this, we hypothesize that explicitly modeling biologically robust nuclear morphology and spatial organization will enable the learning of cancer representations that are resilient to domain shifts. We propose MorphGen (Morphology-Guided Generalization), a method that integrates histopathology images, augmentations, and nuclear segmentation masks within a supervised contrastive learning framework. By aligning latent representations of images and nuclear masks, MorphGen prioritizes diagnostic features such as nuclear and morphological atypia and spatial organization over staining artifacts and domain-specific features. To further enhance out-of-distribution robustness, we incorporate stochastic weight averaging (SWA), steering optimization toward flatter minima. Attention map analyses revealed that MorphGen primarily relies on nuclear morphology, cellular composition, and spatial cell organization within tumors or normal regions for final classification. Finally, we demonstrate resilience of the learned representations to image corruptions (such as staining artifacts) and adversarial attacks, showcasing not only OOD generalization but also addressing critical vulnerabilities in current deep learning systems for digital pathology. Code, datasets, and trained models are available at: https://github.com/hikmatkhan/MorphGen
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Visual Token Pruning for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2509.00320</link>
<guid>https://arxiv.org/abs/2509.00320</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, token pruning, visual tokens, mutual information, computational efficiency

Summary:
Token pruning in Large Multimodal Models (LMMs) is essential for reducing computational and memory costs during inference. This paper proposes a novel approach to token pruning that focuses exclusively on visual tokens. By analyzing the redundancy differences between visual and textual tokens, the proposed method aims to preserve cross-modal alignment and intra-modal diversity. The strategy involves removing visually misaligned tokens based on mutual information and pruning redundant visual tokens by maximizing expected pairwise distances in the embedding space. Experimental results show that the proposed method maintains strong performance while significantly reducing the number of tokens in models like LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a substantial improvement in inference speed by 56.7%. These findings highlight the effectiveness of the proposed visual token pruning strategy in enhancing the efficiency of Large Multimodal Models. 

<br /><br />Summary: <div>
arXiv:2509.00320v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) have achieved significant success across various tasks. These models usually encode visual inputs into dense token sequences, which are then concatenated with textual tokens and jointly processed by a language model. However, the increased token count substantially raises computational and memory costs during inference. Token pruning has emerged as a promising approach to address this issue. Existing token pruning methods often rely on costly calibration or suboptimal importance metrics, leading to redundant retained tokens. In this paper, we analyze the redundancy differences between visual and textual tokens and propose pruning exclusively on visual tokens. Based on this, we propose a visual token pruning strategy that explicitly preserves both cross-modal alignment and intra-modal informational diversity. We introduce a mutual information-based token pruning strategy that removes visual tokens semantically misaligned with textual tokens, effectively preserving the alignment between the visual and textual modalities. To further improve the representational quality of the retained tokens, we additionally prune redundant visual tokens by maximizing the expected pairwise distances in the embedding space, which is solved efficiently with a greedy algorithm. Extensive experiments demonstrate that our method maintains strong performance while reducing tokens by 88.9% on models such as LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference speed.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptoFace: End-to-End Encrypted Face Recognition</title>
<link>https://arxiv.org/abs/2509.00332</link>
<guid>https://arxiv.org/abs/2509.00332</guid>
<content:encoded><![CDATA[
<div> Face recognition, end-to-end encrypted, fully homomorphic encryption, CryptoFace, security <br />
Summary: <br />
This paper introduces CryptoFace, the first end-to-end encrypted face recognition system utilizing fully homomorphic encryption (FHE). It allows secure processing of facial data through all face recognition stages while protecting sensitive biometric information. CryptoFace employs shallow patch convolutional networks for higher-dimensional tensor support, reducing multiplicative depth and inference latency. Parallel FHE evaluation of these networks ensures low latency independent of resolution. By accelerating inference and enhancing verification accuracy on standard benchmarks, CryptoFace outperforms existing FHE neural networks for face recognition, offering robust and proven security. The code for CryptoFace is available at https://github.com/human-analysis/CryptoFace. <div>
arXiv:2509.00332v1 Announce Type: new 
Abstract: Face recognition is central to many authentication, security, and personalized applications. Yet, it suffers from significant privacy risks, particularly arising from unauthorized access to sensitive biometric data. This paper introduces CryptoFace, the first end-to-end encrypted face recognition system with fully homomorphic encryption (FHE). It enables secure processing of facial data across all stages of a face-recognition process--feature extraction, storage, and matching--without exposing raw images or features. We introduce a mixture of shallow patch convolutional networks to support higher-dimensional tensors via patch-based processing while reducing the multiplicative depth and, thus, inference latency. Parallel FHE evaluation of these networks ensures near-resolution-independent latency. On standard face recognition benchmarks, CryptoFace significantly accelerates inference and increases verification accuracy compared to the state-of-the-art FHE neural networks adapted for face recognition. CryptoFace will facilitate secure face recognition systems requiring robust and provable security. The code is available at https://github.com/human-analysis/CryptoFace.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables</title>
<link>https://arxiv.org/abs/2509.00346</link>
<guid>https://arxiv.org/abs/2509.00346</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared image fusion, visible image fusion, real-time fusion devices, LUT-Fuse, distillation

Summary: 
The paper introduces a novel approach, LUT-Fuse, for extremely fast infrared and visible image fusion through distillation to learnable lookup tables. The method utilizes a look-up table structure with low-order approximation encoding and high-level joint contextual scene encoding, suitable for multi-modal fusion. Instead of traditional quantization methods, an efficient LUT distillation strategy is proposed due to the lack of ground truth in multi-modal image fusion. By integrating the multi-modal fusion network into the MM-LUT model, the approach achieves high operational speed, requiring less time compared to current lightweight fusion algorithms. The effectiveness, reliability, and stability of the fusion approach are validated through extensive experiments. Code for the method is available on GitHub at https://github.com/zyb5/LUT-Fuse. 

<br /><br />Summary: <div>
arXiv:2509.00346v1 Announce Type: new 
Abstract: Current advanced research on infrared and visible image fusion primarily focuses on improving fusion performance, often neglecting the applicability on real-time fusion devices. In this paper, we propose a novel approach that towards extremely fast fusion via distillation to learnable lookup tables specifically designed for image fusion, termed as LUT-Fuse. Firstly, we develop a look-up table structure that utilizing low-order approximation encoding and high-level joint contextual scene encoding, which is well-suited for multi-modal fusion. Moreover, given the lack of ground truth in multi-modal image fusion, we naturally proposed the efficient LUT distillation strategy instead of traditional quantization LUT methods. By integrating the performance of the multi-modal fusion network (MM-Net) into the MM-LUT model, our method achieves significant breakthroughs in efficiency and performance. It typically requires less than one-tenth of the time compared to the current lightweight SOTA fusion algorithms, ensuring high operational speed across various scenarios, even in low-power mobile devices. Extensive experiments validate the superiority, reliability, and stability of our fusion approach. The code is available at https://github.com/zyb5/LUT-Fuse.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target-Oriented Single Domain Generalization</title>
<link>https://arxiv.org/abs/2509.00351</link>
<guid>https://arxiv.org/abs/2509.00351</guid>
<content:encoded><![CDATA[
<div> Keywords: Single Domain Generalization, Target-Oriented, Spectral Target Alignment, Visual-Language Models, Feature-space Mixup

Summary: 
Target-Oriented Single Domain Generalization (TO-SDG) addresses the challenge of model generalization under distribution shifts by leveraging textual descriptions of the target domain. The proposed Spectral TARget Alignment (STAR) module uses visual-language models to inject target semantics into source features, aligning them with the deployment domain. STAR recenters image features using a target-anchored subspace derived from the target description and employs spectral projection to retain target cues while discarding source-specific noise. Vision-language distillation aligns backbone features with VLM's semantic geometry, enhancing generalization. Feature-space Mixup ensures smooth transitions between source and target-oriented representations. Experiments across image classification and object detection benchmarks demonstrate STAR's superior performance, highlighting the importance of leveraging textual metadata for robust model deployment in unseen environments.
<br /><br />Summary: <div>
arXiv:2509.00351v1 Announce Type: new 
Abstract: Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data</title>
<link>https://arxiv.org/abs/2509.00353</link>
<guid>https://arxiv.org/abs/2509.00353</guid>
<content:encoded><![CDATA[
<div> AQFusionNet, multimodal deep learning, Air Quality Index, atmospheric imagery, pollutant concentration<br />
Summary:<br />
AQFusionNet is a deep learning framework designed for robust prediction of Air Quality Index (AQI) in regions with limited resources. It integrates ground-level atmospheric imagery and pollutant concentration data using lightweight CNN backbones such as MobileNetV2, ResNet18, and EfficientNet-B0. By combining visual and sensor features through aligned embedding spaces, AQFusionNet achieves high classification accuracy of up to 92.02% and low RMSE of 7.70 with EfficientNet-B0. It outperforms single-modality approaches by 18.5% while maintaining low computational overhead, making it suitable for deployment on edge devices. The model proves to be scalable and practical for AQI monitoring in infrastructure-constrained areas, providing accurate predictions even with limited sensor availability.<br /> <div>
arXiv:2509.00353v1 Announce Type: new 
Abstract: Air pollution monitoring in resource-constrained regions remains challenging due to sparse sensor deployment and limited infrastructure. This work introduces AQFusionNet, a multimodal deep learning framework for robust Air Quality Index (AQI) prediction. The framework integrates ground-level atmospheric imagery with pollutant concentration data using lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features are combined through semantically aligned embedding spaces, enabling accurate and efficient prediction. Experiments on more than 8,000 samples from India and Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines, achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone. The model delivers an 18.5% improvement over single-modality approaches while maintaining low computational overhead, making it suitable for deployment on edge devices. AQFusionNet provides a scalable and practical solution for AQI monitoring in infrastructure-limited environments, offering robust predictive capability even under partial sensor availability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Low-rank Network for Hyperspectral Image Denoising</title>
<link>https://arxiv.org/abs/2509.00356</link>
<guid>https://arxiv.org/abs/2509.00356</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral image denoising, low-rank representation, U-Net architecture, singular value thresholding, iterative refinement

Summary:
The paper introduces a novel iterative low-rank network (ILRNet) for hyperspectral image denoising. ILRNet combines model-driven and data-driven approaches by embedding a rank minimization module within a U-Net architecture. This module transforms feature maps into the wavelet domain and uses singular value thresholding for noise reduction. The parameter related to the thresholding algorithm is adaptively learned from the data, allowing for flexible capture of low-rankness in different scenarios. ILRNet also includes an iterative refinement process that progressively enhances denoised images while preserving details. Experimental results show that ILRNet outperforms existing methods in synthetic and real-world noise removal tasks. <div>
arXiv:2509.00356v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) denoising is a crucial preprocessing step for subsequent tasks. The clean HSI usually reside in a low-dimensional subspace, which can be captured by low-rank and sparse representation, known as the physical prior of HSI. It is generally challenging to adequately use such physical properties for effective denoising while preserving image details. This paper introduces a novel iterative low-rank network (ILRNet) to address these challenges. ILRNet integrates the strengths of model-driven and data-driven approaches by embedding a rank minimization module (RMM) within a U-Net architecture. This module transforms feature maps into the wavelet domain and applies singular value thresholding (SVT) to the low-frequency components during the forward pass, leveraging the spectral low-rankness of HSIs in the feature domain. The parameter, closely related to the hyperparameter of the singular vector thresholding algorithm, is adaptively learned from the data, allowing for flexible and effective capture of low-rankness across different scenarios. Additionally, ILRNet features an iterative refinement process that adaptively combines intermediate denoised HSIs with noisy inputs. This manner ensures progressive enhancement and superior preservation of image details. Experimental results demonstrate that ILRNet achieves state-of-the-art performance in both synthetic and real-world noise removal tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</title>
<link>https://arxiv.org/abs/2509.00357</link>
<guid>https://arxiv.org/abs/2509.00357</guid>
<content:encoded><![CDATA[
<div> Keywords: Surgical video understanding, Computer-Assisted Surgery, Large multimodal model, Spatial focus, Temporal awareness 

Summary: 
The article introduces the SurgLLM framework, a large multimodal model designed for enhancing surgical video understanding in Computer-Assisted Surgery systems. It addresses the limitations of inadequate visual content perception and lack of temporal awareness in existing studies. The framework includes Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) to improve spatial focus through instrument-centric Masked Video Reconstruction and multimodal alignment. Temporal-aware Multimodal Tuning (TM-Tuning) is also introduced to enhance temporal reasoning with interleaved multimodal embeddings. A Surgical Task Dynamic Ensemble is devised to efficiently handle different understanding tasks of surgical videos. Results from experiments on tasks like captioning and VQA demonstrate significant improvements over current approaches, showcasing SurgLLM's effectiveness in versatile surgical video understanding. The source code for SurgLLM is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2509.00357v1 Announce Type: new 
Abstract: Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at https://github.com/franciszchen/SurgLLM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Head and Neck Cancer Dataset for AI-Driven Precision Oncology</title>
<link>https://arxiv.org/abs/2509.00367</link>
<guid>https://arxiv.org/abs/2509.00367</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal dataset, PET/CT, head and neck cancer, tumor segmentation, deep learning<br />
Summary: 
A new publicly available multimodal dataset of annotated Positron Emission Tomography/Computed Tomography (PET/CT) studies for head and neck cancer research has been described. The dataset consists of 1123 FDG-PET/CT studies from patients with histologically confirmed head and neck cancer, sourced from 10 international medical centers. Manual segmentation of primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) was performed by experts, along with providing clinical metadata such as TNM staging, HPV status, demographics, and treatment information. The dataset includes anonymized NifTi files, segmentation masks, radiotherapy dose distribution, and long-term follow-up outcomes. The dataset showcases the potential applications for automated tumor segmentation, recurrence-free survival prediction, and HPV status classification through the utilization of deep learning models like UNet and SegResNet. <div>
arXiv:2509.00367v1 Announce Type: new 
Abstract: We describe a publicly available multimodal dataset of annotated Positron Emission Tomography/Computed Tomography (PET/CT) studies for head and neck cancer research. The dataset includes 1123 FDG-PET/CT studies from patients with histologically confirmed head and neck cancer, acquired from 10 international medical centers. All examinations consisted of co-registered PET/CT scans with varying acquisition protocols, reflecting real-world clinical diversity across institutions. Primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) were manually segmented by experienced radiation oncologists and radiologists following standardized guidelines and quality control measures. We provide anonymized NifTi files of all studies, along with expert-annotated segmentation masks, radiotherapy dose distribution for a subset of patients, and comprehensive clinical metadata. This metadata includes TNM staging, HPV status, demographics (age and gender), long-term follow-up outcomes, survival times, censoring indicators, and treatment information. We demonstrate how this dataset can be used for three key clinical tasks: automated tumor segmentation, recurrence-free survival prediction, and HPV status classification, providing benchmark results using state-of-the-art deep learning models, including UNet, SegResNet, and multimodal prognostic frameworks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs</title>
<link>https://arxiv.org/abs/2509.00371</link>
<guid>https://arxiv.org/abs/2509.00371</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, object hallucination, Visual-Semantic Attention Potential Field, Visual Potential Field Calibration, cross-modal representation space

Summary:
Object hallucination in Multimodal Large Language Models is a persistent challenge, with existing methods often addressing omissions without considering the causes of fabrication hallucinations. This work argues that omission hallucinations stem from mapping visual features to linguistic expressions with insufficient confidence, while fabrication hallucinations result from statistical biases in the training corpus. The proposed Visual-Semantic Attention Potential Field framework elucidates how models infer object presence or absence through visual evidence. The Visual Potential Field Calibration method successfully mitigates omission hallucinations without increasing fabrication hallucinations, addressing a critical oversight in current research and paving the way for more balanced hallucination mitigation strategies. <div>
arXiv:2509.00371v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive advances, yet object hallucination remains a persistent challenge. Existing methods, based on the flawed assumption that omission and fabrication hallucinations share a common cause, often reduce omissions only to trigger more fabrications. In this work, we overturn this view by demonstrating that omission hallucinations arise from insufficient confidence when mapping perceived visual features to linguistic expressions, whereas fabrication hallucinations result from spurious associations within the cross-modal representation space due to statistical biases in the training corpus. Building on findings from visual attention intervention experiments, we propose the Visual-Semantic Attention Potential Field, a conceptual framework that reveals how the model constructs visual evidence to infer the presence or absence of objects. Leveraging this insight, we introduce Visual Potential Field Calibration (VPFC), a plug-and-play hallucination mitigation method that effectively reduces omission hallucinations without introducing additional fabrication hallucinations. Our findings reveal a critical oversight in current object hallucination research and chart new directions for developing more robust and balanced hallucination mitigation strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</title>
<link>https://arxiv.org/abs/2509.00373</link>
<guid>https://arxiv.org/abs/2509.00373</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, adversarial attacks, activation steering, sequence-level preference optimization, model robustness

Summary: 
- Vision Language Models (VLMs) are vulnerable to adversarial attacks, leading to the development of the activation steering defense approach.
- The proposed SPO-VLM framework combines activation-level intervention and policy-level optimization to enhance model robustness.
- In Stage I, adaptive layer-specific steering vectors are computed to suppress harmful behaviors during inference.
- Stage II refines these steering vectors through a sequence-level preference optimization process, integrating toxicity assessment and visual-consistency rewards.
- SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation with deeper policy refinement, improving safety against attacks while maintaining strong performance on benign tasks. 

<br /><br />Summary: <div>
arXiv:2509.00373v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) have demonstrated impressive capabilities in integrating visual and textual information for understanding and reasoning, but remain highly vulnerable to adversarial attacks. While activation steering has emerged as a promising defence, existing approaches often rely on task-specific contrastive prompts to extract harmful directions, which exhibit suboptimal performance and can degrade visual grounding performance. To address these limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM (\textit{SPO-VLM}), a novel two-stage defense framework that combines activation-level intervention with policy-level optimization to enhance model robustness. In \textit{Stage I}, we compute adaptive layer-specific steering vectors from diverse data sources, enabling generalized suppression of harmful behaviors during inference. In \textit{Stage II}, we refine these steering vectors through a sequence-level preference optimization process. This stage integrates automated toxicity assessment, as well as visual-consistency rewards based on caption-image alignment, to achieve safe and semantically grounded text generation. The two-stage structure of SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation in Stage I with deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM enhances safety against attacks via activation steering and preference optimization, while maintaining strong performance on benign tasks without compromising visual understanding capabilities. We will release our code, model weights, and evaluation toolkit to support reproducibility and future research. \textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis</title>
<link>https://arxiv.org/abs/2509.00374</link>
<guid>https://arxiv.org/abs/2509.00374</guid>
<content:encoded><![CDATA[
<div> Keywords: parameter-efficient fine-tuning, 3D point cloud analysis, Adaptive Point-Prompt Tuning (APPT), point embeddings, self-attention mechanisms

Summary:
Parameter-efficient fine-tuning strategies have been successful in 1D textual and 2D visual analysis but remain challenging for 3D models due to scarce data. This paper introduces the Adaptive Point-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with a small number of parameters for direct processing of point clouds without mappings. Point features are converted to embeddings capturing spatial features and positional information. A permutation-invariant feature is used to optimize self-attention mechanisms across different modalities, and a prompt generator dynamically produces prompts for calibrating self-attention. These prompts enhance the foundation model with global structural information, compensating for the lack of structural context in heterogeneous data.<br /><br />Summary: <div>
arXiv:2509.00374v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning strategies for foundation models in 1D textual and 2D visual analysis have demonstrated remarkable efficacy. However, due to the scarcity of point cloud data, pre-training large 3D models remains a challenging task. While many efforts have been made to apply pre-trained visual models to 3D domains through "high-to-low" mapping, these approaches often lead to the loss of spatial geometries and lack a generalizable framework for adapting any modality to 3D. This paper, therefore, attempts to directly leverage point features to calibrate the heterogeneous foundation model of any modality for 3D point cloud analysis. Specifically, we propose the Adaptive Point-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with a modest number of parameters, enabling direct point cloud processing without heterogeneous mappings. We convert raw point clouds into point embeddings by aggregating local geometry to capture spatial features followed by linear layers to ensure seamless utilization of frozen pre-trained models. Given the inherent disorder of point clouds, in contrast to the structured nature of images and language, we employ a permutation-invariant feature to capture the relative positions of point embeddings, thereby obtaining point tokens enriched with location information to optimize self-attention mechanisms. To calibrate self-attention across source domains of any modality to 3D and reduce computational overhead, we introduce a prompt generator that shares weights with the point embedding module, dynamically producing point-prompts without adding additional parameters. These prompts are then concatenated into a frozen foundation model, providing rich global structural information and compensating for the lack of structural context in the heterogeneous data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.00378</link>
<guid>https://arxiv.org/abs/2509.00378</guid>
<content:encoded><![CDATA[
<div> CutMix, data augmentation, diffusion models, high-resolution images, NoiseCutMix
Summary:
NoiseCutMix is a novel data augmentation method that combines the concepts of CutMix and diffusion models to generate natural and high-resolution images. By partially combining the estimated noise from different classes, NoiseCutMix creates diverse augmented data without unnatural boundaries. Classification experiments show the effectiveness of NoiseCutMix compared to conventional augmentation techniques, random image generation using Stable Diffusion, and combinations of methods. The proposed method offers a unique approach to image generation that leverages the strengths of diffusion models and CutMix while addressing the limitations of existing techniques. The codes for NoiseCutMix are publicly available for implementation and further research. <br /><br />Summary: <div>
arXiv:2509.00378v1 Announce Type: new 
Abstract: In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: https://github.com/shumpei-takezaki/NoiseCutMix
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.00379</link>
<guid>https://arxiv.org/abs/2509.00379</guid>
<content:encoded><![CDATA[
<div> domain adaptation, knowledge distillation, semantic segmentation, 3D LiDAR data, autonomous driving 

Summary: 
The article introduces two crossmodal knowledge distillation methods, UDAKD and FSKD, for semantic segmentation of 3D LiDAR data in autonomous driving. By leveraging spatio-temporally synchronized data from cameras and LiDARs, pretrained 2D image models are applied to unlabeled 2D data to align 3D networks with 2D networks without the need for 3D annotations. The focus is on preserving modality-general information while disregarding modality-specific details through self-calibrated convolution on 3D point clouds. Experimental results demonstrate the superior performance of the proposed methods over existing approaches in the field. <div>
arXiv:2509.00379v1 Announce Type: new 
Abstract: Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous driving. Traditional approaches rely on extensive annotated data for point cloud analysis, incurring high costs and time investments. In contrast, realworld image datasets offer abundant availability and substantial scale. To mitigate the burden of annotating 3D LiDAR point clouds, we propose two crossmodal knowledge distillation methods: Unsupervised Domain Adaptation Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge Distillation (FSKD). Leveraging readily available spatio-temporally synchronized data from cameras and LiDARs in autonomous driving scenarios, we directly apply a pretrained 2D image model to unlabeled 2D data. Through crossmodal knowledge distillation with known 2D-3D correspondence, we actively align the output of the 3D network with the corresponding points of the 2D network, thereby obviating the necessity for 3D annotations. Our focus is on preserving modality-general information while filtering out modality-specific details during crossmodal distillation. To achieve this, we deploy self-calibrated convolution on 3D point clouds as the foundation of our domain adaptation module. Rigorous experimentation validates the effectiveness of our proposed methods, consistently surpassing the performance of state-of-the-art approaches in the field.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction</title>
<link>https://arxiv.org/abs/2509.00381</link>
<guid>https://arxiv.org/abs/2509.00381</guid>
<content:encoded><![CDATA[
<div> narrative inquiry, human experience, data analysis, member checking, image generation<br />
Summary:<br />
A new paradigm named NAME is proposed for narrative inquiry, aiming to simplify the process of transforming research documents into coherent story images. The proposed approach includes an actor location and shape module for image generation and robust evaluation metrics to measure perceptual quality and narrative consistency. The method significantly outperforms baseline models, reducing the FID score with minimal data usage. For example, with only 0.96% of the data, the FID score decreased from 195 to 152. The model also achieves a high score on a new evaluation metric introduced. This innovative approach has the potential to revolutionize narrative analysis by improving efficiency and participant satisfaction in the narrative-making process. <div>
arXiv:2509.00381v1 Announce Type: new 
Abstract: Narrative inquiry has been one of the prominent application domains for the analysis of human experience, aiming to know more about the complexity of human society. However, researchers are often required to transform various forms of data into coherent hand-drafted narratives in storied form throughout narrative analysis, which brings an immense burden of data analysis. Participants, too, are expected to engage in member checking and presentation of these narrative products, which involves reviewing and responding to large volumes of documents. Given the dual burden and the need for more efficient and participant-friendly approaches to narrative making and representation, we made a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt to push the field of narrative inquiry. Name is able to transfer research documents into coherent story images, alleviating the cognitive burden of interpreting extensive text-based materials during member checking for both researchers and participants. (ii) We develop an actor location and shape module to facilitate plausible image generation. (iii) We have designed a set of robust evaluation metrics comprising three key dimensions to objectively measure the perceptual quality and narrative consistency of generated characters. Our approach consistently demonstrates state-of-the-art performance across different data partitioning schemes. Remarkably, while the baseline relies on the full 100% of the available data, our method requires only 0.96% yet still reduces the FID score from 195 to 152. Under identical data volumes, our method delivers substantial improvements: for the 70:30 split, the FID score decreases from 175 to 152, and for the 95:5 split, it is nearly halved from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the newly introduced metric, surpassing the baseline score of 2.66.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization</title>
<link>https://arxiv.org/abs/2509.00385</link>
<guid>https://arxiv.org/abs/2509.00385</guid>
<content:encoded><![CDATA[
<div> Top-down Attention Guidance, Egocentric Augmentation, Visual Query Localization, Object Recognition, Egocentric Videos<br />
<br />Summary: <br /> 
The paper introduces HERO-VQL, a method for egocentric visual query localization. It addresses challenges such as viewpoint changes and occlusions in egocentric videos. HERO-VQL uses Top-down Attention Guidance (TAG) for refined attention and Egocentric Augmentation (EgoACT) for diverse query training. TAG leverages high-level context and principal component score maps for localization. EgoACT enhances query diversity by replacing queries with corresponding objects and simulating viewpoint changes. A Consistency Training (CT) loss enforces stable localization across different scenarios. Experiment results on VQ2D dataset show HERO-VQL outperforms baselines in handling egocentric challenges efficiently. <div>
arXiv:2509.00385v1 Announce Type: new 
Abstract: In this work, we tackle the egocentric visual query localization (VQL), where a model should localize the query object in a long-form egocentric video. Frequent and abrupt viewpoint changes in egocentric videos cause significant object appearance variations and partial occlusions, making it difficult for existing methods to achieve accurate localization. To tackle these challenges, we introduce Hierarchical, Egocentric and RObust Visual Query Localization (HERO-VQL), a novel method inspired by human cognitive process in object recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance refines the attention mechanism by leveraging the class token for high-level context and principal component score maps for fine-grained localization. To enhance learning in diverse and challenging matching scenarios, EgoAug enhances query diversity by replacing the query with a randomly selected corresponding object from groundtruth annotations and simulates extreme viewpoint changes by reordering video frames. Additionally, CT loss enforces stable object localization across different augmentation scenarios. Extensive experiments on VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges, significantly outperforming baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction</title>
<link>https://arxiv.org/abs/2509.00395</link>
<guid>https://arxiv.org/abs/2509.00395</guid>
<content:encoded><![CDATA[
<div> constraint, diffusion model, PET reconstruction, low-dose, ultra-low dose

Summary:
The study introduces a Double-Constraint Diffusion Model (DCDM) for ultra-low-dose PET reconstruction. DCDM freezes the weights of a pre-trained diffusion model and adds a double-constraint controller, reducing the number of trainable parameters and enhancing reconstruction flexibility. The Nuclear Transformer Constraint (NTC) and the Encoding Nexus Constraint (ENC) work together to refine the pre-trained model, extract information from low-dose images, and control the reconstruction process. DCDM outperforms existing methods on known dose reduction factors and generalizes well to unknown scenarios, even at ultra-low dose levels. Experimental results on public and clinical datasets demonstrate the effectiveness of DCDM for reducing patient radiation exposure and improving imaging quality in PET reconstruction. <div>
arXiv:2509.00395v1 Announce Type: new 
Abstract: Ultra-low-dose positron emission tomography (PET) reconstruction holds significant potential for reducing patient radiation exposure and shortening examination times. However, it may also lead to increased noise and reduced imaging detail, which could decrease the image quality. In this study, we present a Double-Constraint Diffusion Model (DCDM), which freezes the weights of a pre-trained diffusion model and injects a trainable double-constraint controller into the encoding architecture, greatly reducing the number of trainable parameters for ultra-low-dose PET reconstruction. Unlike full fine-tuning models, DCDM can adapt to different dose levels without retraining all model parameters, thereby improving reconstruction flexibility. Specifically, the two constraint modules, named the Nuclear Transformer Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the pre-trained diffusion model. The NTC leverages the nuclear norm as an approximation for matrix rank minimization, integrates the low-rank property into the Transformer architecture, and enables efficient information extraction from low-dose images and conversion into compressed feature representations in the latent space. Subsequently, the ENC utilizes these compressed feature representations to encode and control the pre-trained diffusion model, ultimately obtaining reconstructed PET images in the pixel space. In clinical reconstruction, the compressed feature representations from NTC help select the most suitable ENC for efficient unknown low-dose PET reconstruction. Experiments conducted on the UDPET public dataset and the Clinical dataset demonstrated that DCDM outperforms state-of-the-art methods on known dose reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving valuable even at ultra-low dose levels, such as 1% of the full dose.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAOVI: Distortion-Aware Omnidirectional Video Inpainting</title>
<link>https://arxiv.org/abs/2509.00396</link>
<guid>https://arxiv.org/abs/2509.00396</guid>
<content:encoded><![CDATA[
<div> omnidirectional videos, video inpainting, deep learning model, distortion-aware, geodesic distance <br />
Summary: <br />
The paper introduces a novel deep learning model, DAOVI, for omnidirectional video inpainting to remove unwanted objects while maintaining spatial and temporal consistency. DAOVI incorporates a module to assess temporal motion information considering geodesic distance and a depth-aware feature propagation module to address geometric distortion in omnidirectional videos. The proposed method surpasses existing techniques in both quantitative and qualitative evaluations. <div>
arXiv:2509.00396v1 Announce Type: new 
Abstract: Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</title>
<link>https://arxiv.org/abs/2509.00403</link>
<guid>https://arxiv.org/abs/2509.00403</guid>
<content:encoded><![CDATA[
<div> Keywords: human avatars, monocular videos, video generative model, fine-grained details, motion reproduction<br />
Summary:
The article introduces a novel framework for reconstructing human avatars from monocular videos. Existing techniques face challenges in capturing dynamic details and generating plausible details from novel viewpoints. The proposed approach leverages the Human4DiT video generative model to generate human motions from alternative perspectives, enriching details in unseen regions and mitigating artifacts. Two strategies are introduced to enhance video generation: injecting physical identity for consistent motion reproduction and using a patch-based denoising algorithm for higher-resolution outputs with finer details. Experimental results demonstrate that the method surpasses current state-of-the-art approaches and validates the effectiveness of the strategies. <div>
arXiv:2509.00403v1 Announce Type: new 
Abstract: We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression</title>
<link>https://arxiv.org/abs/2509.00419</link>
<guid>https://arxiv.org/abs/2509.00419</guid>
<content:encoded><![CDATA[
<div> Keywords: LightVLM, Vision-Language Models, inference acceleration, pyramid token merging, KV Cache compression

Summary:
LightVLM introduces a method to accelerate Vision-Language Models (VLMs) during both encoding and decoding stages without the need for additional training. By utilizing pyramid token merging during encoding, it reduces the number of tokens from different layers to improve efficiency. Moreover, it enhances decoding speed by implementing KV Cache compression, removing unnecessary caches and increasing network throughput. Experimental results demonstrate that LightVLM maintains high performance with significantly fewer image tokens, improving network throughput by 2.02 times and reducing prefilling time by 3.65 times. It enables larger models to infer faster than smaller ones and significantly reduces inference time for generating long text sequences by 3.21 times, outperforming existing methods in real-world applications.<br /><br />Summary: <div>
arXiv:2509.00419v1 Announce Type: new 
Abstract: In this paper, we introduce LightVLM, a simple but effective method that can be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly accelerate the inference process in a training-free manner. We divide the inference procedure of VLMs into two stages, i.e., encoding and decoding, and propose to simultaneously accelerate VLMs in both stages to largely improve model efficiency. During encoding, we propose pyramid token merging to reduce tokens of different LLM layers in a hierarchical manner by finally only keeping a few dominant tokens to achieve high efficiency. During decoding, aimed at reducing the high latency of outputting long sequences, we propose KV Cache compression to remove unnecessary caches to increase the network throughput. Experimental results show that LightVLM successfully retains 100% performance when only preserving 35% image tokens, and maintains around 98% performance when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to infer faster than significantly smaller models (e.g., InternVL2.5 8B), hopefully facilitating the real-world deployment. When generating long text sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by 3.21$\times$, largely outperforming existing methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</title>
<link>https://arxiv.org/abs/2509.00428</link>
<guid>https://arxiv.org/abs/2509.00428</guid>
<content:encoded><![CDATA[
<div> latent modeling, semantic control, fine-grained controllability, diffusion transformers, face generation 

Summary:
Face-MoGLE introduces a novel framework for controllable face generation by utilizing diffusion transformers. It incorporates semantic-decoupled latent modeling through mask-conditioned space factorization for precise attribute manipulation. The framework employs a mixture of global and local experts to capture holistic structure and region-level semantics, enhancing fine-grained controllability. A dynamic gating network produces time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE demonstrates effectiveness in multimodal and monomodal face generation settings, showcasing robust zero-shot generalization capability. The project page at https://github.com/XavierJiezou/Face-MoGLE provides access to the framework for potential applications in generative modeling and security applications. <br /><br />Summary: <div>
arXiv:2509.00428v1 Announce Type: new 
Abstract: Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2509.00442</link>
<guid>https://arxiv.org/abs/2509.00442</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiple instance learning, whole slide images, attention-based methods, transformer models, state space models<br />
Summary:<br />
Multiple instance learning (MIL) is widely used in computational pathology for feature extraction from whole slide images (WSIs). Traditional attention-based MIL methods excel at identifying key patches but may overlook contextual relationships. Transformer models offer interaction modeling but at a high computational cost and risk of overfitting. State space models (SSMs) provide linear complexity but disrupt histological meaning when shuffling patch order. In this study, a novel approach called SemaMIL is introduced, combining Semantic Reordering (SR) for organizing patches based on semantic similarity and a Semantic-guided Retrieval State Space Module (SRSM) for enhancing global modeling. SemaMIL outperforms existing methods in accuracy, requiring fewer floating-point operations and parameters. Tested on four WSI subtype datasets, SemaMIL demonstrates state-of-the-art performance in computational pathology applications. <br /> <div>
arXiv:2509.00442v1 Announce Type: new 
Abstract: Multiple instance learning (MIL) has become the leading approach for extracting discriminative features from whole slide images (WSIs) in computational pathology. Attention-based MIL methods can identify key patches but tend to overlook contextual relationships. Transformer models are able to model interactions but require quadratic computational cost and are prone to overfitting. State space models (SSMs) offer linear complexity, yet shuffling patch order disrupts histological meaning and reduces interpretability. In this work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an adaptive method that clusters and arranges semantically similar patches in sequence through a reversible permutation, with a Semantic-guided Retrieval State Space Module (SRSM) that chooses a representative subset of queries to adjust state space parameters for improved global modeling. Evaluation on four WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves state-of-the-art accuracy with fewer FLOPs and parameters.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stage-wise Adaptive Label Distribution for Facial Age Estimation</title>
<link>https://arxiv.org/abs/2509.00450</link>
<guid>https://arxiv.org/abs/2509.00450</guid>
<content:encoded><![CDATA[
<div> Keywords: label ambiguity, age estimation, adaptive variance modeling, weighted loss function, structured nature

Summary:
The paper introduces the Stage-wise Adaptive Label Distribution Learning (SA-LDL) algorithm to address label ambiguity in age estimation tasks. SA-LDL recognizes the varying degrees of ambiguity present across different age stages by leveraging stage-wise patterns observed in embedding similarities. By integrating stage-wise adaptive variance modeling and a weighted loss function, SA-LDL effectively captures the complex nature of label ambiguity, improving the accuracy and robustness of age estimation. Experimental results on the MORPH-II and FG-NET datasets show that SA-LDL achieves competitive performance with Mean Absolute Error (MAE) values of 1.74 and 2.15, respectively. The proposed method offers a novel approach to handling label ambiguity in age estimation tasks, providing a new perspective on how to improve the performance of age estimation algorithms. 

<br /><br />Summary: <div>
arXiv:2509.00450v1 Announce Type: new 
Abstract: Label ambiguity poses a significant challenge in age estimation tasks. Most existing methods address this issue by modeling correlations between adjacent age groups through label distribution learning. However, they often overlook the varying degrees of ambiguity present across different age stages. In this paper, we propose a Stage-wise Adaptive Label Distribution Learning (SA-LDL) algorithm, which leverages the observation -- revealed through our analysis of embedding similarities between an anchor and all other ages -- that label ambiguity exhibits clear stage-wise patterns. By jointly employing stage-wise adaptive variance modeling and weighted loss function, SA-LDL effectively captures the complex and structured nature of label ambiguity, leading to more accurate and robust age estimation. Extensive experiments demonstrate that SA-LDL achieves competitive performance, with MAE of 1.74 and 2.15 on the MORPH-II and FG-NET datasets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encoder-Only Image Registration</title>
<link>https://arxiv.org/abs/2509.00451</link>
<guid>https://arxiv.org/abs/2509.00451</guid>
<content:encoded><![CDATA[
<div> Convolutional Neural Networks, Image Registration, Deformable Registration, Optical Flow, Efficiency<br />
Summary:<br /> 
- The study analyzes the impact of Convolutional Neural Networks (ConvNets) on deformable image registration, focusing on the Horn-Schunck optical flow equation.
- ConvNets are found to play a crucial role in linearizing local intensities and harmonizing global contrast variations, improving registration performance.
- The Encoder-Only Image Registration (EOIR) framework is proposed, separating feature learning from flow estimation to enhance accuracy-efficiency trade-offs.
- Results across various datasets demonstrate EOIR's effectiveness in achieving superior accuracy-efficiency and accuracy-smoothness trade-offs.
- EOIR provides better efficiency and smoothness compared to existing techniques, making it a promising approach for deformable image registration tasks. <br /> 
Summary: <div>
arXiv:2509.00451v1 Announce Type: new 
Abstract: Learning-based techniques have significantly improved the accuracy and speed of deformable image registration. However, challenges such as reducing computational complexity and handling large deformations persist. To address these challenges, we analyze how convolutional neural networks (ConvNets) influence registration performance using the Horn-Schunck optical flow equation. Supported by prior studies and our empirical experiments, we observe that ConvNets play two key roles in registration: linearizing local intensities and harmonizing global contrast variations. Based on these insights, we propose the Encoder-Only Image Registration (EOIR) framework, designed to achieve a better accuracy-efficiency trade-off. EOIR separates feature learning from flow estimation, employing only a 3-layer ConvNet for feature extraction and a set of 3-layer flow estimators to construct a Laplacian feature pyramid, progressively composing diffeomorphic deformations under a large-deformation model. Results on five datasets across different modalities and anatomical regions demonstrate EOIR's effectiveness, achieving superior accuracy-efficiency and accuracy-smoothness trade-offs. With comparable accuracy, EOIR provides better efficiency and smoothness, and vice versa. The source code of EOIR will be publicly available on https://github.com/XiangChen1994/EOIR.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Decision-Making Capabilities of LLM Agents: An Experimental Study on Jump-Jump Game</title>
<link>https://arxiv.org/abs/2509.00483</link>
<guid>https://arxiv.org/abs/2509.00483</guid>
<content:encoded><![CDATA[
<div> Keywords: Jump-Jump game, decision-making, cognitive aspects, spatial reasoning, strategic planning <br />
Summary: The Jump-Jump game serves as a valuable testing ground for evaluating decision-making abilities of Large Language Models (LLM). By requiring players to judiciously adjust jumping force based on their position and the distance to the target platform, the game engages various cognitive skills such as spatial reasoning, physical modeling, and strategic planning. Players control a character represented by a red circle, navigating it across platforms by calculating the optimal force for each jump to achieve the highest possible score. This simple yet challenging casual game offers insights into the LLM's capability to tackle tasks that involve complex decision-making processes and precise spatial calculations. <div>
arXiv:2509.00483v1 Announce Type: new 
Abstract: The Jump-Jump game, as a simple yet challenging casual game, provides an ideal testing environment for studying LLM decision-making capabilities. The game requires players to precisely control jumping force based on current position and target platform distance, involving multiple cognitive aspects including spatial reasoning, physical modeling, and strategic planning. It illustrates the basic gameplay mechanics of the Jump-Jump game, where the player character (red circle) must jump across platforms with appropriate force to maximize score.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</title>
<link>https://arxiv.org/abs/2509.00484</link>
<guid>https://arxiv.org/abs/2509.00484</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal reward models, video understanding, evaluation, AI-assisted data pipeline

Summary:
Multimodal reward models (MRMs) are essential for training and evaluating Large Vision Language Models (LVLMs). However, existing benchmarks for evaluating MRMs in the video domain lack diversity and comprehensive evaluation dimensions. To address this, VideoRewardBench is introduced as a comprehensive benchmark covering perception, knowledge, reasoning, and safety aspects of video understanding. A high-quality preference dataset is curated, including annotated samples with video-text prompts and responses. Evaluation across 28 MRMs reveals insights such as the limitations of RL training, benefits of inference-time scaling, and varying effects of input video frame count on MRMs. The top-performing models achieve around 57% accuracy, highlighting the challenges in developing effective MRMs for video understanding. VideoRewardBench offers a valuable resource for advancing the evaluation and development of MRMs in the video domain. 

<br /><br />Summary: <div>
arXiv:2509.00484v1 Announce Type: new 
Abstract: Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Focused Video Group Activities Hashing</title>
<link>https://arxiv.org/abs/2509.00490</link>
<guid>https://arxiv.org/abs/2509.00490</guid>
<content:encoded><![CDATA[
<div> activity granularity, video retrieval, spatiotemporal interleaved video hashing, group interactions, multi-focused spatiotemporal video hashing

Summary:<br />
- The paper introduces a new technique called STVH for retrieving group activities in complex scenarios by modeling object dynamics and group interactions.<br />
- STVH captures spatiotemporal evolution on group visual features and positional features to improve video retrieval accuracy.<br />
- To handle the varying requirements of activity and visual features in video retrieval, M-STVH is proposed as an enhanced version of STVH.<br />
- M-STVH incorporates hierarchical feature integration through multi-focused representation learning to focus on activity semantics features and object visual features simultaneously.<br />
- Comparative experiments on public datasets show that both STVH and M-STVH achieve excellent results, demonstrating the effectiveness of the proposed techniques.<br /> 
Summary: <div>
arXiv:2509.00490v1 Announce Type: new 
Abstract: With the explosive growth of video data in various complex scenarios, quickly retrieving group activities has become an urgent problem. However, many tasks can only retrieve videos focusing on an entire video, not the activity granularity. To solve this problem, we propose a new STVH (spatiotemporal interleaved video hashing) technique for the first time. Through a unified framework, the STVH simultaneously models individual object dynamics and group interactions, capturing the spatiotemporal evolution on both group visual features and positional features. Moreover, in real-life video retrieval scenarios, it may sometimes require activity features, while at other times, it may require visual features of objects. We then further propose a novel M-STVH (multi-focused spatiotemporal video hashing) as an enhanced version to handle this difficult task. The advanced method incorporates hierarchical feature integration through multi-focused representation learning, allowing the model to jointly focus on activity semantics features and object visual features. We conducted comparative experiments on publicly available datasets, and both STVH and M-STVH can achieve excellent results.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device Adaptation</title>
<link>https://arxiv.org/abs/2509.00508</link>
<guid>https://arxiv.org/abs/2509.00508</guid>
<content:encoded><![CDATA[
<div> Style gap, Ultrasound images, Image-to-image translation, Token-driven, Downstream tasks

Summary:
TRUST proposes a token-driven dual-stream framework for unpaired image-to-image translation (UI2I) to address the style gap in ultrasound images acquired from different devices. The framework aims to transfer images from a source domain to a target domain while preserving source content and transferring the common style of the target domain. It includes a Token-dRiven (TR) module that selects suitable target tokens for each source token and identifies optimal target tokens for downstream models using a behavior mirror loss. Auxiliary prompts are injected into the source encoder to match content representation with downstream behavior. Experimental results on ultrasound datasets show that TRUST outperforms existing UI2I methods in visual quality and downstream task performance. <div>
arXiv:2509.00508v1 Announce Type: new 
Abstract: Ultrasound images acquired from different devices exhibit diverse styles, resulting in decreased performance of downstream tasks. To mitigate the style gap, unpaired image-to-image (UI2I) translation methods aim to transfer images from a source domain, corresponding to new device acquisitions, to a target domain where a frozen task model has been trained for downstream applications. However, existing UI2I methods have not explicitly considered filtering the most relevant style features, which may result in translated images misaligned with the needs of downstream tasks. In this work, we propose TRUST, a token-driven dual-stream framework that preserves source content while transferring the common style of the target domain, ensuring that content and style remain unblended. Given multiple styles in the target domain, we introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a data view--selecting "suitable" target tokens corresponding to each source token, and (2) a model view--identifying ``optimal" target tokens for the downstream model, guided by a behavior mirror loss. Additionally, we inject auxiliary prompts into the source encoder to match content representation with downstream behavior. Experimental results on ultrasound datasets demonstrate that TRUST outperforms existing UI2I methods in both visual quality and downstream task performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.00509</link>
<guid>https://arxiv.org/abs/2509.00509</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence as a Service, Black-Box Distillation, ATtention-Guided sCaler, DINOv2, API model

Summary:
The article discusses the challenge of training local models using black-box models that do not expose their weights, training data, or logits, a common constraint with AIaaS. A new setting called Black-Box Distillation (B2D) is introduced to enable local model adaptation under realistic constraints. The ATtention-Guided sCaler (ATGC) method is proposed to address the challenge of open-vocabulary models having sensitivity to input resolution, known as the "curse of resolution." ATGC leverages DINOv2 attention maps to select optimal scales for black-box model inference and uses entropy to identify informative scales for pseudo-labelling, resulting in effective distillation. Experiments show significant improvements under black-box supervision across multiple datasets while only requiring one-hot API predictions. The code for ATGC is available on GitHub at https://github.com/yasserben/ATGC. <div>
arXiv:2509.00509v1 Announce Type: new 
Abstract: The rise of Artificial Intelligence as a Service (AIaaS) democratizes access to pre-trained models via Application Programming Interfaces (APIs), but also raises a fundamental question: how can local models be effectively trained using black-box models that do not expose their weights, training data, or logits, a constraint in which current domain adaptation paradigms are impractical ? To address this challenge, we introduce the Black-Box Distillation (B2D) setting, which enables local model adaptation under realistic constraints: (1) the API model is open-vocabulary and trained on large-scale general-purpose data, and (2) access is limited to one-hot predictions only. We identify that open-vocabulary models exhibit significant sensitivity to input resolution, with different object classes being segmented optimally at different scales, a limitation termed the "curse of resolution". Our method, ATtention-Guided sCaler (ATGC), addresses this challenge by leveraging DINOv2 attention maps to dynamically select optimal scales for black-box model inference. ATGC scores the attention maps with entropy to identify informative scales for pseudo-labelling, enabling effective distillation. Experiments demonstrate substantial improvements under black-box supervision across multiple datasets while requiring only one-hot API predictions. Our code is available at https://github.com/yasserben/ATGC.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement</title>
<link>https://arxiv.org/abs/2509.00527</link>
<guid>https://arxiv.org/abs/2509.00527</guid>
<content:encoded><![CDATA[
<div> semantic segmentation, class-incremental learning, catastrophic semantic entanglement, prototype-feature entanglement, language-guided prototypical disentanglement

Summary:
In the study, the authors address the challenge of class-incremental semantic segmentation, focusing on catastrophic semantic entanglement. They propose a Language-inspired Bootstrapped Disentanglement framework (LBD) to tackle Prototype-Feature Entanglement and Background-Increment Entanglement. The framework utilizes pre-trained visual-language models to guide the autonomous disentanglement of features through Language-guided Prototypical Disentanglement and Manifold Mutual Background Disentanglement. By incorporating soft prompt tuning and encoder adaptation modifications, the model bridges the capability gap between dense and sparse tasks, achieving state-of-the-art performance on Pascal VOC and ADE20k datasets, especially in multi-step scenarios. The approach shows promising results in class-incremental semantic segmentation tasks. 

<br /><br />Summary: <div>
arXiv:2509.00527v1 Announce Type: new 
Abstract: Class-Incremental Semantic Segmentation (CISS) requires continuous learning of newly introduced classes while retaining knowledge of past classes. By abstracting mainstream methods into two stages (visual feature extraction and prototype-feature matching), we identify a more fundamental challenge termed catastrophic semantic entanglement. This phenomenon involves Prototype-Feature Entanglement caused by semantic misalignment during the incremental process, and Background-Increment Entanglement due to dynamic data evolution. Existing techniques, which rely on visual feature learning without sufficient cues to distinguish targets, introduce significant noise and errors. To address these issues, we introduce a Language-inspired Bootstrapped Disentanglement framework (LBD). We leverage the prior class semantics of pre-trained visual-language models (e.g., CLIP) to guide the model in autonomously disentangling features through Language-guided Prototypical Disentanglement and Manifold Mutual Background Disentanglement. The former guides the disentangling of new prototypes by treating hand-crafted text features as topological templates, while the latter employs multiple learnable prototypes and mask-pooling-based supervision for background-incremental class disentanglement. By incorporating soft prompt tuning and encoder adaptation modifications, we further bridge the capability gap of CLIP between dense and sparse tasks, achieving state-of-the-art performance on both Pascal VOC and ADE20k, particularly in multi-step scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging</title>
<link>https://arxiv.org/abs/2509.00549</link>
<guid>https://arxiv.org/abs/2509.00549</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, BrainFM, modality-agnostic, multi-task, deep learning

Summary:
BrainFM is a new modality-agnostic, multi-task vision foundation model for human brain imaging. It is designed to address the challenges of generalization in uncalibrated modalities, such as magnetic resonance imaging (MRI). The model utilizes a "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy to improve resilience to variations in image appearance. BrainFM can be applied to five fundamental brain imaging tasks, including image synthesis, anatomy segmentation, scalp-to-cortical distance estimation, bias field estimation, and registration. Evaluation on eleven public datasets shows the model's robustness and effectiveness across all tasks and input modalities. The code for BrainFM is available on GitHub for further exploration and implementation. <div>
arXiv:2509.00549v1 Announce Type: new 
Abstract: Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at https://github.com/jhuldr/BrainFM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection</title>
<link>https://arxiv.org/abs/2509.00578</link>
<guid>https://arxiv.org/abs/2509.00578</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, fine-grained domains, DiffusionDet, Context-Aware Fusion, CarDD benchmark

Summary:
Object detection in challenging visual domains, like vehicle damage assessment, is difficult even for human experts. While DiffusionDet has improved this task using conditional denoising diffusion, it still faces limitations in context-dependent scenarios. To overcome this, Context-Aware Fusion (CAF) is introduced, incorporating global scene context with local proposal features using cross-attention mechanisms. By creating a separate encoder to capture environmental information, each object proposal can now access scene-level understanding. This framework enhances generative detection by allowing object proposals to consider comprehensive environmental information. Experimental results on the CarDD benchmark show that this approach outperforms existing models, setting new benchmarks for context-aware object detection in fine-grained domains.<br /><br />Summary: <div>
arXiv:2509.00578v1 Announce Type: new 
Abstract: Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation</title>
<link>https://arxiv.org/abs/2509.00598</link>
<guid>https://arxiv.org/abs/2509.00598</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, Remote Sensing Segmentation, Global-Local Decoupling, Context-Aware Cropping, Referring Expression Segmentation

Summary:
The article introduces a new framework, DGL-RSIS, for remote sensing segmentation that leverages vision language models. It addresses challenges in transferring VLMs to RS datasets by decoupling visual and textual inputs. The framework includes a Global-Local Decoupling module that separates text inputs into local class nouns and global modifiers, and image inputs into class-agnostic mask proposals. Visual and textual features are aligned at the local scale through context-aware cropping to enrich text inputs with RS-specific knowledge. The framework supports open-vocabulary semantic segmentation by matching enhanced text features with visual features guided by masks. At the global scale, a Cross-Scale Grad-CAM module refines Grad-CAM maps using contextual information from global modifiers. A mask selection module integrates pixel-level Grad-CAM activations into mask-level segmentation output for accurate alignment across global and local dimensions, supporting referring expression segmentation. <div>
arXiv:2509.00598v1 Announce Type: new 
Abstract: The emergence of vision language models (VLMs) has bridged vision and language, enabling joint multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the limited category diversity in RS datasets and the domain gap between natural and RS imagery. Here, we propose a training-free framework, DGL-RSIS, that decouples visual and textual inputs, performing visual-language alignment at both the local semantic and global contextual levels through tailored strategies. Specifically, we first introduce a global-local decoupling (GLD) module, where text inputs are divided into local class nouns and global modifiers using natural language processing (NLP) techniques; image inputs are partitioned into a set of class-agnostic mask proposals via unsupervised mask proposal networks. Second, visual and textual features are aligned at local scale, through a novel context-aware cropping strategy for extracting image patches with proper boundaries and introducing RS-specific knowledge to enrich the text inputs. By matching the enhanced text features with mask-guided visual features, we enable the mask classification, supporting open-vocabulary semantic segmentation (OVSS). Third, at the global scale, we propose a Cross-Scale Grad-CAM module to refine Grad-CAM maps using contextual information from global modifiers. A subsequent mask selection module integrates pixel-level Grad-CAM activations into the mask-level segmentation output, such that accurate and interpretable alignment can be realized across global and local dimensions for referring expression segmentation (RES).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Methane Detection Onboard Satellites</title>
<link>https://arxiv.org/abs/2509.00626</link>
<guid>https://arxiv.org/abs/2509.00626</guid>
<content:encoded><![CDATA[
arXiv:2509.00626v1 Announce Type: new 
Abstract: Methane is a potent greenhouse gas and a major driver of climate change, making its timely detection critical for effective mitigation. Machine learning (ML) deployed onboard satellites can enable rapid detection while reducing downlink costs, supporting faster response systems. Conventional methane detection methods often rely on image processing techniques, such as orthorectification to correct geometric distortions and matched filters to enhance plume signals. We introduce a novel approach that bypasses these preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We find that ML models trained on this dataset achieve performance comparable to those trained on orthorectified data. Moreover, we also train models on an orthorectified dataset, showing that they can outperform the matched filter baseline (mag1c). We release model checkpoints and two ML-ready datasets comprising orthorectified and unorthorectified hyperspectral images from the Earth Surface Mineral Dust Source Investigation (EMIT) sensor at https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at https://github.com/spaceml-org/plume-hunter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2509.00649</link>
<guid>https://arxiv.org/abs/2509.00649</guid>
<content:encoded><![CDATA[
arXiv:2509.00649v1 Announce Type: new 
Abstract: While significant progress has been made in single-view 3D human pose estimation, multi-view 3D human pose estimation remains challenging, particularly in terms of generalizing to new camera configurations. Existing attention-based transformers often struggle to accurately model the spatial arrangement of keypoints, especially in occluded scenarios. Additionally, they tend to overfit specific camera arrangements and visual scenes from training data, resulting in substantial performance drops in new settings. In this study, we introduce a novel Multi-View State Space Modeling framework, named MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the joint spatial sequence at two distinct levels: the feature level from multi-view images and the person keypoint level. We propose a Projective State Space (PSS) block to learn a generalized representation of joint spatial arrangements using state space modeling. Moreover, we modify Mamba's traditional scanning into an effective Grid Token-guided Bidirectional Scanning (GTBS), which is integral to the PSS block. Multiple experiments demonstrate that MV-SSM achieves strong generalization, outperforming state-of-the-art methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP (+38%) on Campus A1 in cross-dataset evaluations. Project Website: https://aviralchharia.github.io/MV-SSM
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains</title>
<link>https://arxiv.org/abs/2509.00658</link>
<guid>https://arxiv.org/abs/2509.00658</guid>
<content:encoded><![CDATA[
arXiv:2509.00658v1 Announce Type: new 
Abstract: Ensuring fairness and robustness in machine learning models remains a challenge, particularly under domain shifts. We present Face4FairShifts, a large-scale facial image benchmark designed to systematically evaluate fairness-aware learning and domain generalization. The dataset includes 100,000 images across four visually distinct domains with 39 annotations within 14 attributes covering demographic and facial features. Through extensive experiments, we analyze model performance under distribution shifts and identify significant gaps. Our findings emphasize the limitations of existing related datasets and the need for more effective fairness-aware domain adaptation techniques. Face4FairShifts provides a comprehensive testbed for advancing equitable and reliable AI systems. The dataset is available online at https://meviuslab.github.io/Face4FairShifts/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Identification and Description of Jewelry Through Computer Vision and Neural Networks for Translators and Interpreters</title>
<link>https://arxiv.org/abs/2509.00661</link>
<guid>https://arxiv.org/abs/2509.00661</guid>
<content:encoded><![CDATA[
arXiv:2509.00661v1 Announce Type: new 
Abstract: Identifying jewelry pieces presents a significant challenge due to the wide range of styles and designs. Currently, precise descriptions are typically limited to industry experts. However, translators and interpreters often require a comprehensive understanding of these items. In this study, we introduce an innovative approach to automatically identify and describe jewelry using neural networks. This method enables translators and interpreters to quickly access accurate information, aiding in resolving queries and gaining essential knowledge about jewelry. Our model operates at three distinct levels of description, employing computer vision techniques and image captioning to emulate expert analysis of accessories. The key innovation involves generating natural language descriptions of jewelry across three hierarchical levels, capturing nuanced details of each piece. Different image captioning architectures are utilized to detect jewels in images and generate descriptions with varying levels of detail. To demonstrate the effectiveness of our approach in recognizing diverse types of jewelry, we assembled a comprehensive database of accessory images. The evaluation process involved comparing various image captioning architectures, focusing particularly on the encoder decoder model, crucial for generating descriptive captions. After thorough evaluation, our final model achieved a captioning accuracy exceeding 90 per cent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model</title>
<link>https://arxiv.org/abs/2509.00664</link>
<guid>https://arxiv.org/abs/2509.00664</guid>
<content:encoded><![CDATA[
arXiv:2509.00664v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation</title>
<link>https://arxiv.org/abs/2509.00665</link>
<guid>https://arxiv.org/abs/2509.00665</guid>
<content:encoded><![CDATA[
arXiv:2509.00665v1 Announce Type: new 
Abstract: Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog, snow, and nighttime) remains highly challenging due to the lack of reliable ground truth and the difficulty of learning from unlabeled real-world data. Existing methods often rely on synthetic adverse data with pseudo-labels, which suffer from domain gaps, or employ self-supervised learning, which violates photometric assumptions in adverse scenarios. In this work, we propose to achieve weather--generalized depth estimation by Parameter--Efficient Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small amount of high--visibility (normal) data. While PEFT has shown strong performance in semantic tasks such as segmentation, it remains underexplored for geometry--centric tasks like depth estimation -- especially in terms of balancing effective adaptation with the preservation of pretrained knowledge. To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy, which structurally decomposes the pretrained weights of VFMs based on two kinds of effective ranks (entropy--rank and stable--rank). In the tuning phase, we adaptively select the proper rank number as well as the task--aware singular directions for initialization, based on the entropy--rank and full--tuned weight; while in the maintaining stage, we enforce a principal direction regularization based on the stable--rank. This design guarantees flexible task adaptation while preserving the strong generalization capability of the pretrained VFM. Extensive experiments on four real--world benchmarks across diverse weather conditions demonstrate that STM not only outperforms existing PEFT methods and full fine--tuning but also surpasses methods trained with adverse synthetic data, and even the depth foundation model
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model</title>
<link>https://arxiv.org/abs/2509.00676</link>
<guid>https://arxiv.org/abs/2509.00676</guid>
<content:encoded><![CDATA[
arXiv:2509.00676v1 Announce Type: new 
Abstract: In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification</title>
<link>https://arxiv.org/abs/2509.00677</link>
<guid>https://arxiv.org/abs/2509.00677</guid>
<content:encoded><![CDATA[
arXiv:2509.00677v1 Announce Type: new 
Abstract: Multimodal fusion has made great progress in the field of remote sensing image classification due to its ability to exploit the complementary spatial-spectral information. Deep learning methods such as CNN and Transformer have been widely used in these domains. State Space Models recently highlighted that prior methods suffer from quadratic computational complexity. As a result, modeling longer-range dependencies of spatial-spectral features imposes an overwhelming burden on the network. Mamba solves this problem by incorporating time-varying parameters into ordinary SSM and performing hardware optimization, but it cannot perform feature fusion directly. In order to make full use of Mamba's low computational burden and explore the potential of internal structure in multimodal feature fusion, we propose Cross State Fusion Mamba (CSFMamba) Network. Specifically, we first design the preprocessing module of remote sensing image information for the needs of Mamba structure, and combine it with CNN to extract multi-layer features. Secondly, a cross-state module based on Mamba operator is creatively designed to fully fuse the feature of the two modalities. The advantages of Mamba and CNN are combined by designing a more powerful backbone. We capture the fusion relationship between HSI and LiDAR modalities with stronger full-image understanding. The experimental results on two datasets of MUUFL and Houston2018 show that the proposed method outperforms the experimental results of Transformer under the premise of reducing the network training burden.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition</title>
<link>https://arxiv.org/abs/2509.00692</link>
<guid>https://arxiv.org/abs/2509.00692</guid>
<content:encoded><![CDATA[
arXiv:2509.00692v1 Announce Type: new 
Abstract: Skeleton-based human action recognition leverages sequences of human joint coordinates to identify actions performed in videos. Owing to the intrinsic spatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs) have been the dominant architecture in this field. However, recent advances in transformer models and masked pretraining frameworks open new avenues for representation learning. In this work, we propose CascadeFormer, a family of two-stage cascading transformers for skeleton-based human action recognition. Our framework consists of a masked pretraining stage to learn generalizable skeleton representations, followed by a cascading fine-tuning stage tailored for discriminative action classification. We evaluate CascadeFormer across three benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achieving competitive performance on all tasks. To promote reproducibility, we release our code and model checkpoints.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision</title>
<link>https://arxiv.org/abs/2509.00700</link>
<guid>https://arxiv.org/abs/2509.00700</guid>
<content:encoded><![CDATA[
arXiv:2509.00700v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) combine a vision encoder and a large language model (LLM) through alignment training, showing strong performance on multimodal tasks. A central component in this architecture is the projection layer, which maps visual features into the LLM's embedding space. Despite its importance, its ability to generalize to unseen visual concepts has not been systematically evaluated. To address this, we propose a benchmark for evaluating projection-layer generalization. We adapt object detection datasets (rich in fine-grained annotations) into a prompting format and design train/test splits with disjoint label sets, enabling precise control over seen and unseen concept separation. Experimental results show that the projection layer retains about 79 to 88 percent of the performance on unseen classes compared to seen ones across various settings, suggesting a non-trivial level of generalization even without explicit alignment supervision on those concepts. We further analyze this behavior through a mechanistic interpretability lens. Our findings indicate that the feed-forward network in the projection layer functions like a key-value memory, processing seen and unseen tokens in similar ways. This study introduces a new evaluation framework for alignment generalization and highlights the potential for efficient VLM training with limited aligned data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning</title>
<link>https://arxiv.org/abs/2509.00745</link>
<guid>https://arxiv.org/abs/2509.00745</guid>
<content:encoded><![CDATA[
arXiv:2509.00745v1 Announce Type: new 
Abstract: Recent advances in deep learning have significantly improved the accuracy of skin lesion classification models, supporting medical diagnoses and promoting equitable healthcare. However, concerns remain about potential biases related to skin color, which can impact diagnostic outcomes. Ensuring fairness is challenging due to difficulties in classifying skin tones, high computational demands, and the complexity of objectively verifying fairness. To address these challenges, we propose a fairness algorithm for skin lesion classification that overcomes the challenges associated with achieving diagnostic fairness across varying skin tones. By calculating the skewness of the feature map in the convolution layer of the VGG (Visual Geometry Group) network and the patches and the heads of the Vision Transformer, our method reduces unnecessary channels related to skin tone, focusing instead on the lesion area. This approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Interpretation of Sparse Autoencoder Features in Vision</title>
<link>https://arxiv.org/abs/2509.00749</link>
<guid>https://arxiv.org/abs/2509.00749</guid>
<content:encoded><![CDATA[
arXiv:2509.00749v1 Announce Type: new 
Abstract: Understanding what sparse auto-encoder (SAE) features in vision transformers truly represent is usually done by inspecting the patches where a feature's activation is highest. However, self-attention mixes information across the entire image, so an activated patch often co-occurs with-but does not cause-the feature's firing. We propose Causal Feature Explanation (CaFE), which leverages Effective Receptive Field (ERF). We consider each activation of an SAE feature to be a target and apply input-attribution methods to identify the image patches that causally drive that activation. Across CLIP-ViT features, ERF maps frequently diverge from naive activation maps, revealing hidden context dependencies (e.g., a "roaring face" feature that requires the co-occurrence of eyes and nose, rather than merely an open mouth). Patch insertion tests confirm that CaFE more effectively recovers or suppresses feature activations than activation-ranked patches. Our results show that CaFE yields more faithful and semantically precise explanations of vision-SAE features, highlighting the risk of misinterpretation when relying solely on activation location.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions</title>
<link>https://arxiv.org/abs/2509.00751</link>
<guid>https://arxiv.org/abs/2509.00751</guid>
<content:encoded><![CDATA[
arXiv:2509.00751v1 Announce Type: new 
Abstract: Event-based image retrieval from free-form captions presents a significant challenge: models must understand not only visual features but also latent event semantics, context, and real-world knowledge. Conventional vision-language retrieval approaches often fall short when captions describe abstract events, implicit causality, temporal context, or contain long, complex narratives. To tackle these issues, we introduce a multi-stage retrieval framework combining dense article retrieval, event-aware language model reranking, and efficient image collection, followed by caption-guided semantic matching and rank-aware selection. We leverage Qwen3 for article search, Qwen3-Reranker for contextual alignment, and Qwen2-VL for precise image scoring. To further enhance performance and robustness, we fuse outputs from multiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves the top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand Challenge, demonstrating the effectiveness of combining language-based reasoning and multimodal retrieval for complex, real-world image understanding. The code is available at https://github.com/vdkhoi20/EVENT-Retriever.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification</title>
<link>https://arxiv.org/abs/2509.00752</link>
<guid>https://arxiv.org/abs/2509.00752</guid>
<content:encoded><![CDATA[
arXiv:2509.00752v1 Announce Type: new 
Abstract: We present a unified vision-language framework tailored for ENT endoscopy image analysis that simultaneously tackles three clinically-relevant tasks: image classification, image-to-image retrieval, and text-to-image retrieval. Unlike conventional CNN-based pipelines that struggle to capture cross-modal semantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it through Low-Rank Adaptation, multi-level CLS token aggregation, and spherical feature interpolation. These components collectively enable efficient fine-tuning on limited medical data while improving representation diversity and semantic alignment across modalities. To bridge the gap between visual inputs and textual diagnostic context, we introduce class-specific natural language prompts that guide the image encoder through a joint training objective combining supervised classification with contrastive learning. We validated our framework through participation in the ACM MM'25 ENTRep Grand Challenge, achieving 95% accuracy and F1-score in classification, Recall@1 of 0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and MRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental benefits of each architectural component, validating the effectiveness of our design for robust multimodal medical understanding in low-resource clinical settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure</title>
<link>https://arxiv.org/abs/2509.00757</link>
<guid>https://arxiv.org/abs/2509.00757</guid>
<content:encoded><![CDATA[
arXiv:2509.00757v1 Announce Type: new 
Abstract: The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No More Sibling Rivalry: Debiasing Human-Object Interaction Detection</title>
<link>https://arxiv.org/abs/2509.00760</link>
<guid>https://arxiv.org/abs/2509.00760</guid>
<content:encoded><![CDATA[
arXiv:2509.00760v1 Announce Type: new 
Abstract: Detection transformers have been applied to human-object interaction (HOI) detection, enhancing the localization and recognition of human-action-object triplets in images. Despite remarkable progress, this study identifies a critical issue-"Toxic Siblings" bias-which hinders the interaction decoder's learning, as numerous similar yet distinct HOI triplets interfere with and even compete against each other both input side and output side to the interaction decoder. This bias arises from high confusion among sibling triplets/categories, where increased similarity paradoxically reduces precision, as one's gain comes at the expense of its toxic sibling's decline. To address this, we propose two novel debiasing learning objectives-"contrastive-then-calibration" and "merge-then-split"-targeting the input and output perspectives, respectively. The former samples sibling-like incorrect HOI triplets and reconstructs them into correct ones, guided by strong positional priors. The latter first learns shared features among sibling categories to distinguish them from other groups, then explicitly refines intra-group differentiation to preserve uniqueness. Experiments show that we significantly outperform both the baseline (+9.18% mAP on HICO-Det) and the state-of-the-art (+3.59% mAP) across various settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</title>
<link>https://arxiv.org/abs/2509.00767</link>
<guid>https://arxiv.org/abs/2509.00767</guid>
<content:encoded><![CDATA[
arXiv:2509.00767v1 Announce Type: new 
Abstract: Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Scalable Face Retrieval via Cancelable Product Quantization</title>
<link>https://arxiv.org/abs/2509.00781</link>
<guid>https://arxiv.org/abs/2509.00781</guid>
<content:encoded><![CDATA[
arXiv:2509.00781v1 Announce Type: new 
Abstract: Despite the ubiquity of modern face retrieval systems, their retrieval stage is often outsourced to third-party entities, posing significant risks to user portrait privacy. Although homomorphic encryption (HE) offers strong security guarantees by enabling arithmetic computations in the cipher space, its high computational inefficiency makes it unsuitable for real-time, real-world applications. To address this issue, we propose Cancelable Product Quantization, a highly efficient framework for secure face representation retrieval. Our hierarchical two-stage framework comprises: (i) a high-throughput cancelable PQ indexing module for fast candidate filtering, and (ii) a fine-grained cipher-space retrieval module for final precise face ranking. A tailored protection mechanism is designed to secure the indexing module for cancelable biometric authentication while ensuring efficiency. Experiments on benchmark datasets demonstrate that our method achieves an decent balance between effectiveness, efficiency and security.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligned Anchor Groups Guided Line Segment Detector</title>
<link>https://arxiv.org/abs/2509.00786</link>
<guid>https://arxiv.org/abs/2509.00786</guid>
<content:encoded><![CDATA[
arXiv:2509.00786v1 Announce Type: new 
Abstract: This paper introduces a novel line segment detector, the Aligned Anchor Groups guided Line Segment Detector (AAGLSD), designed to detect line segments from images with high precision and completeness. The algorithm employs a hierarchical approach to extract candidate pixels with different saliency levels, including regular anchors and aligned anchor groups. AAGLSD initiates from these aligned anchor groups, sequentially linking anchors and updating the currently predicted line segment simultaneously. The final predictions are derived through straightforward validation and merging of adjacent line segments, avoiding complex refinement strategies. AAGLSD is evaluated on various datasets and quantitative experiments demonstrate that the proposed method can effectively extract complete line segments from input images compared to other advanced line segment detectors. The implementation is available at https://github.com/LLiDaBao/AAGLSD.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses</title>
<link>https://arxiv.org/abs/2509.00787</link>
<guid>https://arxiv.org/abs/2509.00787</guid>
<content:encoded><![CDATA[
arXiv:2509.00787v1 Announce Type: new 
Abstract: Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.00789</link>
<guid>https://arxiv.org/abs/2509.00789</guid>
<content:encoded><![CDATA[
arXiv:2509.00789v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have demonstrated impressive spatial reasoning capabilities for autonomous driving, yet existing methods predominantly focus on static scene understanding while neglecting the essential temporal dimension of real-world driving scenarios. To address this critical limitation, we propose the OmniReason framework, which establishes robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and their underlying decision-making processes. Our work makes two fundamental advances: (1) We introduce OmniReason-Data, two large-scale vision-language-action (VLA) datasets with dense spatiotemporal annotations and natural language explanations, generated through a novel hallucination-mitigated auto-labeling pipeline that ensures both physical plausibility and temporal coherence; (2) We develop the OmniReason-Agent architecture, which integrates a sparse temporal memory module for persistent scene context modeling and an explanation generator that produces human-interpretable decision rationales, facilitated by our spatiotemporal knowledge distillation approach that effectively captures spatiotemporal causal reasoning patterns. Comprehensive experiments demonstrate state-of-the-art performance, where OmniReason-Agent achieves significant improvements in both open-loop planning tasks and visual question answering (VQA) benchmarks, while establishing new capabilities for interpretable, temporally-aware autonomous vehicles operating in complex, dynamic environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Iterative RAG for Knowledge Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.00798</link>
<guid>https://arxiv.org/abs/2509.00798</guid>
<content:encoded><![CDATA[
arXiv:2509.00798v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) have significantly advanced multimodal understanding, their performance remains limited on knowledge-intensive visual questions that require external knowledge beyond the image. Retrieval-Augmented Generation (RAG) has become a promising solution for providing models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and update reasoning over newly retrieved knowledge across modalities. At each iteration, MI-RAG leverages an accumulated reasoning record to dynamically formulate a multi-query. These queries then drive a joint search across heterogeneous knowledge bases containing both visually-grounded and textual knowledge. The newly acquired knowledge is synthesized into the reasoning record, progressively refining understanding across iterations. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.00800</link>
<guid>https://arxiv.org/abs/2509.00800</guid>
<content:encoded><![CDATA[
arXiv:2509.00800v1 Announce Type: new 
Abstract: Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification</title>
<link>https://arxiv.org/abs/2509.00808</link>
<guid>https://arxiv.org/abs/2509.00808</guid>
<content:encoded><![CDATA[
arXiv:2509.00808v1 Announce Type: new 
Abstract: Fetal ultrasound standard plane classification is essential for reliable prenatal diagnosis but faces inherent challenges, including low tissue contrast, boundary ambiguity, and operator-dependent image quality variations. To overcome these limitations, we propose a plug-and-play adaptive contrast adjustment module (ACAM), whose core design is inspired by the clinical practice of doctors adjusting image contrast to obtain clearer and more discriminative structural information. The module employs a shallow texture-sensitive network to predict clinically plausible contrast parameters, transforms input images into multiple contrast-enhanced views through differentiable mapping, and fuses them within downstream classifiers. Validated on a multi-center dataset of 12,400 images across six anatomical categories, the module consistently improves performance across diverse models, with accuracy of lightweight models increasing by 2.02 percent, accuracy of traditional models increasing by 1.29 percent, and accuracy of state-of-the-art models increasing by 1.15 percent. The innovation of the module lies in its content-aware adaptation capability, replacing random preprocessing with physics-informed transformations that align with sonographer workflows while improving robustness to imaging heterogeneity through multi-view fusion. This approach effectively bridges low-level image features with high-level semantics, establishing a new paradigm for medical image analysis under real-world image quality variations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</title>
<link>https://arxiv.org/abs/2509.00826</link>
<guid>https://arxiv.org/abs/2509.00826</guid>
<content:encoded><![CDATA[
arXiv:2509.00826v1 Announce Type: new 
Abstract: Efficient adversarial attack methods are critical for assessing the robustness of computer vision models. In this paper, we reconstruct the optimization objective for generating adversarial examples as "maximizing the difference between the non-true labels' probability upper bound and the true label's probability," and propose a gradient-based attack method termed Sequential Difference Maximization (SDM). SDM establishes a three-layer optimization framework of "cycle-stage-step." The processes between cycles and between iterative steps are respectively identical, while optimization stages differ in terms of loss functions: in the initial stage, the negative probability of the true label is used as the loss function to compress the solution space; in subsequent stages, we introduce the Directional Probability Difference Ratio (DPDR) loss function to gradually increase the non-true labels' probability upper bound by compressing the irrelevant labels' probabilities. Experiments demonstrate that compared with previous SOTA methods, SDM not only exhibits stronger attack performance but also achieves higher attack cost-effectiveness. Additionally, SDM can be combined with adversarial training methods to enhance their defensive effects. The code is available at https://github.com/X-L-Liu/SDM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surface Defect Detection with Gabor Filter Using Reconstruction-Based Blurring U-Net-ViT</title>
<link>https://arxiv.org/abs/2509.00827</link>
<guid>https://arxiv.org/abs/2509.00827</guid>
<content:encoded><![CDATA[
arXiv:2509.00827v1 Announce Type: new 
Abstract: This paper proposes a novel approach to enhance the accuracy and reliability of texture-based surface defect detection using Gabor filters and a blurring U-Net-ViT model. By combining the local feature training of U-Net with the global processing of the Vision Transformer(ViT), the model effectively detects defects across various textures. A Gaussian filter-based loss function removes background noise and highlights defect patterns, while Salt-and-Pepper(SP) masking in the training process reinforces texture-defect boundaries, ensuring robust performance in noisy environments. Gabor filters are applied in post-processing to emphasize defect orientation and frequency characteristics. Parameter optimization, including filter size, sigma, wavelength, gamma, and orientation, maximizes performance across datasets like MVTec-AD, Surface Crack Detection, and Marble Surface Anomaly Dataset, achieving an average Area Under the Curve(AUC) of 0.939. The ablation studies validate that the optimal filter size and noise probability significantly enhance defect detection performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring</title>
<link>https://arxiv.org/abs/2509.00831</link>
<guid>https://arxiv.org/abs/2509.00831</guid>
<content:encoded><![CDATA[
arXiv:2509.00831v1 Announce Type: new 
Abstract: Reconstructing dynamic 3D scenes from monocular video has broad applications in AR/VR, robotics, and autonomous navigation, but often fails due to severe motion blur caused by camera and object motion. Existing methods commonly follow a two-step pipeline, where camera poses are first estimated and then 3D Gaussians are optimized. Since blurring artifacts usually undermine pose estimation, pose errors could be accumulated to produce inferior reconstruction results. To address this issue, we introduce a unified optimization framework by incorporating camera poses as learnable parameters complementary to 3DGS attributes for end-to-end optimization. Specifically, we recast camera and object motion as per-primitive SE(3) affine transformations on 3D Gaussians and formulate a unified optimization objective. For stable optimization, we introduce a three-stage training schedule that optimizes camera poses and Gaussians alternatively. Particularly, 3D Gaussians are first trained with poses being fixed, and then poses are optimized with 3D Gaussians being untouched. Finally, all learnable parameters are optimized together. Extensive experiments on the Stereo Blur dataset and challenging real-world sequences demonstrate that our method achieves significant gains in reconstruction quality and pose estimation accuracy over prior dynamic deblurring methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3</title>
<link>https://arxiv.org/abs/2509.00833</link>
<guid>https://arxiv.org/abs/2509.00833</guid>
<content:encoded><![CDATA[
arXiv:2509.00833v1 Announce Type: new 
Abstract: The DINO family of self-supervised vision models has shown remarkable transferability, yet effectively adapting their representations for segmentation remains challenging. Existing approaches often rely on heavy decoders with multi-scale fusion or complex upsampling, which introduce substantial parameter overhead and computational cost. In this work, we propose SegDINO, an efficient segmentation framework that couples a frozen DINOv3 backbone with a lightweight decoder. SegDINO extracts multi-level features from the pretrained encoder, aligns them to a common resolution and channel width, and utilizes a lightweight MLP head to directly predict segmentation masks. This design minimizes trainable parameters while preserving the representational power of foundation features. Extensive experiments across six benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO consistently achieves state-of-the-art performance compared to existing methods. Code is available at https://github.com/script-Yang/SegDINO.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss</title>
<link>https://arxiv.org/abs/2509.00835</link>
<guid>https://arxiv.org/abs/2509.00835</guid>
<content:encoded><![CDATA[
arXiv:2509.00835v1 Announce Type: new 
Abstract: Satellite imagery plays a crucial role in various fields; however, atmospheric interference and haze significantly degrade image clarity and reduce the accuracy of information extraction. To address these challenges, this paper proposes a hybrid dehazing framework that integrates Swin Transformer and U-Net to balance global context learning and local detail restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin Transformer-based Residual-in-Residual Dense Block, in both the encoder and decoder to effectively extract features. This module enables the joint learning of global contextual information and fine spatial structures, which is crucial for structural preservation in satellite image. Furthermore, we introduce a composite loss function that combines L2 loss, guided loss, and a novel watershed loss, which enhances structural boundary preservation and ensures pixel-level accuracy. This architecture enables robust dehazing under diverse atmospheric conditions while maintaining structural consistency across restored images. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically, on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an SSIM of 0.967, which is a significant improvement over existing method. This study provides an effective solution for mitigating atmospheric interference in satellite imagery and highlights its potential applicability across diverse remote sensing applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion</title>
<link>https://arxiv.org/abs/2509.00843</link>
<guid>https://arxiv.org/abs/2509.00843</guid>
<content:encoded><![CDATA[
arXiv:2509.00843v1 Announce Type: new 
Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at https://github.com/YiGuYT/LookBeyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective</title>
<link>https://arxiv.org/abs/2509.00859</link>
<guid>https://arxiv.org/abs/2509.00859</guid>
<content:encoded><![CDATA[
arXiv:2509.00859v1 Announce Type: new 
Abstract: Current quantization-aware training (QAT) methods primarily focus on enhancing the performance of quantized models on in-distribution (I.D) data, while overlooking the potential performance degradation on out-of-distribution (OOD) data. In this paper, we first substantiate this problem through rigorous experiment, showing that QAT can lead to a significant OOD generalization performance degradation. Further, we find the contradiction between the perspective that flatness of loss landscape gives rise to superior OOD generalization and the phenomenon that QAT lead to a sharp loss landscape, can cause the above problem. Therefore, we propose a flatness-oriented QAT method, FQAT, to achieve generalizable QAT. Specifically, i) FQAT introduces a layer-wise freezing mechanism to mitigate the gradient conflict issue between dual optimization objectives (i.e., vanilla QAT and flatness). ii) FQAT proposes an disorder-guided adaptive freezing algorithm to dynamically determines which layers to freeze at each training step, effectively addressing the challenges caused by interference between layers. A gradient disorder metric is designed to help the algorithm identify unstable layers during training. Extensive experiments on influential OOD benchmark demonstrate the superiority of our method over state-of-the-art baselines under both I.D and OOD image classification tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening</title>
<link>https://arxiv.org/abs/2509.00872</link>
<guid>https://arxiv.org/abs/2509.00872</guid>
<content:encoded><![CDATA[
arXiv:2509.00872v1 Announce Type: new 
Abstract: Recent AI-based scoliosis screening methods primarily rely on large-scale silhouette datasets, often neglecting clinically relevant postural asymmetries-key indicators in traditional screening. In contrast, pose data provide an intuitive skeletal representation, enhancing clinical interpretability across various medical applications. However, pose-based scoliosis screening remains underexplored due to two main challenges: (1) the scarcity of large-scale, annotated pose datasets; and (2) the discrete and noise-sensitive nature of raw pose coordinates, which hinders the modeling of subtle asymmetries. To address these limitations, we introduce Scoliosis1K-Pose, a 2D human pose annotation set that extends the original Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050 adolescents. Building on this dataset, we introduce the Dual Representation Framework (DRF), which integrates a continuous skeleton map to preserve spatial structure with a discrete Postural Asymmetry Vector (PAV) that encodes clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA) module further uses the PAV as clinical prior to direct feature extraction from the skeleton map, focusing on clinically meaningful asymmetries. Extensive experiments demonstrate that DRF achieves state-of-the-art performance. Visualizations further confirm that the model leverages clinical asymmetry cues to guide feature extraction and promote synergy between its dual representations. The dataset and code are publicly available at https://zhouzi180.github.io/Scoliosis1K/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotlighter: Revisiting Prompt Tuning from a Representative Mining View</title>
<link>https://arxiv.org/abs/2509.00905</link>
<guid>https://arxiv.org/abs/2509.00905</guid>
<content:encoded><![CDATA[
arXiv:2509.00905v1 Announce Type: new 
Abstract: CLIP's success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual token's activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at https://github.com/greatest-gourmet/Spotlighter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising</title>
<link>https://arxiv.org/abs/2509.00917</link>
<guid>https://arxiv.org/abs/2509.00917</guid>
<content:encoded><![CDATA[
arXiv:2509.00917v1 Announce Type: new 
Abstract: Low-light RAW video denoising is a fundamentally challenging task due to severe signal degradation caused by high sensor gain and short exposure times, which are inherently limited by video frame rate requirements. To address this, we propose DarkVRAI, a novel framework that achieved first place in the AIM 2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary contributions: (1) a successful application of a conditioning scheme for image denoising, which explicitly leverages capture metadata, to video denoising to guide the alignment and denoising processes, and (2) a Burst-Order Selective Scan (BOSS) mechanism that effectively models long-range temporal dependencies within the noisy video sequence. By synergistically combining these components, DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic benchmark dataset, setting a new standard for low-light video denoising.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
<link>https://arxiv.org/abs/2509.00969</link>
<guid>https://arxiv.org/abs/2509.00969</guid>
<content:encoded><![CDATA[
arXiv:2509.00969v1 Announce Type: new 
Abstract: Recent advancements in large video-language models have revolutionized video understanding tasks. However, their efficiency is significantly constrained by processing high volumes of visual tokens. Existing token compression strategies apply a fixed compression ratio, ignoring the variability in semantic density among different video clips. Consequently, this lead to inadequate representation of information-rich clips due to insufficient tokens and unnecessary computation on static or content-poor ones. To address this, we propose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a lightweight language model to describe video clips, converting them into soft caption tokens as visual representations. Trained with our proposed semantic density-aware supervision, LangDC aims to 1) cover key visual cues necessary for downstream task reasoning and 2) dynamically adjust compression ratios based on scene richness, reflected by descriptions length. Our design mimics how humans dynamically express what they see: complex scenes (seeing more) elicit more detailed language to convey nuances (saying more), whereas simpler scenes are described with fewer words. Experimental results show that our method reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive performance. Furthermore, qualitative results demonstrate our approach adaptively adjusts the token compression ratio based on video segment richness.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Integrating Multi-Spectral Imaging with Gaussian Splatting</title>
<link>https://arxiv.org/abs/2509.00989</link>
<guid>https://arxiv.org/abs/2509.00989</guid>
<content:encoded><![CDATA[
arXiv:2509.00989v1 Announce Type: new 
Abstract: We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions</title>
<link>https://arxiv.org/abs/2509.01013</link>
<guid>https://arxiv.org/abs/2509.01013</guid>
<content:encoded><![CDATA[
arXiv:2509.01013v1 Announce Type: new 
Abstract: Rainy weather significantly increases the risk of road accidents due to reduced visibility and vehicle traction. Understanding how experienced drivers adapt their visual perception through gaze behavior under such conditions is critical for designing robust driver monitoring systems (DMS) and for informing advanced driver assistance systems (ADAS). This case study investigates the eye gaze behavior of a driver operating the same highway route under both clear and rainy conditions. To this end, gaze behavior was analyzed by a two-step clustering approach: first, clustering gaze points within 10-second intervals, and then aggregating cluster centroids into meta-clusters. This, along with Markov transition matrices and metrics such as fixation duration, gaze elevation, and azimuth distributions, reveals meaningful behavioral shifts. While the overall gaze behavior focused on the road with occasional mirror checks remains consistent, rainy conditions lead to more frequent dashboard glances, longer fixation durations, and higher gaze elevation, indicating increased cognitive focus. These findings offer valuable insight into visual attention patterns under adverse conditions and highlight the potential of leveraging gaze modeling to aid in the design of more robust ADAS and DMS.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef</title>
<link>https://arxiv.org/abs/2509.01019</link>
<guid>https://arxiv.org/abs/2509.01019</guid>
<content:encoded><![CDATA[
arXiv:2509.01019v1 Announce Type: new 
Abstract: Coral reefs are on the brink of collapse, with climate change, ocean acidification, and pollution leading to a projected 70-90% loss of coral species within the next decade. Restoration efforts are crucial, but their success hinges on introducing automation to upscale efforts. We present automated deployment of coral re-seeding devices powered by artificial intelligence, computer vision, and robotics. Specifically, we perform automated substrate classification, enabling detection of areas of the seafloor suitable for coral growth, thus significantly reducing reliance on human experts and increasing the range and efficiency of restoration. Real-world testing of the algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%, sub-image patch classification of 89.1%, and real-time model inference at 5.5 frames per second. Further, we present and publicly contribute a large collection of annotated substrate image data to foster future research in this area.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>